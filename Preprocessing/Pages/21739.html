<!DOCTYPE html>

<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="/xmlui/themes/Mirage2/images/favicon.ico" rel="shortcut icon"/>
<link href="/xmlui/themes/Mirage2/images/apple-touch-icon.png" rel="apple-touch-icon"/>
<meta content="DSpace 5.8" name="Generator"/>
<link href="https://fonts.googleapis.com/css?family=PT+Sans:400,700|Ubuntu:400,500,700italic|Droid+Sans:400,700" rel="stylesheet" type="text/css"/>
<link href="/xmlui/themes/Mirage2/styles/main.css" rel="stylesheet"/>
<link href="https://papyrus.bib.umontreal.ca:443/xmlui/description.xml" rel="search" title="DSpace" type="application/opensearchdescription+xml"/>
<script>
                //Clear default text of emty text areas on focus
                function tFocus(element)
                {
                if (element.value == ''){element.value='';}
                }
                //Clear default text of emty text areas on submit
                function tSubmit(form)
                {
                var defaultedElements = document.getElementsByTagName("textarea");
                for (var i=0; i != defaultedElements.length; i++){
                if (defaultedElements[i].value == ''){
                defaultedElements[i].value='';}}
                }
                //Disable pressing 'enter' key to submit a form (otherwise pressing 'enter' causes a submission to start over)
                function disableEnterKey(e)
                {
                var key;

                if(window.event)
                key = window.event.keyCode;     //Internet Explorer
                else
                key = e.which;     //Firefox and Netscape

                if(key == 13)  //if "Enter" pressed, then disable!
                return false;
                else
                return true;
                }
            </script><!--[if lt IE 9]>
                <script src="/xmlui/themes/Mirage2/vendor/html5shiv/dist/html5shiv.js"> </script>
                <script src="/xmlui/themes/Mirage2/vendor/respond/respond.min.js"> </script>
                <![endif]--><script src="/xmlui/themes/Mirage2/vendor/modernizr/modernizr.js"> </script><script src="https://www.bib.umontreal.ca/modele/urgence-udem/1.0.0/urgence-udem.js"></script>
<title>Learning and time : on using memory and curricula for language understanding</title>
<meta content="Dr8axyf94eWBJ8HQjNW4eKxTumAAPsNbvRwpJYMbgGs" name="google-site-verification"/>
<link href="http://purl.org/dc/terms/" rel="schema.DCTERMS">
<link href="http://purl.org/dc/elements/1.1/" rel="schema.DC">
<meta content="Bengio, Yoshua" name="DC.contributor">
<meta content="Gulcehre, Caglar" name="DC.creator">
<meta content="2019-05-13T18:59:58Z" name="DCTERMS.dateAccepted" scheme="DCTERMS.W3CDTF">
<meta content="NO_RESTRICTION" name="DCTERMS.available" scheme="DCTERMS.W3CDTF" xml:lang="fr">
<meta content="2019-05-13T18:59:58Z" name="DCTERMS.available" scheme="DCTERMS.W3CDTF">
<meta content="2019-03-13" name="DCTERMS.issued" scheme="DCTERMS.W3CDTF">
<meta content="2018-05" name="DC.date" scheme="DCTERMS.W3CDTF"/>
<meta content="http://hdl.handle.net/1866/21739" name="DC.identifier" scheme="DCTERMS.URI"/>
<meta content="Deep learning" name="DC.subject" xml:lang="fr"/>
<meta content="Curriculum learning" name="DC.subject" xml:lang="fr"/>
<meta content="Optimization" name="DC.subject" xml:lang="fr"/>
<meta content="Natural language processing" name="DC.subject" xml:lang="fr"/>
<meta content="Neural networks" name="DC.subject" xml:lang="fr"/>
<meta content="Recurrent neural networks" name="DC.subject" xml:lang="fr"/>
<meta content="Apprentissage profond" name="DC.subject" xml:lang="fr"/>
<meta content="Apprentissage du curriculum" name="DC.subject" xml:lang="fr"/>
<meta content="Traitement du langage naturel" name="DC.subject" xml:lang="fr"/>
<meta content="Language modelling" name="DC.subject" xml:lang="fr"/>
<meta content="Memory" name="DC.subject" xml:lang="fr"/>
<meta content="Modélisation du langage" name="DC.subject" xml:lang="fr"/>
<meta content="Machine translation" name="DC.subject" xml:lang="fr"/>
<meta content="Traduction automatique" name="DC.subject" xml:lang="fr"/>
<meta content="Optimisation" name="DC.subject" xml:lang="fr"/>
<meta content="Réseaux de neurones" name="DC.subject" xml:lang="fr"/>
<meta content="Réseaux de neurones récurrents" name="DC.subject" xml:lang="fr"/>
<meta content="Mémoire" name="DC.subject" xml:lang="fr"/>
<meta content="Applied Sciences - Artificial Intelligence / Sciences appliqués et technologie - Intelligence artificielle (UMI : 0800)" name="DC.subject" xml:lang="fr"/>
<meta content="Learning and time : on using memory and curricula for language understanding" name="DC.title" xml:lang="fr"/>
<meta content="Thèse ou mémoire / Thesis or Dissertation" name="DC.type"/>
<meta content="Cette thèse présente quelques-unes des étapes entreprises pour pouvoir un jour résoudre le problème de la compréhension du langage naturel et d’apprentissage de dépendances à long terme, dans le but de développer de meilleurs algorithmes d’intelligence artificielle. Cette thèse est écrite comme une thèse par articles, et contient cinq publications scientifiques. Chacun de ces articles propose un nouveau modèle ou algorithme et démontre leur efficacité sur des problèmes qui impliquent des dépendances à long terme ou la compréhension du langage naturel. Malgré le fait que quelque uns de ces modèles n’ont été testés que sur une seule tâche (comme la traduction automatique neuronale), les méthodes proposées sont généralement applicables dans d’autres domaines et sur d’autres tâches.

Dans l’introduction de la thèse, nous expliquons quelques concepts fondamentaux de l'entraînement de réseaux de neurones appliqués sur des données séquentielles. Tout d'abord, nous présentons succinctement les réseaux de neurones, puis, de façon plus détaillé, certains algorithmes et méthodes utilisés à travers cette thèse.

Dans notre premier article, nous proposons une nouvelle méthode permettant d'utiliser la grande quantité de données monolingue disponible afin d'entraîner des modèles de traduction. Nous avons accompli cela en entraînant d’abord un modèle Long short-term memory (LSTM) sur un large corpus monolingue. Nous lions ensuite la sortie de la couche cachée du modèle avec celle d’un décodeur d’un modèle de traduction automatique. Ce dernier utilise un mécanisme d’attention et est entièrement entraîné par descente de gradient. Nous avons montré que la méthode proposée peut augmenter la performance des modèles de traduction automatique neuronale de façon significative sur les tâches où peu de données multilingues sont disponibles. Notre approche augmente également l’efficacité de l’utilisation des données dans les systèmes de traduction automatique. Nous montrons aussi des améliorations sur les paires de langues suivantes: turc-anglais, allemand-anglais, chinois-anglais et tchèque-anglais.

Dans notre deuxième article, nous proposons une approche pour aborder le problème des mots rares dans plusieurs tâches du traitement des langages. Notre approche modifie l’architecture habituelle des modèles encodeur-décodeur avec attention, en remplaçant la couche softmax du décodeur par notre couche pointer-softmax. Celle-ci permet au décodeur de pointer à différents endroits dans la phrase d’origine. Notre modèle apprend à alterner entre copier un mot de la phrase d’origine et prédire un mot provenant d’une courte liste de mots prédéfinie, de manière probabiliste. L’approche que nous avons proposée est entièrement entraînable par descente de gradient et n’utilise qu’un objectif de maximum de vraisemblance sur les tâches de traduction. Nous avons aussi montré que le pointer-softmax aide de manière significative aux tâches de traduction et de synthèse de documents.

Dans notre article  &quot;Plan, Attend, Generate: Planning for Sequence-to-Sequence Models&quot;, nous proposons deux approches pour apprendre l’alignement dans les modèles entraînés sur des séquences. Lorsque la longueur de l’entrée et celle de la sortie sont trop grandes, apprendre les alignements peut être très difficile. La raison est que lorsque le décodeur est trop puissant, il a tendance à ignorer l’alignement des mots pour ne se concentrer que sur le dernier mot de la séquence d’entrée. Nous avons proposé une nouvelle approche, inspirée d’un algorithme d’apprentissage par renforcement, en ajoutant explicitement un mécanisme de planification au décodeur. Ce nouveau mécanisme planifie à l’avance l’alignement pour les k prochaines prédictions. Notre modèle apprend également un plan de correction pour déterminer lorsqu’il est nécessaire de recalculer les alignements. Notre approche peut apprendre de haut niveaux d’abstraction au point de vue temporel et nous montrons que les alignements sont généralement de meilleure qualité. Nous obtenons également des gains de performance significatifs comparativement à notre modèle de référence, malgré le fait que nos modèles ont moins de paramètres et qu’ils aient été entraînés moins longtemps.

Dans notre article &quot;Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes&quot;, nous proposons une nouvelle approche pour ajouter de manière explicite un mécanisme de mémoire aux réseaux de neurones. Contrairement aux RNNs conventionnels, la mémoire n’est pas seulement représentée au niveau des activations du réseau, mais également dans une mémoire externe. Notre modèle, D-NTM, utilise un mécanisme d’adressage plus simple que les Neural Turing Machine (NTM) en utilisant des paires clé-valeur. Nous montrons que les modèles disposant de ce nouveau mécanisme peuvent plus efficacement apprendre les dépendances à long terme, en plus de mieux généraliser. Nous obtenons des améliorations sur plusieurs tâches incluant entre autres la réponse aux questions sur bAbI, le raisonnement avec implication, MNIST permuté, ainsi que des tâches synthétiques.

Dans notre article &quot;Noisy Activation Functions&quot;, nous proposons une nouvelle fonction d’activation, qui rend les activations stochastiques en leur ajoutant du bruit. Notre motivation dans cet article est d’aborder les problèmes d’optimisation qui surviennent lorsque nous utilisons des fonctions d’activation qui saturent, comme celles généralement utilisées dans les RNNs. Notre approche permet d’utiliser des fonctions d’activation linéaires par morceaux sur les RNNs à porte. Nous montrons des améliorations pour un grand nombre de tâches sans effectuer de recherche d'hyper paramètres intensive. Nous montrons également que supprimer le bruit dans les fonctions d’activation a un profond impact sur l’optimisation." name="DCTERMS.abstract" xml:lang="fr"/>
<meta content='The goal of this thesis is to present some of the small steps taken on the path towards solving natural language understanding and learning long-term dependencies to develop artificial intelligence algorithms that can reason with language. This thesis is written as a thesis by articles and contains five articles. Each article in this thesis proposes a new model or algorithm and demonstrates the efficiency of the proposed approach to solve problems that involve long-term dependencies or require natural language understanding. Although some of the models are tested on a particular task (such as neural machine translation), the proposed methods in this thesis are generally applicable to other domains and tasks (and have been used in the literature).

In the introduction of this thesis, we introduce some of the fundamental concepts behind training sequence models using neural networks. We first provide a brief introduction to neural networks and then dive into details of the some of approaches and algorithms that are used throughout this thesis.

In our first article, we propose a novel method to utilize the abundant amount of available monolingual data for training neural machine translation models. We have accomplished this goal by first training a long short-term memory (LSTM) language model on a large monolingual corpus and then fusing the outputs or the hidden states of the LSTM language model with the decoder of the neural machine translation model. Our neural machine translation model is trained end to end with an attention mechanism. We have shown that our proposed approaches can improve the performance of the neural machine translation models significantly on the rare resource translation tasks and our approach improved the data-efficiency of the end to end neural machine translation systems. We report improvements on Turkish-English (Tr-En), German-English (De-En), Chinese-English (Zh-En) and Czech-English (Cz-En) translation tasks.

In our second paper, we propose an approach to address the problem of rare words for natural language processing tasks. Our approach augments the encoder-decoder architecture with attention model by replacing the final softmax layer with our proposed pointer-softmax layer that creates pointers to the source sentences as the decoder translates. In the case of pointer-softmax, our model learns to switch between copying a word from the source and predicting a word from a shortlist vocabulary in a probabilistic manner. Our proposed approach is end-to-end trainable with a single maximum likelihood objective of the NMT model. We have also shown that it improves the performance of summarization and the neural machine translation model. We report significant improvements in machine translation and summarization tasks.

In our "Plan, Attend, Generate: Planning for Sequence-to-Sequence Models" paper, we propose two new approaches to learn alignments in a sequence to sequence model. If the input and the source context is very long, learning the alignments for a sequence to sequence model can be difficult. In particular, because when the decoder is a large network, it can learn to ignore the alignments and attend more on the last token of the input sequence. We propose a new approach which is inspired by a hierarchical reinforcement learning algorithm and extend our model with an explicit planning mechanism. The proposed alignment mechanism plans and computes the alignments for the next $k$ tokens in the decoder. Our model also learns a commitment plan to decide when to recompute the alignment matrix. Our proposed approach can learn high-level temporal abstractions, and we show that it qualitatively learns better alignments. We also achieve significant improvements over our baseline despite using smaller models and with less training.

In "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes," we propose a new approach for augmenting neural networks with an explicit memory mechanism. As opposed to conventional RNNs, the memory is not only represented in the activations of the neural network but in an external memory that can be accessed via the neural network controller. Our model, D-NTM uses a more straightforward memory addressing mechanism than NTM which is achieved by using key-value pairs for each memory cell. We find out that the models augmented with an external memory mechanism can learn tasks that involve long-term dependencies more efficiently and achieve better generalization. We achieve improvements on many tasks including but not limited to episodic question answering on bAbI, reasoning with entailment, permuted MNIST task and synthetic tasks.

In our "Noisy Activation Functions" paper, we propose a novel activation function that makes the activations stochastic by injecting a particular form of noise to them. Our motivation in this paper is to address the optimization problem of commonly used saturating activation functions that are used with the recurrent neural networks. Our approach enables us to use piece-wise linear activation functions on the gated recurrent neural network models. We show improvements in a wide range of tasks without doing any extensive hyperparameter search by a drop-in replacement. We also show that annealing the noise of the activation function can have a profound continuation-like effect on the optimization of the network.' name="DCTERMS.abstract" xml:lang="fr"/>
<meta content="eng" name="DCTERMS.language" xml:lang="fr"/>
<meta content="Deep learning; Curriculum learning; Optimization; Natural language processing; Neural networks; Recurrent neural networks; Apprentissage profond; Apprentissage du curriculum; Traitement du langage naturel; Language modelling; Memory; Modélisation du langage; Machine translation; Traduction automatique; Optimisation; Réseaux de neurones; Réseaux de neurones récurrents; Mémoire; Applied Sciences - Artificial Intelligence / Sciences appliqués et technologie - Intelligence artificielle (UMI : 0800); Thèse ou mémoire / Thesis or Dissertation; Informatique" name="citation_keywords"/>
<meta content="Learning and time : on using memory and curricula for language understanding" name="citation_title"/>
<meta content="eng" name="citation_language"/>
<meta content="Gulcehre, Caglar" name="citation_author"/>
<meta content="https://papyrus.bib.umontreal.ca/xmlui/bitstream/1866/21739/10/Gulcehre_Caglar_2018_these.pdf" name="citation_pdf_url"/>
<meta content="2019-03-13" name="citation_date"/>
<meta content="https://papyrus.bib.umontreal.ca/xmlui/handle/1866/21739" name="citation_abstract_html_url"/>
<script type="text/x-mathjax-config">
                    MathJax.Hub.Config({
                      tex2jax: {
                        inlineMath: [['\\(','\\)']],
                        ignoreClass: "detail-field-data|detailtable|exception"
                      },
                      TeX: {
                        Macros: {
                          AA: '{\\mathring A}'
                        }
                      }
                    });
                </script><script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"> </script>
</meta></meta></meta></meta></meta></meta></link></link></head><body>
<header>
<div class="navbar navbar-default navbar-static-top" role="navigation">
<div class="container">
<div class="hidden-xs hidden-sm hidden-md" id="um-bandeau" role="banner">
<nav class="um-nav um-nav-hidden" role="navigation">
<span class="hidden">Liens externes</span>
<ul>
<li>
<a href="https://www.umontreal.ca/#udemwww-search-personne">Directories</a>
</li>
<li>
<a href="http://www.umontreal.ca/repertoires/facultes.html">Faculties</a>
</li>
<li>
<a href="http://www.bib.umontreal.ca/">Libraries</a>
</li>
<li>
<a href="http://plancampus.umontreal.ca/">Campus maps</a>
</li>
<li>
<a href="http://www.umontreal.ca/index/az.html">Sites A to Z</a>
</li>
<li>
<a>My UdeM</a>
<ul>
<li>
<a href="https://monudem.umontreal.ca/">My UdeM Portal</a>
</li>
<li>
<a href="https://outlook.umontreal.ca">My email</a>
</li>
<li>
<a href="https://studium.umontreal.ca/">StudiUM</a>
</li>
</ul>
</li>
</ul>
</nav>
<form action="https://google.com/cse" class="um-recherche" id="um-recherche" role="search">
<input name="cx" type="hidden" value="011926736769028447783:qlpu3so2kqq"/><input name="ie" type="hidden" value="ISO-8859-1"/><label class="hidden" for="um-boite-recherche">Search in : </label><span class="um-boite-bouton"><input class="um-boite" id="um-boite-recherche" maxlength="255" name="q" placeholder="UdeM" title="Type your search text" type="text" value=""/><input alt="Search in : " class="um-bouton" src="/xmlui/themes/Mirage2/images/bib-umontreal/loupe.gif" type="image"/></span>
</form>
</div>
<div class="hidden-xs hidden-sm visible-md visible-lg" id="papyrus-deco">
<img alt="Dessin du pavillon Roger Gaudry/Sketch of Roger Gaudry Building" border="0" height="89px" src="/xmlui/themes/Mirage2/images/UdeM.gif" title="Dessin du pavillon Roger Gaudry/Sketch of Roger Gaudry Building" width="193px"/></div>
<div class="navbar-header">
<button class="navbar-toggle" data-toggle="offcanvas" type="button"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand hidden-xs hidden-sm visible-md visible-lg um-logo-md-lg" href="http://www.umontreal.ca/english/index.html"><img alt="University Home page" src="/xmlui/themes/Mirage2/images/logo-UdeM.svg" title="Back to University Home page"/></a><a class="navbar-brand hidden-xs visible-sm hidden-md hidden-lg um-logo-sm" href="http://www.umontreal.ca/english/index.html"><img alt="University Home page" src="/xmlui/themes/Mirage2/images/logo-UdeM.svg" title="Back to University Home page"/></a><a class="navbar-brand visible-xs hidden-sm hidden-md hidden-lg um-logo-xs" href="http://www.umontreal.ca/english/index.html"><img alt="University Home page" src="/xmlui/themes/Mirage2/images/logo-UdeM.svg" title="Back to University Home page"/></a>
<div class="navbar-brand separateur visible-xs hidden-sm hidden-md hidden-lg um-sep-site-xs"></div>
<div class="navbar-brand separateur hidden-xs visible-sm hidden-md hidden-lg um-sep-site-sm"></div>
<div class="navbar-brand separateur visible-md visible-lg hidden-xs hidden-sm um-sep-site-md-lg"></div>
<div class="navbar-brand visible-md visible-lg hidden-xs hidden-sm um-titre-site-md-lg">
<span class="papyrus-signature">Papyrus</span> :
                                Institutional Repository</div>
<div class="navbar-brand hidden-xs visible-sm hidden-md hidden-lg um-titre-site-sm">
<span class="papyrus-signature">Papyrus</span>
<br/>Institutional Repository</div>
<div class="navbar-brand visible-xs hidden-sm hidden-md hidden-lg um-titre-site-xs">
<span class="papyrus-signature">Papyrus</span>
</div>
<div class="navbar-header pull-right visible-xs hidden-sm hidden-md hidden-lg">
<ul class="nav nav-pills pull-left">
<li class="dropdown" id="ds-language-selection-xs">
<button class="dropdown-toggle navbar-toggle navbar-link" data-toggle="dropdown" href="#" id="language-dropdown-toggle-xs" role="button"><b aria-hidden="true" class="visible-xs glyphicon glyphicon-globe"></b></button>
<ul aria-labelledby="language-dropdown-toggle-xs" class="dropdown-menu pull-right" data-no-collapse="true" role="menu">
<li role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/21739?locale-attribute=fr&amp;show=full">français</a>
</li>
<li class="disabled" role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/21739?locale-attribute=en&amp;show=full">English</a>
</li>
</ul>
</li>
<li>
<form action="/xmlui/login" method="get" style="display: inline">
<button class="navbar-toggle navbar-link"><b aria-hidden="true" class="visible-xs glyphicon glyphicon-user"></b></button>
</form>
</li>
</ul>
</div>
</div>
<div class="navbar-header pull-right hidden-xs">
<div class="nav-language-and-login-lg hidden-xs hidden-sm hidden-md">
<ul class="nav navbar-nav pull-right">
<li class="dropdown" id="ds-language-selection">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" id="language-dropdown-toggle" role="button"><span class="hidden-xs">English <b class="caret"></b></span></a>
<ul aria-labelledby="language-dropdown-toggle" class="dropdown-menu pull-right" data-no-collapse="true" role="menu">
<li role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/21739?locale-attribute=fr&amp;show=full">français</a>
</li>
<li class="disabled" role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/21739?locale-attribute=en&amp;show=full">English</a>
</li>
</ul>
</li>
</ul>
<ul class="nav navbar-nav pull-right">
<li>
<a href="/xmlui/login"><span class="hidden-xs">Login</span></a>
</li>
</ul>
</div>
<div class="hidden-lg">
<ul class="nav navbar-nav pull-right">
<li class="dropdown" id="ds-language-selection">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" id="language-dropdown-toggle" role="button"><span class="hidden-xs">English <b class="caret"></b></span></a>
<ul aria-labelledby="language-dropdown-toggle" class="dropdown-menu pull-right" data-no-collapse="true" role="menu">
<li role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/21739?locale-attribute=fr&amp;show=full">français</a>
</li>
<li class="disabled" role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/21739?locale-attribute=en&amp;show=full">English</a>
</li>
</ul>
</li>
</ul>
<ul class="nav navbar-nav pull-right">
<li>
<a href="/xmlui/login"><span class="hidden-xs">Login</span></a>
</li>
</ul>
</div>
<button class="navbar-toggle visible-sm" data-toggle="offcanvas" type="button"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button>
</div>
</div>
</div>
</header>
<div class="trail-wrapper hidden-print">
<div class="container">
<div class="row">
<div class="col-xs-12">
<div class="breadcrumb dropdown visible-xs">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" id="trail-dropdown-toggle" role="button">View Item <b class="caret"></b></a>
<ul aria-labelledby="trail-dropdown-toggle" class="dropdown-menu" role="menu">
<li role="presentation">
<a href="/xmlui/" role="menuitem"><i aria-hidden="true" class="glyphicon glyphicon-home"></i>  Home</a>
</li>
<li role="presentation">
<a href="/xmlui/handle/1866/3010" role="menuitem">Faculté des arts et des sciences</a>
</li>
<li role="presentation">
<a href="/xmlui/handle/1866/2958" role="menuitem">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle</a>
</li>
<li role="presentation">
<a href="/xmlui/handle/1866/3001" role="menuitem">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle - Thèses et mémoires</a>
</li>
<li class="disabled" role="presentation">
<a href="#" role="menuitem">View Item</a>
</li>
</ul>
</div>
<ul class="breadcrumb hidden-xs">
<li>
<i aria-hidden="true" class="glyphicon glyphicon-home"></i>  <a href="/xmlui/">Home</a>
</li>
<li>
<a href="/xmlui/handle/1866/3010">Faculté des arts et des sciences</a>
</li>
<li>
<a href="/xmlui/handle/1866/2958">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle</a>
</li>
<li>
<a href="/xmlui/handle/1866/3001">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle - Thèses et mémoires</a>
</li>
<li class="active">View Item</li>
</ul>
</div>
</div>
</div>
</div>
<div class="hidden" id="no-js-warning-wrapper">
<div id="no-js-warning">
<div class="notice failure">JavaScript is disabled for your browser. Some features of this site may not work without it.</div>
</div>
</div>
<div class="container" id="main-container">
<div class="row row-offcanvas row-offcanvas-left">
<div class="horizontal-slider clearfix">
<div class="col-xs-6 col-sm-3 sidebar-offcanvas" id="sidebar" role="navigation">
<div class="word-break hidden-print" id="ds-options">
<div class="ds-option-set" id="ds-search-option">
<form action="/xmlui/discover" class="" id="ds-search-form" method="post">
<fieldset>
<div class="input-group">
<input class="ds-text-field form-control" name="query" placeholder="Search" type="text"/><span class="input-group-btn"><button class="ds-button-field btn" title="Go"><span aria-hidden="true" class="glyphicon glyphicon-search"></span></button></span>
</div>
<div class="radio">
<label><input checked="" id="ds-search-form-scope-all" name="scope" type="radio" value=""/>Search Papyrus</label>
</div>
<div class="radio">
<label><input id="ds-search-form-scope-container" name="scope" type="radio" value="1866/3001"/>Search this Collection</label>
</div>
</fieldset>
</form>
</div>
<h2 class="ds-option-set-head h6">My Account</h2>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_account">
<div class="list-group-item ds-option details_Papyrus" id="aspect_eperson_Navigation_item_details_Papyrus">To submit an item or subscribe to email alerts.</div>
<div class="list-group-item ds-option">
<span class="bold"><a href="/xmlui/login">Login</a></span>
</div>
<a class="list-group-item ds-option" href="/xmlui/register">New user?</a>
</div>
<h2 class="ds-option-set-head h6">Browse</h2>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_browse">
<a class="list-group-item active"><span class="h5 list-group-item-heading h5">All of Papyrus</span></a><a class="list-group-item ds-option" href="/xmlui/community-list">Communities and Collections</a><a class="list-group-item ds-option" href="/xmlui/browse?type=title">Titles</a><a class="list-group-item ds-option" href="/xmlui/browse?type=dateissued">Issue Dates</a><a class="list-group-item ds-option" href="/xmlui/browse?type=author">Authors</a><a class="list-group-item ds-option" href="/xmlui/browse?type=advisor">Advisors</a><a class="list-group-item ds-option" href="/xmlui/browse?type=subject">Subjects</a><a class="list-group-item ds-option" href="/xmlui/browse?type=discipline">Disciplines</a><a class="list-group-item ds-option" href="/xmlui/browse?type=affiliation">Affiliation</a><a class="list-group-item ds-option" href="/xmlui/browse?type=titleindex">Titles index</a><a class="list-group-item active"><span class="h5 list-group-item-heading h5">This Collection</span></a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=title">Titles</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=dateissued">Issue Dates</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=author">Authors</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=advisor">Advisors</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=subject">Subjects</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=discipline">Disciplines</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=affiliation">Affiliation</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=titleindex">Titles index</a>
</div>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_context"></div>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_administrative"></div>
<div class="list-group" id="aspect_discovery_Navigation_list_discovery"></div>
<h2 class="ds-option-set-head h6">Statistics</h2>
<div class="list-group" id="aspect_statistics_Navigation_list_statistics">
<a class="list-group-item ds-option" href="/xmlui/handle/1866/21739/statistics">View Usage Statistics</a>
</div>
</div>
</div>
<div class="col-xs-12 col-sm-12 col-md-9 main-content">
<div>
<div class="ds-static-div primary" id="aspect_artifactbrowser_ItemViewer_div_item-view">
<p class="ds-paragraph item-view-toggle item-view-toggle-top">
<span class="item-view-toggle"><a href="/xmlui/handle/1866/21739">Show item record</a></span>
</p>
<!-- External Metadata URL: cocoon://metadata/handle/1866/21739/mets.xml-->
<h2 class="page-header first-page-header">Learning and time : on using memory and curricula for language understanding</h2>
<div class="ds-table-responsive">
<table class="ds-includeSet-table detailtable table table-striped table-hover">
<tr class="ds-table-row odd">
<td class="label-cell">dc.contributor.advisor</td><td class="word-break">Bengio, Yoshua</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.contributor.author</td><td class="word-break">Gulcehre, Caglar</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.date.accessioned</td><td class="word-break">2019-05-13T18:59:58Z</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.date.available</td><td class="word-break">NO_RESTRICTION</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.date.available</td><td class="word-break">2019-05-13T18:59:58Z</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.date.issued</td><td class="word-break">2019-03-13</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.date.submitted</td><td class="word-break">2018-05</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.identifier.uri</td><td class="word-break">http://hdl.handle.net/1866/21739</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Deep learning</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Curriculum learning</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Optimization</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Natural language processing</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Neural networks</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Recurrent neural networks</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Apprentissage profond</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Apprentissage du curriculum</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Traitement du langage naturel</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Language modelling</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Memory</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Modélisation du langage</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Machine translation</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Traduction automatique</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Optimisation</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Réseaux de neurones</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Réseaux de neurones récurrents</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Mémoire</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject.other</td><td class="word-break">Applied Sciences - Artificial Intelligence / Sciences appliqués et technologie - Intelligence artificielle (UMI : 0800)</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.title</td><td class="word-break">Learning and time : on using memory and curricula for language understanding</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.type</td><td class="word-break">Thèse ou mémoire / Thesis or Dissertation</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">etd.degree.discipline</td><td class="word-break">Informatique</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">etd.degree.grantor</td><td class="word-break">Université de Montréal</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">etd.degree.level</td><td class="word-break">Doctorat / Doctoral</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">etd.degree.name</td><td class="word-break">Ph. D.</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dcterms.abstract</td><td class="word-break">Cette thèse présente quelques-unes des étapes entreprises pour pouvoir un jour résoudre le problème de la compréhension du langage naturel et d’apprentissage de dépendances à long terme, dans le but de développer de meilleurs algorithmes d’intelligence artificielle. Cette thèse est écrite comme une thèse par articles, et contient cinq publications scientifiques. Chacun de ces articles propose un nouveau modèle ou algorithme et démontre leur efficacité sur des problèmes qui impliquent des dépendances à long terme ou la compréhension du langage naturel. Malgré le fait que quelque uns de ces modèles n’ont été testés que sur une seule tâche (comme la traduction automatique neuronale), les méthodes proposées sont généralement applicables dans d’autres domaines et sur d’autres tâches.

Dans l’introduction de la thèse, nous expliquons quelques concepts fondamentaux de l'entraînement de réseaux de neurones appliqués sur des données séquentielles. Tout d'abord, nous présentons succinctement les réseaux de neurones, puis, de façon plus détaillé, certains algorithmes et méthodes utilisés à travers cette thèse.

Dans notre premier article, nous proposons une nouvelle méthode permettant d'utiliser la grande quantité de données monolingue disponible afin d'entraîner des modèles de traduction. Nous avons accompli cela en entraînant d’abord un modèle Long short-term memory (LSTM) sur un large corpus monolingue. Nous lions ensuite la sortie de la couche cachée du modèle avec celle d’un décodeur d’un modèle de traduction automatique. Ce dernier utilise un mécanisme d’attention et est entièrement entraîné par descente de gradient. Nous avons montré que la méthode proposée peut augmenter la performance des modèles de traduction automatique neuronale de façon significative sur les tâches où peu de données multilingues sont disponibles. Notre approche augmente également l’efficacité de l’utilisation des données dans les systèmes de traduction automatique. Nous montrons aussi des améliorations sur les paires de langues suivantes: turc-anglais, allemand-anglais, chinois-anglais et tchèque-anglais.

Dans notre deuxième article, nous proposons une approche pour aborder le problème des mots rares dans plusieurs tâches du traitement des langages. Notre approche modifie l’architecture habituelle des modèles encodeur-décodeur avec attention, en remplaçant la couche softmax du décodeur par notre couche pointer-softmax. Celle-ci permet au décodeur de pointer à différents endroits dans la phrase d’origine. Notre modèle apprend à alterner entre copier un mot de la phrase d’origine et prédire un mot provenant d’une courte liste de mots prédéfinie, de manière probabiliste. L’approche que nous avons proposée est entièrement entraînable par descente de gradient et n’utilise qu’un objectif de maximum de vraisemblance sur les tâches de traduction. Nous avons aussi montré que le pointer-softmax aide de manière significative aux tâches de traduction et de synthèse de documents.

Dans notre article  "Plan, Attend, Generate: Planning for Sequence-to-Sequence Models", nous proposons deux approches pour apprendre l’alignement dans les modèles entraînés sur des séquences. Lorsque la longueur de l’entrée et celle de la sortie sont trop grandes, apprendre les alignements peut être très difficile. La raison est que lorsque le décodeur est trop puissant, il a tendance à ignorer l’alignement des mots pour ne se concentrer que sur le dernier mot de la séquence d’entrée. Nous avons proposé une nouvelle approche, inspirée d’un algorithme d’apprentissage par renforcement, en ajoutant explicitement un mécanisme de planification au décodeur. Ce nouveau mécanisme planifie à l’avance l’alignement pour les k prochaines prédictions. Notre modèle apprend également un plan de correction pour déterminer lorsqu’il est nécessaire de recalculer les alignements. Notre approche peut apprendre de haut niveaux d’abstraction au point de vue temporel et nous montrons que les alignements sont généralement de meilleure qualité. Nous obtenons également des gains de performance significatifs comparativement à notre modèle de référence, malgré le fait que nos modèles ont moins de paramètres et qu’ils aient été entraînés moins longtemps.

Dans notre article "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", nous proposons une nouvelle approche pour ajouter de manière explicite un mécanisme de mémoire aux réseaux de neurones. Contrairement aux RNNs conventionnels, la mémoire n’est pas seulement représentée au niveau des activations du réseau, mais également dans une mémoire externe. Notre modèle, D-NTM, utilise un mécanisme d’adressage plus simple que les Neural Turing Machine (NTM) en utilisant des paires clé-valeur. Nous montrons que les modèles disposant de ce nouveau mécanisme peuvent plus efficacement apprendre les dépendances à long terme, en plus de mieux généraliser. Nous obtenons des améliorations sur plusieurs tâches incluant entre autres la réponse aux questions sur bAbI, le raisonnement avec implication, MNIST permuté, ainsi que des tâches synthétiques.

Dans notre article "Noisy Activation Functions", nous proposons une nouvelle fonction d’activation, qui rend les activations stochastiques en leur ajoutant du bruit. Notre motivation dans cet article est d’aborder les problèmes d’optimisation qui surviennent lorsque nous utilisons des fonctions d’activation qui saturent, comme celles généralement utilisées dans les RNNs. Notre approche permet d’utiliser des fonctions d’activation linéaires par morceaux sur les RNNs à porte. Nous montrons des améliorations pour un grand nombre de tâches sans effectuer de recherche d'hyper paramètres intensive. Nous montrons également que supprimer le bruit dans les fonctions d’activation a un profond impact sur l’optimisation.</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dcterms.abstract</td><td class="word-break">The goal of this thesis is to present some of the small steps taken on the path towards solving natural language understanding and learning long-term dependencies to develop artificial intelligence algorithms that can reason with language. This thesis is written as a thesis by articles and contains five articles. Each article in this thesis proposes a new model or algorithm and demonstrates the efficiency of the proposed approach to solve problems that involve long-term dependencies or require natural language understanding. Although some of the models are tested on a particular task (such as neural machine translation), the proposed methods in this thesis are generally applicable to other domains and tasks (and have been used in the literature).

In the introduction of this thesis, we introduce some of the fundamental concepts behind training sequence models using neural networks. We first provide a brief introduction to neural networks and then dive into details of the some of approaches and algorithms that are used throughout this thesis.

In our first article, we propose a novel method to utilize the abundant amount of available monolingual data for training neural machine translation models. We have accomplished this goal by first training a long short-term memory (LSTM) language model on a large monolingual corpus and then fusing the outputs or the hidden states of the LSTM language model with the decoder of the neural machine translation model. Our neural machine translation model is trained end to end with an attention mechanism. We have shown that our proposed approaches can improve the performance of the neural machine translation models significantly on the rare resource translation tasks and our approach improved the data-efficiency of the end to end neural machine translation systems. We report improvements on Turkish-English (Tr-En), German-English (De-En), Chinese-English (Zh-En) and Czech-English (Cz-En) translation tasks.

In our second paper, we propose an approach to address the problem of rare words for natural language processing tasks. Our approach augments the encoder-decoder architecture with attention model by replacing the final softmax layer with our proposed pointer-softmax layer that creates pointers to the source sentences as the decoder translates. In the case of pointer-softmax, our model learns to switch between copying a word from the source and predicting a word from a shortlist vocabulary in a probabilistic manner. Our proposed approach is end-to-end trainable with a single maximum likelihood objective of the NMT model. We have also shown that it improves the performance of summarization and the neural machine translation model. We report significant improvements in machine translation and summarization tasks.

In our "Plan, Attend, Generate: Planning for Sequence-to-Sequence Models" paper, we propose two new approaches to learn alignments in a sequence to sequence model. If the input and the source context is very long, learning the alignments for a sequence to sequence model can be difficult. In particular, because when the decoder is a large network, it can learn to ignore the alignments and attend more on the last token of the input sequence. We propose a new approach which is inspired by a hierarchical reinforcement learning algorithm and extend our model with an explicit planning mechanism. The proposed alignment mechanism plans and computes the alignments for the next $k$ tokens in the decoder. Our model also learns a commitment plan to decide when to recompute the alignment matrix. Our proposed approach can learn high-level temporal abstractions, and we show that it qualitatively learns better alignments. We also achieve significant improvements over our baseline despite using smaller models and with less training.

In "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes," we propose a new approach for augmenting neural networks with an explicit memory mechanism. As opposed to conventional RNNs, the memory is not only represented in the activations of the neural network but in an external memory that can be accessed via the neural network controller. Our model, D-NTM uses a more straightforward memory addressing mechanism than NTM which is achieved by using key-value pairs for each memory cell. We find out that the models augmented with an external memory mechanism can learn tasks that involve long-term dependencies more efficiently and achieve better generalization. We achieve improvements on many tasks including but not limited to episodic question answering on bAbI, reasoning with entailment, permuted MNIST task and synthetic tasks.

In our "Noisy Activation Functions" paper, we propose a novel activation function that makes the activations stochastic by injecting a particular form of noise to them. Our motivation in this paper is to address the optimization problem of commonly used saturating activation functions that are used with the recurrent neural networks. Our approach enables us to use piece-wise linear activation functions on the gated recurrent neural network models. We show improvements in a wide range of tasks without doing any extensive hyperparameter search by a drop-in replacement. We also show that annealing the noise of the activation function can have a profound continuation-like effect on the optimization of the network.</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dcterms.language</td><td class="word-break">eng</td><td>fr</td>
</tr>
</table>
</div>
<span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft_id=http%3A%2F%2Fhdl.handle.net%2F1866%2F21739&amp;rfr_id=info%3Asid%2Fdspace.org%3Arepository&amp;rft.degree=Informatique&amp;rft.degree=Universit%C3%A9+de+Montr%C3%A9al&amp;rft.degree=Doctorat+%2F+Doctoral&amp;rft.degree=Ph.+D."> ﻿ 
        </span>
<h3>Files in this item</h3>
<div class="file-list">
<div class="file-wrapper row">
<div class="col-xs-6 col-sm-3">
<div class="thumbnail-wrapper">
<a class="image-link" href="/xmlui/bitstream/handle/1866/21739/Gulcehre_Caglar_2018_these.pdf?sequence=10&amp;isAllowed=y"><img alt="Thumbnail" class="thumbnail-papyrus" src="/xmlui/bitstream/handle/1866/21739/Gulcehre_Caglar_2018_these.pdf.jpg?sequence=12&amp;isAllowed=y"/></a>
<div class="imageoverlay">
<img height="49" src="/xmlui/themes/Mirage2/images/mimes/mime_icone_pdf.png" style="height: 49px;" width="44"/></div>
</div>
</div>
<div class="col-xs-6 col-sm-7">
<dl class="file-metadata dl-horizontal">
<dt>Name:</dt>
<dd class="word-break" title="Gulcehre_Caglar_2018_these.pdf">
<a href="/xmlui/bitstream/handle/1866/21739/Gulcehre_Caglar_2018_these.pdf?sequence=10&amp;isAllowed=y">Gulcehre_Caglar_2018_these.pdf</a>
</dd>
<dt>Size:</dt>
<dd class="word-break">20.73Mb</dd>
<dt>Format:</dt>
<dd class="word-break">PDF</dd>
<dt>Description:</dt>
<dd class="word-break" title="Thèse">Thèse</dd>
</dl>
</div>
</div>
</div>
<h3 class="ds-list-head">This item appears in the following Collection(s)</h3>
<ul class="ds-referenceSet-list">
<!-- External Metadata URL: cocoon://metadata/handle/1866/2621/mets.xml-->
<li>
<a href="/xmlui/handle/1866/2621">Thèses et mémoires électroniques de l’Université de Montréal</a> [18270]<br/>
</li>
<!-- External Metadata URL: cocoon://metadata/handle/1866/3001/mets.xml-->
<li>
<a href="/xmlui/handle/1866/3001">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle - Thèses et mémoires</a> [795]<br/>
</li>
</ul>
<p class="ds-paragraph item-view-toggle item-view-toggle-bottom">
<span class="item-view-toggle"><a href="/xmlui/handle/1866/21739">Show item record</a></span>
</p>
</div>
</div>
<div class="visible-xs visible-sm">
<footer>
<div class="row">
<hr/>
<div class="col-xs-7 col-sm-8">
<div>
<a href="http://www.dspace.org/" target="_blank">DSpace software</a>
                        [version 5.8 XMLUI],
                        copyright © 2002-2015  <a href="http://www.duraspace.org/" target="_blank">DuraSpace</a>
</div>
<div class="hidden-print">
<a href="/xmlui/contact">Contact Us</a> | <a href="/xmlui/feedback">Send Feedback</a>
</div>
</div>
<div class="col-xs-5 col-sm-4 hidden-print">
<div class="pull-right">
<script type="text/javascript"> 
										      (function(d, t) { 
										        var s = d.createElement(t), options = {'domain':'papyrus.bib.umontreal.ca','style': '16','container':'entrust-net-seal'}; 
										        s.src = 'https://seal.entrust.net/sealv2.js'; 
										        s.async = true; 
										        var scr = d.getElementsByTagName(t)[0], par = scr.parentNode; par.insertBefore(s, scr); 
										        s.onload = s.onreadystatechange = function() { 
										        var rs = this.readyState; if (rs) if (rs != 'complete') if (rs != 'loaded') return; 
										        try{goEntrust(options)} catch (e) {} }; 
										        })(document, 'script'); 
										</script>
<div id="entrust-net-seal">
<a href="https://www.entrust.com/ssl-certificates/">Certificat SSL / SSL Certificate</a>
</div>
</div>
</div>
</div>
<div class="row bib-footer hidden-print">
<div class="col-xs-12 col-sm-12 col-md-4">
<a class="footerEXLlink" href="http://www.bib.umontreal.ca"><img alt="les bibliothèques/UdeM" src="/xmlui/themes/Mirage2/images/propulse-par.png"/></a>
</div>
<div class="hidden-xs col-sm-12 col-md-8">
<nav>
<ul>
<li>
<a href="https://www.urgence.umontreal.ca/">Emergency</a>
</li>
<li>
<a href="http://www.carrieres.umontreal.ca/">Careers</a>
</li>
<li>
<a href="https://outlook.umontreal.ca/">My email</a>
</li>
<li>
<a href="https://studium.umontreal.ca/">StudiUM</a>
</li>
<li>
<a href="http://itunesu.umontreal.ca/">iTunes U</a>
</li>
<li>
<a href="http://www.bib.umontreal.ca/a-propos/nous-ecrire.htm">Contact us</a>
</li>
<li>
<a href="https://www.facebook.com/bibUdeM"><img alt="Facebook" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Facebook.png"/></a>
</li>
<li>
<a href="https://www.youtube.com/user/BibliothequesUdeM"><img alt="YouTube" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-YouTube.png"/></a>
</li>
<li>
<a href="https://twitter.com/bibUdeM"><img alt="Twitter" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Twitter.png"/></a>
</li>
<li>
<a href="https://www.nouvelles.umontreal.ca/frontpage/rss.html"><img alt="University RSS" src="/xmlui/themes/Mirage2/images/bib-umontreal/icone-syndication-14x14.png"/></a>
</li>
</ul>
</nav>
</div>
</div>
<a class="hidden" href="/xmlui/htmlmap"> </a>
<p> </p>
</footer>
</div>
</div>
</div>
</div>
<div class="hidden-xs hidden-sm">
<footer>
<div class="row">
<hr/>
<div class="col-xs-7 col-sm-8">
<div>
<a href="http://www.dspace.org/" target="_blank">DSpace software</a>
                        [version 5.8 XMLUI],
                        copyright © 2002-2015  <a href="http://www.duraspace.org/" target="_blank">DuraSpace</a>
</div>
<div class="hidden-print">
<a href="/xmlui/contact">Contact Us</a> | <a href="/xmlui/feedback">Send Feedback</a>
</div>
</div>
<div class="col-xs-5 col-sm-4 hidden-print">
<div class="pull-right">
<script type="text/javascript"> 
										      (function(d, t) { 
										        var s = d.createElement(t), options = {'domain':'papyrus.bib.umontreal.ca','style': '16','container':'entrust-net-seal'}; 
										        s.src = 'https://seal.entrust.net/sealv2.js'; 
										        s.async = true; 
										        var scr = d.getElementsByTagName(t)[0], par = scr.parentNode; par.insertBefore(s, scr); 
										        s.onload = s.onreadystatechange = function() { 
										        var rs = this.readyState; if (rs) if (rs != 'complete') if (rs != 'loaded') return; 
										        try{goEntrust(options)} catch (e) {} }; 
										        })(document, 'script'); 
										</script>
<div id="entrust-net-seal">
<a href="https://www.entrust.com/ssl-certificates/">Certificat SSL / SSL Certificate</a>
</div>
</div>
</div>
</div>
<div class="row bib-footer hidden-print">
<div class="col-xs-12 col-sm-12 col-md-4">
<a class="footerEXLlink" href="http://www.bib.umontreal.ca"><img alt="les bibliothèques/UdeM" src="/xmlui/themes/Mirage2/images/propulse-par.png"/></a>
</div>
<div class="hidden-xs col-sm-12 col-md-8">
<nav>
<ul>
<li>
<a href="https://www.urgence.umontreal.ca/">Emergency</a>
</li>
<li>
<a href="http://www.carrieres.umontreal.ca/">Careers</a>
</li>
<li>
<a href="https://outlook.umontreal.ca/">My email</a>
</li>
<li>
<a href="https://studium.umontreal.ca/">StudiUM</a>
</li>
<li>
<a href="http://itunesu.umontreal.ca/">iTunes U</a>
</li>
<li>
<a href="http://www.bib.umontreal.ca/a-propos/nous-ecrire.htm">Contact us</a>
</li>
<li>
<a href="https://www.facebook.com/bibUdeM"><img alt="Facebook" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Facebook.png"/></a>
</li>
<li>
<a href="https://www.youtube.com/user/BibliothequesUdeM"><img alt="YouTube" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-YouTube.png"/></a>
</li>
<li>
<a href="https://twitter.com/bibUdeM"><img alt="Twitter" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Twitter.png"/></a>
</li>
<li>
<a href="https://www.nouvelles.umontreal.ca/frontpage/rss.html"><img alt="University RSS" src="/xmlui/themes/Mirage2/images/bib-umontreal/icone-syndication-14x14.png"/></a>
</li>
</ul>
</nav>
</div>
</div>
<a class="hidden" href="/xmlui/htmlmap"> </a>
<p> </p>
</footer>
</div>
</div>
<script src="https://www.google.com/jsapi"> </script><script>if(!window.DSpace){window.DSpace={};}window.DSpace.context_path='/xmlui';window.DSpace.theme_path='/xmlui/themes/Mirage2/';</script><script src="/xmlui/themes/Mirage2/scripts/theme.js"> </script><script>
                  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

                  ga('create', 'UA-305727-1', 'papyrus.bib.umontreal.ca');
                  ga('send', 'pageview');
           </script><script>         
             var details_Papyrus =  $("#aspect_viewArtifacts_Navigation_list_account .details_Papyrus").text();
             $("#aspect_viewArtifacts_Navigation_list_account a[href='/xmlui/login']").attr("title",details_Papyrus);
        </script>
</body></html>
