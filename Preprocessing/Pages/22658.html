<!DOCTYPE html>

<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="/xmlui/themes/Mirage2/images/favicon.ico" rel="shortcut icon"/>
<link href="/xmlui/themes/Mirage2/images/apple-touch-icon.png" rel="apple-touch-icon"/>
<meta content="DSpace 5.8" name="Generator"/>
<link href="https://fonts.googleapis.com/css?family=PT+Sans:400,700|Ubuntu:400,500,700italic|Droid+Sans:400,700" rel="stylesheet" type="text/css"/>
<link href="/xmlui/themes/Mirage2/styles/main.css" rel="stylesheet"/>
<link href="https://papyrus.bib.umontreal.ca:443/xmlui/description.xml" rel="search" title="DSpace" type="application/opensearchdescription+xml"/>
<script>
                //Clear default text of emty text areas on focus
                function tFocus(element)
                {
                if (element.value == ''){element.value='';}
                }
                //Clear default text of emty text areas on submit
                function tSubmit(form)
                {
                var defaultedElements = document.getElementsByTagName("textarea");
                for (var i=0; i != defaultedElements.length; i++){
                if (defaultedElements[i].value == ''){
                defaultedElements[i].value='';}}
                }
                //Disable pressing 'enter' key to submit a form (otherwise pressing 'enter' causes a submission to start over)
                function disableEnterKey(e)
                {
                var key;

                if(window.event)
                key = window.event.keyCode;     //Internet Explorer
                else
                key = e.which;     //Firefox and Netscape

                if(key == 13)  //if "Enter" pressed, then disable!
                return false;
                else
                return true;
                }
            </script><!--[if lt IE 9]>
                <script src="/xmlui/themes/Mirage2/vendor/html5shiv/dist/html5shiv.js"> </script>
                <script src="/xmlui/themes/Mirage2/vendor/respond/respond.min.js"> </script>
                <![endif]--><script src="/xmlui/themes/Mirage2/vendor/modernizr/modernizr.js"> </script><script src="https://www.bib.umontreal.ca/modele/urgence-udem/1.0.0/urgence-udem.js"></script>
<title>Feature extraction on faces : from landmark localization to depth estimation</title>
<meta content="Dr8axyf94eWBJ8HQjNW4eKxTumAAPsNbvRwpJYMbgGs" name="google-site-verification"/>
<link href="http://purl.org/dc/terms/" rel="schema.DCTERMS">
<link href="http://purl.org/dc/elements/1.1/" rel="schema.DC">
<meta content="Vincent, Pascal" name="DC.contributor">
<meta content="Pal, Christopher" name="DC.contributor">
<meta content="Honari, Sina" name="DC.creator">
<meta content="2019-11-27T20:09:50Z" name="DCTERMS.dateAccepted" scheme="DCTERMS.W3CDTF">
<meta content="NO_RESTRICTION" name="DCTERMS.available" scheme="DCTERMS.W3CDTF" xml:lang="fr">
<meta content="2019-11-27T20:09:50Z" name="DCTERMS.available" scheme="DCTERMS.W3CDTF">
<meta content="2019-06-19" name="DCTERMS.issued" scheme="DCTERMS.W3CDTF"/>
<meta content="2018-12" name="DC.date" scheme="DCTERMS.W3CDTF"/>
<meta content="http://hdl.handle.net/1866/22658" name="DC.identifier" scheme="DCTERMS.URI"/>
<meta content="Neural networks" name="DC.subject" xml:lang="fr"/>
<meta content="Deep learning" name="DC.subject" xml:lang="fr"/>
<meta content="Convolutional networks" name="DC.subject" xml:lang="fr"/>
<meta content="Supervised learning" name="DC.subject" xml:lang="fr"/>
<meta content="Unsupervised learning" name="DC.subject" xml:lang="fr"/>
<meta content="Semi-supervised learning" name="DC.subject" xml:lang="fr"/>
<meta content="Coarse-to-fine architectures" name="DC.subject" xml:lang="fr"/>
<meta content="Landmark localization" name="DC.subject" xml:lang="fr"/>
<meta content="Depth estimation" name="DC.subject" xml:lang="fr"/>
<meta content="Face rotation" name="DC.subject" xml:lang="fr"/>
<meta content="Face replacement" name="DC.subject" xml:lang="fr"/>
<meta content="Réseaux neuronaux" name="DC.subject" xml:lang="fr"/>
<meta content="Apprentissage profond" name="DC.subject" xml:lang="fr"/>
<meta content="Réseaux neuronaux de convolution" name="DC.subject" xml:lang="fr"/>
<meta content="Apprentissage supervisé" name="DC.subject" xml:lang="fr"/>
<meta content="Apprentissage non-supervisé" name="DC.subject" xml:lang="fr"/>
<meta content="Apprentissage semi-supervisé" name="DC.subject" xml:lang="fr"/>
<meta content="Architectures grossières à fines" name="DC.subject" xml:lang="fr"/>
<meta content="Localisation de points clés" name="DC.subject" xml:lang="fr"/>
<meta content="Estimation de la profondeur" name="DC.subject" xml:lang="fr"/>
<meta content="Rotation de visage" name="DC.subject" xml:lang="fr"/>
<meta content="Échange de visage" name="DC.subject" xml:lang="fr"/>
<meta content="Applied Sciences - Artificial Intelligence / Sciences appliqués et technologie - Intelligence artificielle (UMI : 0800)" name="DC.subject" xml:lang="fr"/>
<meta content="Feature extraction on faces : from landmark localization to depth estimation" name="DC.title" xml:lang="fr"/>
<meta content="Thèse ou mémoire / Thesis or Dissertation" name="DC.type"/>
<meta content="Le sujet de cette thèse porte sur les algorithmes d'apprentissage qui extraient les caractéristiques importantes des visages. Les caractéristiques d’intérêt principal sont des points clés;  
La localisation en deux dimensions (2D) ou en trois dimensions (3D) de traits importants du visage telles que le centre des yeux, le bout du nez et les coins de la bouche. Les points clés sont utilisés pour résoudre des tâches complexes qui ne peuvent pas être résolues directement ou qui requièrent du guidage pour l’obtention de performances améliorées, telles que la reconnaissance de poses ou de gestes, le suivi ou la vérification du visage. L'application des modèles présentés dans cette thèse concerne les images du visage; cependant, les algorithmes proposés sont plus généraux et peuvent être appliqués aux points clés de d'autres objets, tels que les mains, le corps ou des objets fabriqués par l'homme. Cette thèse est écrite par article et explore différentes techniques pour résoudre plusieurs aspects de la localisation de points clés.

Dans le premier article, nous démêlons l'identité et l'expression d'un visage donné pour apprendre une distribution à priori sur l'ensemble des points clés. Cette distribution à priori est ensuite combinée avec un classifieur discriminant qui apprend une distribution de probabilité indépendante par point clé. Le modèle combiné est capable d'expliquer les différences dans les expressions pour une même représentation d'identité.

Dans le deuxième article, nous proposons une architecture qui vise à conserver les caractéristiques d’images pour effectuer des tâches qui nécessitent une haute précision au niveau des pixels, telles que la localisation de points clés ou la segmentation d’images. L’architecture proposée extrait progressivement les caractéristiques les plus grossières dans les étapes d'encodage pour obtenir des informations plus globales sur l’image. Ensuite, il étend les caractéristiques grossières pour revenir à la résolution de l'image originale en recombinant les caractéristiques du chemin d'encodage. Le modèle, appelé Réseaux de Recombinaison, a obtenu l’état de l’art sur plusieurs jeux de données, tout en accélérant le temps d’apprentissage.

Dans le troisième article, nous visons à améliorer la localisation des points clés lorsque peu d'images comportent des étiquettes sur des points clés. En particulier, nous exploitons une forme plus faible d’étiquettes qui sont plus faciles à acquérir ou plus abondantes tel que l'émotion ou la pose de la tête. Pour ce faire, nous proposons une architecture permettant la rétropropagation du gradient des étiquettes les plus faibles à travers des points clés, ainsi entraînant le réseau de localisation des points clés. Nous proposons également une composante de coût non supervisée qui permet des prédictions de points clés équivariantes en fonction des transformations appliquées à l'image, sans avoir les vraies étiquettes des points clés. Ces techniques ont considérablement amélioré les performances tout en réduisant le pourcentage d'images étiquetées par points clés.

Finalement, dans le dernier article, nous proposons un algorithme d'apprentissage permettant d'estimer la profondeur des points clés sans aucune supervision de la profondeur. Nous y parvenons en faisant correspondre les points clés de deux visages en les transformant l'un vers l'autre. Cette transformation nécessite une estimation de la profondeur sur un visage, ainsi que une transformation affine qui transforme le premier visage au deuxième. Nous démontrons que notre formulation ne nécessite que la profondeur et que les paramètres affines peuvent être estimés avec un solution analytique impliquant les points clés augmentés par profondeur. Même en l'absence de supervision directe de la profondeur, la technique proposée extrait des valeurs de profondeur raisonnables qui diffèrent des vraies valeurs de profondeur par un facteur d'échelle et de décalage. Nous démontrons des applications d'estimation de profondeur pour la tâche de rotation de visage, ainsi que celle d'échange de visage." name="DCTERMS.abstract" xml:lang="fr"/>
<meta content="This thesis focuses on learning algorithms that extract important features from faces. The features of main interest are landmarks; the two dimensional (2D) or three dimensional (3D) locations of important facial features such as eye centers, nose tip, and mouth corners. Landmarks are used to solve complex tasks that cannot be solved directly or require guidance for enhanced performance, such as pose or gesture recognition, tracking, or face verification. The application of the models presented in this thesis is on facial images; however, the algorithms proposed are more general and can be applied to the landmarks of other forms of objects, such as hands, full body or man-made objects. This thesis is written by article and explores different techniques to solve various aspects of landmark localization. 

In the first article, we disentangle identity and expression of a given face to learn a prior distribution over the joint set of landmarks. This prior is then merged with a discriminative classifier that learns an independent probability distribution per landmark. The merged model is capable of explaining differences in expressions for the same identity representation.

In the second article, we propose an architecture that aims at uncovering image features to do tasks that require high pixel-level accuracy, such as landmark localization or image segmentation. 
The proposed architecture gradually extracts coarser features in its encoding steps to get more global information over the image and then it expands the coarse features back to the image resolution by recombining the features of the encoding path. The model, termed Recombinator Networks, obtained state-of-the-art on several datasets, while also speeding up training.

In the third article, we aim at improving landmark localization when only a few images with labelled landmarks are available. In particular, we leverage a weaker form of data labels that are easier to acquire or more abundantly available such as emotion or head pose. To do so, we propose an architecture to backpropagate gradients of the weaker labels through landmarks, effectively training the landmark localization network. We also propose an unsupervised loss component which makes equivariant landmark predictions with respect to transformations applied to the image without having ground truth landmark labels. These techniques improved performance considerably when we have a low percentage of labelled images with landmarks.

Finally, in the last article, we propose a learning algorithm to estimate the depth of the landmarks without any depth supervision. We do so by matching landmarks of two faces through transforming one to another. This transformation requires estimation of depth on one face and an affine transformation that maps the first face to the second one. Our formulation, which only requires depth estimation and affine parameters, can be estimated as a closed form solution of the 2D landmarks and the estimated depth. Even without direct depth supervision, the proposed technique extracts reasonable depth values that differ from the ground truth depth values by a scale and a shift. We demonstrate applications of the estimated depth in face rotation and face replacement tasks." name="DCTERMS.abstract" xml:lang="fr"/>
<meta content="eng" name="DCTERMS.language" xml:lang="fr"/>
<meta content="Neural networks; Deep learning; Convolutional networks; Supervised learning; Unsupervised learning; Semi-supervised learning; Coarse-to-fine architectures; Landmark localization; Depth estimation; Face rotation; Face replacement; Réseaux neuronaux; Apprentissage profond; Réseaux neuronaux de convolution; Apprentissage supervisé; Apprentissage non-supervisé; Apprentissage semi-supervisé; Architectures grossières à fines; Localisation de points clés; Estimation de la profondeur; Rotation de visage; Échange de visage; Applied Sciences - Artificial Intelligence / Sciences appliqués et technologie - Intelligence artificielle (UMI : 0800); Thèse ou mémoire / Thesis or Dissertation; Informatique" name="citation_keywords"/>
<meta content="Feature extraction on faces : from landmark localization to depth estimation" name="citation_title"/>
<meta content="eng" name="citation_language"/>
<meta content="Honari, Sina" name="citation_author"/>
<meta content="https://papyrus.bib.umontreal.ca/xmlui/bitstream/1866/22658/2/Honari_Sina_2018_these.pdf" name="citation_pdf_url"/>
<meta content="2019-06-19" name="citation_date"/>
<meta content="https://papyrus.bib.umontreal.ca/xmlui/handle/1866/22658" name="citation_abstract_html_url"/>
<script type="text/x-mathjax-config">
                    MathJax.Hub.Config({
                      tex2jax: {
                        inlineMath: [['\\(','\\)']],
                        ignoreClass: "detail-field-data|detailtable|exception"
                      },
                      TeX: {
                        Macros: {
                          AA: '{\\mathring A}'
                        }
                      }
                    });
                </script><script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"> </script>
</meta></meta></meta></meta></meta></meta></link></link></head><body>
<header>
<div class="navbar navbar-default navbar-static-top" role="navigation">
<div class="container">
<div class="hidden-xs hidden-sm hidden-md" id="um-bandeau" role="banner">
<nav class="um-nav um-nav-hidden" role="navigation">
<span class="hidden">Liens externes</span>
<ul>
<li>
<a href="https://www.umontreal.ca/#udemwww-search-personne">Directories</a>
</li>
<li>
<a href="http://www.umontreal.ca/repertoires/facultes.html">Faculties</a>
</li>
<li>
<a href="http://www.bib.umontreal.ca/">Libraries</a>
</li>
<li>
<a href="http://plancampus.umontreal.ca/">Campus maps</a>
</li>
<li>
<a href="http://www.umontreal.ca/index/az.html">Sites A to Z</a>
</li>
<li>
<a>My UdeM</a>
<ul>
<li>
<a href="https://monudem.umontreal.ca/">My UdeM Portal</a>
</li>
<li>
<a href="https://outlook.umontreal.ca">My email</a>
</li>
<li>
<a href="https://studium.umontreal.ca/">StudiUM</a>
</li>
</ul>
</li>
</ul>
</nav>
<form action="https://google.com/cse" class="um-recherche" id="um-recherche" role="search">
<input name="cx" type="hidden" value="011926736769028447783:qlpu3so2kqq"/><input name="ie" type="hidden" value="ISO-8859-1"/><label class="hidden" for="um-boite-recherche">Search in : </label><span class="um-boite-bouton"><input class="um-boite" id="um-boite-recherche" maxlength="255" name="q" placeholder="UdeM" title="Type your search text" type="text" value=""/><input alt="Search in : " class="um-bouton" src="/xmlui/themes/Mirage2/images/bib-umontreal/loupe.gif" type="image"/></span>
</form>
</div>
<div class="hidden-xs hidden-sm visible-md visible-lg" id="papyrus-deco">
<img alt="Dessin du pavillon Roger Gaudry/Sketch of Roger Gaudry Building" border="0" height="89px" src="/xmlui/themes/Mirage2/images/UdeM.gif" title="Dessin du pavillon Roger Gaudry/Sketch of Roger Gaudry Building" width="193px"/></div>
<div class="navbar-header">
<button class="navbar-toggle" data-toggle="offcanvas" type="button"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand hidden-xs hidden-sm visible-md visible-lg um-logo-md-lg" href="http://www.umontreal.ca/english/index.html"><img alt="University Home page" src="/xmlui/themes/Mirage2/images/logo-UdeM.svg" title="Back to University Home page"/></a><a class="navbar-brand hidden-xs visible-sm hidden-md hidden-lg um-logo-sm" href="http://www.umontreal.ca/english/index.html"><img alt="University Home page" src="/xmlui/themes/Mirage2/images/logo-UdeM.svg" title="Back to University Home page"/></a><a class="navbar-brand visible-xs hidden-sm hidden-md hidden-lg um-logo-xs" href="http://www.umontreal.ca/english/index.html"><img alt="University Home page" src="/xmlui/themes/Mirage2/images/logo-UdeM.svg" title="Back to University Home page"/></a>
<div class="navbar-brand separateur visible-xs hidden-sm hidden-md hidden-lg um-sep-site-xs"></div>
<div class="navbar-brand separateur hidden-xs visible-sm hidden-md hidden-lg um-sep-site-sm"></div>
<div class="navbar-brand separateur visible-md visible-lg hidden-xs hidden-sm um-sep-site-md-lg"></div>
<div class="navbar-brand visible-md visible-lg hidden-xs hidden-sm um-titre-site-md-lg">
<span class="papyrus-signature">Papyrus</span> :
                                Institutional Repository</div>
<div class="navbar-brand hidden-xs visible-sm hidden-md hidden-lg um-titre-site-sm">
<span class="papyrus-signature">Papyrus</span>
<br/>Institutional Repository</div>
<div class="navbar-brand visible-xs hidden-sm hidden-md hidden-lg um-titre-site-xs">
<span class="papyrus-signature">Papyrus</span>
</div>
<div class="navbar-header pull-right visible-xs hidden-sm hidden-md hidden-lg">
<ul class="nav nav-pills pull-left">
<li class="dropdown" id="ds-language-selection-xs">
<button class="dropdown-toggle navbar-toggle navbar-link" data-toggle="dropdown" href="#" id="language-dropdown-toggle-xs" role="button"><b aria-hidden="true" class="visible-xs glyphicon glyphicon-globe"></b></button>
<ul aria-labelledby="language-dropdown-toggle-xs" class="dropdown-menu pull-right" data-no-collapse="true" role="menu">
<li role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/22658?locale-attribute=fr&amp;show=full">français</a>
</li>
<li class="disabled" role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/22658?locale-attribute=en&amp;show=full">English</a>
</li>
</ul>
</li>
<li>
<form action="/xmlui/login" method="get" style="display: inline">
<button class="navbar-toggle navbar-link"><b aria-hidden="true" class="visible-xs glyphicon glyphicon-user"></b></button>
</form>
</li>
</ul>
</div>
</div>
<div class="navbar-header pull-right hidden-xs">
<div class="nav-language-and-login-lg hidden-xs hidden-sm hidden-md">
<ul class="nav navbar-nav pull-right">
<li class="dropdown" id="ds-language-selection">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" id="language-dropdown-toggle" role="button"><span class="hidden-xs">English <b class="caret"></b></span></a>
<ul aria-labelledby="language-dropdown-toggle" class="dropdown-menu pull-right" data-no-collapse="true" role="menu">
<li role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/22658?locale-attribute=fr&amp;show=full">français</a>
</li>
<li class="disabled" role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/22658?locale-attribute=en&amp;show=full">English</a>
</li>
</ul>
</li>
</ul>
<ul class="nav navbar-nav pull-right">
<li>
<a href="/xmlui/login"><span class="hidden-xs">Login</span></a>
</li>
</ul>
</div>
<div class="hidden-lg">
<ul class="nav navbar-nav pull-right">
<li class="dropdown" id="ds-language-selection">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" id="language-dropdown-toggle" role="button"><span class="hidden-xs">English <b class="caret"></b></span></a>
<ul aria-labelledby="language-dropdown-toggle" class="dropdown-menu pull-right" data-no-collapse="true" role="menu">
<li role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/22658?locale-attribute=fr&amp;show=full">français</a>
</li>
<li class="disabled" role="presentation">
<a href="https://papyrus.bib.umontreal.ca:443/xmlui/handle/1866/22658?locale-attribute=en&amp;show=full">English</a>
</li>
</ul>
</li>
</ul>
<ul class="nav navbar-nav pull-right">
<li>
<a href="/xmlui/login"><span class="hidden-xs">Login</span></a>
</li>
</ul>
</div>
<button class="navbar-toggle visible-sm" data-toggle="offcanvas" type="button"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button>
</div>
</div>
</div>
</header>
<div class="trail-wrapper hidden-print">
<div class="container">
<div class="row">
<div class="col-xs-12">
<div class="breadcrumb dropdown visible-xs">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" id="trail-dropdown-toggle" role="button">View Item <b class="caret"></b></a>
<ul aria-labelledby="trail-dropdown-toggle" class="dropdown-menu" role="menu">
<li role="presentation">
<a href="/xmlui/" role="menuitem"><i aria-hidden="true" class="glyphicon glyphicon-home"></i>  Home</a>
</li>
<li role="presentation">
<a href="/xmlui/handle/1866/3010" role="menuitem">Faculté des arts et des sciences</a>
</li>
<li role="presentation">
<a href="/xmlui/handle/1866/2958" role="menuitem">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle</a>
</li>
<li role="presentation">
<a href="/xmlui/handle/1866/3001" role="menuitem">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle - Thèses et mémoires</a>
</li>
<li class="disabled" role="presentation">
<a href="#" role="menuitem">View Item</a>
</li>
</ul>
</div>
<ul class="breadcrumb hidden-xs">
<li>
<i aria-hidden="true" class="glyphicon glyphicon-home"></i>  <a href="/xmlui/">Home</a>
</li>
<li>
<a href="/xmlui/handle/1866/3010">Faculté des arts et des sciences</a>
</li>
<li>
<a href="/xmlui/handle/1866/2958">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle</a>
</li>
<li>
<a href="/xmlui/handle/1866/3001">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle - Thèses et mémoires</a>
</li>
<li class="active">View Item</li>
</ul>
</div>
</div>
</div>
</div>
<div class="hidden" id="no-js-warning-wrapper">
<div id="no-js-warning">
<div class="notice failure">JavaScript is disabled for your browser. Some features of this site may not work without it.</div>
</div>
</div>
<div class="container" id="main-container">
<div class="row row-offcanvas row-offcanvas-left">
<div class="horizontal-slider clearfix">
<div class="col-xs-6 col-sm-3 sidebar-offcanvas" id="sidebar" role="navigation">
<div class="word-break hidden-print" id="ds-options">
<div class="ds-option-set" id="ds-search-option">
<form action="/xmlui/discover" class="" id="ds-search-form" method="post">
<fieldset>
<div class="input-group">
<input class="ds-text-field form-control" name="query" placeholder="Search" type="text"/><span class="input-group-btn"><button class="ds-button-field btn" title="Go"><span aria-hidden="true" class="glyphicon glyphicon-search"></span></button></span>
</div>
<div class="radio">
<label><input checked="" id="ds-search-form-scope-all" name="scope" type="radio" value=""/>Search Papyrus</label>
</div>
<div class="radio">
<label><input id="ds-search-form-scope-container" name="scope" type="radio" value="1866/3001"/>Search this Collection</label>
</div>
</fieldset>
</form>
</div>
<h2 class="ds-option-set-head h6">My Account</h2>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_account">
<div class="list-group-item ds-option details_Papyrus" id="aspect_eperson_Navigation_item_details_Papyrus">To submit an item or subscribe to email alerts.</div>
<div class="list-group-item ds-option">
<span class="bold"><a href="/xmlui/login">Login</a></span>
</div>
<a class="list-group-item ds-option" href="/xmlui/register">New user?</a>
</div>
<h2 class="ds-option-set-head h6">Browse</h2>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_browse">
<a class="list-group-item active"><span class="h5 list-group-item-heading h5">All of Papyrus</span></a><a class="list-group-item ds-option" href="/xmlui/community-list">Communities and Collections</a><a class="list-group-item ds-option" href="/xmlui/browse?type=title">Titles</a><a class="list-group-item ds-option" href="/xmlui/browse?type=dateissued">Issue Dates</a><a class="list-group-item ds-option" href="/xmlui/browse?type=author">Authors</a><a class="list-group-item ds-option" href="/xmlui/browse?type=advisor">Advisors</a><a class="list-group-item ds-option" href="/xmlui/browse?type=subject">Subjects</a><a class="list-group-item ds-option" href="/xmlui/browse?type=discipline">Disciplines</a><a class="list-group-item ds-option" href="/xmlui/browse?type=affiliation">Affiliation</a><a class="list-group-item ds-option" href="/xmlui/browse?type=titleindex">Titles index</a><a class="list-group-item active"><span class="h5 list-group-item-heading h5">This Collection</span></a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=title">Titles</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=dateissued">Issue Dates</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=author">Authors</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=advisor">Advisors</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=subject">Subjects</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=discipline">Disciplines</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=affiliation">Affiliation</a><a class="list-group-item ds-option" href="/xmlui/handle/1866/3001/browse?type=titleindex">Titles index</a>
</div>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_context"></div>
<div class="list-group" id="aspect_viewArtifacts_Navigation_list_administrative"></div>
<div class="list-group" id="aspect_discovery_Navigation_list_discovery"></div>
<h2 class="ds-option-set-head h6">Statistics</h2>
<div class="list-group" id="aspect_statistics_Navigation_list_statistics">
<a class="list-group-item ds-option" href="/xmlui/handle/1866/22658/statistics">View Usage Statistics</a>
</div>
</div>
</div>
<div class="col-xs-12 col-sm-12 col-md-9 main-content">
<div>
<div class="ds-static-div primary" id="aspect_artifactbrowser_ItemViewer_div_item-view">
<p class="ds-paragraph item-view-toggle item-view-toggle-top">
<span class="item-view-toggle"><a href="/xmlui/handle/1866/22658">Show item record</a></span>
</p>
<!-- External Metadata URL: cocoon://metadata/handle/1866/22658/mets.xml-->
<h2 class="page-header first-page-header">Feature extraction on faces : from landmark localization to depth estimation</h2>
<div class="ds-table-responsive">
<table class="ds-includeSet-table detailtable table table-striped table-hover">
<tr class="ds-table-row odd">
<td class="label-cell">dc.contributor.advisor</td><td class="word-break">Vincent, Pascal</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.contributor.advisor</td><td class="word-break">Pal, Christopher</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.contributor.author</td><td class="word-break">Honari, Sina</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.date.accessioned</td><td class="word-break">2019-11-27T20:09:50Z</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.date.available</td><td class="word-break">NO_RESTRICTION</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.date.available</td><td class="word-break">2019-11-27T20:09:50Z</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.date.issued</td><td class="word-break">2019-06-19</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.date.submitted</td><td class="word-break">2018-12</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.identifier.uri</td><td class="word-break">http://hdl.handle.net/1866/22658</td><td></td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Neural networks</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Deep learning</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Convolutional networks</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Supervised learning</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Unsupervised learning</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Semi-supervised learning</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Coarse-to-fine architectures</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Landmark localization</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Depth estimation</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Face rotation</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Face replacement</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Réseaux neuronaux</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Apprentissage profond</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Réseaux neuronaux de convolution</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Apprentissage supervisé</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Apprentissage non-supervisé</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Apprentissage semi-supervisé</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Architectures grossières à fines</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Localisation de points clés</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Estimation de la profondeur</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject</td><td class="word-break">Rotation de visage</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.subject</td><td class="word-break">Échange de visage</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.subject.other</td><td class="word-break">Applied Sciences - Artificial Intelligence / Sciences appliqués et technologie - Intelligence artificielle (UMI : 0800)</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dc.title</td><td class="word-break">Feature extraction on faces : from landmark localization to depth estimation</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dc.type</td><td class="word-break">Thèse ou mémoire / Thesis or Dissertation</td><td></td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">etd.degree.discipline</td><td class="word-break">Informatique</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">etd.degree.grantor</td><td class="word-break">Université de Montréal</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">etd.degree.level</td><td class="word-break">Doctorat / Doctoral</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">etd.degree.name</td><td class="word-break">Ph. D.</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dcterms.abstract</td><td class="word-break">Le sujet de cette thèse porte sur les algorithmes d'apprentissage qui extraient les caractéristiques importantes des visages. Les caractéristiques d’intérêt principal sont des points clés;  
La localisation en deux dimensions (2D) ou en trois dimensions (3D) de traits importants du visage telles que le centre des yeux, le bout du nez et les coins de la bouche. Les points clés sont utilisés pour résoudre des tâches complexes qui ne peuvent pas être résolues directement ou qui requièrent du guidage pour l’obtention de performances améliorées, telles que la reconnaissance de poses ou de gestes, le suivi ou la vérification du visage. L'application des modèles présentés dans cette thèse concerne les images du visage; cependant, les algorithmes proposés sont plus généraux et peuvent être appliqués aux points clés de d'autres objets, tels que les mains, le corps ou des objets fabriqués par l'homme. Cette thèse est écrite par article et explore différentes techniques pour résoudre plusieurs aspects de la localisation de points clés.

Dans le premier article, nous démêlons l'identité et l'expression d'un visage donné pour apprendre une distribution à priori sur l'ensemble des points clés. Cette distribution à priori est ensuite combinée avec un classifieur discriminant qui apprend une distribution de probabilité indépendante par point clé. Le modèle combiné est capable d'expliquer les différences dans les expressions pour une même représentation d'identité.

Dans le deuxième article, nous proposons une architecture qui vise à conserver les caractéristiques d’images pour effectuer des tâches qui nécessitent une haute précision au niveau des pixels, telles que la localisation de points clés ou la segmentation d’images. L’architecture proposée extrait progressivement les caractéristiques les plus grossières dans les étapes d'encodage pour obtenir des informations plus globales sur l’image. Ensuite, il étend les caractéristiques grossières pour revenir à la résolution de l'image originale en recombinant les caractéristiques du chemin d'encodage. Le modèle, appelé Réseaux de Recombinaison, a obtenu l’état de l’art sur plusieurs jeux de données, tout en accélérant le temps d’apprentissage.

Dans le troisième article, nous visons à améliorer la localisation des points clés lorsque peu d'images comportent des étiquettes sur des points clés. En particulier, nous exploitons une forme plus faible d’étiquettes qui sont plus faciles à acquérir ou plus abondantes tel que l'émotion ou la pose de la tête. Pour ce faire, nous proposons une architecture permettant la rétropropagation du gradient des étiquettes les plus faibles à travers des points clés, ainsi entraînant le réseau de localisation des points clés. Nous proposons également une composante de coût non supervisée qui permet des prédictions de points clés équivariantes en fonction des transformations appliquées à l'image, sans avoir les vraies étiquettes des points clés. Ces techniques ont considérablement amélioré les performances tout en réduisant le pourcentage d'images étiquetées par points clés.

Finalement, dans le dernier article, nous proposons un algorithme d'apprentissage permettant d'estimer la profondeur des points clés sans aucune supervision de la profondeur. Nous y parvenons en faisant correspondre les points clés de deux visages en les transformant l'un vers l'autre. Cette transformation nécessite une estimation de la profondeur sur un visage, ainsi que une transformation affine qui transforme le premier visage au deuxième. Nous démontrons que notre formulation ne nécessite que la profondeur et que les paramètres affines peuvent être estimés avec un solution analytique impliquant les points clés augmentés par profondeur. Même en l'absence de supervision directe de la profondeur, la technique proposée extrait des valeurs de profondeur raisonnables qui diffèrent des vraies valeurs de profondeur par un facteur d'échelle et de décalage. Nous démontrons des applications d'estimation de profondeur pour la tâche de rotation de visage, ainsi que celle d'échange de visage.</td><td>fr</td>
</tr>
<tr class="ds-table-row even">
<td class="label-cell">dcterms.abstract</td><td class="word-break">This thesis focuses on learning algorithms that extract important features from faces. The features of main interest are landmarks; the two dimensional (2D) or three dimensional (3D) locations of important facial features such as eye centers, nose tip, and mouth corners. Landmarks are used to solve complex tasks that cannot be solved directly or require guidance for enhanced performance, such as pose or gesture recognition, tracking, or face verification. The application of the models presented in this thesis is on facial images; however, the algorithms proposed are more general and can be applied to the landmarks of other forms of objects, such as hands, full body or man-made objects. This thesis is written by article and explores different techniques to solve various aspects of landmark localization. 

In the first article, we disentangle identity and expression of a given face to learn a prior distribution over the joint set of landmarks. This prior is then merged with a discriminative classifier that learns an independent probability distribution per landmark. The merged model is capable of explaining differences in expressions for the same identity representation.

In the second article, we propose an architecture that aims at uncovering image features to do tasks that require high pixel-level accuracy, such as landmark localization or image segmentation. 
The proposed architecture gradually extracts coarser features in its encoding steps to get more global information over the image and then it expands the coarse features back to the image resolution by recombining the features of the encoding path. The model, termed Recombinator Networks, obtained state-of-the-art on several datasets, while also speeding up training.

In the third article, we aim at improving landmark localization when only a few images with labelled landmarks are available. In particular, we leverage a weaker form of data labels that are easier to acquire or more abundantly available such as emotion or head pose. To do so, we propose an architecture to backpropagate gradients of the weaker labels through landmarks, effectively training the landmark localization network. We also propose an unsupervised loss component which makes equivariant landmark predictions with respect to transformations applied to the image without having ground truth landmark labels. These techniques improved performance considerably when we have a low percentage of labelled images with landmarks.

Finally, in the last article, we propose a learning algorithm to estimate the depth of the landmarks without any depth supervision. We do so by matching landmarks of two faces through transforming one to another. This transformation requires estimation of depth on one face and an affine transformation that maps the first face to the second one. Our formulation, which only requires depth estimation and affine parameters, can be estimated as a closed form solution of the 2D landmarks and the estimated depth. Even without direct depth supervision, the proposed technique extracts reasonable depth values that differ from the ground truth depth values by a scale and a shift. We demonstrate applications of the estimated depth in face rotation and face replacement tasks.</td><td>fr</td>
</tr>
<tr class="ds-table-row odd">
<td class="label-cell">dcterms.language</td><td class="word-break">eng</td><td>fr</td>
</tr>
</table>
</div>
<span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft_id=http%3A%2F%2Fhdl.handle.net%2F1866%2F22658&amp;rfr_id=info%3Asid%2Fdspace.org%3Arepository&amp;rft.degree=Informatique&amp;rft.degree=Universit%C3%A9+de+Montr%C3%A9al&amp;rft.degree=Doctorat+%2F+Doctoral&amp;rft.degree=Ph.+D."> ﻿ 
        </span>
<h3>Files in this item</h3>
<div class="file-list">
<div class="file-wrapper row">
<div class="col-xs-6 col-sm-3">
<div class="thumbnail-wrapper">
<a class="image-link" href="/xmlui/bitstream/handle/1866/22658/Honari_Sina_2018_these.pdf?sequence=2&amp;isAllowed=y"><img alt="Thumbnail" class="thumbnail-papyrus" src="/xmlui/bitstream/handle/1866/22658/Honari_Sina_2018_these.pdf.jpg?sequence=4&amp;isAllowed=y"/></a>
<div class="imageoverlay">
<img height="49" src="/xmlui/themes/Mirage2/images/mimes/mime_icone_pdf.png" style="height: 49px;" width="44"/></div>
</div>
</div>
<div class="col-xs-6 col-sm-7">
<dl class="file-metadata dl-horizontal">
<dt>Name:</dt>
<dd class="word-break" title="Honari_Sina_2018_these.pdf">
<a href="/xmlui/bitstream/handle/1866/22658/Honari_Sina_2018_these.pdf?sequence=2&amp;isAllowed=y">Honari_Sina_2018_these.pdf</a>
</dd>
<dt>Size:</dt>
<dd class="word-break">12.99Mb</dd>
<dt>Format:</dt>
<dd class="word-break">PDF</dd>
<dt>Description:</dt>
<dd class="word-break" title="Thèse">Thèse</dd>
</dl>
</div>
</div>
</div>
<h3 class="ds-list-head">This item appears in the following Collection(s)</h3>
<ul class="ds-referenceSet-list">
<!-- External Metadata URL: cocoon://metadata/handle/1866/2621/mets.xml-->
<li>
<a href="/xmlui/handle/1866/2621">Thèses et mémoires électroniques de l’Université de Montréal</a> [18270]<br/>
</li>
<!-- External Metadata URL: cocoon://metadata/handle/1866/3001/mets.xml-->
<li>
<a href="/xmlui/handle/1866/3001">Faculté des arts et des sciences – Département d'informatique et de recherche opérationnelle - Thèses et mémoires</a> [795]<br/>
</li>
</ul>
<p class="ds-paragraph item-view-toggle item-view-toggle-bottom">
<span class="item-view-toggle"><a href="/xmlui/handle/1866/22658">Show item record</a></span>
</p>
</div>
</div>
<div class="visible-xs visible-sm">
<footer>
<div class="row">
<hr/>
<div class="col-xs-7 col-sm-8">
<div>
<a href="http://www.dspace.org/" target="_blank">DSpace software</a>
                        [version 5.8 XMLUI],
                        copyright © 2002-2015  <a href="http://www.duraspace.org/" target="_blank">DuraSpace</a>
</div>
<div class="hidden-print">
<a href="/xmlui/contact">Contact Us</a> | <a href="/xmlui/feedback">Send Feedback</a>
</div>
</div>
<div class="col-xs-5 col-sm-4 hidden-print">
<div class="pull-right">
<script type="text/javascript"> 
										      (function(d, t) { 
										        var s = d.createElement(t), options = {'domain':'papyrus.bib.umontreal.ca','style': '16','container':'entrust-net-seal'}; 
										        s.src = 'https://seal.entrust.net/sealv2.js'; 
										        s.async = true; 
										        var scr = d.getElementsByTagName(t)[0], par = scr.parentNode; par.insertBefore(s, scr); 
										        s.onload = s.onreadystatechange = function() { 
										        var rs = this.readyState; if (rs) if (rs != 'complete') if (rs != 'loaded') return; 
										        try{goEntrust(options)} catch (e) {} }; 
										        })(document, 'script'); 
										</script>
<div id="entrust-net-seal">
<a href="https://www.entrust.com/ssl-certificates/">Certificat SSL / SSL Certificate</a>
</div>
</div>
</div>
</div>
<div class="row bib-footer hidden-print">
<div class="col-xs-12 col-sm-12 col-md-4">
<a class="footerEXLlink" href="http://www.bib.umontreal.ca"><img alt="les bibliothèques/UdeM" src="/xmlui/themes/Mirage2/images/propulse-par.png"/></a>
</div>
<div class="hidden-xs col-sm-12 col-md-8">
<nav>
<ul>
<li>
<a href="https://www.urgence.umontreal.ca/">Emergency</a>
</li>
<li>
<a href="http://www.carrieres.umontreal.ca/">Careers</a>
</li>
<li>
<a href="https://outlook.umontreal.ca/">My email</a>
</li>
<li>
<a href="https://studium.umontreal.ca/">StudiUM</a>
</li>
<li>
<a href="http://itunesu.umontreal.ca/">iTunes U</a>
</li>
<li>
<a href="http://www.bib.umontreal.ca/a-propos/nous-ecrire.htm">Contact us</a>
</li>
<li>
<a href="https://www.facebook.com/bibUdeM"><img alt="Facebook" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Facebook.png"/></a>
</li>
<li>
<a href="https://www.youtube.com/user/BibliothequesUdeM"><img alt="YouTube" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-YouTube.png"/></a>
</li>
<li>
<a href="https://twitter.com/bibUdeM"><img alt="Twitter" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Twitter.png"/></a>
</li>
<li>
<a href="https://www.nouvelles.umontreal.ca/frontpage/rss.html"><img alt="University RSS" src="/xmlui/themes/Mirage2/images/bib-umontreal/icone-syndication-14x14.png"/></a>
</li>
</ul>
</nav>
</div>
</div>
<a class="hidden" href="/xmlui/htmlmap"> </a>
<p> </p>
</footer>
</div>
</div>
</div>
</div>
<div class="hidden-xs hidden-sm">
<footer>
<div class="row">
<hr/>
<div class="col-xs-7 col-sm-8">
<div>
<a href="http://www.dspace.org/" target="_blank">DSpace software</a>
                        [version 5.8 XMLUI],
                        copyright © 2002-2015  <a href="http://www.duraspace.org/" target="_blank">DuraSpace</a>
</div>
<div class="hidden-print">
<a href="/xmlui/contact">Contact Us</a> | <a href="/xmlui/feedback">Send Feedback</a>
</div>
</div>
<div class="col-xs-5 col-sm-4 hidden-print">
<div class="pull-right">
<script type="text/javascript"> 
										      (function(d, t) { 
										        var s = d.createElement(t), options = {'domain':'papyrus.bib.umontreal.ca','style': '16','container':'entrust-net-seal'}; 
										        s.src = 'https://seal.entrust.net/sealv2.js'; 
										        s.async = true; 
										        var scr = d.getElementsByTagName(t)[0], par = scr.parentNode; par.insertBefore(s, scr); 
										        s.onload = s.onreadystatechange = function() { 
										        var rs = this.readyState; if (rs) if (rs != 'complete') if (rs != 'loaded') return; 
										        try{goEntrust(options)} catch (e) {} }; 
										        })(document, 'script'); 
										</script>
<div id="entrust-net-seal">
<a href="https://www.entrust.com/ssl-certificates/">Certificat SSL / SSL Certificate</a>
</div>
</div>
</div>
</div>
<div class="row bib-footer hidden-print">
<div class="col-xs-12 col-sm-12 col-md-4">
<a class="footerEXLlink" href="http://www.bib.umontreal.ca"><img alt="les bibliothèques/UdeM" src="/xmlui/themes/Mirage2/images/propulse-par.png"/></a>
</div>
<div class="hidden-xs col-sm-12 col-md-8">
<nav>
<ul>
<li>
<a href="https://www.urgence.umontreal.ca/">Emergency</a>
</li>
<li>
<a href="http://www.carrieres.umontreal.ca/">Careers</a>
</li>
<li>
<a href="https://outlook.umontreal.ca/">My email</a>
</li>
<li>
<a href="https://studium.umontreal.ca/">StudiUM</a>
</li>
<li>
<a href="http://itunesu.umontreal.ca/">iTunes U</a>
</li>
<li>
<a href="http://www.bib.umontreal.ca/a-propos/nous-ecrire.htm">Contact us</a>
</li>
<li>
<a href="https://www.facebook.com/bibUdeM"><img alt="Facebook" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Facebook.png"/></a>
</li>
<li>
<a href="https://www.youtube.com/user/BibliothequesUdeM"><img alt="YouTube" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-YouTube.png"/></a>
</li>
<li>
<a href="https://twitter.com/bibUdeM"><img alt="Twitter" src="/xmlui/themes/Mirage2/images/bib-umontreal/logo-Twitter.png"/></a>
</li>
<li>
<a href="https://www.nouvelles.umontreal.ca/frontpage/rss.html"><img alt="University RSS" src="/xmlui/themes/Mirage2/images/bib-umontreal/icone-syndication-14x14.png"/></a>
</li>
</ul>
</nav>
</div>
</div>
<a class="hidden" href="/xmlui/htmlmap"> </a>
<p> </p>
</footer>
</div>
</div>
<script src="https://www.google.com/jsapi"> </script><script>if(!window.DSpace){window.DSpace={};}window.DSpace.context_path='/xmlui';window.DSpace.theme_path='/xmlui/themes/Mirage2/';</script><script src="/xmlui/themes/Mirage2//xmlui/themes/Mirage2//xmlui/themes/Mirage2/scripts/theme.jss/theme.jsscripts/theme.jseme.js"> </script><script>
                  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

                  ga('create', 'UA-305727-1', 'papyrus.bib.umontreal.ca');
                  ga('send', 'pageview');
           </script><script>         
             var details_Papyrus =  $("#aspect_viewArtifacts_Navigation_list_account .details_Papyrus").text();
             $("#aspect_viewArtifacts_Navigation_list_account a[href='/xmlui/login']").attr("title",details_Papyrus);
        </script>
</body></html>
