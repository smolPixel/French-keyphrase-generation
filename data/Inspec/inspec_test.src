{"title": "Twenty years of the literature on acquiring out-of-print materials", "abstract": "This article reviews the last two-and-a-half decades of literature on acquiring out-of-print materials to assess recurring issues and identify changing practices. The out-of-print literature is uniform in its assertion that libraries need to acquire o.p. materials to replace worn or damaged copies, to replace missing copies, to duplicate copies of heavily used materials, to fill gaps in collections, to strengthen weak collections, to continue to develop strong collections, and to provide materials for new courses, new programs, and even entire new libraries", "id": "0", "src": "twenty years of the literature on acquiring out of print materials . this article reviews the last two and a half decades of literature on acquiring out of print materials to assess recurring issues and identify changing practices . the out of print literature is uniform in its assertion that libraries need to acquire o . p . materials to replace worn or damaged copies , to replace missing copies , to duplicate copies of heavily used materials , to fill gaps in collections , to strengthen weak collections , to continue to develop strong collections , and to provide materials for new courses , new programs , and even entire new libraries"}
{"title": "A new method of systemological analysis coordinated with the procedure of", "abstract": "object-oriented design. II For pt.I. see Vestn. KhGPU, no.81, p.15-18 (2000). The paper presents the results of development of an object-oriented systemological method used to design complex systems. A formal system representation, as well as an axiomatics of the calculus of systems as functional flow-type objects based on a Node-Function-Object class hierarchy are proposed. A formalized NFO/UFO analysis algorithm and CASE tools used to support it are considered", "id": "1", "src": "a new method of systemological analysis coordinated with the procedure of . object oriented design . ii for pt . i . see vestn . khgpu , no . <digit> , p . <digit> <digit> ( <digit> ) . the paper presents the results of development of an object oriented systemological method used to design complex systems . a formal system representation , as well as an axiomatics of the calculus of systems as functional flow type objects based on a node function object class hierarchy are proposed . a formalized nfo ufo analysis algorithm and case tools used to support it are considered"}
{"title": "Mathematical fundamentals of constructing fuzzy Bayesian inference techniques", "abstract": "Problems and an associated technique for developing a Bayesian approach to decision-making in the case of fuzzy data are presented. The concept of fuzzy and pseudofuzzy quantities is introduced and main operations with pseudofuzzy quantities are considered. The basic relationships and the principal concepts of the Bayesian decision procedure based on the modus-ponens rule are proposed. Some problems concerned with the practical realization of the fuzzy Bayesian method are considered", "id": "2", "src": "mathematical fundamentals of constructing fuzzy bayesian inference techniques . problems and an associated technique for developing a bayesian approach to decision making in the case of fuzzy data are presented . the concept of fuzzy and pseudofuzzy quantities is introduced and main operations with pseudofuzzy quantities are considered . the basic relationships and the principal concepts of the bayesian decision procedure based on the modus ponens rule are proposed . some problems concerned with the practical realization of the fuzzy bayesian method are considered"}
{"title": "Solution of the safe problem on (0,1)-matrices", "abstract": "A safe problem with mn locks is studied. It is reduced to a system of linear equations in the modulo 2 residue class. There are three possible variants defined by the numbers m and n evenness, with only one of them having a solution. In two other cases, correction of the initial state of the safe insuring a solution is proposed", "id": "3", "src": "solution of the safe problem on ( 0 , 1 ) matrices . a safe problem with mn locks is studied . it is reduced to a system of linear equations in the modulo 2 residue class . there are three possible variants defined by the numbers m and n evenness , with only one of them having a solution . in two other cases , correction of the initial state of the safe insuring a solution is proposed"}
{"title": "Accelerated simulation of the steady-state availability of non-Markovian", "abstract": "systems A general accelerated simulation method for evaluation of the steady-state availability of non-Markovian systems is proposed. It is applied to the investigation of a class of systems with repair. Numerical examples are given", "id": "4", "src": "accelerated simulation of the steady state availability of non markovian . systems a general accelerated simulation method for evaluation of the steady state availability of non markovian systems is proposed . it is applied to the investigation of a class of systems with repair . numerical examples are given"}
{"title": "Computational finite-element schemes for optimal control of an elliptic system", "abstract": "with conjugation conditions New optimal control problems are considered for distributed systems described by elliptic equations with conjugate conditions and a quadratic minimized function. Highly accurate computational discretization schemes are constructed for the case where a feasible control set u/sub delta / coincides with the full Hilbert space u of controls", "id": "5", "src": "computational finite element schemes for optimal control of an elliptic system . with conjugation conditions new optimal control problems are considered for distributed systems described by elliptic equations with conjugate conditions and a quadratic minimized function . highly accurate computational discretization schemes are constructed for the case where a feasible control set u sub delta coincides with the full hilbert space u of controls"}
{"title": "Identification of states of complex systems with estimation of admissible", "abstract": "measurement errors on the basis of fuzzy information The problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. Some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system", "id": "6", "src": "identification of states of complex systems with estimation of admissible . measurement errors on the basis of fuzzy information the problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered . some estimates of a maximally admissible degree of measurement error are obtained that make it possible , using the apparatus of fuzzy set theory , to correctly identify the current state of a system"}
{"title": "A new approach to the decomposition of Boolean functions by the method of", "abstract": "q-partitions.II. Repeated decomposition For pt.I. see Upr. Sist. Mash., no. 6, p. 29-42 (1999). A new approach to the decomposition of Boolean,functions that depend on n variables and are represented in various forms is considered. The approach is based on the method of q-partitioning of minterms and on the introduced concept of a decomposition clone. The theorem on simple disjunctive decomposition of full and partial functions is formulated. The approach proposed is illustrated by examples", "id": "7", "src": "a new approach to the decomposition of boolean functions by the method of . q partitions . ii . repeated decomposition for pt . i . see upr . sist . mash . , no . 6 , p . <digit> <digit> ( <digit> ) . a new approach to the decomposition of boolean , functions that depend on n variables and are represented in various forms is considered . the approach is based on the method of q partitioning of minterms and on the introduced concept of a decomposition clone . the theorem on simple disjunctive decomposition of full and partial functions is formulated . the approach proposed is illustrated by examples"}
{"title": "Nonlinear extrapolation algorithm for realization of a scalar random process", "abstract": "A method of construction of a nonlinear extrapolation algorithm is proposed. This method makes it possible to take into account any nonlinear random dependences that exist in an investigated process and are described by mixed central moment functions. The method is based on the V. S. Pugachev canonical decomposition apparatus. As an example, the problem of nonlinear extrapolation is solved for a moment function of third order", "id": "8", "src": "nonlinear extrapolation algorithm for realization of a scalar random process . a method of construction of a nonlinear extrapolation algorithm is proposed . this method makes it possible to take into account any nonlinear random dependences that exist in an investigated process and are described by mixed central moment functions . the method is based on the v . s . pugachev canonical decomposition apparatus . as an example , the problem of nonlinear extrapolation is solved for a moment function of third order"}
{"title": "A method for solution of systems of linear algebraic equations with", "abstract": "m-dimensional lambda -matrices A system of linear algebraic equations with m-dimensional lambda -matrices is considered. The proposed method of searching for the solution of this system lies in reducing it to a numerical system of a special kind", "id": "9", "src": "a method for solution of systems of linear algebraic equations with . m dimensional lambda matrices a system of linear algebraic equations with m dimensional lambda matrices is considered . the proposed method of searching for the solution of this system lies in reducing it to a numerical system of a special kind"}
{"title": "Compatibility of systems of linear constraints over the set of natural numbers", "abstract": "Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types", "id": "10", "src": "compatibility of systems of linear constraints over the set of natural numbers . criteria of compatibility of a system of linear diophantine equations , strict inequations , and nonstrict inequations are considered . upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given . these criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types"}
{"title": "Books on demand: just-in-time acquisitions", "abstract": "The Purdue University Libraries Interlibrary Loan unit proposed a pilot project to purchase patrons' loan requests from Amazon. com, lend them to the patrons, and then add the titles to the collection. Staff analyzed previous monograph loans, developed ordering criteria, implemented the proposal as a pilot project for six months, and evaluated the resulting patron comments, statistics, and staff perceptions. As a result of enthusiastic patron comments and a review of the project statistics, the program was extended", "id": "11", "src": "books on demand just in time acquisitions . the purdue university libraries interlibrary loan unit proposed a pilot project to purchase patrons ' loan requests from amazon . com , lend them to the patrons , and then add the titles to the collection . staff analyzed previous monograph loans , developed ordering criteria , implemented the proposal as a pilot project for six months , and evaluated the resulting patron comments , statistics , and staff perceptions . as a result of enthusiastic patron comments and a review of the project statistics , the program was extended"}
{"title": "New lower bounds of the size of error-correcting codes for the Z-channel", "abstract": "Optimization problems on graphs are formulated to obtain new lower bounds of the size of error-correcting codes for the Z-channel", "id": "12", "src": "new lower bounds of the size of error correcting codes for the z channel . optimization problems on graphs are formulated to obtain new lower bounds of the size of error correcting codes for the z channel"}
{"title": "Descriptological foundations of programming", "abstract": "Descriptological foundations of programming are constructed. An explication of the concept of a descriptive process is given. The operations of introduction and elimination of abstraction at the level of processes are refined. An intensional concept of a bipolar function is introduced. An explication of the concept of introduction and extraction of abstraction at the bipole level is given. On this basis, a complete set of descriptological operations is constructed", "id": "13", "src": "descriptological foundations of programming . descriptological foundations of programming are constructed . an explication of the concept of a descriptive process is given . the operations of introduction and elimination of abstraction at the level of processes are refined . an intensional concept of a bipolar function is introduced . an explication of the concept of introduction and extraction of abstraction at the bipole level is given . on this basis , a complete set of descriptological operations is constructed"}
{"title": "Precoded OFDM with adaptive vector channel allocation for scalable video", "abstract": "transmission over frequency-selective fading channels Orthogonal frequency division multiplexing (OFDM) has been applied in broadband wireline and wireless systems for high data rate transmission where severe intersymbol interference (ISI) always occurs. The conventional OFDM system provides advantages through conversion of an ISI channel into ISI-free subchannels at multiple frequency bands. However, it may suffer from channel spectral nulls and heavy data rate overhead due to cyclic prefix insertion. Previously, a new OFDM framework, the precoded OFDM, has been proposed to mitigate the above two problems through precoding and conversion of an ISI channel into ISI-free vector channels. In this paper, we consider the application of the precoded OFDM system to efficient scalable video transmission. We propose to enhance the precoded OFDM system with adaptive vector channel allocation to provide stronger protection against errors to more important layers in the layered bit stream structure of scalable video. The more critical layers, or equivalently, the lower layers, are allocated vector channels of higher transmission quality. The channel quality is characterized by Frobenius norm metrics; based on channel estimation at the receiver. The channel allocation information is fed back periodically to the transmitter through a control channel. Simulation results have demonstrated the robustness of the proposed scheme to noise and fading inherent in wireless channels", "id": "14", "src": "precoded ofdm with adaptive vector channel allocation for scalable video . transmission over frequency selective fading channels orthogonal frequency division multiplexing ( ofdm ) has been applied in broadband wireline and wireless systems for high data rate transmission where severe intersymbol interference ( isi ) always occurs . the conventional ofdm system provides advantages through conversion of an isi channel into isi free subchannels at multiple frequency bands . however , it may suffer from channel spectral nulls and heavy data rate overhead due to cyclic prefix insertion . previously , a new ofdm framework , the precoded ofdm , has been proposed to mitigate the above two problems through precoding and conversion of an isi channel into isi free vector channels . in this paper , we consider the application of the precoded ofdm system to efficient scalable video transmission . we propose to enhance the precoded ofdm system with adaptive vector channel allocation to provide stronger protection against errors to more important layers in the layered bit stream structure of scalable video . the more critical layers , or equivalently , the lower layers , are allocated vector channels of higher transmission quality . the channel quality is characterized by frobenius norm metrics based on channel estimation at the receiver . the channel allocation information is fed back periodically to the transmitter through a control channel . simulation results have demonstrated the robustness of the proposed scheme to noise and fading inherent in wireless channels"}
{"title": "I-WAP: an intelligent WAP site management system", "abstract": "The popularity regarding wireless communications is such that more and more WAP sites have been developed with wireless markup language (WML). Meanwhile, to translate hypertext markup language (HTML) pages into proper WML ones becomes imperative since it is difficult for WAP users to read most contents designed for PC users via their mobile phone screens. However, for those sites that have been maintained with hypertext markup language (HTML), considerable time and manpower costs will be incurred to rebuild them with WML. In this paper, we propose an intelligent WAP site management system to cope with these problems. With the help of the intelligent management system, the original contents of HTML Web sites can be automatically translated to proper WAP content in an efficient way. As a consequence, the costs associated with maintaining WAP sites could be significantly reduced. The management system also allows the system manager to define the relevance of numerals and keywords for removing unimportant or meaningless contents. The original contents will be reduced and reorganized to fit the size of mobile phone screens, thus reducing the communication cost and enhancing readability. Numerical results gained through various experiments have evinced the effective performance of the WAP management system", "id": "15", "src": "i wap an intelligent wap site management system . the popularity regarding wireless communications is such that more and more wap sites have been developed with wireless markup language ( wml ) . meanwhile , to translate hypertext markup language ( html ) pages into proper wml ones becomes imperative since it is difficult for wap users to read most contents designed for pc users via their mobile phone screens . however , for those sites that have been maintained with hypertext markup language ( html ) , considerable time and manpower costs will be incurred to rebuild them with wml . in this paper , we propose an intelligent wap site management system to cope with these problems . with the help of the intelligent management system , the original contents of html web sites can be automatically translated to proper wap content in an efficient way . as a consequence , the costs associated with maintaining wap sites could be significantly reduced . the management system also allows the system manager to define the relevance of numerals and keywords for removing unimportant or meaningless contents . the original contents will be reduced and reorganized to fit the size of mobile phone screens , thus reducing the communication cost and enhancing readability . numerical results gained through various experiments have evinced the effective performance of the wap management system"}
{"title": "A framework of electronic tendering for government procurement: a lesson", "abstract": "learned in Taiwan To render government procurement efficient, transparent, nondiscriminating, and accountable, an electronic government procurement system is required. Accordingly, Taiwan government procurement law (TGPL) states that suppliers may employ electronic devices to forward a tender. This investigation demonstrates how the electronic government procurement system functions and reengineers internal procurement processes, which in turn benefits both government bodies and vendors. The system features explored herein include posting/receiving bids via the Internet, vendor registration, certificate authorization, contract development tools, bid/request for proposal (RFP) development, online bidding, and online payment, all of which can be integrated easily within most existing information infrastructures", "id": "16", "src": "a framework of electronic tendering for government procurement a lesson . learned in taiwan to render government procurement efficient , transparent , nondiscriminating , and accountable , an electronic government procurement system is required . accordingly , taiwan government procurement law ( tgpl ) states that suppliers may employ electronic devices to forward a tender . this investigation demonstrates how the electronic government procurement system functions and reengineers internal procurement processes , which in turn benefits both government bodies and vendors . the system features explored herein include posting receiving bids via the internet , vendor registration , certificate authorization , contract development tools , bid request for proposal ( rfp ) development , online bidding , and online payment , all of which can be integrated easily within most existing information infrastructures"}
{"title": "The development of a mobile manipulator imaging system for bridge crack", "abstract": "inspection A mobile manipulator imaging system is developed for the automation of bridge crack inspection. During bridge safety inspections, an eyesight inspection is made for preliminary evaluation and screening before a more precise inspection. The inspection for cracks is an important part of the preliminary evaluation. Currently, the inspectors must stand on the platform of a bridge inspection vehicle or a temporarily erected scaffolding to examine the underside of a bridge. However, such a procedure is risky. To help automate the bridge crack inspection process, we installed two CCD cameras and a four-axis manipulator system on a mobile vehicle. The parallel cameras are used to detect cracks. The manipulator system is equipped with binocular charge coupled devices (CCD) for examining structures that may not be accessible to the eye. The system also reduces the danger of accidents to the human inspectors. The manipulator system consists of four arms. Balance weights are placed at the ends of arms 2 and 4, respectively, to maintain the center of gravity during operation. Mechanically, arms 2 and 4 can revolve smoothly. Experiments indicated that the system could be useful for bridge crack inspections", "id": "17", "src": "the development of a mobile manipulator imaging system for bridge crack . inspection a mobile manipulator imaging system is developed for the automation of bridge crack inspection . during bridge safety inspections , an eyesight inspection is made for preliminary evaluation and screening before a more precise inspection . the inspection for cracks is an important part of the preliminary evaluation . currently , the inspectors must stand on the platform of a bridge inspection vehicle or a temporarily erected scaffolding to examine the underside of a bridge . however , such a procedure is risky . to help automate the bridge crack inspection process , we installed two ccd cameras and a four axis manipulator system on a mobile vehicle . the parallel cameras are used to detect cracks . the manipulator system is equipped with binocular charge coupled devices ( ccd ) for examining structures that may not be accessible to the eye . the system also reduces the danger of accidents to the human inspectors . the manipulator system consists of four arms . balance weights are placed at the ends of arms 2 and 4 , respectively , to maintain the center of gravity during operation . mechanically , arms 2 and 4 can revolve smoothly . experiments indicated that the system could be useful for bridge crack inspections"}
{"title": "Integrating building management system and facilities management on the", "abstract": "Internet Recently, it is of great interest to adopt the Internet/intranet to develop building management systems (BMS) and facilities management systems (FMS). This paper addresses two technical issues: the Web-based access (including database integration) and the integration of BMS and FMS. These should be addressed for accessing BMS remotely via the Internet, integrating control networks using the Internet protocols and infrastructures, and using Internet/intranet for building facilities management. An experimental Internet-enabled system that integrates building and facilities management systems has been developed and tested. This system integrated open control networks with the Internet and is developed utilizing the embedded Web server, the PC Web server and the Distributed Component Object Model (DCOM) software development technology on the platform of an open control network. Three strategies for interconnecting BMS local networks via Internet/intranet are presented and analyzed", "id": "18", "src": "integrating building management system and facilities management on the . internet recently , it is of great interest to adopt the internet intranet to develop building management systems ( bms ) and facilities management systems ( fms ) . this paper addresses two technical issues the web based access ( including database integration ) and the integration of bms and fms . these should be addressed for accessing bms remotely via the internet , integrating control networks using the internet protocols and infrastructures , and using internet intranet for building facilities management . an experimental internet enabled system that integrates building and facilities management systems has been developed and tested . this system integrated open control networks with the internet and is developed utilizing the embedded web server , the pc web server and the distributed component object model ( dcom ) software development technology on the platform of an open control network . three strategies for interconnecting bms local networks via internet intranet are presented and analyzed"}
{"title": "Modelling user acceptance of building management systems", "abstract": "This study examines user acceptance of building management systems (BMS) using a questionnaire survey. These systems are crucial for optimising building performance and yet it has been widely reported that users are not making full use of their systems' facilities. Established models of technology acceptance have been employed in this research, and the positive influence of user perceptions of ease of use and compatibility has been demonstrated. Previous research has indicated differing levels of importance of perceived ease of use relative to other factors. Here, perceived ease of use is shown generally to be more important, though the balance between this and compatibility is moderated by the user perceptions of voluntariness", "id": "19", "src": "modelling user acceptance of building management systems . this study examines user acceptance of building management systems ( bms ) using a questionnaire survey . these systems are crucial for optimising building performance and yet it has been widely reported that users are not making full use of their systems ' facilities . established models of technology acceptance have been employed in this research , and the positive influence of user perceptions of ease of use and compatibility has been demonstrated . previous research has indicated differing levels of importance of perceived ease of use relative to other factors . here , perceived ease of use is shown generally to be more important , though the balance between this and compatibility is moderated by the user perceptions of voluntariness"}
{"title": "Estimating populations for collective dose calculations", "abstract": "The collective dose provides an estimate of the effects of facility operations on the public based on an estimate of the population in the area. Geographic information system software, electronic population data resources, and a personal computer were used to develop estimates of population within 80 km radii of two sites", "id": "20", "src": "estimating populations for collective dose calculations . the collective dose provides an estimate of the effects of facility operations on the public based on an estimate of the population in the area . geographic information system software , electronic population data resources , and a personal computer were used to develop estimates of population within <digit> km radii of two sites"}
{"title": "A new graphical user interface for fast construction of computation phantoms", "abstract": "and MCNP calculations: application to calibration of in vivo measurement systems Reports on a new utility for development of computational phantoms for Monte Carlo calculations and data analysis for in vivo measurements of radionuclides deposited in tissues. The individual properties of each worker can be acquired for a rather precise geometric representation of his (her) anatomy, which is particularly important for low energy gamma ray emitting sources such as thorium, uranium, plutonium and other actinides. The software enables automatic creation of an MCNP input data file based on scanning data. The utility includes segmentation of images obtained with either computed tomography or magnetic resonance imaging by distinguishing tissues according to their signal (brightness) and specification of the source and detector. In addition, a coupling of individual voxels within the tissue is used to reduce the memory demand and to increase the calculational speed. The utility was tested for low energy emitters in plastic and biological tissues as well as for computed tomography and magnetic resonance imaging scanning information", "id": "21", "src": "a new graphical user interface for fast construction of computation phantoms . and mcnp calculations application to calibration of in vivo measurement systems reports on a new utility for development of computational phantoms for monte carlo calculations and data analysis for in vivo measurements of radionuclides deposited in tissues . the individual properties of each worker can be acquired for a rather precise geometric representation of his ( her ) anatomy , which is particularly important for low energy gamma ray emitting sources such as thorium , uranium , plutonium and other actinides . the software enables automatic creation of an mcnp input data file based on scanning data . the utility includes segmentation of images obtained with either computed tomography or magnetic resonance imaging by distinguishing tissues according to their signal ( brightness ) and specification of the source and detector . in addition , a coupling of individual voxels within the tissue is used to reduce the memory demand and to increase the calculational speed . the utility was tested for low energy emitters in plastic and biological tissues as well as for computed tomography and magnetic resonance imaging scanning information"}
{"title": "The acquisition of out-of-print music", "abstract": "Non-specialist librarians are alerted to factors important in the successful acquisition of out-of-print music, both scholarly editions and performance editions. The appropriate technical music vocabulary, the music publishing industry, specialized publishers and vendors, and methods of acquisition of out-of-print printed music are introduced, and the need for familiarity with them is emphasized", "id": "22", "src": "the acquisition of out of print music . non specialist librarians are alerted to factors important in the successful acquisition of out of print music , both scholarly editions and performance editions . the appropriate technical music vocabulary , the music publishing industry , specialized publishers and vendors , and methods of acquisition of out of print printed music are introduced , and the need for familiarity with them is emphasized"}
{"title": "General solution of a density functionally gradient piezoelectric cantilever", "abstract": "and its applications We have used the plane strain theory of transversely isotropic bodies to study a piezoelectric cantilever. In order to find the general solution of a density functionally gradient piezoelectric cantilever, we have used the inverse method (i.e. the Airy stress function method). We have obtained the stress and induction functions in the form of polynomials as well as the general solution of the beam. Based on this general solution, we have deduced the solutions of the cantilever under different loading conditions. Furthermore, as applications of this general solution in engineering, we have studied the tip deflection and blocking force of a piezoelectric cantilever actuator. Finally, we have addressed a method to determine the density distribution profile for a given piezoelectric material", "id": "23", "src": "general solution of a density functionally gradient piezoelectric cantilever . and its applications we have used the plane strain theory of transversely isotropic bodies to study a piezoelectric cantilever . in order to find the general solution of a density functionally gradient piezoelectric cantilever , we have used the inverse method ( i . e . the airy stress function method ) . we have obtained the stress and induction functions in the form of polynomials as well as the general solution of the beam . based on this general solution , we have deduced the solutions of the cantilever under different loading conditions . furthermore , as applications of this general solution in engineering , we have studied the tip deflection and blocking force of a piezoelectric cantilever actuator . finally , we have addressed a method to determine the density distribution profile for a given piezoelectric material"}
{"title": "Recording quantum properties of light in a long-lived atomic spin state:", "abstract": "towards quantum memory We report an experiment on mapping a quantum state of light onto the ground state spin of an ensemble of Cs atoms with the lifetime of 2 ms. Recording of one of the two quadrature phase operators of light is demonstrated with vacuum and squeezed states of light. The sensitivity of the mapping procedure at the level of approximately 1 photon/sec per Hz is shown. The results pave the road towards complete (storing both quadrature phase observables) quantum memory for Gaussian states of light. The experiment also sheds new light on fundamental limits of sensitivity of the magneto-optical resonance method", "id": "24", "src": "recording quantum properties of light in a long lived atomic spin state . towards quantum memory we report an experiment on mapping a quantum state of light onto the ground state spin of an ensemble of cs atoms with the lifetime of 2 ms . recording of one of the two quadrature phase operators of light is demonstrated with vacuum and squeezed states of light . the sensitivity of the mapping procedure at the level of approximately 1 photon sec per hz is shown . the results pave the road towards complete ( storing both quadrature phase observables ) quantum memory for gaussian states of light . the experiment also sheds new light on fundamental limits of sensitivity of the magneto optical resonance method"}
{"title": "Comprehensive encoding and decoupling solution to problems of decoherence and", "abstract": "design in solid-state quantum computing Proposals for scalable quantum computing devices suffer not only from decoherence due to the interaction with their environment, but also from severe engineering constraints. Here we introduce a practical solution to these major concerns, addressing solid-state proposals in particular. Decoherence is first reduced by encoding a logical qubit into two qubits, then completely eliminated by an efficient set of decoupling pulse sequences. The same encoding removes the need for single-qubit operations, which pose a difficult design constraint. We further show how the dominant decoherence processes can be identified empirically, in order to optimize the decoupling pulses", "id": "25", "src": "comprehensive encoding and decoupling solution to problems of decoherence and . design in solid state quantum computing proposals for scalable quantum computing devices suffer not only from decoherence due to the interaction with their environment , but also from severe engineering constraints . here we introduce a practical solution to these major concerns , addressing solid state proposals in particular . decoherence is first reduced by encoding a logical qubit into two qubits , then completely eliminated by an efficient set of decoupling pulse sequences . the same encoding removes the need for single qubit operations , which pose a difficult design constraint . we further show how the dominant decoherence processes can be identified empirically , in order to optimize the decoupling pulses"}
{"title": "Social percolation and the influence of mass media", "abstract": "In the marketing model of Solomon and Weisbuch, people buy a product only if their neighbours tell them of its quality, and if this quality is higher than their own quality expectations. Now we introduce additional information from the mass media, which is analogous to the ghost field in percolation theory. The mass media shift the percolative phase transition observed in the model, and decrease the time after which the stationary state is reached", "id": "26", "src": "social percolation and the influence of mass media . in the marketing model of solomon and weisbuch , people buy a product only if their neighbours tell them of its quality , and if this quality is higher than their own quality expectations . now we introduce additional information from the mass media , which is analogous to the ghost field in percolation theory . the mass media shift the percolative phase transition observed in the model , and decrease the time after which the stationary state is reached"}
{"title": "Estimating long-range dependence: finite sample properties and confidence", "abstract": "intervals A major issue in financial economics is the behavior of asset returns over long horizons. Various estimators of long-range dependence have been proposed. Even though some have known asymptotic properties, it is important to test their accuracy by using simulated series of different lengths. We test R/S analysis, detrended fluctuation analysis and periodogram regression methods on samples drawn from Gaussian white noise. The DFA statistics turns out to be the unanimous winner. Unfortunately, no asymptotic distribution theory has been derived for this statistics so far. We were able, however, to construct empirical (i.e. approximate) confidence intervals for all three methods. The obtained values differ largely from heuristic values proposed by some authors for the R/S statistics and are very close to asymptotic values for the periodogram regression method", "id": "27", "src": "estimating long range dependence finite sample properties and confidence . intervals a major issue in financial economics is the behavior of asset returns over long horizons . various estimators of long range dependence have been proposed . even though some have known asymptotic properties , it is important to test their accuracy by using simulated series of different lengths . we test r s analysis , detrended fluctuation analysis and periodogram regression methods on samples drawn from gaussian white noise . the dfa statistics turns out to be the unanimous winner . unfortunately , no asymptotic distribution theory has been derived for this statistics so far . we were able , however , to construct empirical ( i . e . approximate ) confidence intervals for all three methods . the obtained values differ largely from heuristic values proposed by some authors for the r s statistics and are very close to asymptotic values for the periodogram regression method"}
{"title": "Simulation of evacuation processes using a bionics-inspired cellular automaton", "abstract": "model for pedestrian dynamics We present simulations of evacuation processes using a recently introduced cellular automaton model for pedestrian dynamics. This model applies a bionics approach to describe the interaction between the pedestrians using ideas from chemotaxis. Here we study a rather simple situation, namely the evacuation from a large room with one or two doors. It is shown that the variation of the model parameters allows to describe different types of behaviour, from regular to panic. We find a non-monotonic dependence of the evacuation times on the coupling constants. These times depend on the strength of the herding behaviour, with minimal evacuation times for some intermediate values of the couplings, i.e., a proper combination of herding and use of knowledge about the shortest way to the exit", "id": "28", "src": "simulation of evacuation processes using a bionics inspired cellular automaton . model for pedestrian dynamics we present simulations of evacuation processes using a recently introduced cellular automaton model for pedestrian dynamics . this model applies a bionics approach to describe the interaction between the pedestrians using ideas from chemotaxis . here we study a rather simple situation , namely the evacuation from a large room with one or two doors . it is shown that the variation of the model parameters allows to describe different types of behaviour , from regular to panic . we find a non monotonic dependence of the evacuation times on the coupling constants . these times depend on the strength of the herding behaviour , with minimal evacuation times for some intermediate values of the couplings , i . e . , a proper combination of herding and use of knowledge about the shortest way to the exit"}
{"title": "Dynamical transition to periodic motions of a recurrent bus induced by nonstops", "abstract": "We study the dynamical behavior of a recurrent bus on a circular route with many bus stops when the recurrent bus passes some bus stops without stopping. The recurrent time (one period) is described in terms of a nonlinear map. It is shown that the recurrent bus exhibits the complex periodic behaviors. The dynamical transitions to periodic motions occur by increasing nonstops. The periodic motions depend on the property of an attractor of the nonlinear map. The period n of the attractor varies sensitively with the number of nonstops", "id": "29", "src": "dynamical transition to periodic motions of a recurrent bus induced by nonstops . we study the dynamical behavior of a recurrent bus on a circular route with many bus stops when the recurrent bus passes some bus stops without stopping . the recurrent time ( one period ) is described in terms of a nonlinear map . it is shown that the recurrent bus exhibits the complex periodic behaviors . the dynamical transitions to periodic motions occur by increasing nonstops . the periodic motions depend on the property of an attractor of the nonlinear map . the period n of the attractor varies sensitively with the number of nonstops"}
{"title": "The two populations' cellular automata model with predation based on the Penna", "abstract": "model In Penna's (1995) single-species asexual bit-string model of biological ageing, the Verhulst factor has too strong a restraining effect on the development of the population. Danuta Makowiec gave an improved model based on the lattice, where the restraining factor of the four neighbours take the place of the Verhulst factor. Here, we discuss the two populations' Penna model with predation on the planar lattice of two dimensions. A cellular automata model containing movable wolves and sheep has been built. The results show that both the quantity of the wolves and the sheep fluctuate in accordance with the law that one quantity increases while the other one decreases", "id": "30", "src": "the two populations ' cellular automata model with predation based on the penna . model in penna ' s ( <digit> ) single species asexual bit string model of biological ageing , the verhulst factor has too strong a restraining effect on the development of the population . danuta makowiec gave an improved model based on the lattice , where the restraining factor of the four neighbours take the place of the verhulst factor . here , we discuss the two populations ' penna model with predation on the planar lattice of two dimensions . a cellular automata model containing movable wolves and sheep has been built . the results show that both the quantity of the wolves and the sheep fluctuate in accordance with the law that one quantity increases while the other one decreases"}
{"title": "Option pricing from path integral for non-Gaussian fluctuations. Natural", "abstract": "martingale and application to truncated Levy distributions Within a path integral formalism for non-Gaussian price fluctuations, we set up a simple stochastic calculus and derive a natural martingale for option pricing from the wealth balance of options, stocks, and bonds. The resulting formula is evaluated for truncated Levy distributions", "id": "31", "src": "option pricing from path integral for non gaussian fluctuations . natural . martingale and application to truncated levy distributions within a path integral formalism for non gaussian price fluctuations , we set up a simple stochastic calculus and derive a natural martingale for option pricing from the wealth balance of options , stocks , and bonds . the resulting formula is evaluated for truncated levy distributions"}
{"title": "Quantum market games", "abstract": "We propose a quantum-like description of markets and economics. The approach has roots in the recently developed quantum game theory", "id": "32", "src": "quantum market games . we propose a quantum like description of markets and economics . the approach has roots in the recently developed quantum game theory"}
{"title": "On the emergence of rules in neural networks", "abstract": "A simple associationist neural network learns to factor abstract rules (i.e., grammars) from sequences of arbitrary input symbols by inventing abstract representations that accommodate unseen symbol sets as well as unseen but similar grammars. The neural network is shown to have the ability to transfer grammatical knowledge to both new symbol vocabularies and new grammars. Analysis of the state-space shows that the network learns generalized abstract structures of the input and is not simply memorizing the input strings. These representations are context sensitive, hierarchical, and based on the state variable of the finite-state machines that the neural network has learned. Generalization to new symbol sets or grammars arises from the spatial nature of the internal representations used by the network, allowing new symbol sets to be encoded close to symbol sets that have already been learned in the hidden unit space of the network. The results are counter to the arguments that learning algorithms based on weight adaptation after each exemplar presentation (such as the long term potentiation found in the mammalian nervous system) cannot in principle extract symbolic knowledge from positive examples as prescribed by prevailing human linguistic theory and evolutionary psychology", "id": "33", "src": "on the emergence of rules in neural networks . a simple associationist neural network learns to factor abstract rules ( i . e . , grammars ) from sequences of arbitrary input symbols by inventing abstract representations that accommodate unseen symbol sets as well as unseen but similar grammars . the neural network is shown to have the ability to transfer grammatical knowledge to both new symbol vocabularies and new grammars . analysis of the state space shows that the network learns generalized abstract structures of the input and is not simply memorizing the input strings . these representations are context sensitive , hierarchical , and based on the state variable of the finite state machines that the neural network has learned . generalization to new symbol sets or grammars arises from the spatial nature of the internal representations used by the network , allowing new symbol sets to be encoded close to symbol sets that have already been learned in the hidden unit space of the network . the results are counter to the arguments that learning algorithms based on weight adaptation after each exemplar presentation ( such as the long term potentiation found in the mammalian nervous system ) cannot in principle extract symbolic knowledge from positive examples as prescribed by prevailing human linguistic theory and evolutionary psychology"}
{"title": "Streaming, disruptive interference and power-law behavior in the exit dynamics", "abstract": "of confined pedestrians We analyze the exit dynamics of pedestrians who are initially confined in a room. Pedestrians are modeled as cellular automata and compete to escape via a known exit at the soonest possible time. A pedestrian could move forward, backward, left or right within each iteration time depending on adjacent cell vacancy and in accordance with simple rules that determine the compulsion to move and physical capability relative to his neighbors. The arching signatures of jamming were observed and the pedestrians exited in bursts of various sizes. Power-law behavior is found in the burst-size frequency distribution for exit widths w greater than one cell dimension (w > 1). The slope of the power-law curve varies with w from -1.3092 (w = 2) to -1.0720 (w = 20). Streaming which is a diffusive behavior, arises in large burst sizes and is more likely in a single-exit room with w = 1 and leads to a counterintuitive result wherein an average exit throughput Q is obtained that is higher than with w = 2, 3, or 4. For a two-exit room (w = 1), Q is not greater than twice the yield of a single-exit room. If the doors are not separated far enough (< 4w), Q becomes even significantly less due to a collective slow-down that emerges among pedestrians crossing in each other's path (disruptive interference effect). For the same w and door number, Q is also higher with relaxed pedestrians than with anxious ones", "id": "34", "src": "streaming , disruptive interference and power law behavior in the exit dynamics . of confined pedestrians we analyze the exit dynamics of pedestrians who are initially confined in a room . pedestrians are modeled as cellular automata and compete to escape via a known exit at the soonest possible time . a pedestrian could move forward , backward , left or right within each iteration time depending on adjacent cell vacancy and in accordance with simple rules that determine the compulsion to move and physical capability relative to his neighbors . the arching signatures of jamming were observed and the pedestrians exited in bursts of various sizes . power law behavior is found in the burst size frequency distribution for exit widths w greater than one cell dimension ( w > 1 ) . the slope of the power law curve varies with w from 1 . <digit> ( w 2 ) to 1 . <digit> ( w <digit> ) . streaming which is a diffusive behavior , arises in large burst sizes and is more likely in a single exit room with w 1 and leads to a counterintuitive result wherein an average exit throughput q is obtained that is higher than with w 2 , 3 , or 4 . for a two exit room ( w 1 ) , q is not greater than twice the yield of a single exit room . if the doors are not separated far enough ( < 4w ) , q becomes even significantly less due to a collective slow down that emerges among pedestrians crossing in each other ' s path ( disruptive interference effect ) . for the same w and door number , q is also higher with relaxed pedestrians than with anxious ones"}
{"title": "The influence of tollbooths on highway traffic", "abstract": "We study the effects of tollbooths on the traffic flow. The highway traffic is simulated by the Nagel-Schreckenberg model. Various types of toll collection are examined, which can be characterized either by a waiting time or a reduced speed. A first-order phase transition is observed. The phase separation results a saturated flow, which is observed as a plateau region in the fundamental diagram. The effects of lane expansion near the tollbooth are examined. The full capacity of a highway can be restored. The emergence of vehicle queuing is studied. Besides the numerical results, we also obtain analytical expressions for various quantities. The numerical simulations can be well described by the analytical formulas. We also discuss the influence on the travel time and its variance. The tollbooth increases the travel time but decreases its variance. The differences between long- and short-distance travelers are also discussed", "id": "35", "src": "the influence of tollbooths on highway traffic . we study the effects of tollbooths on the traffic flow . the highway traffic is simulated by the nagel schreckenberg model . various types of toll collection are examined , which can be characterized either by a waiting time or a reduced speed . a first order phase transition is observed . the phase separation results a saturated flow , which is observed as a plateau region in the fundamental diagram . the effects of lane expansion near the tollbooth are examined . the full capacity of a highway can be restored . the emergence of vehicle queuing is studied . besides the numerical results , we also obtain analytical expressions for various quantities . the numerical simulations can be well described by the analytical formulas . we also discuss the influence on the travel time and its variance . the tollbooth increases the travel time but decreases its variance . the differences between long and short distance travelers are also discussed"}
{"title": "The Bagsik Oscillator without complex numbers", "abstract": "We argue that the analysis of the so-called Bagsik Oscillator, recently published by Piotrowski and Sladkowski (2001), is erroneous due to: (1) the incorrect banking data used and (2) the application of statistical mechanism apparatus to processes that are totally deterministic", "id": "36", "src": "the bagsik oscillator without complex numbers . we argue that the analysis of the so called bagsik oscillator , recently published by piotrowski and sladkowski ( <digit> ) , is erroneous due to ( 1 ) the incorrect banking data used and ( 2 ) the application of statistical mechanism apparatus to processes that are totally deterministic"}
{"title": "The variance of firm growth rates: the 'scaling' puzzle", "abstract": "Recent evidence suggests that a power-law relationship exists between a firm's size and the variance of its growth rate. The flatness of the relation is regarded as puzzling, in that it suggests that large firms are not much more stable than small firms. It has been suggested that the powerlaw nature of the relationship reflects the presence of some form of correlation of growth rates across the firm's constituent businesses. Here, it is shown that a model of independent businesses which allows for the fact that these businesses vary in size, as modelled by a simple 'partitions of integers' model, provides a good representation of what is observed empirically", "id": "37", "src": "the variance of firm growth rates the ' scaling ' puzzle . recent evidence suggests that a power law relationship exists between a firm ' s size and the variance of its growth rate . the flatness of the relation is regarded as puzzling , in that it suggests that large firms are not much more stable than small firms . it has been suggested that the powerlaw nature of the relationship reflects the presence of some form of correlation of growth rates across the firm ' s constituent businesses . here , it is shown that a model of independent businesses which allows for the fact that these businesses vary in size , as modelled by a simple ' partitions of integers ' model , provides a good representation of what is observed empirically"}
{"title": "Antipersistent Markov behavior in foreign exchange markets", "abstract": "A quantitative check of efficiency in US dollar/Deutsche mark exchange rates is developed using high-frequency (tick by tick) data. The antipersistent Markov behavior of log-price fluctuations of given size implies, in principle, the possibility of a statistical forecast. We introduce and measure the available information of the quote sequence, and we show how it can be profitable following a particular trading rule", "id": "38", "src": "antipersistent markov behavior in foreign exchange markets . a quantitative check of efficiency in us dollar deutsche mark exchange rates is developed using high frequency ( tick by tick ) data . the antipersistent markov behavior of log price fluctuations of given size implies , in principle , the possibility of a statistical forecast . we introduce and measure the available information of the quote sequence , and we show how it can be profitable following a particular trading rule"}
{"title": "Stock market dynamics", "abstract": "We elucidate on several empirical statistical observations of stock market returns. Moreover, we find that these properties are recurrent and are also present in invariant measures of low-dimensional dynamical systems. Thus, we propose that the returns are modeled by the first Poincare return time of a low-dimensional chaotic trajectory. This modeling, which captures the recurrent properties of the return fluctuations, is able to predict well the evolution of the observed statistical quantities. In addition, it explains the reason for which stocks present simultaneously dynamical properties and high uncertainties. In our analysis, we use data from the S&P 500 index and the Brazilian stock Telebras", "id": "39", "src": "stock market dynamics . we elucidate on several empirical statistical observations of stock market returns . moreover , we find that these properties are recurrent and are also present in invariant measures of low dimensional dynamical systems . thus , we propose that the returns are modeled by the first poincare return time of a low dimensional chaotic trajectory . this modeling , which captures the recurrent properties of the return fluctuations , is able to predict well the evolution of the observed statistical quantities . in addition , it explains the reason for which stocks present simultaneously dynamical properties and high uncertainties . in our analysis , we use data from the s&p <digit> index and the brazilian stock telebras"}
{"title": "Application of nonlinear time series analysis techniques to high-frequency", "abstract": "currency exchange data In this work we have applied nonlinear time series analysis to high-frequency currency exchange data. The time series studied are the exchange rates between the US Dollar and 18 other foreign currencies from within and without the Euro zone. Our goal was to determine if their dynamical behaviours were in some way correlated. The nonexistence of stationarity called for the application of recurrence quantification analysis as a tool for this analysis, and is based on the definition of several parameters that allow for the quantification of recurrence plots. The method was checked using the European Monetary System currency exchanges. The results show, as expected, the high correlation between the currencies that are part of the Euro, but also a strong correlation between the Japanese Yen, the Canadian Dollar and the British Pound. Singularities of the series are also demonstrated taking into account historical events, in 1996, in the Euro zone", "id": "40", "src": "application of nonlinear time series analysis techniques to high frequency . currency exchange data in this work we have applied nonlinear time series analysis to high frequency currency exchange data . the time series studied are the exchange rates between the us dollar and <digit> other foreign currencies from within and without the euro zone . our goal was to determine if their dynamical behaviours were in some way correlated . the nonexistence of stationarity called for the application of recurrence quantification analysis as a tool for this analysis , and is based on the definition of several parameters that allow for the quantification of recurrence plots . the method was checked using the european monetary system currency exchanges . the results show , as expected , the high correlation between the currencies that are part of the euro , but also a strong correlation between the japanese yen , the canadian dollar and the british pound . singularities of the series are also demonstrated taking into account historical events , in <digit> , in the euro zone"}
{"title": "Modeling daily realized futures volatility with singular spectrum analysis", "abstract": "Using singular spectrum analysis (SSA), we model the realized volatility and logarithmic standard deviations of two important futures return series. The realized volatility and logarithmic standard deviations are constructed following the methodology of Andersen et al. [J. Am. Stat. Ass. 96 (2001) 42-55] using intra-day transaction data. We find that SSA decomposes the volatility series quite well and effectively captures both the market trend (accounting for about 34-38% of the total variance in the series) and, more importantly, a number of underlying market periodicities. Reliable identification of any periodicities is extremely important for options pricing and risk management and we believe that SSA can be a useful addition to the financial practitioners' toolbox", "id": "41", "src": "modeling daily realized futures volatility with singular spectrum analysis . using singular spectrum analysis ( ssa ) , we model the realized volatility and logarithmic standard deviations of two important futures return series . the realized volatility and logarithmic standard deviations are constructed following the methodology of andersen et al . j . am . stat . ass . <digit> ( <digit> ) <digit> <digit> using intra day transaction data . we find that ssa decomposes the volatility series quite well and effectively captures both the market trend ( accounting for about <digit> <digit> of the total variance in the series ) and , more importantly , a number of underlying market periodicities . reliable identification of any periodicities is extremely important for options pricing and risk management and we believe that ssa can be a useful addition to the financial practitioners ' toolbox"}
{"title": "Phase control of higher-order squeezing of a quantum field", "abstract": "In a recent experiment [Phys. Rev. Lett. 88 (2002) 023601], phase-dependent photon statistics in a c.w. system has been observed in the mixing of a coherent field with a two-photon source. Their system has the advantage over other atomic transition-based fluorescent systems. In this paper, we examine further the squeezing properties of higher-order quantum fluctuations in one of the quadrature components of the combined field in this system. We demonstrate that efficient and lasting higher-order squeezing effects could be observed with proper choice of the relative phase between the pump and coherent fields. This nonclassical feature is attributed to a constructive two-photon interference. Relationship between the second- and higher-order squeezing of the field is discussed", "id": "42", "src": "phase control of higher order squeezing of a quantum field . in a recent experiment phys . rev . lett . <digit> ( <digit> ) <digit> , phase dependent photon statistics in a c . w . system has been observed in the mixing of a coherent field with a two photon source . their system has the advantage over other atomic transition based fluorescent systems . in this paper , we examine further the squeezing properties of higher order quantum fluctuations in one of the quadrature components of the combined field in this system . we demonstrate that efficient and lasting higher order squeezing effects could be observed with proper choice of the relative phase between the pump and coherent fields . this nonclassical feature is attributed to a constructive two photon interference . relationship between the second and higher order squeezing of the field is discussed"}
{"title": "Modeling self-consistent multi-class dynamic traffic flow", "abstract": "In this study, we present a systematic self-consistent multiclass multilane traffic model derived from the vehicular Boltzmann equation and the traffic dispersion model. The multilane domain is considered as a two-dimensional space and the interaction among vehicles in the domain is described by a dispersion model. The reason we consider a multilane domain as a two-dimensional space is that the driving behavior of road users may not be restricted by lanes, especially motorcyclists. The dispersion model, which is a nonlinear Poisson equation, is derived from the car-following theory and the equilibrium assumption. Under the concept that all kinds of users share the finite section, the density is distributed on a road by the dispersion model. In addition, the dynamic evolution of the traffic flow is determined by the systematic gas-kinetic model derived from the Boltzmann equation. Multiplying Boltzmann equation by the zeroth, first- and second-order moment functions, integrating both side of the equation and using chain rules, we can derive continuity, motion and variance equation, respectively. However, the second-order moment function, which is the square of the individual velocity, is employed by previous researches does not have physical meaning in traffic flow", "id": "43", "src": "modeling self consistent multi class dynamic traffic flow . in this study , we present a systematic self consistent multiclass multilane traffic model derived from the vehicular boltzmann equation and the traffic dispersion model . the multilane domain is considered as a two dimensional space and the interaction among vehicles in the domain is described by a dispersion model . the reason we consider a multilane domain as a two dimensional space is that the driving behavior of road users may not be restricted by lanes , especially motorcyclists . the dispersion model , which is a nonlinear poisson equation , is derived from the car following theory and the equilibrium assumption . under the concept that all kinds of users share the finite section , the density is distributed on a road by the dispersion model . in addition , the dynamic evolution of the traffic flow is determined by the systematic gas kinetic model derived from the boltzmann equation . multiplying boltzmann equation by the zeroth , first and second order moment functions , integrating both side of the equation and using chain rules , we can derive continuity , motion and variance equation , respectively . however , the second order moment function , which is the square of the individual velocity , is employed by previous researches does not have physical meaning in traffic flow"}
{"title": "Mixture of experts classification using a hierarchical mixture model", "abstract": "A three-level hierarchical mixture model for classification is presented that models the following data generation process: (1) the data are generated by a finite number of sources (clusters), and (2) the generation mechanism of each source assumes the existence of individual internal class-labeled sources (subclusters of the external cluster). The model estimates the posterior probability of class membership similar to a mixture of experts classifier. In order to learn the parameters of the model, we have developed a general training approach based on maximum likelihood that results in two efficient training algorithms. Compared to other classification mixture models, the proposed hierarchical model exhibits several advantages and provides improved classification performance as indicated by the experimental results", "id": "44", "src": "mixture of experts classification using a hierarchical mixture model . a three level hierarchical mixture model for classification is presented that models the following data generation process ( 1 ) the data are generated by a finite number of sources ( clusters ) , and ( 2 ) the generation mechanism of each source assumes the existence of individual internal class labeled sources ( subclusters of the external cluster ) . the model estimates the posterior probability of class membership similar to a mixture of experts classifier . in order to learn the parameters of the model , we have developed a general training approach based on maximum likelihood that results in two efficient training algorithms . compared to other classification mixture models , the proposed hierarchical model exhibits several advantages and provides improved classification performance as indicated by the experimental results"}
{"title": "eMarketing: restaurant Web sites that click", "abstract": "A number of global companies have adopted electronic commerce as a means of reducing transaction related expenditures, connecting with current and potential customers, and enhancing revenues and profitability. If a restaurant is to have an Internet presence, what aspects of the business should be highlighted? Food service companies that have successfully ventured onto the web have employed assorted web-based technologies to create a powerful marketing tool of unparalleled strength. Historically, it has been difficult to create a set of criteria against which to evaluate website effectiveness. As practitioners consider additional resources for website development, the effectiveness of e-marketing investment becomes increasingly important. Care must be exercised to ensure that the quality of the site adheres to high standards and incorporates evolving technology, as appropriate. Developing a coherent website strategy, including an effective website design, are proving critical to an effective web presence", "id": "45", "src": "emarketing restaurant web sites that click . a number of global companies have adopted electronic commerce as a means of reducing transaction related expenditures , connecting with current and potential customers , and enhancing revenues and profitability . if a restaurant is to have an internet presence , what aspects of the business should be highlighted food service companies that have successfully ventured onto the web have employed assorted web based technologies to create a powerful marketing tool of unparalleled strength . historically , it has been difficult to create a set of criteria against which to evaluate website effectiveness . as practitioners consider additional resources for website development , the effectiveness of e marketing investment becomes increasingly important . care must be exercised to ensure that the quality of the site adheres to high standards and incorporates evolving technology , as appropriate . developing a coherent website strategy , including an effective website design , are proving critical to an effective web presence"}
{"title": "Exploring developments in Web based relationship marketing within the hotel", "abstract": "industry This paper provides a content analysis study of the application of World Wide Web marketing by the hotel industry. There is a lack of historical perspective on industry related Web marketing applications and this paper attempts to resolve this with a two-year follow-up case study of the changing use of the Web to develop different types of relationships. Specifically, the aims are: (1) to identify key changes in the way hotels are using the Web; (2) to look for evidence of the adoption of a relationship marketing (RM) model as a strategy for the development of hotel Web sites and the use of new technologies; and, (3) To investigate the use of multimedia in hotel Web sites. The development and strategic exploitation of the Internet has transformed the basis of marketing. Using the evidence from a Web content survey this study reveals the way relationships are being created and managed within the hotel industry by its use of the Web as a marketing tool. The authors have collected evidence by means of a descriptive study on the way hotels build and create relationships with their Web presence delivering multimedia information as well as channel and interactive means of communication. In addition a strategic framework is offered as the means to describe the mechanism and orientation of Web based marketing by hotels. The study utilizes a model by Gilbert (1996) as a means of developing a measurement instrument to allow a content analysis of the current approach by hotels to the development of Web sites. The results indicate hotels are aware of the new uses of Web technology and are promoting hotel products in the global electronic market in new and sophisticated ways", "id": "46", "src": "exploring developments in web based relationship marketing within the hotel . industry this paper provides a content analysis study of the application of world wide web marketing by the hotel industry . there is a lack of historical perspective on industry related web marketing applications and this paper attempts to resolve this with a two year follow up case study of the changing use of the web to develop different types of relationships . specifically , the aims are ( 1 ) to identify key changes in the way hotels are using the web ( 2 ) to look for evidence of the adoption of a relationship marketing ( rm ) model as a strategy for the development of hotel web sites and the use of new technologies and , ( 3 ) to investigate the use of multimedia in hotel web sites . the development and strategic exploitation of the internet has transformed the basis of marketing . using the evidence from a web content survey this study reveals the way relationships are being created and managed within the hotel industry by its use of the web as a marketing tool . the authors have collected evidence by means of a descriptive study on the way hotels build and create relationships with their web presence delivering multimedia information as well as channel and interactive means of communication . in addition a strategic framework is offered as the means to describe the mechanism and orientation of web based marketing by hotels . the study utilizes a model by gilbert ( <digit> ) as a means of developing a measurement instrument to allow a content analysis of the current approach by hotels to the development of web sites . the results indicate hotels are aware of the new uses of web technology and are promoting hotel products in the global electronic market in new and sophisticated ways"}
{"title": "Online auctions: dynamic pricing and the lodging industry", "abstract": "The traditional channels of distribution for overnight accommodation are rapidly being displaced by Web site scripting, online intermediaries, and specialty brokers. Businesses that pioneered Internet usage relied on it as a sales and marketing alternative to predecessor product distribution channels. As such, Web sites replace the traditional trading model to the Internet. Web-enabled companies are popular because the medium renders the process faster, less costly, highly reliable, and secure. Auction-based models impact business models by converting the price setting mechanism from supplier-centric to market-centric and transforming the trading model from \"one to many\" to \"many to many.\" Historically, pricing was based on the cost of production plus a margin of profit. Traditionally, as products and services move through the supply chain, from the producer to the consumer, various intermediaries added their share of profit to the price. As Internet based mediums of distribution become more prevalent, traditional pricing models are being supplanted with dynamic pricing. A dynamic pricing model represents a flexible system that changes prices not only from product to product, but also from customer to customer and transaction to transaction. Many industry leaders are skeptical of the long run impact of online auctions on lodging industry profit margins, despite the fact pricing theory suggests that an increase in the flow of information results in efficient market pricing. The future of such endeavors remains promising, but controversial", "id": "47", "src": "online auctions dynamic pricing and the lodging industry . the traditional channels of distribution for overnight accommodation are rapidly being displaced by web site scripting , online intermediaries , and specialty brokers . businesses that pioneered internet usage relied on it as a sales and marketing alternative to predecessor product distribution channels . as such , web sites replace the traditional trading model to the internet . web enabled companies are popular because the medium renders the process faster , less costly , highly reliable , and secure . auction based models impact business models by converting the price setting mechanism from supplier centric to market centric and transforming the trading model from one to many to many to many . historically , pricing was based on the cost of production plus a margin of profit . traditionally , as products and services move through the supply chain , from the producer to the consumer , various intermediaries added their share of profit to the price . as internet based mediums of distribution become more prevalent , traditional pricing models are being supplanted with dynamic pricing . a dynamic pricing model represents a flexible system that changes prices not only from product to product , but also from customer to customer and transaction to transaction . many industry leaders are skeptical of the long run impact of online auctions on lodging industry profit margins , despite the fact pricing theory suggests that an increase in the flow of information results in efficient market pricing . the future of such endeavors remains promising , but controversial"}
{"title": "Affine invariants of convex polygons", "abstract": "In this correspondence, we prove that the affine invariants, for image registration and object recognition, proposed recently by Yang and Cohen (see ibid., vol.8, no.7, p.934-46, July 1999) are algebraically dependent. We show how to select an independent and complete set of the invariants. The use of this new set leads to a significant reduction of the computing complexity without decreasing the discrimination power", "id": "48", "src": "affine invariants of convex polygons . in this correspondence , we prove that the affine invariants , for image registration and object recognition , proposed recently by yang and cohen ( see ibid . , vol . 8 , no . 7 , p . <digit> <digit> , july <digit> ) are algebraically dependent . we show how to select an independent and complete set of the invariants . the use of this new set leads to a significant reduction of the computing complexity without decreasing the discrimination power"}
{"title": "Real-time implementation of a new low-memory SPIHT image coding algorithm using", "abstract": "DSP chip Among all algorithms based on wavelet transform and zerotree quantization, Said and Pearlman's (1996) set partitioning in hierarchical trees (SPIHT) algorithm is well-known for its simplicity and efficiency. This paper deals with the real-time implementation of SPIHT algorithm using DSP chip. In order to facilitate the implementation and improve the codec's performance, some relative issues are thoroughly discussed, such as the optimization of program structure to speed up the wavelet decomposition. SPIHT's high memory requirement is a major drawback for hardware implementation. In this paper, we modify the original SPIHT algorithm by presenting two new concepts-number of error bits and absolute zerotree. Consequently, the memory cost is significantly reduced. We also introduce a new method to control the coding process by number of error bits. Our experimental results show that the implementation meets common requirement of real-time video coding and is proven to be a practical and efficient DSP solution", "id": "49", "src": "real time implementation of a new low memory spiht image coding algorithm using . dsp chip among all algorithms based on wavelet transform and zerotree quantization , said and pearlman ' s ( <digit> ) set partitioning in hierarchical trees ( spiht ) algorithm is well known for its simplicity and efficiency . this paper deals with the real time implementation of spiht algorithm using dsp chip . in order to facilitate the implementation and improve the codec ' s performance , some relative issues are thoroughly discussed , such as the optimization of program structure to speed up the wavelet decomposition . spiht ' s high memory requirement is a major drawback for hardware implementation . in this paper , we modify the original spiht algorithm by presenting two new concepts number of error bits and absolute zerotree . consequently , the memory cost is significantly reduced . we also introduce a new method to control the coding process by number of error bits . our experimental results show that the implementation meets common requirement of real time video coding and is proven to be a practical and efficient dsp solution"}
{"title": "Efficient computation of local geometric moments", "abstract": "Local moments have attracted attention as local features in applications such as edge detection and texture segmentation. The main reason for this is that they are inherently integral-based features, so that their use reduces the effect of uncorrelated noise. The computation of local moments, when viewed as a neighborhood operation, can be interpreted as a convolution of the image with a set of masks. Nevertheless, moments computed inside overlapping windows are not independent and convolution does not take this fact into account. By introducing a matrix formulation and the concept of accumulation moments, this paper presents an algorithm which is computationally much more efficient than convolving and yet as simple", "id": "50", "src": "efficient computation of local geometric moments . local moments have attracted attention as local features in applications such as edge detection and texture segmentation . the main reason for this is that they are inherently integral based features , so that their use reduces the effect of uncorrelated noise . the computation of local moments , when viewed as a neighborhood operation , can be interpreted as a convolution of the image with a set of masks . nevertheless , moments computed inside overlapping windows are not independent and convolution does not take this fact into account . by introducing a matrix formulation and the concept of accumulation moments , this paper presents an algorithm which is computationally much more efficient than convolving and yet as simple"}
{"title": "Adaptive image denoising using scale and space consistency", "abstract": "This paper proposes a new method for image denoising with edge preservation, based on image multiresolution decomposition by a redundant wavelet transform. In our approach, edges are implicitly located and preserved in the wavelet domain, whilst image noise is filtered out. At each resolution level, the image edges are estimated by gradient magnitudes (obtained from the wavelet coefficients), which are modeled probabilistically, and a shrinkage function is assembled based on the model obtained. Joint use of space and scale consistency is applied for better preservation of edges. The shrinkage functions are combined to preserve edges that appear simultaneously at several resolutions, and geometric constraints are applied to preserve edges that are not isolated. The proposed technique produces a filtered version of the original image, where homogeneous regions appear separated by well-defined edges. Possible applications include image presegmentation, and image denoising", "id": "51", "src": "adaptive image denoising using scale and space consistency . this paper proposes a new method for image denoising with edge preservation , based on image multiresolution decomposition by a redundant wavelet transform . in our approach , edges are implicitly located and preserved in the wavelet domain , whilst image noise is filtered out . at each resolution level , the image edges are estimated by gradient magnitudes ( obtained from the wavelet coefficients ) , which are modeled probabilistically , and a shrinkage function is assembled based on the model obtained . joint use of space and scale consistency is applied for better preservation of edges . the shrinkage functions are combined to preserve edges that appear simultaneously at several resolutions , and geometric constraints are applied to preserve edges that are not isolated . the proposed technique produces a filtered version of the original image , where homogeneous regions appear separated by well defined edges . possible applications include image presegmentation , and image denoising"}
{"title": "Tracking nonparameterized object contours in video", "abstract": "We propose a new method for contour tracking in video. The inverted distance transform of the edge map is used as an edge indicator function for contour detection. Using the concept of topographical distance, the watershed segmentation can be formulated as a minimization. This new viewpoint gives a way to combine the results of the watershed algorithm on different surfaces. In particular, our algorithm determines the contour as a combination of the current edge map and the contour, predicted from the tracking result in the previous frame. We also show that the problem of background clutter can be relaxed by taking the object motion into account. The compensation with object motion allows to detect and remove spurious edges in background. The experimental results confirm the expected advantages of the proposed method over the existing approaches", "id": "52", "src": "tracking nonparameterized object contours in video . we propose a new method for contour tracking in video . the inverted distance transform of the edge map is used as an edge indicator function for contour detection . using the concept of topographical distance , the watershed segmentation can be formulated as a minimization . this new viewpoint gives a way to combine the results of the watershed algorithm on different surfaces . in particular , our algorithm determines the contour as a combination of the current edge map and the contour , predicted from the tracking result in the previous frame . we also show that the problem of background clutter can be relaxed by taking the object motion into account . the compensation with object motion allows to detect and remove spurious edges in background . the experimental results confirm the expected advantages of the proposed method over the existing approaches"}
{"title": "Multilayered image representation: application to image compression", "abstract": "The main contribution of this work is a new paradigm for image representation and image compression. We describe a new multilayered representation technique for images. An image is parsed into a superposition of coherent layers: piecewise smooth regions layer, textures layer, etc. The multilayered decomposition algorithm consists in a cascade of compressions applied successively to the image itself and to the residuals that resulted from the previous compressions. During each iteration of the algorithm, we code the residual part in a lossy way: we only retain the most significant structures of the residual part, which results in a sparse representation. Each layer is encoded independently with a different transform, or basis, at a different bitrate, and the combination of the compressed layers can always be reconstructed in a meaningful way. The strength of the multilayer approach comes from the fact that different sets of basis functions complement each others: some of the basis functions will give reasonable account of the large trend of the data, while others will catch the local transients, or the oscillatory patterns. This multilayered representation has a lot of beautiful applications in image understanding, and image and video coding. We have implemented the algorithm and we have studied its capabilities", "id": "53", "src": "multilayered image representation application to image compression . the main contribution of this work is a new paradigm for image representation and image compression . we describe a new multilayered representation technique for images . an image is parsed into a superposition of coherent layers piecewise smooth regions layer , textures layer , etc . the multilayered decomposition algorithm consists in a cascade of compressions applied successively to the image itself and to the residuals that resulted from the previous compressions . during each iteration of the algorithm , we code the residual part in a lossy way we only retain the most significant structures of the residual part , which results in a sparse representation . each layer is encoded independently with a different transform , or basis , at a different bitrate , and the combination of the compressed layers can always be reconstructed in a meaningful way . the strength of the multilayer approach comes from the fact that different sets of basis functions complement each others some of the basis functions will give reasonable account of the large trend of the data , while others will catch the local transients , or the oscillatory patterns . this multilayered representation has a lot of beautiful applications in image understanding , and image and video coding . we have implemented the algorithm and we have studied its capabilities"}
{"title": "Combining spatial and scale-space techniques for edge detection to provide a", "abstract": "spatially adaptive wavelet-based noise filtering algorithm New methods for detecting edges in an image using spatial and scale-space domains are proposed. A priori knowledge about geometrical characteristics of edges is used to assign a probability factor to the chance of any pixel being on an edge. An improved double thresholding technique is introduced for spatial domain filtering. Probabilities that pixels belong to a given edge are assigned based on pixel similarity across gradient amplitudes, gradient phases and edge connectivity. The scale-space approach uses dynamic range compression to allow wavelet correlation over a wider range of scales. A probabilistic formulation is used to combine the results obtained from filtering in each domain to provide a final edge probability image which has the advantages of both spatial and scale-space domain methods. Decomposing this edge probability image with the same wavelet as the original image permits the generation of adaptive filters that can recognize the characteristics of the edges in all wavelet detail and approximation images regardless of scale. These matched filters permit significant reduction in image noise without contributing to edge distortion. The spatially adaptive wavelet noise-filtering algorithm is qualitatively and quantitatively compared to a frequency domain and two wavelet based noise suppression algorithms using both natural and computer generated noisy images", "id": "54", "src": "combining spatial and scale space techniques for edge detection to provide a . spatially adaptive wavelet based noise filtering algorithm new methods for detecting edges in an image using spatial and scale space domains are proposed . a priori knowledge about geometrical characteristics of edges is used to assign a probability factor to the chance of any pixel being on an edge . an improved double thresholding technique is introduced for spatial domain filtering . probabilities that pixels belong to a given edge are assigned based on pixel similarity across gradient amplitudes , gradient phases and edge connectivity . the scale space approach uses dynamic range compression to allow wavelet correlation over a wider range of scales . a probabilistic formulation is used to combine the results obtained from filtering in each domain to provide a final edge probability image which has the advantages of both spatial and scale space domain methods . decomposing this edge probability image with the same wavelet as the original image permits the generation of adaptive filters that can recognize the characteristics of the edges in all wavelet detail and approximation images regardless of scale . these matched filters permit significant reduction in image noise without contributing to edge distortion . the spatially adaptive wavelet noise filtering algorithm is qualitatively and quantitatively compared to a frequency domain and two wavelet based noise suppression algorithms using both natural and computer generated noisy images"}
{"title": "Computational capacity of an odorant discriminator: the linear separability of", "abstract": "curves We introduce and study an artificial neural network inspired by the probabilistic receptor affinity distribution model of olfaction. Our system consists of N sensory neurons whose outputs converge on a single processing linear threshold element. The system's aim is to model discrimination of a single target odorant from a large number p of background odorants within a range of odorant concentrations. We show that this is possible provided p does not exceed a critical value p/sub c/ and calculate the critical capacity alpha c=p/sub c//N. The critical capacity depends on the range of concentrations in which the discrimination is to be accomplished. If the olfactory bulb may be thought of as a collection of such processing elements, each responsible for the discrimination of a single odorant, our study provides a quantitative analysis of the potential computational properties of the olfactory bulb. The mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves, embedded in a large-dimensional space. This is accomplished here by a numerical study, using a method that signals whether the discrimination task is realizable, together with a finite-size scaling analysis", "id": "55", "src": "computational capacity of an odorant discriminator the linear separability of . curves we introduce and study an artificial neural network inspired by the probabilistic receptor affinity distribution model of olfaction . our system consists of n sensory neurons whose outputs converge on a single processing linear threshold element . the system ' s aim is to model discrimination of a single target odorant from a large number p of background odorants within a range of odorant concentrations . we show that this is possible provided p does not exceed a critical value p sub c and calculate the critical capacity alpha c p sub c n . the critical capacity depends on the range of concentrations in which the discrimination is to be accomplished . if the olfactory bulb may be thought of as a collection of such processing elements , each responsible for the discrimination of a single odorant , our study provides a quantitative analysis of the potential computational properties of the olfactory bulb . the mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves , embedded in a large dimensional space . this is accomplished here by a numerical study , using a method that signals whether the discrimination task is realizable , together with a finite size scaling analysis"}
{"title": "Lossy to lossless object-based coding of 3-D MRI data", "abstract": "We propose a fully three-dimensional (3-D) object-based coding system exploiting the diagnostic relevance of the different regions of the volumetric data for rate allocation. The data are first decorrelated via a 3-D discrete wavelet transform. The implementation via the lifting steps scheme allows to map integer-to-integer values, enabling lossless coding, and facilitates the definition of the object-based inverse transform. The coding process assigns disjoint segments of the bitstream to the different objects, which can be independently accessed and reconstructed at any up-to-lossless quality. Two fully 3-D coding strategies are considered: embedded zerotree coding (EZW-3D) and multidimensional layered zero coding (MLZC), both generalized for region of interest (ROI)-based processing. In order to avoid artifacts along region boundaries, some extra coefficients must be encoded for each object. This gives rise to an overheading of the bitstream with respect to the case where the volume is encoded as a whole. The amount of such extra information depends on both the filter length and the decomposition depth. The system is characterized on a set of head magnetic resonance images. Results show that MLZC and EZW-3D have competitive performances. In particular, the best MLZC mode outperforms the others state-of-the-art techniques on one of the datasets for which results are available in the literature", "id": "56", "src": "lossy to lossless object based coding of 3 d mri data . we propose a fully three dimensional ( 3 d ) object based coding system exploiting the diagnostic relevance of the different regions of the volumetric data for rate allocation . the data are first decorrelated via a 3 d discrete wavelet transform . the implementation via the lifting steps scheme allows to map integer to integer values , enabling lossless coding , and facilitates the definition of the object based inverse transform . the coding process assigns disjoint segments of the bitstream to the different objects , which can be independently accessed and reconstructed at any up to lossless quality . two fully 3 d coding strategies are considered embedded zerotree coding ( ezw 3d ) and multidimensional layered zero coding ( mlzc ) , both generalized for region of interest ( roi ) based processing . in order to avoid artifacts along region boundaries , some extra coefficients must be encoded for each object . this gives rise to an overheading of the bitstream with respect to the case where the volume is encoded as a whole . the amount of such extra information depends on both the filter length and the decomposition depth . the system is characterized on a set of head magnetic resonance images . results show that mlzc and ezw 3d have competitive performances . in particular , the best mlzc mode outperforms the others state of the art techniques on one of the datasets for which results are available in the literature"}
{"title": "Optimal linear control in stabilizer design", "abstract": "The most common method of improving stability of the power system is the synthesis of the turbine and generator control systems, because of the high effectiveness and relatively low cost of these elements. The synthesis and construction of the effective synchronous generator and turbine controller is a very difficult task. This paper describes the seven step mu -synthesis approach to PSS design enabling the synchronous generator to remain stable over a wide range of system operating conditions", "id": "57", "src": "optimal linear control in stabilizer design . the most common method of improving stability of the power system is the synthesis of the turbine and generator control systems , because of the high effectiveness and relatively low cost of these elements . the synthesis and construction of the effective synchronous generator and turbine controller is a very difficult task . this paper describes the seven step mu synthesis approach to pss design enabling the synchronous generator to remain stable over a wide range of system operating conditions"}
{"title": "Verifying resonant grounding in distribution systems", "abstract": "The authors describe RESFAL, a software tool that can check on the behavior of distribution network resonant grounding systems with regard to compensation coil tuning and to fault detection", "id": "58", "src": "verifying resonant grounding in distribution systems . the authors describe resfal , a software tool that can check on the behavior of distribution network resonant grounding systems with regard to compensation coil tuning and to fault detection"}
{"title": "Power electronics spark new simulation challenges", "abstract": "This article discusses some of the changes that have taken place in power systems and explores some of the inherent requirements for simulation technologies in order to keep up with this rapidly changing environment. The authors describe how energy utilities are realizing that, with the appropriate tools, they can train and sustain engineers who can maintain a great insight into system dynamics", "id": "59", "src": "power electronics spark new simulation challenges . this article discusses some of the changes that have taken place in power systems and explores some of the inherent requirements for simulation technologies in order to keep up with this rapidly changing environment . the authors describe how energy utilities are realizing that , with the appropriate tools , they can train and sustain engineers who can maintain a great insight into system dynamics"}
{"title": "Deriving model parameters from field test measurements [generator control", "abstract": "simulation] A major component of any power system simulation is the generating plant. The purpose of DeriveAssist is to speed up the parameter derivation process and to allow engineers less versed in parameter matching and identification to get involved in the process of power plant electric generator modelling", "id": "60", "src": "deriving model parameters from field test measurements generator control . simulation a major component of any power system simulation is the generating plant . the purpose of deriveassist is to speed up the parameter derivation process and to allow engineers less versed in parameter matching and identification to get involved in the process of power plant electric generator modelling"}
{"title": "Prospective on computer applications in power", "abstract": "The so-called \"deregulation\" and restructuring of the electric power industry have made it very difficult to keep up with industry changes and have made it much more difficult to envision the future. In this article, current key issues and major developments of the past few years are reviewed to provide perspective, and prospects for future computer applications in power are suggested. Technology changes are occurring at an exponential rate. The interconnected bulk electric systems are becoming integrated with vast networked information systems. This article discusses the skills that will be needed by future power engineers to keep pace with these developments and trends", "id": "61", "src": "prospective on computer applications in power . the so called deregulation and restructuring of the electric power industry have made it very difficult to keep up with industry changes and have made it much more difficult to envision the future . in this article , current key issues and major developments of the past few years are reviewed to provide perspective , and prospects for future computer applications in power are suggested . technology changes are occurring at an exponential rate . the interconnected bulk electric systems are becoming integrated with vast networked information systems . this article discusses the skills that will be needed by future power engineers to keep pace with these developments and trends"}
{"title": "Control centers are here to stay", "abstract": "Despite changes with different structures, market rules, and uncertainties, a control center must always be in place to maintain the security, reliability, and quality of electric service. This article focuses on the energy management system (EMS) control center, identifying the major functions that have become standard components of every application software package. The two most important control center functions, security control and load-following control, guarantee the continuity of electric service, which after all, is the end-product of the utility business. New technology trends in the design of control center infrastructures are emerging in the liberalized environment of the energy market. An example of a control center infrastructure is described. The article ends with a concern for the security of the control center itself", "id": "62", "src": "control centers are here to stay . despite changes with different structures , market rules , and uncertainties , a control center must always be in place to maintain the security , reliability , and quality of electric service . this article focuses on the energy management system ( ems ) control center , identifying the major functions that have become standard components of every application software package . the two most important control center functions , security control and load following control , guarantee the continuity of electric service , which after all , is the end product of the utility business . new technology trends in the design of control center infrastructures are emerging in the liberalized environment of the energy market . an example of a control center infrastructure is described . the article ends with a concern for the security of the control center itself"}
{"title": "Virus hunting", "abstract": "We all appreciate the need for, and hopefully we have all deployed, anti-virus software. The good news is that AV software has come a long way fast. Four or so years ago it was true to write that AV software could not detect Trojan Horses and similar intrusion attempts. Now it can and does. McAfee's VirusScan, for example, goes one further; it detects viruses, worms and Trojan Horses and deploys itself as a firewall to filter data packets, control access to Internet resources, activate rule sets for specific applications, in general to protect against hackers. But like so much software, we use it with little thought as to how it came to do its job. Behind the scenes there is an army of top notch programmers trying to stay ahead of the baddies who, at the last count, had produced some 60,000 viruses", "id": "63", "src": "virus hunting . we all appreciate the need for , and hopefully we have all deployed , anti virus software . the good news is that av software has come a long way fast . four or so years ago it was true to write that av software could not detect trojan horses and similar intrusion attempts . now it can and does . mcafee ' s virusscan , for example , goes one further it detects viruses , worms and trojan horses and deploys itself as a firewall to filter data packets , control access to internet resources , activate rule sets for specific applications , in general to protect against hackers . but like so much software , we use it with little thought as to how it came to do its job . behind the scenes there is an army of top notch programmers trying to stay ahead of the baddies who , at the last count , had produced some <digit> , <digit> viruses"}
{"title": "Integration is key - an introduction to enterprise application integration", "abstract": "(EAI) technology Over the past few years, numerous organisations have invested in the latest software applications to drive their business forward. But many are now finding that these systems are becoming redundant on their own. The key to staying ahead of the competition in today's current climate is now to integrate all of these systems, says Justin Opie, Portfolio Director at Imark Communications", "id": "64", "src": "integration is key an introduction to enterprise application integration . ( eai ) technology over the past few years , numerous organisations have invested in the latest software applications to drive their business forward . but many are now finding that these systems are becoming redundant on their own . the key to staying ahead of the competition in today ' s current climate is now to integrate all of these systems , says justin opie , portfolio director at imark communications"}
{"title": "Managing system risk", "abstract": "Companies are increasingly required to provide assurance that their systems are secure and conform to commercial security standards. Senior business managers are ultimately responsible for the security of their corporate systems and for the implications in the event of a failure. Businesses will be exposed to unquantified security risks unless they have a formal risk management framework in place to enable risks to be identified, evaluated and managed. Failure to assess and manage risks can lead to a business suffering serious financial impacts, commercial embarrassment and fines or sanctions from regulators. This is both a key responsibility and opportunity for Management Services Practitioners", "id": "65", "src": "managing system risk . companies are increasingly required to provide assurance that their systems are secure and conform to commercial security standards . senior business managers are ultimately responsible for the security of their corporate systems and for the implications in the event of a failure . businesses will be exposed to unquantified security risks unless they have a formal risk management framework in place to enable risks to be identified , evaluated and managed . failure to assess and manage risks can lead to a business suffering serious financial impacts , commercial embarrassment and fines or sanctions from regulators . this is both a key responsibility and opportunity for management services practitioners"}
{"title": "On optimality in auditory information processing", "abstract": "We study limits for the detection and estimation of weak sinusoidal signals in the primary part of the mammalian auditory system using a stochastic Fitzhugh-Nagumo model and an action-recovery model for synaptic depression. Our overall model covers the chain from a hair cell to a point just after the synaptic connection with a cell in the cochlear nucleus. The information processing performance of the system is evaluated using so-called phi -divergences from statistics that quantify \"dissimilarity\" between probability measures and are intimately related to a number of fundamental limits in statistics and information theory (IT). We show that there exists a set of parameters that can optimize several important phi -divergences simultaneously and that this set corresponds to a constant quiescent firing rate (QFR) of the spiral ganglion neuron. The optimal value of the QFR is frequency dependent but is essentially independent of the amplitude of the signal (for small amplitudes). Consequently, optimal processing according to several standard IT criteria can be accomplished for this model if and only if the parameters are \"tuned\" to values that correspond to one and the same QFR. This offers a new explanation for the QFR and can provide new insight into the role played by several other parameters of the peripheral auditory system", "id": "66", "src": "on optimality in auditory information processing . we study limits for the detection and estimation of weak sinusoidal signals in the primary part of the mammalian auditory system using a stochastic fitzhugh nagumo model and an action recovery model for synaptic depression . our overall model covers the chain from a hair cell to a point just after the synaptic connection with a cell in the cochlear nucleus . the information processing performance of the system is evaluated using so called phi divergences from statistics that quantify dissimilarity between probability measures and are intimately related to a number of fundamental limits in statistics and information theory ( it ) . we show that there exists a set of parameters that can optimize several important phi divergences simultaneously and that this set corresponds to a constant quiescent firing rate ( qfr ) of the spiral ganglion neuron . the optimal value of the qfr is frequency dependent but is essentially independent of the amplitude of the signal ( for small amplitudes ) . consequently , optimal processing according to several standard it criteria can be accomplished for this model if and only if the parameters are tuned to values that correspond to one and the same qfr . this offers a new explanation for the qfr and can provide new insight into the role played by several other parameters of the peripheral auditory system"}
{"title": "Electrical facility construction work for information network structuring by", "abstract": "the use of sewage conduits To confront the advent of the advanced information society, there has been a pressing demand for the adjustment of the communications infrastructure and the structuring of the information network by utilizing the sewage conduits. The City of Tokyo is promoting a project by the name of the sewer optical fiber teleway (SOFT) network plan. According to this plan, the total distance of the optical fiber network laid in the sewer conduits is scheduled to reach about 470 km by the end of March 2000. At the final stage, this distance will reach 800 km as a whole. We completed the construction work for the information control facilities scattered in 11 places inclusive of the Treatment Site S, with the intention to adjust and extend the information transmission network laid through the above-mentioned optical fiber network, to be used exclusively by the Bureau of Sewerage. This construction work is described in the paper", "id": "67", "src": "electrical facility construction work for information network structuring by . the use of sewage conduits to confront the advent of the advanced information society , there has been a pressing demand for the adjustment of the communications infrastructure and the structuring of the information network by utilizing the sewage conduits . the city of tokyo is promoting a project by the name of the sewer optical fiber teleway ( soft ) network plan . according to this plan , the total distance of the optical fiber network laid in the sewer conduits is scheduled to reach about <digit> km by the end of march <digit> . at the final stage , this distance will reach <digit> km as a whole . we completed the construction work for the information control facilities scattered in <digit> places inclusive of the treatment site s , with the intention to adjust and extend the information transmission network laid through the above mentioned optical fiber network , to be used exclusively by the bureau of sewerage . this construction work is described in the paper"}
{"title": "A framework for evaluating the data-hiding capacity of image sources", "abstract": "An information-theoretic model for image watermarking and data hiding is presented in this paper. Previous theoretical results are used to characterize the fundamental capacity limits of image watermarking and data-hiding systems. Capacity is determined by the statistical model used for the host image, by the distortion constraints on the data hider and the attacker, and by the information available to the data hider, to the attacker, and to the decoder. We consider autoregressive, block-DCT, and wavelet statistical models for images and compute data-hiding capacity for compressed and uncompressed host-image sources. Closed-form expressions are obtained under sparse-model approximations. Models for geometric attacks and distortion measures that are invariant to such attacks are considered", "id": "68", "src": "a framework for evaluating the data hiding capacity of image sources . an information theoretic model for image watermarking and data hiding is presented in this paper . previous theoretical results are used to characterize the fundamental capacity limits of image watermarking and data hiding systems . capacity is determined by the statistical model used for the host image , by the distortion constraints on the data hider and the attacker , and by the information available to the data hider , to the attacker , and to the decoder . we consider autoregressive , block dct , and wavelet statistical models for images and compute data hiding capacity for compressed and uncompressed host image sources . closed form expressions are obtained under sparse model approximations . models for geometric attacks and distortion measures that are invariant to such attacks are considered"}
{"title": "Geometrically invariant watermarking using feature points", "abstract": "This paper presents a new approach for watermarking of digital images providing robustness to geometrical distortions. The weaknesses of classical watermarking methods to geometrical distortions are outlined first. Geometrical distortions can be decomposed into two classes: global transformations such as rotations and translations and local transformations such as the StirMark attack. An overview of existing self-synchronizing schemes is then presented. Theses schemes can use periodical properties of the mark, invariant properties of transforms, template insertion, or information provided by the original image to counter geometrical distortions. Thereafter, a new class of watermarking schemes using the image content is presented. We propose an embedding and detection scheme where the mark is bound with a content descriptor defined by salient points. Three different types of feature points are studied and their robustness to geometrical transformations is evaluated to develop an enhanced detector. The embedding of the signature is done by extracting feature points of the image and performing a Delaunay tessellation on the set of points. The mark is embedded using a classical additive scheme inside each triangle of the tessellation. The detection is done using correlation properties on the different triangles. The performance of the presented scheme is evaluated after JPEG compression, geometrical attack and transformations. Results show that the fact that the scheme is robust to these different manipulations. Finally, in our concluding remarks, we analyze the different perspectives of such content-based watermarking scheme", "id": "69", "src": "geometrically invariant watermarking using feature points . this paper presents a new approach for watermarking of digital images providing robustness to geometrical distortions . the weaknesses of classical watermarking methods to geometrical distortions are outlined first . geometrical distortions can be decomposed into two classes global transformations such as rotations and translations and local transformations such as the stirmark attack . an overview of existing self synchronizing schemes is then presented . theses schemes can use periodical properties of the mark , invariant properties of transforms , template insertion , or information provided by the original image to counter geometrical distortions . thereafter , a new class of watermarking schemes using the image content is presented . we propose an embedding and detection scheme where the mark is bound with a content descriptor defined by salient points . three different types of feature points are studied and their robustness to geometrical transformations is evaluated to develop an enhanced detector . the embedding of the signature is done by extracting feature points of the image and performing a delaunay tessellation on the set of points . the mark is embedded using a classical additive scheme inside each triangle of the tessellation . the detection is done using correlation properties on the different triangles . the performance of the presented scheme is evaluated after jpeg compression , geometrical attack and transformations . results show that the fact that the scheme is robust to these different manipulations . finally , in our concluding remarks , we analyze the different perspectives of such content based watermarking scheme"}
{"title": "Color plane interpolation using alternating projections", "abstract": "Most commercial digital cameras use color filter arrays to sample red, green, and blue colors according to a specific pattern. At the location of each pixel only one color sample is taken, and the values of the other colors must be interpolated using neighboring samples. This color plane interpolation is known as demosaicing; it is one of the important tasks in a digital camera pipeline. If demosaicing is not performed appropriately, images suffer from highly visible color artifacts. In this paper we present a new demosaicing technique that uses inter-channel correlation effectively in an alternating-projections scheme. We have compared this technique with six state-of-the-art demosaicing techniques, and it outperforms all of them, both visually and in terms of mean square error", "id": "70", "src": "color plane interpolation using alternating projections . most commercial digital cameras use color filter arrays to sample red , green , and blue colors according to a specific pattern . at the location of each pixel only one color sample is taken , and the values of the other colors must be interpolated using neighboring samples . this color plane interpolation is known as demosaicing it is one of the important tasks in a digital camera pipeline . if demosaicing is not performed appropriately , images suffer from highly visible color artifacts . in this paper we present a new demosaicing technique that uses inter channel correlation effectively in an alternating projections scheme . we have compared this technique with six state of the art demosaicing techniques , and it outperforms all of them , both visually and in terms of mean square error"}
{"title": "A comparison of computational color constancy Algorithms. II. Experiments with", "abstract": "image data For pt.I see ibid., vol. 11, no.9, p.972-84 (2002). We test a number of the leading computational color constancy algorithms using a comprehensive set of images. These were of 33 different scenes under 11 different sources representative of common illumination conditions. The algorithms studied include two gray world methods, a version of the Retinex method, several variants of Forsyth's (1990) gamut-mapping method, Cardei et al.'s (2000) neural net method, and Finlayson et al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel and Finlayson 2000). We discuss a number of issues in applying color constancy ideas to image data, and study in depth the effect of different preprocessing strategies. We compare the performance of the algorithms on image data with their performance on synthesized data. All data used for this study are available online at http://www.cs.sfu.ca/~color/data, and implementations for most of the algorithms are also available (http://www.cs.sfu.ca/~color/code). Experiments with synthesized data (part one of this paper) suggested that the methods which emphasize the use of the input data statistics, specifically color by correlation and the neural net algorithm, are potentially the most effective at estimating the chromaticity of the scene illuminant. Unfortunately, we were unable to realize comparable performance on real images. Here exploiting pixel intensity proved to be more beneficial than exploiting the details of image chromaticity statistics, and the three-dimensional (3-D) gamut-mapping algorithms gave the best performance", "id": "71", "src": "a comparison of computational color constancy algorithms . ii . experiments with . image data for pt . i see ibid . , vol . <digit> , no . 9 , p . <digit> <digit> ( <digit> ) . we test a number of the leading computational color constancy algorithms using a comprehensive set of images . these were of <digit> different scenes under <digit> different sources representative of common illumination conditions . the algorithms studied include two gray world methods , a version of the retinex method , several variants of forsyth ' s ( <digit> ) gamut mapping method , cardei et al . ' s ( <digit> ) neural net method , and finlayson et al . ' s color by correlation method ( finlayson et al . <digit> , <digit> hubel and finlayson <digit> ) . we discuss a number of issues in applying color constancy ideas to image data , and study in depth the effect of different preprocessing strategies . we compare the performance of the algorithms on image data with their performance on synthesized data . all data used for this study are available online at http www . cs . sfu . ca color data , and implementations for most of the algorithms are also available ( http www . cs . sfu . ca color code ) . experiments with synthesized data ( part one of this paper ) suggested that the methods which emphasize the use of the input data statistics , specifically color by correlation and the neural net algorithm , are potentially the most effective at estimating the chromaticity of the scene illuminant . unfortunately , we were unable to realize comparable performance on real images . here exploiting pixel intensity proved to be more beneficial than exploiting the details of image chromaticity statistics , and the three dimensional ( 3 d ) gamut mapping algorithms gave the best performance"}
{"title": "A comparison of computational color constancy algorithms. I: Methodology and", "abstract": "experiments with synthesized data We introduce a context for testing computational color constancy, specify our approach to the implementation of a number of the leading algorithms, and report the results of three experiments using synthesized data. Experiments using synthesized data are important because the ground truth is known, possible confounds due to camera characterization and pre-processing are absent, and various factors affecting color constancy can be efficiently investigated because they can be manipulated individually and precisely. The algorithms chosen for close study include two gray world methods, a limiting case of a version of the Retinex method, a number of variants of Forsyth's (1990) gamut-mapping method, Cardei et al.'s (2000) neural net method, and Finlayson et al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel and Finlayson 2000) . We investigate the ability of these algorithms to make estimates of three different color constancy quantities: the chromaticity of the scene illuminant, the overall magnitude of that illuminant, and a corrected, illumination invariant, image. We consider algorithm performance as a function of the number of surfaces in scenes generated from reflectance spectra, the relative effect on the algorithms of added specularities, and the effect of subsequent clipping of the data. All data is available on-line at http://www.cs.sfu.ca/~color/data, and implementations for most of the algorithms are also available (http://www.cs.sfu.ca/~color/code)", "id": "72", "src": "a comparison of computational color constancy algorithms . i methodology and . experiments with synthesized data we introduce a context for testing computational color constancy , specify our approach to the implementation of a number of the leading algorithms , and report the results of three experiments using synthesized data . experiments using synthesized data are important because the ground truth is known , possible confounds due to camera characterization and pre processing are absent , and various factors affecting color constancy can be efficiently investigated because they can be manipulated individually and precisely . the algorithms chosen for close study include two gray world methods , a limiting case of a version of the retinex method , a number of variants of forsyth ' s ( <digit> ) gamut mapping method , cardei et al . ' s ( <digit> ) neural net method , and finlayson et al . ' s color by correlation method ( finlayson et al . <digit> , <digit> hubel and finlayson <digit> ) . we investigate the ability of these algorithms to make estimates of three different color constancy quantities the chromaticity of the scene illuminant , the overall magnitude of that illuminant , and a corrected , illumination invariant , image . we consider algorithm performance as a function of the number of surfaces in scenes generated from reflectance spectra , the relative effect on the algorithms of added specularities , and the effect of subsequent clipping of the data . all data is available on line at http www . cs . sfu . ca color data , and implementations for most of the algorithms are also available ( http www . cs . sfu . ca color code )"}
{"title": "Quality image metrics for synthetic images based on perceptual color", "abstract": "differences Due to the improvement of image rendering processes, and the increasing importance of quantitative comparisons among synthetic color images, it is essential to define perceptually based metrics which enable to objectively assess the visual quality of digital simulations. In response to this need, this paper proposes a new methodology for the determination of an objective image quality metric, and gives an answer to this problem through three metrics. This methodology is based on the LLAB color space for perception of color in complex images, a modification of the CIELab1976 color space. The first metric proposed is a pixel by pixel metric which introduces a local distance map between two images. The second metric associates, to a pair of images, a global value. Finally, the third metric uses a recursive subdivision of the images to obtain an adaptative distance map, rougher but less expensive to compute than the first method", "id": "73", "src": "quality image metrics for synthetic images based on perceptual color . differences due to the improvement of image rendering processes , and the increasing importance of quantitative comparisons among synthetic color images , it is essential to define perceptually based metrics which enable to objectively assess the visual quality of digital simulations . in response to this need , this paper proposes a new methodology for the determination of an objective image quality metric , and gives an answer to this problem through three metrics . this methodology is based on the llab color space for perception of color in complex images , a modification of the cielab1976 color space . the first metric proposed is a pixel by pixel metric which introduces a local distance map between two images . the second metric associates , to a pair of images , a global value . finally , the third metric uses a recursive subdivision of the images to obtain an adaptative distance map , rougher but less expensive to compute than the first method"}
{"title": "Exact controllability of shells in minimal time", "abstract": "We prove an exact controllability result for thin cups using the Fourier method and recent improvements of Ingham (1936) type theorems", "id": "74", "src": "exact controllability of shells in minimal time . we prove an exact controllability result for thin cups using the fourier method and recent improvements of ingham ( <digit> ) type theorems"}
{"title": "A friction compensator for pneumatic control valves", "abstract": "A procedure that compensates for static friction (stiction) in pneumatic control valves is presented. The compensation is obtained by adding pulses to the control signal. The characteristics of the pulses are determined from the control action. The compensator is implemented in industrial controllers and control systems, and the industrial experiences show that the procedure reduces the control error during stick-slip motion significantly compared to standard control without stiction compensation", "id": "75", "src": "a friction compensator for pneumatic control valves . a procedure that compensates for static friction ( stiction ) in pneumatic control valves is presented . the compensation is obtained by adding pulses to the control signal . the characteristics of the pulses are determined from the control action . the compensator is implemented in industrial controllers and control systems , and the industrial experiences show that the procedure reduces the control error during stick slip motion significantly compared to standard control without stiction compensation"}
{"title": "Performance comparison between PID and dead-time compensating controllers", "abstract": "This paper is intended to answer the question: \"When can a simple dead-time compensator be expected to perform better than a PID?\". The performance criterion used is the integrated absolute error (IAE). It is compared for PI and PID controllers and a simple dead-time compensator (DTC) when a step load disturbance is applied at the plant input. Both stable and integrating processes are considered. For a fair comparison the controllers should provide equal robustness in some sense. Here, as a measure of robustness, the H/sub infinity / norm of the sum of the absolute values of the sensitivity function and the complementary sensitivity function is used. Performance of the DTC's is given also as a function of dead-time margin (D/sub M/)", "id": "76", "src": "performance comparison between pid and dead time compensating controllers . this paper is intended to answer the question when can a simple dead time compensator be expected to perform better than a pid . the performance criterion used is the integrated absolute error ( iae ) . it is compared for pi and pid controllers and a simple dead time compensator ( dtc ) when a step load disturbance is applied at the plant input . both stable and integrating processes are considered . for a fair comparison the controllers should provide equal robustness in some sense . here , as a measure of robustness , the h sub infinity norm of the sum of the absolute values of the sensitivity function and the complementary sensitivity function is used . performance of the dtc ' s is given also as a function of dead time margin ( d sub m )"}
{"title": "Waiting for the wave to crest [wavelength services]", "abstract": "Wavelength services have been hyped ad nauseam for years. But despite their quick turn-up time and impressive margins, such services have yet to live up to the industry's expectations. The reasons for this lukewarm reception are many, not the least of which is the confusion that still surrounds the technology, but most industry observers are still convinced that wavelength services with ultimately flourish", "id": "77", "src": "waiting for the wave to crest wavelength services . wavelength services have been hyped ad nauseam for years . but despite their quick turn up time and impressive margins , such services have yet to live up to the industry ' s expectations . the reasons for this lukewarm reception are many , not the least of which is the confusion that still surrounds the technology , but most industry observers are still convinced that wavelength services with ultimately flourish"}
{"title": "Adaptive state feedback control for a class of linear systems with unknown bounds of uncertainties", "abstract": "The problem of adaptive robust stabilization for a class of linear time-varying systems with disturbance and nonlinear uncertainties is considered. The bounds of the disturbance and uncertainties are assumed to be unknown, being even arbitrary. For such uncertain dynamical systems, the adaptive robust state feedback controller is obtained. And the resulting closed-loop systems are asymptotically stable in theory. Moreover, an adaptive robust state feedback control scheme is given. The scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. Finally, simulations show that the control scheme is effective", "id": "78", "src": "adaptive state feedback control for a class of linear systems with unknown bounds of uncertainties . the problem of adaptive robust stabilization for a class of linear time varying systems with disturbance and nonlinear uncertainties is considered . the bounds of the disturbance and uncertainties are assumed to be unknown , being even arbitrary . for such uncertain dynamical systems , the adaptive robust state feedback controller is obtained . and the resulting closed loop systems are asymptotically stable in theory . moreover , an adaptive robust state feedback control scheme is given . the scheme ensures the closed loop systems exponentially practically stable and can be used in practical engineering . finally , simulations show that the control scheme is effective"}
{"title": "Preintegration lateral inhibition enhances unsupervised learning", "abstract": "A large and influential class of neural network architectures uses postintegration lateral inhibition as a mechanism for competition. We argue that these algorithms are computationally deficient in that they fail to generate, or learn, appropriate perceptual representations under certain circumstances. An alternative neural network architecture is presented here in which nodes compete for the right to receive inputs rather than for the right to generate outputs. This form of competition, implemented through preintegration lateral inhibition, does provide appropriate coding properties and can be used to learn such representations efficiently. Furthermore, this architecture is consistent with both neuroanatomical and neuropsychological data. We thus argue that preintegration lateral inhibition has computational advantages over conventional neural network architectures while remaining equally biologically plausible", "id": "79", "src": "preintegration lateral inhibition enhances unsupervised learning . a large and influential class of neural network architectures uses postintegration lateral inhibition as a mechanism for competition . we argue that these algorithms are computationally deficient in that they fail to generate , or learn , appropriate perceptual representations under certain circumstances . an alternative neural network architecture is presented here in which nodes compete for the right to receive inputs rather than for the right to generate outputs . this form of competition , implemented through preintegration lateral inhibition , does provide appropriate coding properties and can be used to learn such representations efficiently . furthermore , this architecture is consistent with both neuroanatomical and neuropsychological data . we thus argue that preintegration lateral inhibition has computational advantages over conventional neural network architectures while remaining equally biologically plausible"}
{"title": "Generalized predictive control for non-uniformly sampled systems", "abstract": "In this paper, we study digital control systems with non-uniform updating and sampling patterns, which include multirate sampled-data systems as special cases. We derive lifted models in the state-space domain. The main obstacle for generalized predictive control (GPC) design using the lifted models is the so-called causality constraint. Taking into account this design constraint, we propose a new GPC algorithm, which results in optimal causal control laws for the non-uniformly sampled systems. The solution applies immediately to multirate sampled-data systems where rates are integer multiples of some base period", "id": "80", "src": "generalized predictive control for non uniformly sampled systems . in this paper , we study digital control systems with non uniform updating and sampling patterns , which include multirate sampled data systems as special cases . we derive lifted models in the state space domain . the main obstacle for generalized predictive control ( gpc ) design using the lifted models is the so called causality constraint . taking into account this design constraint , we propose a new gpc algorithm , which results in optimal causal control laws for the non uniformly sampled systems . the solution applies immediately to multirate sampled data systems where rates are integer multiples of some base period"}
{"title": "A simple graphic approach for observer decomposition", "abstract": "Based upon the proposition that the roles of inputs and outputs in a physical system and those in the corresponding output-injection observer do not really have to be consistent, a systematic procedure is developed in this work to properly divide a set of sparse system models and measurement models into a number of independent subsets with the help of a visual aid. Several smaller sub-observers can then be constructed accordingly to replace the original one. The size of each sub-observer may be further reduced by strategically selecting one or more appended states. These techniques are shown to be quite effective in relieving on-line computation load of the output-injection observers and also in identifying detectable sub-systems", "id": "81", "src": "a simple graphic approach for observer decomposition . based upon the proposition that the roles of inputs and outputs in a physical system and those in the corresponding output injection observer do not really have to be consistent , a systematic procedure is developed in this work to properly divide a set of sparse system models and measurement models into a number of independent subsets with the help of a visual aid . several smaller sub observers can then be constructed accordingly to replace the original one . the size of each sub observer may be further reduced by strategically selecting one or more appended states . these techniques are shown to be quite effective in relieving on line computation load of the output injection observers and also in identifying detectable sub systems"}
{"title": "A new subspace identification approach based on principal component analysis", "abstract": "Principal component analysis (PCA) has been widely used for monitoring complex industrial processes with multiple variables and diagnosing process and sensor faults. The objective of this paper is to develop a new subspace identification algorithm that gives consistent model estimates under the errors-in-variables (EIV) situation. In this paper, we propose a new subspace identification approach using principal component analysis. PCA naturally falls into the category of EIV formulation, which resembles total least squares and allows for errors in both process input and output. We propose to use PCA to determine the system observability subspace, the matrices and the system order for an EIV formulation. Standard PCA is modified with instrumental variables in order to achieve consistent estimates of the system matrices. The proposed subspace identification method is demonstrated using a simulated process and a real industrial process for model identification and order determination. For comparison the MOESP algorithm and N4SID algorithm are used as benchmarks to demonstrate the advantages of the proposed PCA based subspace model identification (SMI) algorithm", "id": "82", "src": "a new subspace identification approach based on principal component analysis . principal component analysis ( pca ) has been widely used for monitoring complex industrial processes with multiple variables and diagnosing process and sensor faults . the objective of this paper is to develop a new subspace identification algorithm that gives consistent model estimates under the errors in variables ( eiv ) situation . in this paper , we propose a new subspace identification approach using principal component analysis . pca naturally falls into the category of eiv formulation , which resembles total least squares and allows for errors in both process input and output . we propose to use pca to determine the system observability subspace , the matrices and the system order for an eiv formulation . standard pca is modified with instrumental variables in order to achieve consistent estimates of the system matrices . the proposed subspace identification method is demonstrated using a simulated process and a real industrial process for model identification and order determination . for comparison the moesp algorithm and n4sid algorithm are used as benchmarks to demonstrate the advantages of the proposed pca based subspace model identification ( smi ) algorithm"}
{"title": "Nonlinear modeling and adaptive fuzzy control of MCFC stack", "abstract": "To improve availability and performance of fuel cells, the operating temperature of the molten carbonate fuel cells (MCFC) stack should be controlled within a specified range. However, most existing models of MCFC are not ready to be applied in synthesis. In the paper, a radial basis function neural networks identification model of a MCFC stack is developed based on the input-output sampled data. An adaptive fuzzy control procedure for the temperature of the MCFC stack is also developed. The parameters of the fuzzy control system are regulated by back-propagation algorithm, and the rule database of the fuzzy system is also adaptively adjusted by the nearest-neighbor-clustering algorithm. Finally using the neural networks model of MCFC stack, the simulation results of the control algorithm are presented. The results show the effectiveness of the proposed modeling and design procedures for the MCFC stack based on neural networks identification and the novel adaptive fuzzy control", "id": "83", "src": "nonlinear modeling and adaptive fuzzy control of mcfc stack . to improve availability and performance of fuel cells , the operating temperature of the molten carbonate fuel cells ( mcfc ) stack should be controlled within a specified range . however , most existing models of mcfc are not ready to be applied in synthesis . in the paper , a radial basis function neural networks identification model of a mcfc stack is developed based on the input output sampled data . an adaptive fuzzy control procedure for the temperature of the mcfc stack is also developed . the parameters of the fuzzy control system are regulated by back propagation algorithm , and the rule database of the fuzzy system is also adaptively adjusted by the nearest neighbor clustering algorithm . finally using the neural networks model of mcfc stack , the simulation results of the control algorithm are presented . the results show the effectiveness of the proposed modeling and design procedures for the mcfc stack based on neural networks identification and the novel adaptive fuzzy control"}
{"title": "New paradigms for interactive 3D volume segmentation", "abstract": "We present a new virtual reality-based interaction metaphor for semi-automatic segmentation of medical 3D volume data. The mouse-based, manual initialization of deformable surfaces in 3D represents a major bottleneck in interactive segmentation. In our multi-modal system we enhance this process with additional sensory feedback. A 3D haptic device is used to extract the centreline of a tubular structure. Based on the obtained path a cylinder with varying diameter is generated, which in turn is used as the initial guess for a deformable surface", "id": "84", "src": "new paradigms for interactive 3d volume segmentation . we present a new virtual reality based interaction metaphor for semi automatic segmentation of medical 3d volume data . the mouse based , manual initialization of deformable surfaces in 3d represents a major bottleneck in interactive segmentation . in our multi modal system we enhance this process with additional sensory feedback . a 3d haptic device is used to extract the centreline of a tubular structure . based on the obtained path a cylinder with varying diameter is generated , which in turn is used as the initial guess for a deformable surface"}
{"title": "State-of-the-art in orthopaedic surgical navigation with a focus on medical", "abstract": "image modalities This paper presents a review of surgical navigation systems in orthopaedics and categorizes these systems according to the image modalities that are used for the visualization of surgical action. Medical images used to be an essential part of surgical education and documentation as well as diagnosis and operation planning over many years. With the recent introduction of navigation techniques in orthopaedic surgery, a new field of application has been opened. Today surgical navigation systems - also known as image-guided surgery systems - are available for various applications in orthopaedic surgery. They visualize the position and orientation of surgical instruments as graphical overlays onto a medical image of the operated anatomy on a computer monitor. Preoperative image data such as computed tomography scans or intra operatively generated images (for example, ultrasonic, endoscopic or fluoroscopic images) are suitable for this purpose. A new category of medical images termed 'surgeon-defined anatomy' has been developed that exclusively relies upon the usage of navigation technology. Points on the anatomy are digitized interactively by the surgeon and are used to build up an abstract geometrical model of the bony structures to be operated on. This technique may be used when no other image data is available or appropriate for a given application", "id": "85", "src": "state of the art in orthopaedic surgical navigation with a focus on medical . image modalities this paper presents a review of surgical navigation systems in orthopaedics and categorizes these systems according to the image modalities that are used for the visualization of surgical action . medical images used to be an essential part of surgical education and documentation as well as diagnosis and operation planning over many years . with the recent introduction of navigation techniques in orthopaedic surgery , a new field of application has been opened . today surgical navigation systems also known as image guided surgery systems are available for various applications in orthopaedic surgery . they visualize the position and orientation of surgical instruments as graphical overlays onto a medical image of the operated anatomy on a computer monitor . preoperative image data such as computed tomography scans or intra operatively generated images ( for example , ultrasonic , endoscopic or fluoroscopic images ) are suitable for this purpose . a new category of medical images termed ' surgeon defined anatomy ' has been developed that exclusively relies upon the usage of navigation technology . points on the anatomy are digitized interactively by the surgeon and are used to build up an abstract geometrical model of the bony structures to be operated on . this technique may be used when no other image data is available or appropriate for a given application"}
{"title": "Lung metastasis detection and visualization on CT images: a knowledge-based", "abstract": "method A solution to the problem of lung metastasis detection on computed tomography (CT) scans of the thorax is presented. A knowledge-based top-down approach for image interpretation is used. The method is inspired by the manner in which a radiologist and radiotherapist interpret CT images before radiotherapy is planned. A two-dimensional followed by a three-dimensional analysis is performed. The algorithm first detects the thorax contour, the lungs and the ribs, which further help the detection of metastases. Thus, two types of tumors are detected: nodules and metastases located at the lung extremities. A method to visualize the anatomical structures segmented is also presented. The system was tested on 20 patients (988 total images) from the Oncology Department of La Chaux-de-Fonds Hospital and the results show that the method is reliable as a computer-aided diagnostic tool for clinical purpose in an oncology department", "id": "86", "src": "lung metastasis detection and visualization on ct images a knowledge based . method a solution to the problem of lung metastasis detection on computed tomography ( ct ) scans of the thorax is presented . a knowledge based top down approach for image interpretation is used . the method is inspired by the manner in which a radiologist and radiotherapist interpret ct images before radiotherapy is planned . a two dimensional followed by a three dimensional analysis is performed . the algorithm first detects the thorax contour , the lungs and the ribs , which further help the detection of metastases . thus , two types of tumors are detected nodules and metastases located at the lung extremities . a method to visualize the anatomical structures segmented is also presented . the system was tested on <digit> patients ( <digit> total images ) from the oncology department of la chaux de fonds hospital and the results show that the method is reliable as a computer aided diagnostic tool for clinical purpose in an oncology department"}
{"title": "The creation of a high-fidelity finite element model of the kidney for use in", "abstract": "trauma research A detailed finite element model of the human kidney for trauma research has been created directly from the National Library of Medicine Visible Human Female (VHF) Project data set. An image segmentation and organ reconstruction software package has been developed and employed to transform the 2D VHF images into a 3D polygonal representation. Nonuniform rational B-spline (NURBS) surfaces were then mapped to the polygonal surfaces, and were finally utilized to create a robust 3D hexahedral finite element mesh within a commercially available meshing software. The model employs a combined viscoelastic and hyperelastic material model to successfully simulate the behaviour of biological soft tissues. The finite element model was then validated for use in biomechanical research", "id": "87", "src": "the creation of a high fidelity finite element model of the kidney for use in . trauma research a detailed finite element model of the human kidney for trauma research has been created directly from the national library of medicine visible human female ( vhf ) project data set . an image segmentation and organ reconstruction software package has been developed and employed to transform the 2d vhf images into a 3d polygonal representation . nonuniform rational b spline ( nurbs ) surfaces were then mapped to the polygonal surfaces , and were finally utilized to create a robust 3d hexahedral finite element mesh within a commercially available meshing software . the model employs a combined viscoelastic and hyperelastic material model to successfully simulate the behaviour of biological soft tissues . the finite element model was then validated for use in biomechanical research"}
{"title": "Building 3D anatomical scenes on the Web", "abstract": "We propose a new service for building user-defined 3D anatomical structures on the Web. The Web server is connected to a database storing more than 1000 3D anatomical models reconstructed from the Visible Human. Users may combine existing models as well as planar oblique slices in order to create their own structured anatomical scenes. Furthermore, they may record sequences of scene construction and visualization actions. These actions enable the server to construct high-quality video animations, downloadable by the user. Professionals and students in anatomy, medicine and related disciplines are invited to use the server and create their own anatomical scenes", "id": "88", "src": "building 3d anatomical scenes on the web . we propose a new service for building user defined 3d anatomical structures on the web . the web server is connected to a database storing more than <digit> 3d anatomical models reconstructed from the visible human . users may combine existing models as well as planar oblique slices in order to create their own structured anatomical scenes . furthermore , they may record sequences of scene construction and visualization actions . these actions enable the server to construct high quality video animations , downloadable by the user . professionals and students in anatomy , medicine and related disciplines are invited to use the server and create their own anatomical scenes"}
{"title": "A survey of interactive mesh-cutting techniques and a new method for", "abstract": "implementing generalized interactive mesh cutting using virtual tools In our experience, mesh-cutting methods can be distinguished by how their solutions address the following major issues: definition of the cut path, primitive removal and re-meshing, number of new primitives created, when re-meshing is performed, and representation of the cutting tool. Many researchers have developed schemes for interactive mesh cutting with the goals of reducing the number of new primitives created, creating new primitives with good aspect ratios, avoiding a disconnected mesh structure between primitives in the cut path, and representing the path traversed by the tool as accurately as possible. The goal of this paper is to explain how, by using a very simple framework, one can build a generalized cutting scheme. This method allows for any arbitrary cut to be made within a virtual object, and can simulate cutting surface, layered surface or tetrahedral objects using a virtual scalpel, scissors, or loop cautery tool. This method has been implemented in a real-time, haptic-rate surgical simulation system allowing arbitrary cuts to be made on high-resolution patient-specific models", "id": "89", "src": "a survey of interactive mesh cutting techniques and a new method for . implementing generalized interactive mesh cutting using virtual tools in our experience , mesh cutting methods can be distinguished by how their solutions address the following major issues definition of the cut path , primitive removal and re meshing , number of new primitives created , when re meshing is performed , and representation of the cutting tool . many researchers have developed schemes for interactive mesh cutting with the goals of reducing the number of new primitives created , creating new primitives with good aspect ratios , avoiding a disconnected mesh structure between primitives in the cut path , and representing the path traversed by the tool as accurately as possible . the goal of this paper is to explain how , by using a very simple framework , one can build a generalized cutting scheme . this method allows for any arbitrary cut to be made within a virtual object , and can simulate cutting surface , layered surface or tetrahedral objects using a virtual scalpel , scissors , or loop cautery tool . this method has been implemented in a real time , haptic rate surgical simulation system allowing arbitrary cuts to be made on high resolution patient specific models"}
{"title": "Correction to construction of panoramic image mosaics with global and local", "abstract": "alignment For original paper see ibid., vol. 36, no. 2, p. 101-30 (2000). The authors had given a method for the construction of panoramic image mosaics with global and local alignment. Unfortunately a mistake had led to an incorrect equation which whilst making little difference in many cases, for faster (and assured) convergence, the correct formulae given here should be used", "id": "90", "src": "correction to construction of panoramic image mosaics with global and local . alignment for original paper see ibid . , vol . <digit> , no . 2 , p . <digit> <digit> ( <digit> ) . the authors had given a method for the construction of panoramic image mosaics with global and local alignment . unfortunately a mistake had led to an incorrect equation which whilst making little difference in many cases , for faster ( and assured ) convergence , the correct formulae given here should be used"}
{"title": "Scale-invariant segmentation of dynamic contrast-enhanced perfusion MR images", "abstract": "with inherent scale selection Selection of the best set of scales is problematic when developing signal-driven approaches for pixel-based image segmentation. Often, different possibly conflicting criteria need to be fulfilled in order to obtain the best trade-off between uncertainty (variance) and location accuracy. The optimal set of scales depends on several factors: the noise level present in the image material, the prior distribution of the different types of segments, the class-conditional distributions associated with each type of segment as well as the actual size of the (connected) segments. We analyse, theoretically and through experiments, the possibility of using the overall and class-conditional error rates as criteria for selecting the optimal sampling of the linear and morphological scale spaces. It is shown that the overall error rate is optimized by taking the prior class distribution in the image material into account. However, a uniform (ignorant) prior distribution ensures constant class-conditional error rates. Consequently, we advocate for a uniform prior class distribution when an uncommitted, scale-invariant segmentation approach is desired. Experiments with a neural net classifier developed for segmentation of dynamic magnetic resonance (MR) images, acquired with a paramagnetic tracer, support the theoretical results. Furthermore, the experiments show that the addition of spatial features to the classifier, extracted from the linear or morphological scale spaces, improves the segmentation result compared to a signal-driven approach based solely on the dynamic MR signal. The segmentation results obtained from the two types of features are compared using two novel quality measures that characterize spatial properties of labelled images", "id": "91", "src": "scale invariant segmentation of dynamic contrast enhanced perfusion mr images . with inherent scale selection selection of the best set of scales is problematic when developing signal driven approaches for pixel based image segmentation . often , different possibly conflicting criteria need to be fulfilled in order to obtain the best trade off between uncertainty ( variance ) and location accuracy . the optimal set of scales depends on several factors the noise level present in the image material , the prior distribution of the different types of segments , the class conditional distributions associated with each type of segment as well as the actual size of the ( connected ) segments . we analyse , theoretically and through experiments , the possibility of using the overall and class conditional error rates as criteria for selecting the optimal sampling of the linear and morphological scale spaces . it is shown that the overall error rate is optimized by taking the prior class distribution in the image material into account . however , a uniform ( ignorant ) prior distribution ensures constant class conditional error rates . consequently , we advocate for a uniform prior class distribution when an uncommitted , scale invariant segmentation approach is desired . experiments with a neural net classifier developed for segmentation of dynamic magnetic resonance ( mr ) images , acquired with a paramagnetic tracer , support the theoretical results . furthermore , the experiments show that the addition of spatial features to the classifier , extracted from the linear or morphological scale spaces , improves the segmentation result compared to a signal driven approach based solely on the dynamic mr signal . the segmentation results obtained from the two types of features are compared using two novel quality measures that characterize spatial properties of labelled images"}
{"title": "Innovative phase unwrapping algorithm: hybrid approach", "abstract": "We present a novel algorithm based on a hybrid of the global and local treatment of a wrapped map. The proposed algorithm is especially effective for the unwrapping of speckle-coded interferogram contour maps. In contrast to earlier unwrapping algorithms by region, we propose a local discontinuity-restoring criterion to serve as the preprocessor or postprocessor of our hybrid algorithm, which makes the unwrapping by region much easier and more efficient. With this hybrid algorithm, a robust, stable, and especially time effective phase unwrapping can be achieved. Additionally, the criterion and limitation of this hybrid algorithm are fully described. The robustness, stability, and speed of this hybrid algorithm are also studied. The proposed algorithm can be easily upgraded with minor modifications to solve the unwrapping problem of maps with phase inconsistency. Both numerical simulation and experimental applications demonstrate the effectiveness of the proposed algorithm", "id": "92", "src": "innovative phase unwrapping algorithm hybrid approach . we present a novel algorithm based on a hybrid of the global and local treatment of a wrapped map . the proposed algorithm is especially effective for the unwrapping of speckle coded interferogram contour maps . in contrast to earlier unwrapping algorithms by region , we propose a local discontinuity restoring criterion to serve as the preprocessor or postprocessor of our hybrid algorithm , which makes the unwrapping by region much easier and more efficient . with this hybrid algorithm , a robust , stable , and especially time effective phase unwrapping can be achieved . additionally , the criterion and limitation of this hybrid algorithm are fully described . the robustness , stability , and speed of this hybrid algorithm are also studied . the proposed algorithm can be easily upgraded with minor modifications to solve the unwrapping problem of maps with phase inconsistency . both numerical simulation and experimental applications demonstrate the effectiveness of the proposed algorithm"}
{"title": "Strain contouring using Gabor filters: principle and algorithm", "abstract": "Moire interferometry is a powerful technique for high sensitivity in-plane deformation contouring. However, from an engineering viewpoint, the derivatives of displacement, i.e., strain, are the desired parameter. Thus there is a need to differentiate the displacement field. Optical and digital methods have been proposed for this differentiation. Optical methods provide contours that still need to be quantified, while digital methods suffer from drawbacks inherent in the digital differentiation process. We describe a novel approach of strain segmentation for the moire pattern using a multichannel Gabor filter. Appropriate filter design allows for user-specific segmentation, which is essentially in engineering design and analysis", "id": "93", "src": "strain contouring using gabor filters principle and algorithm . moire interferometry is a powerful technique for high sensitivity in plane deformation contouring . however , from an engineering viewpoint , the derivatives of displacement , i . e . , strain , are the desired parameter . thus there is a need to differentiate the displacement field . optical and digital methods have been proposed for this differentiation . optical methods provide contours that still need to be quantified , while digital methods suffer from drawbacks inherent in the digital differentiation process . we describe a novel approach of strain segmentation for the moire pattern using a multichannel gabor filter . appropriate filter design allows for user specific segmentation , which is essentially in engineering design and analysis"}
{"title": "Novel denoising algorithm for obtaining a superresolved position estimation", "abstract": "We present a new algorithm that uses the randomness of the noise pattern to achieve high positioning accuracy by applying a modified averaging operation. Using the suggested approach, noise sensitivity of the positioning accuracy can be significantly reduced. This new improved algorithm can improve the performances of tracking systems used for military as well as civil applications. The concept is demonstrated theoretically as well as by optical experiment", "id": "94", "src": "novel denoising algorithm for obtaining a superresolved position estimation . we present a new algorithm that uses the randomness of the noise pattern to achieve high positioning accuracy by applying a modified averaging operation . using the suggested approach , noise sensitivity of the positioning accuracy can be significantly reduced . this new improved algorithm can improve the performances of tracking systems used for military as well as civil applications . the concept is demonstrated theoretically as well as by optical experiment"}
{"title": "Adaptive filtering for noise reduction in hue saturation intensity color space", "abstract": "Even though the hue saturation intensity (HSI) color model has been widely used in color image processing and analysis, the conversion formulas from the RGB color model to HSI are nonlinear and complicated in comparison with the conversion formulas of other color models. When an RGB image is degraded by random Gaussian noise, this nonlinearity leads to a nonuniform noise distribution in HSI, making accurate image analysis more difficult. We have analyzed the noise characteristics of the HSI color model and developed an adaptive spatial filtering method to reduce the magnitude of noise and the nonuniformity of noise variance in the HSI color space. With this adaptive filtering method, the filter kernel for each pixel is dynamically adjusted, depending on the values of intensity and saturation. In our experiments we have filtered the saturation and hue components and generated edge maps from color gradients. We have found that by using the adaptive filtering method, the minimum error rate in edge detection improves by approximately 15%", "id": "95", "src": "adaptive filtering for noise reduction in hue saturation intensity color space . even though the hue saturation intensity ( hsi ) color model has been widely used in color image processing and analysis , the conversion formulas from the rgb color model to hsi are nonlinear and complicated in comparison with the conversion formulas of other color models . when an rgb image is degraded by random gaussian noise , this nonlinearity leads to a nonuniform noise distribution in hsi , making accurate image analysis more difficult . we have analyzed the noise characteristics of the hsi color model and developed an adaptive spatial filtering method to reduce the magnitude of noise and the nonuniformity of noise variance in the hsi color space . with this adaptive filtering method , the filter kernel for each pixel is dynamically adjusted , depending on the values of intensity and saturation . in our experiments we have filtered the saturation and hue components and generated edge maps from color gradients . we have found that by using the adaptive filtering method , the minimum error rate in edge detection improves by approximately <digit>"}
{"title": "Optical recognition of three-dimensional objects with scale invariance using a", "abstract": "classical convergent correlator We present a real-time method for recognizing three-dimensional (3-D) objects with scale invariance. The 3-D information of the objects is codified in deformed fringe patterns using the Fourier transform profilometry technique and is correlated using a classical convergent correlator. The scale invariance property is achieved using two different approaches: the Mellin radial harmonic decomposition and the logarithmic radial harmonic filter. Thus, the method is invariant for changes in the scale of the 3-D target within a defined interval of scale factors. Experimental results show the utility of the proposed method", "id": "96", "src": "optical recognition of three dimensional objects with scale invariance using a . classical convergent correlator we present a real time method for recognizing three dimensional ( 3 d ) objects with scale invariance . the 3 d information of the objects is codified in deformed fringe patterns using the fourier transform profilometry technique and is correlated using a classical convergent correlator . the scale invariance property is achieved using two different approaches the mellin radial harmonic decomposition and the logarithmic radial harmonic filter . thus , the method is invariant for changes in the scale of the 3 d target within a defined interval of scale factors . experimental results show the utility of the proposed method"}
{"title": "Fully automatic algorithm for region of interest location in camera calibration", "abstract": "We present an automatic method for region of interest (ROI) location in camera calibration used in computer vision inspection. An intelligent ROI location algorithm based on the Radon transform is developed to automate the calibration process. The algorithm remains robust even if the anchor target has a notable rotation angle in the target plane. This method functions well although the anchor target is not carefully positioned. Several improvement methods are studied to avoid the algorithm's huge time/space consumption problem. The algorithm runs about 100 times faster if these improvement methods are applied. Using this method fully automatic camera calibration is achieved without human interactive ROI specification. Experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically", "id": "97", "src": "fully automatic algorithm for region of interest location in camera calibration . we present an automatic method for region of interest ( roi ) location in camera calibration used in computer vision inspection . an intelligent roi location algorithm based on the radon transform is developed to automate the calibration process . the algorithm remains robust even if the anchor target has a notable rotation angle in the target plane . this method functions well although the anchor target is not carefully positioned . several improvement methods are studied to avoid the algorithm ' s huge time space consumption problem . the algorithm runs about <digit> times faster if these improvement methods are applied . using this method fully automatic camera calibration is achieved without human interactive roi specification . experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically"}
{"title": "Autofocus system for microscope", "abstract": "A technique is developed for microscope autofocusing, which is called the eccentric light beam approach with high resolution, wide focusing range, and compact construction. The principle is described. The theoretical formula of the eccentric light beam approach deduced can be applied not only to an object lens whose objective plane is just at the focal plane, but also to an object lens whose objective plane is not at the focal plane. The experimental setup uses a semiconductor laser device as the light source. The laser beam that enters into the microscope is eccentric with the main light axis. A defocused signal is acquired by a symmetrical silicon photocell for the change of the reflected light position caused by differential amplification and processed by a microprocessor. Then the electric signal is power-amplified and drives a dc motor, which moves a fine working platform to an automatic focus of the microscope. The result of the experiments shows a +or-0.1- mu m precision of autofocusing for a range of +or-500- mu m defocusing. The system has high reliability and can meet the requirements of various accurate micro measurement systems", "id": "98", "src": "autofocus system for microscope . a technique is developed for microscope autofocusing , which is called the eccentric light beam approach with high resolution , wide focusing range , and compact construction . the principle is described . the theoretical formula of the eccentric light beam approach deduced can be applied not only to an object lens whose objective plane is just at the focal plane , but also to an object lens whose objective plane is not at the focal plane . the experimental setup uses a semiconductor laser device as the light source . the laser beam that enters into the microscope is eccentric with the main light axis . a defocused signal is acquired by a symmetrical silicon photocell for the change of the reflected light position caused by differential amplification and processed by a microprocessor . then the electric signal is power amplified and drives a dc motor , which moves a fine working platform to an automatic focus of the microscope . the result of the experiments shows a +or 0 . 1 mu m precision of autofocusing for a range of +or <digit> mu m defocusing . the system has high reliability and can meet the requirements of various accurate micro measurement systems"}
{"title": "Design and implementation of a 3-D mapping system for highly irregular shaped", "abstract": "objects with application to semiconductor manufacturing The basic technology for a robotic system is developed to automate the packing of polycrystalline silicon nuggets into fragile fused silica crucible in Czochralski (melt pulling) semiconductor wafer production. The highly irregular shapes of the nuggets and the packing constraints make this a difficult and challenging task. It requires the delicate manipulation and packing of highly irregular polycrystalline silicon nuggets into a fragile fused silica crucible. For this application, a dual optical 3-D surface mapping system that uses active laser triangulation has been developed and successfully tested. One part of the system measures the geometry profile of a nugget being packed and the other the profile of the nuggets already in the crucible. A resolution of 1 mm with 15-KHz sampling frequency is achieved. Data from the system are used by the packing algorithm, which determines optimal nugget placement. The key contribution is to describe the design and implementation of an efficient and robust 3-D imaging system to map highly irregular shaped objects using conventional components in context of real commercial manufacturing processes", "id": "99", "src": "design and implementation of a 3 d mapping system for highly irregular shaped . objects with application to semiconductor manufacturing the basic technology for a robotic system is developed to automate the packing of polycrystalline silicon nuggets into fragile fused silica crucible in czochralski ( melt pulling ) semiconductor wafer production . the highly irregular shapes of the nuggets and the packing constraints make this a difficult and challenging task . it requires the delicate manipulation and packing of highly irregular polycrystalline silicon nuggets into a fragile fused silica crucible . for this application , a dual optical 3 d surface mapping system that uses active laser triangulation has been developed and successfully tested . one part of the system measures the geometry profile of a nugget being packed and the other the profile of the nuggets already in the crucible . a resolution of 1 mm with <digit> khz sampling frequency is achieved . data from the system are used by the packing algorithm , which determines optimal nugget placement . the key contribution is to describe the design and implementation of an efficient and robust 3 d imaging system to map highly irregular shaped objects using conventional components in context of real commercial manufacturing processes"}
{"title": "Effective moving cast shadow detection for monocular color traffic image", "abstract": "sequences For an accurate scene analysis using monocular color traffic image sequences, a robust segmentation of moving vehicles from the stationary background is generally required. However, the presence of moving cast shadow may lead to an inaccurate vehicle segmentation, and as a result, may lead to further erroneous scene analysis. We propose an effective method for the detection of moving cast shadow. By observing the characteristics of cast shadow in the luminance, chrominance, gradient density, and geometry domains, a combined probability map, called a shadow confidence score (SCS), is obtained. From the edge map of the input image, each edge pixel is examined to determine whether it belongs to the vehicle region based on its neighboring SCSs. The cast shadow is identified as those regions with high SCSs, which are outside the convex hull of the selected vehicle edge pixels. The proposed method is tested on 100 vehicle images taken under different lighting conditions (sunny and cloudy), viewing angles (roadside and overhead), vehicle sizes (small, medium, and large), and colors (similar to the road and not). The results indicate that an average error rate of around 14% is obtained while the lowest error rate is around 3% for large vehicles", "id": "100", "src": "effective moving cast shadow detection for monocular color traffic image . sequences for an accurate scene analysis using monocular color traffic image sequences , a robust segmentation of moving vehicles from the stationary background is generally required . however , the presence of moving cast shadow may lead to an inaccurate vehicle segmentation , and as a result , may lead to further erroneous scene analysis . we propose an effective method for the detection of moving cast shadow . by observing the characteristics of cast shadow in the luminance , chrominance , gradient density , and geometry domains , a combined probability map , called a shadow confidence score ( scs ) , is obtained . from the edge map of the input image , each edge pixel is examined to determine whether it belongs to the vehicle region based on its neighboring scss . the cast shadow is identified as those regions with high scss , which are outside the convex hull of the selected vehicle edge pixels . the proposed method is tested on <digit> vehicle images taken under different lighting conditions ( sunny and cloudy ) , viewing angles ( roadside and overhead ) , vehicle sizes ( small , medium , and large ) , and colors ( similar to the road and not ) . the results indicate that an average error rate of around <digit> is obtained while the lowest error rate is around 3 for large vehicles"}
{"title": "Estimation of error in curvature computation on multi-scale free-form surfaces", "abstract": "A novel technique for multi-scale curvature computation on a free-form 3-D surface is presented. This is achieved by convolving local parametrisations of the surface with 2-D Gaussian filters iteratively. In our technique, semigeodesic coordinates are constructed at each vertex of the mesh. Smoothing results are shown for 3-D surfaces with different shapes indicating that surface noise is eliminated and surface details are removed gradually. A number of evolution properties of 3-D surfaces are described. Next, the surface Gaussian and mean curvature values are estimated accurately at multiple scales which are then mapped to colours and displayed directly on the surface. The performance of the technique when selecting different directions as an arbitrary direction for the geodesic at each vertex are also presented. The results indicate that the error observed for the estimation of Gaussian and mean curvatures is quite low after only one iteration. Furthermore, as the surface is smoothed iteratively, the error is further reduced. The results also show that the estimation error of Gaussian curvature is less than that of mean curvature. Our experiments demonstrate that estimation of smoothed surface curvatures are very accurate and not affected by the arbitrary direction of the first geodesic line when constructing semigeodesic coordinates. Our technique is independent of the underlying triangulation and is also more efficient than volumetric diffusion techniques since 2-D rather than 3-D convolutions are employed. Finally, the method presented here is a generalisation of the Curvature Scale Space method for 2-D contours. The CSS method has outperformed comparable techniques within the MPEG-7 evaluation framework. As a result, it has been selected for inclusion in the MPEG-7 package of standards", "id": "101", "src": "estimation of error in curvature computation on multi scale free form surfaces . a novel technique for multi scale curvature computation on a free form 3 d surface is presented . this is achieved by convolving local parametrisations of the surface with 2 d gaussian filters iteratively . in our technique , semigeodesic coordinates are constructed at each vertex of the mesh . smoothing results are shown for 3 d surfaces with different shapes indicating that surface noise is eliminated and surface details are removed gradually . a number of evolution properties of 3 d surfaces are described . next , the surface gaussian and mean curvature values are estimated accurately at multiple scales which are then mapped to colours and displayed directly on the surface . the performance of the technique when selecting different directions as an arbitrary direction for the geodesic at each vertex are also presented . the results indicate that the error observed for the estimation of gaussian and mean curvatures is quite low after only one iteration . furthermore , as the surface is smoothed iteratively , the error is further reduced . the results also show that the estimation error of gaussian curvature is less than that of mean curvature . our experiments demonstrate that estimation of smoothed surface curvatures are very accurate and not affected by the arbitrary direction of the first geodesic line when constructing semigeodesic coordinates . our technique is independent of the underlying triangulation and is also more efficient than volumetric diffusion techniques since 2 d rather than 3 d convolutions are employed . finally , the method presented here is a generalisation of the curvature scale space method for 2 d contours . the css method has outperformed comparable techniques within the mpeg 7 evaluation framework . as a result , it has been selected for inclusion in the mpeg 7 package of standards"}
{"title": "Restoration of broadband imagery steered with a liquid-crystal optical phased", "abstract": "array In many imaging applications, it is highly desirable to replace mechanical beam-steering components (i.e., mirrors and gimbals) with a nonmechanical device. One such device is a nematic liquid crystal optical phased array (LCOPA). An LCOPA can implement a blazed phase grating to steer the incident light. However, when a phase grating is used in a broadband imaging system, two adverse effects can occur. First, dispersion will cause different incident wavelengths arriving at the same angle to be steered to different output angles, causing chromatic aberrations in the image plane. Second, the device will steer energy not only to the first diffraction order, but to others as well. This multiple-order effect results in multiple copies of the scene appearing in the image plane. We describe a digital image restoration technique designed to overcome these degradations. The proposed postprocessing technique is based on a Wiener deconvolution filter. The technique, however, is applicable only to scenes containing objects with approximately constant reflectivities over the spectral region of interest. Experimental results are presented to demonstrate the effectiveness of this technique", "id": "102", "src": "restoration of broadband imagery steered with a liquid crystal optical phased . array in many imaging applications , it is highly desirable to replace mechanical beam steering components ( i . e . , mirrors and gimbals ) with a nonmechanical device . one such device is a nematic liquid crystal optical phased array ( lcopa ) . an lcopa can implement a blazed phase grating to steer the incident light . however , when a phase grating is used in a broadband imaging system , two adverse effects can occur . first , dispersion will cause different incident wavelengths arriving at the same angle to be steered to different output angles , causing chromatic aberrations in the image plane . second , the device will steer energy not only to the first diffraction order , but to others as well . this multiple order effect results in multiple copies of the scene appearing in the image plane . we describe a digital image restoration technique designed to overcome these degradations . the proposed postprocessing technique is based on a wiener deconvolution filter . the technique , however , is applicable only to scenes containing objects with approximately constant reflectivities over the spectral region of interest . experimental results are presented to demonstrate the effectiveness of this technique"}
{"title": "One-step digit-set-restricted modified signed-digit adder using an incoherent", "abstract": "correlator based on a shared content-addressable memory An efficient one-step digit-set-restricted modified signed-digit (MSD) adder based on symbolic substitution is presented. In this technique, carry propagation is avoided by introducing reference digits to restrict the intermediate carry and sum digits to {1,0} and {0,1}, respectively. The proposed technique requires significantly fewer minterms and simplifies system complexity compared to the reported one-step MSD addition techniques. An incoherent correlator based on an optoelectronic shared content-addressable memory processor is suggested to perform the addition operation. In this technique, only one set of minterms needs to be stored, independent of the operand length", "id": "103", "src": "one step digit set restricted modified signed digit adder using an incoherent . correlator based on a shared content addressable memory an efficient one step digit set restricted modified signed digit ( msd ) adder based on symbolic substitution is presented . in this technique , carry propagation is avoided by introducing reference digits to restrict the intermediate carry and sum digits to 1 , 0 and 0 , 1 , respectively . the proposed technique requires significantly fewer minterms and simplifies system complexity compared to the reported one step msd addition techniques . an incoherent correlator based on an optoelectronic shared content addressable memory processor is suggested to perform the addition operation . in this technique , only one set of minterms needs to be stored , independent of the operand length"}
{"title": "Two-step integral imaging for orthoscopic three-dimensional imaging with", "abstract": "improved viewing resolution We present a two-step integral imaging system to obtain 3-D orthoscopic real images. By adopting a nonstationary micro-optics technique, we demonstrate experimentally the potential usefulness of two-step integral imaging", "id": "104", "src": "two step integral imaging for orthoscopic three dimensional imaging with . improved viewing resolution we present a two step integral imaging system to obtain 3 d orthoscopic real images . by adopting a nonstationary micro optics technique , we demonstrate experimentally the potential usefulness of two step integral imaging"}
{"title": "Diffraction limit for a circular mask with a periodic rectangular apertures", "abstract": "array A mask with periodic apertures imaging system is adopted very widely and plays a leading role in modern technology for uses such as pinhole cameras, coded imaging systems, optical information processing, etc. because of its high resolution, its infinite depth of focus, and its usefulness over a broad frequency spectra ranging from visible light to X-rays and gamma rays. While the masks with periodic apertures investigated in the literature are limited only to far-field diffraction, they do not take the shift of apertures within the mask into consideration. Therefore the derivation of the far-field diffraction for a single aperture cannot be applied to a mask with periodic apertures. The far-field diffraction formula modified for a multiaperture mask has been proposed in the past, the analysis remains too complicated to offer some practical guidance for mask design. We study a circular mask with periodic rectangular apertures and develop an easier way to interpret it. First, the near-field diffraction intensity of a circular aperture is calculated by means of Lommel's function. Then the convolution of the circular mask diffraction with periodic rectangular apertures is put together, and we can present a simple mathematical tool to analyze the mask properties including the intensity distribution, blurring aberration, and the criterion of defining the far- or near-field diffraction. This concept can also be expanded to analyze different types of masks with the arbitrarily shaped apertures", "id": "105", "src": "diffraction limit for a circular mask with a periodic rectangular apertures . array a mask with periodic apertures imaging system is adopted very widely and plays a leading role in modern technology for uses such as pinhole cameras , coded imaging systems , optical information processing , etc . because of its high resolution , its infinite depth of focus , and its usefulness over a broad frequency spectra ranging from visible light to x rays and gamma rays . while the masks with periodic apertures investigated in the literature are limited only to far field diffraction , they do not take the shift of apertures within the mask into consideration . therefore the derivation of the far field diffraction for a single aperture cannot be applied to a mask with periodic apertures . the far field diffraction formula modified for a multiaperture mask has been proposed in the past , the analysis remains too complicated to offer some practical guidance for mask design . we study a circular mask with periodic rectangular apertures and develop an easier way to interpret it . first , the near field diffraction intensity of a circular aperture is calculated by means of lommel ' s function . then the convolution of the circular mask diffraction with periodic rectangular apertures is put together , and we can present a simple mathematical tool to analyze the mask properties including the intensity distribution , blurring aberration , and the criterion of defining the far or near field diffraction . this concept can also be expanded to analyze different types of masks with the arbitrarily shaped apertures"}
{"title": "Binocular model for figure-ground segmentation in translucent and occluding", "abstract": "images A Fourier-based solution to the problem of figure-ground segmentation in short baseline binocular image pairs is presented. Each image is modeled as an additive composite of two component images that exhibit a spatial shift due to the binocular parallax. The segmentation is accomplished by decoupling each Fourier component in one of the resultant additive images into its two constituent phasors, allocating each to its appropriate object-specific spectrum, and then reconstructing the foreground and background using the inverse Fourier transform. It is shown that the foreground and background shifts can be computed from the differences of the magnitudes and phases of the Fourier transform of the binocular image pair. While the model is based on translucent objects, it also works with occluding objects", "id": "106", "src": "binocular model for figure ground segmentation in translucent and occluding . images a fourier based solution to the problem of figure ground segmentation in short baseline binocular image pairs is presented . each image is modeled as an additive composite of two component images that exhibit a spatial shift due to the binocular parallax . the segmentation is accomplished by decoupling each fourier component in one of the resultant additive images into its two constituent phasors , allocating each to its appropriate object specific spectrum , and then reconstructing the foreground and background using the inverse fourier transform . it is shown that the foreground and background shifts can be computed from the differences of the magnitudes and phases of the fourier transform of the binocular image pair . while the model is based on translucent objects , it also works with occluding objects"}
{"title": "Multispectral color image capture using a liquid crystal tunable filter", "abstract": "We describe the experimental setup of a multispectral color image acquisition system consisting of a professional monochrome CCD camera and a tunable filter in which the spectral transmittance can be controlled electronically. We perform a spectral characterization of the acquisition system taking into account the acquisition noise. To convert the camera output signals to device-independent color data, two main approaches are proposed and evaluated. One consists in applying regression methods to convert from the K camera outputs to a device-independent color space such as CIEXYZ or CIELAB. Another method is based on a spectral model of the acquisition system. By inverting the model using a principal eigenvector approach, we estimate the spectral reflectance of each pixel of the imaged surface", "id": "107", "src": "multispectral color image capture using a liquid crystal tunable filter . we describe the experimental setup of a multispectral color image acquisition system consisting of a professional monochrome ccd camera and a tunable filter in which the spectral transmittance can be controlled electronically . we perform a spectral characterization of the acquisition system taking into account the acquisition noise . to convert the camera output signals to device independent color data , two main approaches are proposed and evaluated . one consists in applying regression methods to convert from the k camera outputs to a device independent color space such as ciexyz or cielab . another method is based on a spectral model of the acquisition system . by inverting the model using a principal eigenvector approach , we estimate the spectral reflectance of each pixel of the imaged surface"}
{"title": "Iterative regularized least-mean mixed-norm image restoration", "abstract": "We develop a regularized mixed-norm image restoration algorithm to deal with various types of noise. A mixed-norm functional is introduced, which combines the least mean square (LMS) and the least mean fourth (LMF) functionals, as well as a smoothing functional. Two regularization parameters are introduced: one to determine the relative importance of the LMS and LMF functionals, which is a function of the kurtosis, and another to determine the relative importance of the smoothing functional. The two parameters are chosen in such a way that the proposed functional is convex, so that a unique minimizer exists. An iterative algorithm is utilized for obtaining the solution, and its convergence is analyzed. The novelty of the proposed algorithm is that no knowledge of the noise distribution is required, and the relative contributions of the LMS, the LMF, and the smoothing functionals are adjusted based on the partially restored image. Experimental results demonstrate the effectiveness of the proposed algorithm", "id": "108", "src": "iterative regularized least mean mixed norm image restoration . we develop a regularized mixed norm image restoration algorithm to deal with various types of noise . a mixed norm functional is introduced , which combines the least mean square ( lms ) and the least mean fourth ( lmf ) functionals , as well as a smoothing functional . two regularization parameters are introduced one to determine the relative importance of the lms and lmf functionals , which is a function of the kurtosis , and another to determine the relative importance of the smoothing functional . the two parameters are chosen in such a way that the proposed functional is convex , so that a unique minimizer exists . an iterative algorithm is utilized for obtaining the solution , and its convergence is analyzed . the novelty of the proposed algorithm is that no knowledge of the noise distribution is required , and the relative contributions of the lms , the lmf , and the smoothing functionals are adjusted based on the partially restored image . experimental results demonstrate the effectiveness of the proposed algorithm"}
{"title": "Motion estimation using modified dynamic programming", "abstract": "A new method for computing precise estimates of the motion vector field of moving objects in a sequence of images is proposed. Correspondence vector-field computation is formulated as a matching optimization problem for multiple dynamic images. The proposed method is a heuristic modification of dynamic programming applied to the 2-D optimization problem. Motion-vector-field estimates using real movie images demonstrate good performance of the algorithm in terms of dynamic motion analysis", "id": "109", "src": "motion estimation using modified dynamic programming . a new method for computing precise estimates of the motion vector field of moving objects in a sequence of images is proposed . correspondence vector field computation is formulated as a matching optimization problem for multiple dynamic images . the proposed method is a heuristic modification of dynamic programming applied to the 2 d optimization problem . motion vector field estimates using real movie images demonstrate good performance of the algorithm in terms of dynamic motion analysis"}
{"title": "Centroid detection based on optical correlation", "abstract": "We propose three correlation-based methods to simultaneously detect the centroids of multiple objects in an input scene. The first method is based on the modulus of the moment function, the second method is based on squaring the moment function, and the third method works with a single intensity filter. These methods are invariant to changes in the position, orientation, and scale of the object and result in good noise-smoothing performance. We use spatial light modulators (SLMs) to directly implement the input of the image and filter information for the purpose of these approaches. We present results showing simulations from different approaches and provide comparisons between optical-correlation- and digital-moment-based methods. Experimental results corresponding to an optical correlator using SLMs for the centroid detection are also presented", "id": "110", "src": "centroid detection based on optical correlation . we propose three correlation based methods to simultaneously detect the centroids of multiple objects in an input scene . the first method is based on the modulus of the moment function , the second method is based on squaring the moment function , and the third method works with a single intensity filter . these methods are invariant to changes in the position , orientation , and scale of the object and result in good noise smoothing performance . we use spatial light modulators ( slms ) to directly implement the input of the image and filter information for the purpose of these approaches . we present results showing simulations from different approaches and provide comparisons between optical correlation and digital moment based methods . experimental results corresponding to an optical correlator using slms for the centroid detection are also presented"}
{"title": "Block truncation image bit plane coding", "abstract": "Block truncation coding (BTC) is a successful image compression technique due to its simple and fast computational burden. The bit rate is fixed to 2.0 bits/pixel, whose performance is moderate in terms of compression ratio compared to other compression schemes such as discrete cosine transform (DCT), vector quantization (VQ), wavelet transform coding (WTC), etc. Two kinds of overheads are required for BTC coding: bit plane and quantization values, respectively. A new technique is presented to reduce the bit plane overhead. Conventional bit plane overhead is 1.0 bits/pixel; we decrease it to 0.734 bits/pixel while maintaining the same decoded quality as absolute moment BTC (AMBTC) does for the \"Lena\" image. Compared to other published bit plane coding strategies, the proposed method outperforms all of the existing methods", "id": "111", "src": "block truncation image bit plane coding . block truncation coding ( btc ) is a successful image compression technique due to its simple and fast computational burden . the bit rate is fixed to 2 . 0 bits pixel , whose performance is moderate in terms of compression ratio compared to other compression schemes such as discrete cosine transform ( dct ) , vector quantization ( vq ) , wavelet transform coding ( wtc ) , etc . two kinds of overheads are required for btc coding bit plane and quantization values , respectively . a new technique is presented to reduce the bit plane overhead . conventional bit plane overhead is 1 . 0 bits pixel we decrease it to 0 . <digit> bits pixel while maintaining the same decoded quality as absolute moment btc ( ambtc ) does for the lena image . compared to other published bit plane coding strategies , the proposed method outperforms all of the existing methods"}
{"title": "Plenoptic image editing", "abstract": "This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D scene. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D scene had itself been modified. The modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter a scene's plenoptic function in a physically-consistent way, thereby affecting scene appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an scene's plenoptic function from an incomplete set of camera viewpoints", "id": "112", "src": "plenoptic image editing . this paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3d scene . the distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the ( unknown ) 3d scene had itself been modified . the modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations . the approach is useful first as a power assist that enables a user to quickly modify many images by editing just a few , and second as a means for constructing and editing image based scene representations by manipulating a set of photographs . the approach works by extending operations like image painting , scissoring , and morphing so that they alter a scene ' s plenoptic function in a physically consistent way , thereby affecting scene appearance from all viewpoints simultaneously . a key element in realizing these operations is a new volumetric decomposition technique for reconstructing an scene ' s plenoptic function from an incomplete set of camera viewpoints"}
{"title": "Elimination of zero-order diffraction in digital holography", "abstract": "A simple method to suppress the zero-order diffraction in the reconstructed image of digital holography is presented. In this method, the Laplacian of a detected hologram is used instead of the hologram itself for numerical reconstruction by computing the discrete Fresnel integral. This method can significantly improve the image quality and give better resolution and higher accuracy of the reconstructed image. The main advantages of this method are its simplicity in experimental requirements and convenience in data processing", "id": "113", "src": "elimination of zero order diffraction in digital holography . a simple method to suppress the zero order diffraction in the reconstructed image of digital holography is presented . in this method , the laplacian of a detected hologram is used instead of the hologram itself for numerical reconstruction by computing the discrete fresnel integral . this method can significantly improve the image quality and give better resolution and higher accuracy of the reconstructed image . the main advantages of this method are its simplicity in experimental requirements and convenience in data processing"}
{"title": "Efficient two-level image thresholding method based on Bayesian formulation and", "abstract": "the maximum entropy principle An efficient method for two-level thresholding is proposed based on the Bayes formula and the maximum entropy principle, in which no assumptions of the image histogram are made. An alternative criterion is derived based on maximizing entropy and used for speeding up the searching algorithm. Five forms of conditional probability distributions-simple, linear, parabola concave, parabola convex, and S-function-are employed and compared to each other for optimal threshold determination. The effect of precision on optimal threshold determination is discussed and a trade-off precision epsilon =0.001 is selected experimentally. Our experiments demonstrate that the proposed method achieves a significant improvement in speed from 26 to 57 times faster than the exhaustive search method", "id": "114", "src": "efficient two level image thresholding method based on bayesian formulation and . the maximum entropy principle an efficient method for two level thresholding is proposed based on the bayes formula and the maximum entropy principle , in which no assumptions of the image histogram are made . an alternative criterion is derived based on maximizing entropy and used for speeding up the searching algorithm . five forms of conditional probability distributions simple , linear , parabola concave , parabola convex , and s function are employed and compared to each other for optimal threshold determination . the effect of precision on optimal threshold determination is discussed and a trade off precision epsilon 0 . <digit> is selected experimentally . our experiments demonstrate that the proposed method achieves a significant improvement in speed from <digit> to <digit> times faster than the exhaustive search method"}
{"title": "Adaptive digital watermarking using fuzzy logic techniques", "abstract": "Digital watermarking has been proposed for copyright protection in our digital society. We propose an adaptive digital watermarking scheme based on the human visual system model and a fuzzy logic technique. The fuzzy logic approach is employed to obtain the different strengths and lengths of a watermark by the local characteristics of the image in our proposed scheme. In our experiments, this scheme provides a more robust and imperceptible watermark", "id": "115", "src": "adaptive digital watermarking using fuzzy logic techniques . digital watermarking has been proposed for copyright protection in our digital society . we propose an adaptive digital watermarking scheme based on the human visual system model and a fuzzy logic technique . the fuzzy logic approach is employed to obtain the different strengths and lengths of a watermark by the local characteristics of the image in our proposed scheme . in our experiments , this scheme provides a more robust and imperceptible watermark"}
{"title": "Optical encoding of color three-dimensional correlation", "abstract": "Three-dimensional (3D) correlation of color images, considering the color distribution as the third dimension, has been shown to be useful for color pattern recognition tasks. Nevertheless, 3D correlation cannot be directly performed on an optical correlator, that can only process two-dimensional (2D) signals. We propose a method to encode 3D functions onto 2D ones in such a way that the Fourier transform and correlation of these signals, that can be optically performed, encode the 3D Fourier transform and correlation of the 3D signals. The theory for the encoding is given and experimental results obtained in an optical correlator are shown", "id": "116", "src": "optical encoding of color three dimensional correlation . three dimensional ( 3d ) correlation of color images , considering the color distribution as the third dimension , has been shown to be useful for color pattern recognition tasks . nevertheless , 3d correlation cannot be directly performed on an optical correlator , that can only process two dimensional ( 2d ) signals . we propose a method to encode 3d functions onto 2d ones in such a way that the fourier transform and correlation of these signals , that can be optically performed , encode the 3d fourier transform and correlation of the 3d signals . the theory for the encoding is given and experimental results obtained in an optical correlator are shown"}
{"title": "Search for efficient solutions of multi-criterion problems by target-level", "abstract": "method The target-level method is considered for solving continuous multi-criterion maximization problems. In the first step, the decision-maker specifies a target-level point (the desired criterion values); then in the set of vector evaluations we seek points that are closest to the target point in the Chebyshev metric. The vector evaluations obtained in this way are in general weakly efficient. To identify the efficient evaluations, the second step maximizes the sum of the criteria on the set generated in step 1. We prove the relationship between the evaluations and decisions obtained by the proposed procedure, on the one hand, and the efficient (weakly efficient) evaluations and decisions, on the other hand. If the Edgeworth-Pareto hull of the set of vector evaluations is convex, the set of efficient vector evaluations can be approximated by the proposed method", "id": "117", "src": "search for efficient solutions of multi criterion problems by target level . method the target level method is considered for solving continuous multi criterion maximization problems . in the first step , the decision maker specifies a target level point ( the desired criterion values ) then in the set of vector evaluations we seek points that are closest to the target point in the chebyshev metric . the vector evaluations obtained in this way are in general weakly efficient . to identify the efficient evaluations , the second step maximizes the sum of the criteria on the set generated in step 1 . we prove the relationship between the evaluations and decisions obtained by the proposed procedure , on the one hand , and the efficient ( weakly efficient ) evaluations and decisions , on the other hand . if the edgeworth pareto hull of the set of vector evaluations is convex , the set of efficient vector evaluations can be approximated by the proposed method"}
{"title": "Computer processing of data on mental impairments during the acute period of", "abstract": "concussion The article presents results of computer processing of experimental information obtained from patients during the acute period of concussion. A number of computational procedures are described", "id": "118", "src": "computer processing of data on mental impairments during the acute period of . concussion the article presents results of computer processing of experimental information obtained from patients during the acute period of concussion . a number of computational procedures are described"}
{"title": "Regularization of linear regression problems", "abstract": "The study considers robust estimation of linear regression parameters by the regularization method, the pseudoinverse method, and the Bayesian method allowing for correlations and errors in the data. Regularizing algorithms are constructed and their relationship with pseudoinversion, the Bayesian approach, and BLUE is investigated", "id": "119", "src": "regularization of linear regression problems . the study considers robust estimation of linear regression parameters by the regularization method , the pseudoinverse method , and the bayesian method allowing for correlations and errors in the data . regularizing algorithms are constructed and their relationship with pseudoinversion , the bayesian approach , and blue is investigated"}
{"title": "Choice from a three-element set: some lessons of the 2000 presidential campaign", "abstract": "in the United States We consider the behavior of four choice rules - plurality voting, approval voting, Borda count, and self-consistent choice - when applied to choose the best option from a three-element set. It is assumed that the two main options are preferred by a large majority of the voters, while the third option gets a very small number of votes and influences the election outcome only when the two main options receive a close number of votes. When used to rate the main options, Borda count and self-consistent choice contain terms that allow both for the \"strength of preferences\" of the voters and the rating of the main candidates by voters who vote for the third option. In this way, it becomes possible to determine more reliably the winner when plurality voting or approval voting produce close results", "id": "120", "src": "choice from a three element set some lessons of the <digit> presidential campaign . in the united states we consider the behavior of four choice rules plurality voting , approval voting , borda count , and self consistent choice when applied to choose the best option from a three element set . it is assumed that the two main options are preferred by a large majority of the voters , while the third option gets a very small number of votes and influences the election outcome only when the two main options receive a close number of votes . when used to rate the main options , borda count and self consistent choice contain terms that allow both for the strength of preferences of the voters and the rating of the main candidates by voters who vote for the third option . in this way , it becomes possible to determine more reliably the winner when plurality voting or approval voting produce close results"}
{"title": "An inverse problem for a model of a hierarchical structure", "abstract": "We consider the inverse problem for the identification of the coefficient in a parabolic equation. The model is applied to describe the functioning of a hierarchical structure; it is also relevant for heat-conduction theory. Unique solvability of the inverse problem is proved", "id": "121", "src": "an inverse problem for a model of a hierarchical structure . we consider the inverse problem for the identification of the coefficient in a parabolic equation . the model is applied to describe the functioning of a hierarchical structure it is also relevant for heat conduction theory . unique solvability of the inverse problem is proved"}
{"title": "Self-calibration from image derivatives", "abstract": "This study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters, where the image formation is modeled with a linear pinhole-camera model. The equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters. A set of parameters encoding the latter component is linearly related to the flow, and from these parameters the calibration can be determined. However, as for discrete motion, in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components. Geometrically, the ambiguity takes the form of a part of the rotational component being parallel to the translational component, and thus the scene can be reconstructed only up to a projective transformation. In general, for full calibration at least four successive image frames are necessary, with the 3D rotation changing between the measurements. The geometric analysis gives rise to a direct self-calibration method that avoids computation of optical flow or point correspondences and uses only normal flow measurements. New constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives, and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated. The calibration parameters are then estimated from the rotational components of several flow fields. As the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks, or as a tool for analyzing image sequences in large video databases", "id": "122", "src": "self calibration from image derivatives . this study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters , where the image formation is modeled with a linear pinhole camera model . the equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters . a set of parameters encoding the latter component is linearly related to the flow , and from these parameters the calibration can be determined . however , as for discrete motion , in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components . geometrically , the ambiguity takes the form of a part of the rotational component being parallel to the translational component , and thus the scene can be reconstructed only up to a projective transformation . in general , for full calibration at least four successive image frames are necessary , with the 3d rotation changing between the measurements . the geometric analysis gives rise to a direct self calibration method that avoids computation of optical flow or point correspondences and uses only normal flow measurements . new constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives , and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated . the calibration parameters are then estimated from the rotational components of several flow fields . as the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks , or as a tool for analyzing image sequences in large video databases"}
{"title": "Inverse problems for a mathematical model of ion exchange in a compressible ion", "abstract": "exchanger A mathematical model of ion exchange is considered, allowing for ion exchanger compression in the process of ion exchange. Two inverse problems are investigated for this model, unique solvability is proved, and numerical solution methods are proposed. The efficiency of the proposed methods is demonstrated by a numerical experiment", "id": "123", "src": "inverse problems for a mathematical model of ion exchange in a compressible ion . exchanger a mathematical model of ion exchange is considered , allowing for ion exchanger compression in the process of ion exchange . two inverse problems are investigated for this model , unique solvability is proved , and numerical solution methods are proposed . the efficiency of the proposed methods is demonstrated by a numerical experiment"}
{"title": "Application of multiprocessor systems for computation of jets", "abstract": "The article describes the implementation of methods for numerical solution of gas-dynamic problems on a wide class of multiprocessor systems, conventionally characterized as \"cluster\" systems. A standard data-transfer interface - the so-called message passing interface - is used for parallelization of application algorithms among processors. Simulation of jets escaping into a low-pressure region is chosen as a computational example", "id": "124", "src": "application of multiprocessor systems for computation of jets . the article describes the implementation of methods for numerical solution of gas dynamic problems on a wide class of multiprocessor systems , conventionally characterized as cluster systems . a standard data transfer interface the so called message passing interface is used for parallelization of application algorithms among processors . simulation of jets escaping into a low pressure region is chosen as a computational example"}
{"title": "Hybrid simulation of space plasmas: models with massless fluid representation", "abstract": "of electrons. IV. Kelvin-Helmholtz instability For pt.III. see Prikl. Mat. Informatika, MAKS Press, no. 4, p. 5-56 (2000). This is a survey of the literature on hybrid simulation of the Kelvin-Helmholtz instability. We start with a brief review of the theory: the simplest model of the instability - a transition layer in the form of a tangential discontinuity; compressibility of the medium; finite size of the velocity shear region; pressure anisotropy. We then describe the electromagnetic hybrid model (ions as particles and electrons as a massless fluid) and the main numerical schemes. We review the studies on two-dimensional and three-dimensional hybrid simulation of the process of particle mixing across the magnetopause shear layer driven by the onset of a Kelvin-Helmholtz instability. The article concludes with a survey of literature on hybrid simulation of the Kelvin-Helmholtz instability in finite-size objects: jets moving across the magnetic field in the middle of the field reversal layer; interaction between a magnetized plasma flow and a cylindrical plasma source with zero own magnetic field", "id": "125", "src": "hybrid simulation of space plasmas models with massless fluid representation . of electrons . iv . kelvin helmholtz instability for pt . iii . see prikl . mat . informatika , maks press , no . 4 , p . 5 <digit> ( <digit> ) . this is a survey of the literature on hybrid simulation of the kelvin helmholtz instability . we start with a brief review of the theory the simplest model of the instability a transition layer in the form of a tangential discontinuity compressibility of the medium finite size of the velocity shear region pressure anisotropy . we then describe the electromagnetic hybrid model ( ions as particles and electrons as a massless fluid ) and the main numerical schemes . we review the studies on two dimensional and three dimensional hybrid simulation of the process of particle mixing across the magnetopause shear layer driven by the onset of a kelvin helmholtz instability . the article concludes with a survey of literature on hybrid simulation of the kelvin helmholtz instability in finite size objects jets moving across the magnetic field in the middle of the field reversal layer interaction between a magnetized plasma flow and a cylindrical plasma source with zero own magnetic field"}
{"title": "Limits for computational electromagnetics codes imposed by computer", "abstract": "architecture The algorithmic complexity of the innermost loops that determine the complexity of algorithms in computational electromagnetics (CEM) codes are analyzed according to their operation count and the impact of the underlying computer hardware. As memory chips are much slower than arithmetic processors, codes that involve a high data movement compared to the number of arithmetic operations are executed comparatively slower. Hence, matrix-matrix multiplications are much faster than matrix-vector multiplications. It is seen that it is not sufficient to compare only the complexity, but also the actual performance of algorithms to judge on faster execution. Implications involve FDTD loops, LU factorizations, and iterative solvers for dense matrices. Run times on two reference platforms, namely an Athlon 900 MHz and an HP PA 8600 processor, verify the findings", "id": "126", "src": "limits for computational electromagnetics codes imposed by computer . architecture the algorithmic complexity of the innermost loops that determine the complexity of algorithms in computational electromagnetics ( cem ) codes are analyzed according to their operation count and the impact of the underlying computer hardware . as memory chips are much slower than arithmetic processors , codes that involve a high data movement compared to the number of arithmetic operations are executed comparatively slower . hence , matrix matrix multiplications are much faster than matrix vector multiplications . it is seen that it is not sufficient to compare only the complexity , but also the actual performance of algorithms to judge on faster execution . implications involve fdtd loops , lu factorizations , and iterative solvers for dense matrices . run times on two reference platforms , namely an athlon <digit> mhz and an hp pa <digit> processor , verify the findings"}
{"title": "Three-dimensional geometrical optics code for indoor propagation", "abstract": "This paper presents a program, GO 3D, for computing the fields of a transmitter in an indoor environment using geometrical optics. The program uses an \"image tree\" data structure to construct the images needed to compute all the rays carrying fields above a preset \"threshold\" value, no matter how many reflections are needed. The paper briefly describes the input file required to define wall construction, the floor plan, the transmitter, and the receiver locations. A case study consisting of a long corridor with a small room on one side is used to demonstrate the features of the GO 3D program", "id": "127", "src": "three dimensional geometrical optics code for indoor propagation . this paper presents a program , go 3d , for computing the fields of a transmitter in an indoor environment using geometrical optics . the program uses an image tree data structure to construct the images needed to compute all the rays carrying fields above a preset threshold value , no matter how many reflections are needed . the paper briefly describes the input file required to define wall construction , the floor plan , the transmitter , and the receiver locations . a case study consisting of a long corridor with a small room on one side is used to demonstrate the features of the go 3d program"}
{"title": "Building a better game through dynamic programming: a Flip analysis", "abstract": "Flip is a solitaire board game produced by craft woodworkers. We analyze Flip and suggest modifications to the rules to make the game more marketable. In addition to being an interesting application of dynamic programming, this case shows the use of operations research in managerial decision making", "id": "128", "src": "building a better game through dynamic programming a flip analysis . flip is a solitaire board game produced by craft woodworkers . we analyze flip and suggest modifications to the rules to make the game more marketable . in addition to being an interesting application of dynamic programming , this case shows the use of operations research in managerial decision making"}
{"title": "Designing and delivering a university course - a process (or operations)", "abstract": "management perspective With over 30 years of academic experience in both engineering and management faculties, involving trial and error experimentation in teaching as well as reading relevant literature and observing other instructors in action, the author has accumulated a number of ideas, regarding the preparation and delivery of a university course, that should be of interest to other instructors. This should be particularly the case for those individuals who have had little or no teaching experience (e.g. those whose graduate education was recently completed at research-oriented institutions providing little guidance with respect to teaching). A particular perspective is used to convey the ideas, namely one of viewing the preparation and delivery of a course as two major processes that should provide outputs or outcomes that are of value to a number of customers, in particular, students", "id": "129", "src": "designing and delivering a university course a process ( or operations ) . management perspective with over <digit> years of academic experience in both engineering and management faculties , involving trial and error experimentation in teaching as well as reading relevant literature and observing other instructors in action , the author has accumulated a number of ideas , regarding the preparation and delivery of a university course , that should be of interest to other instructors . this should be particularly the case for those individuals who have had little or no teaching experience ( e . g . those whose graduate education was recently completed at research oriented institutions providing little guidance with respect to teaching ) . a particular perspective is used to convey the ideas , namely one of viewing the preparation and delivery of a course as two major processes that should provide outputs or outcomes that are of value to a number of customers , in particular , students"}
{"title": "A generalized PERT/CPM implementation in a spreadsheet", "abstract": "This paper describes the implementation of the traditional PERT/CPM algorithm for finding the critical path in a project network in a spreadsheet. The problem is of importance due to the recent shift of attention to using the spreadsheet environment as a vehicle for delivering management science/operations research (MS/OR) techniques to end-users", "id": "130", "src": "a generalized pert cpm implementation in a spreadsheet . this paper describes the implementation of the traditional pert cpm algorithm for finding the critical path in a project network in a spreadsheet . the problem is of importance due to the recent shift of attention to using the spreadsheet environment as a vehicle for delivering management science operations research ( ms or ) techniques to end users"}
{"title": "An object-oriented version of SIMLIB (a simple simulation package)", "abstract": "This paper introduces an object-oriented version of SIMLIB (an easy-to-understand discrete-event simulation package). The object-oriented version is preferable to the original procedural language versions of SIMLIB in that it is easier to understand and teach simulation from an object point of view. A single-server queue simulation is demonstrated using the object-oriented SIMLIB", "id": "131", "src": "an object oriented version of simlib ( a simple simulation package ) . this paper introduces an object oriented version of simlib ( an easy to understand discrete event simulation package ) . the object oriented version is preferable to the original procedural language versions of simlib in that it is easier to understand and teach simulation from an object point of view . a single server queue simulation is demonstrated using the object oriented simlib"}
{"title": "The maximum possible EVPI", "abstract": "In this paper we calculate the maximum expected value of perfect information (EVPI) for any probability distribution for the states of the world. This maximum EVPI is an upper bound for the EVPI with given probabilities and thus an upper bound for any partial information about the states of the world", "id": "132", "src": "the maximum possible evpi . in this paper we calculate the maximum expected value of perfect information ( evpi ) for any probability distribution for the states of the world . this maximum evpi is an upper bound for the evpi with given probabilities and thus an upper bound for any partial information about the states of the world"}
{"title": "Geotensity: combining motion and lighting for 3D surface reconstruction", "abstract": "This paper is about automatically reconstructing the full 3D surface of an object observed in motion by a single static camera. Based on the two paradigms, structure from motion and linear intensity subspaces, we introduce the geotensity constraint that governs the relationship between four or more images of a moving object. We show that it is possible in theory to solve for 3D Lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources. The surface may or may not be textured. We then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera", "id": "133", "src": "geotensity combining motion and lighting for 3d surface reconstruction . this paper is about automatically reconstructing the full 3d surface of an object observed in motion by a single static camera . based on the two paradigms , structure from motion and linear intensity subspaces , we introduce the geotensity constraint that governs the relationship between four or more images of a moving object . we show that it is possible in theory to solve for 3d lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources . the surface may or may not be textured . we then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera"}
{"title": "Who Wants To Be A Millionaire(R): The classroom edition", "abstract": "This paper introduces a version of the internationally popular television game show Who Wants To Be A Millionaire(R) that has been created for use in the classroom using Microsoft PowerPoint(R). A suggested framework for its classroom use is presented, instructions on operating and editing the classroom version of Who Wants To Be A Millionaire(R) are provided, and sample feedback from students who have played the classroom version of Who Wants To Be A Millionaire(R) is offered", "id": "134", "src": "who wants to be a millionaire ( r ) the classroom edition . this paper introduces a version of the internationally popular television game show who wants to be a millionaire ( r ) that has been created for use in the classroom using microsoft powerpoint ( r ) . a suggested framework for its classroom use is presented , instructions on operating and editing the classroom version of who wants to be a millionaire ( r ) are provided , and sample feedback from students who have played the classroom version of who wants to be a millionaire ( r ) is offered"}
{"title": "Who wants to see a $million error?", "abstract": "Inspired by the popular television show \"Who Wants to Be a Millionaire?\", this case discusses the monetary decisions contestants face on a game consisting of 15 increasingly difficult multiple choice questions. Since the game continues as long as a contestant answers correctly, this case, at its core, is one of sequential decision analysis, amenable to analysis via stochastic dynamic programming. The case is also suitable for a course dealing with single decision analysis, allowing for discussion of utility theory and Bayesian probability revision. In developing a story line for the case, the author has sprinkled in much background material on probability and statistics. This material is placed in a historical context, illuminating some of the influential scholars involved in the development of these subjects as well as the birth of operations research and the management sciences", "id": "135", "src": "who wants to see a million error . inspired by the popular television show who wants to be a millionaire , this case discusses the monetary decisions contestants face on a game consisting of <digit> increasingly difficult multiple choice questions . since the game continues as long as a contestant answers correctly , this case , at its core , is one of sequential decision analysis , amenable to analysis via stochastic dynamic programming . the case is also suitable for a course dealing with single decision analysis , allowing for discussion of utility theory and bayesian probability revision . in developing a story line for the case , the author has sprinkled in much background material on probability and statistics . this material is placed in a historical context , illuminating some of the influential scholars involved in the development of these subjects as well as the birth of operations research and the management sciences"}
{"title": "Blitzograms - interactive histograms", "abstract": "As computers become ever faster, more and more procedures that were once viewed as iterative will continue to become instantaneous. The blitzogram is the application of this trend to histograms, which the author hopes will lead to a better tacit understanding of probability distributions among both students and managers. And this is not just an academic exercise. Commercial Monte Carlo simulation packages like @RISK and Crystal Ball, and my INSIGHT.xla are widely available", "id": "136", "src": "blitzograms interactive histograms . as computers become ever faster , more and more procedures that were once viewed as iterative will continue to become instantaneous . the blitzogram is the application of this trend to histograms , which the author hopes will lead to a better tacit understanding of probability distributions among both students and managers . and this is not just an academic exercise . commercial monte carlo simulation packages like risk and crystal ball , and my insight . xla are widely available"}
{"title": "Teaching management science with spreadsheets: From decision models to decision", "abstract": "support The 1990s were a decade of enormous change for management science (MS) educators. While the outlook at the beginning of the decade was somewhat bleak, the renaissance in MS education brought about by the use of spreadsheets as the primary delivery vehicle for quantitative modeling techniques has resulted in a much brighter future. This paper takes inventory of the current state of MS education and suggests some promising new directions in the area of decision support systems for MS educators to consider for the future", "id": "137", "src": "teaching management science with spreadsheets from decision models to decision . support the 1990s were a decade of enormous change for management science ( ms ) educators . while the outlook at the beginning of the decade was somewhat bleak , the renaissance in ms education brought about by the use of spreadsheets as the primary delivery vehicle for quantitative modeling techniques has resulted in a much brighter future . this paper takes inventory of the current state of ms education and suggests some promising new directions in the area of decision support systems for ms educators to consider for the future"}
{"title": "Teaching modeling in management science", "abstract": "This essay discusses how we can most effectively teach Management Science to students in MBA or similar programs who will be, at best, part-time practitioners of these arts. I take as a working hypothesis the radical proposition that the heart of Management Science itself is not the impressive array of tools that have been built up over the years (optimization, simulation, decision analysis, queuing, and so on) but rather the art of reasoning logically with formal models. I believe it is necessary with this group of students to teach basic modeling skills, and in fact it is only when such students have these basic skills as a foundation that they are prepared to acquire the more sophisticated skills needed to employ Management Science. In this paper I present a hierarchy of modeling skills, from numeracy skills through sophisticated Management Science skills, as a framework within which to plan courses for the occasional practitioner", "id": "138", "src": "teaching modeling in management science . this essay discusses how we can most effectively teach management science to students in mba or similar programs who will be , at best , part time practitioners of these arts . i take as a working hypothesis the radical proposition that the heart of management science itself is not the impressive array of tools that have been built up over the years ( optimization , simulation , decision analysis , queuing , and so on ) but rather the art of reasoning logically with formal models . i believe it is necessary with this group of students to teach basic modeling skills , and in fact it is only when such students have these basic skills as a foundation that they are prepared to acquire the more sophisticated skills needed to employ management science . in this paper i present a hierarchy of modeling skills , from numeracy skills through sophisticated management science skills , as a framework within which to plan courses for the occasional practitioner"}
{"title": "Causes of the decline of the business school management science course", "abstract": "The business school management science course is suffering serious decline. The traditional model- and algorithm-based course fails to meet the needs of MBA programs and students. Poor student mathematical preparation is a reality, and is not an acceptable justification for poor teaching outcomes. Management science Ph.D.s are often poorly prepared to teach in a general management program, having more experience and interest in algorithms than management. The management science profession as a whole has focused its attention on algorithms and a narrow subset of management problems for which they are most applicable. In contrast, MBA's rarely encounter problems that are suitable for straightforward application of management science tools, living instead in a world where problems are ill-defined, data is scarce, time is short, politics is dominant, and rational \"decision makers\" are non-existent. The root cause of the profession's failure to address these issues seems to be (in Russell Ackoff's words) a habit of professional introversion that caused the profession to be uninterested in what MBA's really do on the job and how management science can help them", "id": "139", "src": "causes of the decline of the business school management science course . the business school management science course is suffering serious decline . the traditional model and algorithm based course fails to meet the needs of mba programs and students . poor student mathematical preparation is a reality , and is not an acceptable justification for poor teaching outcomes . management science ph . d . s are often poorly prepared to teach in a general management program , having more experience and interest in algorithms than management . the management science profession as a whole has focused its attention on algorithms and a narrow subset of management problems for which they are most applicable . in contrast , mba ' s rarely encounter problems that are suitable for straightforward application of management science tools , living instead in a world where problems are ill defined , data is scarce , time is short , politics is dominant , and rational decision makers are non existent . the root cause of the profession ' s failure to address these issues seems to be ( in russell ackoff ' s words ) a habit of professional introversion that caused the profession to be uninterested in what mba ' s really do on the job and how management science can help them"}
{"title": "Gifts to a science academic librarian", "abstract": "Gifts, by their altruistic nature, perfectly fit into the environment of universities and academic libraries. As a university's community and general public continue to donate materials, libraries accept donations willingly, both in-kind and monetary. Eight steps of gift processing are listed in the paper. Positive and negative aspects of gift acceptance are discussed. Gifts bring value for academic libraries. Gifts can be considered additional routes to contribute to library collections without direct purchases, options to add money to the library budget, and the cement of social relationships. But, unfortunately, large donations are time-consuming, labor-intensive and costly to process. Great amounts of staff time and processing space are two main negative aspects that cause concern and put the value of gift acceptance under consideration by librarians. Some strategies in handling gifts are recommended. To be effective, academic science librarians need to approach gifts as an investment. Librarians are not to be forced by moral and public notions and should be able to make professional decisions in evaluating proposed collections", "id": "140", "src": "gifts to a science academic librarian . gifts , by their altruistic nature , perfectly fit into the environment of universities and academic libraries . as a university ' s community and general public continue to donate materials , libraries accept donations willingly , both in kind and monetary . eight steps of gift processing are listed in the paper . positive and negative aspects of gift acceptance are discussed . gifts bring value for academic libraries . gifts can be considered additional routes to contribute to library collections without direct purchases , options to add money to the library budget , and the cement of social relationships . but , unfortunately , large donations are time consuming , labor intensive and costly to process . great amounts of staff time and processing space are two main negative aspects that cause concern and put the value of gift acceptance under consideration by librarians . some strategies in handling gifts are recommended . to be effective , academic science librarians need to approach gifts as an investment . librarians are not to be forced by moral and public notions and should be able to make professional decisions in evaluating proposed collections"}
{"title": "Four factors influencing the fair market value of out-of print books. 2", "abstract": "Fot pt.1 see ibid., p.71-8 (2002). Data from the fifty-six titles examined qualitatively in the Patterson study are examined quantitatively. In addition to the four factors of edition, condition, dust jacket, and autograph that were hypothesized to influence the value of a book, four other factors for which information was available in the data were examined", "id": "141", "src": "four factors influencing the fair market value of out of print books . 2 . fot pt . 1 see ibid . , p . <digit> 8 ( <digit> ) . data from the fifty six titles examined qualitatively in the patterson study are examined quantitatively . in addition to the four factors of edition , condition , dust jacket , and autograph that were hypothesized to influence the value of a book , four other factors for which information was available in the data were examined"}
{"title": "Four factors influencing the fair market value of out-of-print books.1", "abstract": "Four factors (edition, condition, dust jacket, and autograph) that are hypothesized to influence the value of books are identified and linked to basic economic principles, which are explained. A sample of fifty-six titles is qualitatively examined to test the hypothesis", "id": "142", "src": "four factors influencing the fair market value of out of print books . 1 . four factors ( edition , condition , dust jacket , and autograph ) that are hypothesized to influence the value of books are identified and linked to basic economic principles , which are explained . a sample of fifty six titles is qualitatively examined to test the hypothesis"}
{"title": "Acquiring materials in the history of science, technology, and medicine", "abstract": "This article provides detailed advice on acquiring new, out-of-print, and rare materials in the history of science, technology, and medicine for the beginner in these fields. The focus is on the policy formation, basic reference tools, and methods of collection development and acquisitions that are the necessary basis for success in this endeavor", "id": "143", "src": "acquiring materials in the history of science , technology , and medicine . this article provides detailed advice on acquiring new , out of print , and rare materials in the history of science , technology , and medicine for the beginner in these fields . the focus is on the policy formation , basic reference tools , and methods of collection development and acquisitions that are the necessary basis for success in this endeavor"}
{"title": "Information architecture: looking ahead", "abstract": "It may be a bit strange to consider where the field of information architecture (IA) is headed. After all, many would argue that it's too new to be considered as a field at all, or that it is mislabeled, and by no means is there a widely accepted definition of what information architecture actually is. Practicing information architects probably number in the thousands, and this vibrant group is already building various forms of communal infrastructure, ranging from an IA journal and a self-organizing \"library\" of resources to a passel of local professional groups and degree-granting academic programs. So the profession has achieved a beachhead that will enable it to stabilize and perhaps even grow during these difficult times", "id": "144", "src": "information architecture looking ahead . it may be a bit strange to consider where the field of information architecture ( ia ) is headed . after all , many would argue that it ' s too new to be considered as a field at all , or that it is mislabeled , and by no means is there a widely accepted definition of what information architecture actually is . practicing information architects probably number in the thousands , and this vibrant group is already building various forms of communal infrastructure , ranging from an ia journal and a self organizing library of resources to a passel of local professional groups and degree granting academic programs . so the profession has achieved a beachhead that will enable it to stabilize and perhaps even grow during these difficult times"}
{"title": "Decisions, decisions, decisions: a tale of special collections in the small", "abstract": "academic library A case study of a special collections department in a small academic library and how its collections have been acquired and developed over the years is described. It looks at the changes that have occurred in the academic environment and what effect, if any, these changes may have had on the department and how it has adapted to them. It raises questions about development and acquisitions policies and procedures", "id": "145", "src": "decisions , decisions , decisions a tale of special collections in the small . academic library a case study of a special collections department in a small academic library and how its collections have been acquired and developed over the years is described . it looks at the changes that have occurred in the academic environment and what effect , if any , these changes may have had on the department and how it has adapted to them . it raises questions about development and acquisitions policies and procedures"}
{"title": "Acquisitions in the James Ford Bell Library", "abstract": "This article presents basic acquisitions philosophy and approaches in a noted special collection, with commentary on \"just saying no\" and on how the electronic revolution has changed the acquisition of special collections materials", "id": "146", "src": "acquisitions in the james ford bell library . this article presents basic acquisitions philosophy and approaches in a noted special collection , with commentary on just saying no and on how the electronic revolution has changed the acquisition of special collections materials"}
{"title": "Underground poetry, collecting poetry, and the librarian", "abstract": "A powerful encounter with underground poetry and its important role in poetry, literature, and culture is discussed. The acquisitions difficulties encountered in the unique publishing world of underground poetry are introduced. Strategies for acquiring underground poetry for library collections are proposed, including total immersion and local focus, with accompanying action", "id": "147", "src": "underground poetry , collecting poetry , and the librarian . a powerful encounter with underground poetry and its important role in poetry , literature , and culture is discussed . the acquisitions difficulties encountered in the unique publishing world of underground poetry are introduced . strategies for acquiring underground poetry for library collections are proposed , including total immersion and local focus , with accompanying action"}
{"title": "On emotion and bounded rationality: reply to Hanoch", "abstract": "The author refers to the comment made by Hanoch (see ibid. vol.49 (2000)) on his model of bounded rationality and the role of the Yerkes-Dodson law and emotional arousal in it. The author points out that Hanoch's comment, however, conspicuously fails to challenge - much less contradict - the central hypothesis of his paper. In addition, several of Hanoch's criticisms are based on a wrong characterization of the positions", "id": "148", "src": "on emotion and bounded rationality reply to hanoch . the author refers to the comment made by hanoch ( see ibid . vol . <digit> ( <digit> ) ) on his model of bounded rationality and the role of the yerkes dodson law and emotional arousal in it . the author points out that hanoch ' s comment , however , conspicuously fails to challenge much less contradict the central hypothesis of his paper . in addition , several of hanoch ' s criticisms are based on a wrong characterization of the positions"}
{"title": "The effects of emotions on bounded rationality: a comment on Kaufman", "abstract": "Bruce Kaufman's article (1999), \"Emotional arousal as a source of bounded rationality\", objective is to present an additional source of bounded rationality, one that is not due to cognitive constraints, but to high emotional arousal. In doing so, Kaufman is following a long tradition of thinkers who have contrasted emotion with reason, claiming, for the most part, that emotions are a violent force hindering rational thinking. This paper aims to challenge Kaufman's unidimensional idea regarding the connection between high emotional arousal and decision making", "id": "149", "src": "the effects of emotions on bounded rationality a comment on kaufman . bruce kaufman ' s article ( <digit> ) , emotional arousal as a source of bounded rationality , objective is to present an additional source of bounded rationality , one that is not due to cognitive constraints , but to high emotional arousal . in doing so , kaufman is following a long tradition of thinkers who have contrasted emotion with reason , claiming , for the most part , that emotions are a violent force hindering rational thinking . this paper aims to challenge kaufman ' s unidimensional idea regarding the connection between high emotional arousal and decision making"}
{"title": "Emotion and self-control", "abstract": "A biology-based model of choice is used to examine time-inconsistent preferences and the problem of self-control. Emotion is shown to be the biological substrate of choice, in that emotional systems assign value to 'goods' in the environment and also facilitate the learning of expectations regarding alternative options for acquiring those goods. A third major function of the emotional choice systems is motivation. Self-control is shown to be the result of a problem with the inhibition of the motive force of emotion, where this inhibition is necessary for higher level deliberation", "id": "150", "src": "emotion and self control . a biology based model of choice is used to examine time inconsistent preferences and the problem of self control . emotion is shown to be the biological substrate of choice , in that emotional systems assign value to ' goods ' in the environment and also facilitate the learning of expectations regarding alternative options for acquiring those goods . a third major function of the emotional choice systems is motivation . self control is shown to be the result of a problem with the inhibition of the motive force of emotion , where this inhibition is necessary for higher level deliberation"}
{"title": "Product and process innovations in the life cycle of an industry", "abstract": "Filson (2001) uses industry-level data on firm numbers, price, quantity and quality along with an equilibrium model of industry evolution to estimate the nature and effects of quality and cost improvements in the personal computer industry and four other new industries. This paper studies the personal computer industry in more detail and shows that the model explains some peculiar patterns that cannot be explained by previous life-cycle models. The model estimates are evaluated using historical studies of the evolution of the personal computer industry and patterns that require further model development are described", "id": "151", "src": "product and process innovations in the life cycle of an industry . filson ( <digit> ) uses industry level data on firm numbers , price , quantity and quality along with an equilibrium model of industry evolution to estimate the nature and effects of quality and cost improvements in the personal computer industry and four other new industries . this paper studies the personal computer industry in more detail and shows that the model explains some peculiar patterns that cannot be explained by previous life cycle models . the model estimates are evaluated using historical studies of the evolution of the personal computer industry and patterns that require further model development are described"}
{"title": "A comparison of the discounted utility model and hyperbolic discounting models", "abstract": "in the case of social and private intertemporal preferences for health Whilst there is substantial evidence that hyperbolic discounting models describe intertemporal preferences for monetary outcomes better than the discounted utility (DU) model, there is only very limited evidence in the context of health outcomes. This study elicits private and social intertemporal preferences for non-fatal changes in health. Specific functional forms of the DU model and three hyperbolic models are fitted. The results show that the stationarity axiom is violated, and that the hyperbolic models fit the data better than the DU model. Intertemporal preferences for private and social decisions are found to be very similar", "id": "152", "src": "a comparison of the discounted utility model and hyperbolic discounting models . in the case of social and private intertemporal preferences for health whilst there is substantial evidence that hyperbolic discounting models describe intertemporal preferences for monetary outcomes better than the discounted utility ( du ) model , there is only very limited evidence in the context of health outcomes . this study elicits private and social intertemporal preferences for non fatal changes in health . specific functional forms of the du model and three hyperbolic models are fitted . the results show that the stationarity axiom is violated , and that the hyperbolic models fit the data better than the du model . intertemporal preferences for private and social decisions are found to be very similar"}
{"title": "Modeling the labor market as an evolving institution: model ARTEMIS", "abstract": "A stylized French labor market is modeled as an endogenously evolving institution. Boundedly rational firms and individuals strive to decrease the cost or increase utility. The labor market is coordinated by a search process and decentralized setting of hiring standards, but intermediaries can speed up matching. The model reproduces the dynamics of the gross flows and spectacular changes in mobility patterns of some demographic groups when the oil crisis in the 1970's occurred, notably the sudden decline of the integration in good jobs. The internal labor markets of large firms are shown to increase unemployment if the secondary (temporary or bad) jobs do not exist", "id": "153", "src": "modeling the labor market as an evolving institution model artemis . a stylized french labor market is modeled as an endogenously evolving institution . boundedly rational firms and individuals strive to decrease the cost or increase utility . the labor market is coordinated by a search process and decentralized setting of hiring standards , but intermediaries can speed up matching . the model reproduces the dynamics of the gross flows and spectacular changes in mobility patterns of some demographic groups when the oil crisis in the <digit> ' s occurred , notably the sudden decline of the integration in good jobs . the internal labor markets of large firms are shown to increase unemployment if the secondary ( temporary or bad ) jobs do not exist"}
{"title": "The ultimate control group", "abstract": "Empirical research on the organization of firms requires that firms be classified on the basis of their control structures. This should be done in a way that can potentially be made operational. It is easy to identify the ultimate controller of a hierarchical organization, and the literature has largely focused on this case. However, many organizational structures mix hierarchy with collective choice procedures such as voting, or use circular structures under which superiors are accountable to their subordinates. The author develops some analytic machinery that can be used to map the authority structures of such organizations, and show that under mild restrictions there is a well-defined ultimate control group. The results are consistent with intuitions about the nature of control in familiar economic settings", "id": "154", "src": "the ultimate control group . empirical research on the organization of firms requires that firms be classified on the basis of their control structures . this should be done in a way that can potentially be made operational . it is easy to identify the ultimate controller of a hierarchical organization , and the literature has largely focused on this case . however , many organizational structures mix hierarchy with collective choice procedures such as voting , or use circular structures under which superiors are accountable to their subordinates . the author develops some analytic machinery that can be used to map the authority structures of such organizations , and show that under mild restrictions there is a well defined ultimate control group . the results are consistent with intuitions about the nature of control in familiar economic settings"}
{"title": "Information architecture for bilingual Web sites", "abstract": "Creating an information architecture for a bilingual Web site presents particular challenges beyond those that exist for single and multilanguage sites. This article reports work in progress on the development of a content-based bilingual Web site to facilitate the sharing of resources and information between Speech and Language Therapists. The development of the information architecture is based on a combination of two aspects: an abstract structural analysis of existing bilingual Web designs focusing on the presentation of bilingual material, and a bilingual card-sorting activity conducted with potential users. Issues for bilingual developments are discussed, and some observations are made regarding the use of card-sorting activities", "id": "155", "src": "information architecture for bilingual web sites . creating an information architecture for a bilingual web site presents particular challenges beyond those that exist for single and multilanguage sites . this article reports work in progress on the development of a content based bilingual web site to facilitate the sharing of resources and information between speech and language therapists . the development of the information architecture is based on a combination of two aspects an abstract structural analysis of existing bilingual web designs focusing on the presentation of bilingual material , and a bilingual card sorting activity conducted with potential users . issues for bilingual developments are discussed , and some observations are made regarding the use of card sorting activities"}
{"title": "Modularity in technology and organization", "abstract": "The paper is an attempt to raid both the literature on modular design and the literature on property rights to create the outlines of a modularity theory of the firm. Such a theory will look at firms, and other organizations, in terms of the partitioning of rights-understood as protected spheres of authority-among cooperating parties. It will assert that organizations reflect nonmodular structures, that is, structures in which decision rights, rights of alienation, and residual claims to income do not all reside in the same hands", "id": "156", "src": "modularity in technology and organization . the paper is an attempt to raid both the literature on modular design and the literature on property rights to create the outlines of a modularity theory of the firm . such a theory will look at firms , and other organizations , in terms of the partitioning of rights understood as protected spheres of authority among cooperating parties . it will assert that organizations reflect nonmodular structures , that is , structures in which decision rights , rights of alienation , and residual claims to income do not all reside in the same hands"}
{"title": "Designing a new urban Internet", "abstract": "The parallel between designing a Web site and the construction of a building is a familiar one, but how often do we think of the Internet as having parks and streets? It would be absurd to say that the Internet could ever take the place of real, livable communities; however, it is safe to say that the context for using the Internet is on a path of change. As the Internet evolves beyond a simple linkage of disparate Web sites and applications, the challenge for Information Architects is establishing a process by which to structure, organize, and design networked environments. The principles that guide New Urbanism can offer much insight into networked electronic environment design. At the core of every New Urbanism principle is the idea of \"wholeness\"-of making sure that neighborhoods and communities are knit together in a way that supports civic activities, economic development, efficient ecosystems, aesthetic beauty, and human interaction", "id": "157", "src": "designing a new urban internet . the parallel between designing a web site and the construction of a building is a familiar one , but how often do we think of the internet as having parks and streets it would be absurd to say that the internet could ever take the place of real , livable communities however , it is safe to say that the context for using the internet is on a path of change . as the internet evolves beyond a simple linkage of disparate web sites and applications , the challenge for information architects is establishing a process by which to structure , organize , and design networked environments . the principles that guide new urbanism can offer much insight into networked electronic environment design . at the core of every new urbanism principle is the idea of wholeness of making sure that neighborhoods and communities are knit together in a way that supports civic activities , economic development , efficient ecosystems , aesthetic beauty , and human interaction"}
{"title": "Three-dimensional optimum design of the cooling lines of injection moulds based", "abstract": "on boundary element design sensitivity analysis A three-dimensional numerical simulation using the boundary element method is proposed, which can predict the cavity temperature distributions in the cooling stage of injection moulding. Then, choosing the radii and positions of cooling lines as design variables, the boundary integral sensitivity formulations are deduced. For the optimum design of cooling lines, the squared difference between the objective temperature and temperature of the cavity is taken as the objective function. Based on the optimization techniques with design sensitivity analysis, an iterative algorithm to reach the minimum value of the objective function is introduced, which leads to the optimum design of cooling lines at the same time", "id": "158", "src": "three dimensional optimum design of the cooling lines of injection moulds based . on boundary element design sensitivity analysis a three dimensional numerical simulation using the boundary element method is proposed , which can predict the cavity temperature distributions in the cooling stage of injection moulding . then , choosing the radii and positions of cooling lines as design variables , the boundary integral sensitivity formulations are deduced . for the optimum design of cooling lines , the squared difference between the objective temperature and temperature of the cavity is taken as the objective function . based on the optimization techniques with design sensitivity analysis , an iterative algorithm to reach the minimum value of the objective function is introduced , which leads to the optimum design of cooling lines at the same time"}
{"title": "Managing safety and strategic stocks to improve materials requirements planning", "abstract": "performance This paper provides a methodology for managing safety and strategic stocks in materials requirements planning (MRP) environments to face uncertainty in market demand. A set of recommended guidelines suggest where to position, how to dimension and when to replenish both safety and strategic stocks. Trade-offs between stock positioning and dimensioning and between stock positioning and replenishment order triggering are outlined. The study reveals also that most of the decisions are system specific, so that they should be evaluated in a quantitative manner through simulation. A case study is reported, where the benefits from adopting the new proposed methodology lie in achieving the target service level even under peak demand conditions, with the value of safety stocks as a whole growing only by about 20 per cent", "id": "159", "src": "managing safety and strategic stocks to improve materials requirements planning . performance this paper provides a methodology for managing safety and strategic stocks in materials requirements planning ( mrp ) environments to face uncertainty in market demand . a set of recommended guidelines suggest where to position , how to dimension and when to replenish both safety and strategic stocks . trade offs between stock positioning and dimensioning and between stock positioning and replenishment order triggering are outlined . the study reveals also that most of the decisions are system specific , so that they should be evaluated in a quantitative manner through simulation . a case study is reported , where the benefits from adopting the new proposed methodology lie in achieving the target service level even under peak demand conditions , with the value of safety stocks as a whole growing only by about <digit> per cent"}
{"title": "Innovative manufacture of impulse turbine blades for wave energy power", "abstract": "conversion An innovative approach to the manufacture of impulse turbine blades using rapid prototyping, fused decomposition modelling (FDM), is presented. These blades were designed and manufactured by the Wave Energy Research Team (WERT) at the University of Limerick for the experimental analysis of a 0.6 m impulse turbine with fixed guide vanes for wave energy power conversion. The computer aided design/manufacture (CAD/CAM) package Pro-Engineer 2000i was used for three-dimensional solid modelling of the individual blades. A detailed finite element analysis of the blades under centrifugal loads was performed using Pro-Mechanica. based on this analysis and FDM machine capabilities, blades were redesigned. Finally, Pro-E data were transferred to an FDM machine for the manufacture of turbine blades. The objective of this paper is to present the innovative method used to design, modify and manufacture blades in a time and cost effective manner using a concurrent engineering approach", "id": "160", "src": "innovative manufacture of impulse turbine blades for wave energy power . conversion an innovative approach to the manufacture of impulse turbine blades using rapid prototyping , fused decomposition modelling ( fdm ) , is presented . these blades were designed and manufactured by the wave energy research team ( wert ) at the university of limerick for the experimental analysis of a 0 . 6 m impulse turbine with fixed guide vanes for wave energy power conversion . the computer aided design manufacture ( cad cam ) package pro engineer 2000i was used for three dimensional solid modelling of the individual blades . a detailed finite element analysis of the blades under centrifugal loads was performed using pro mechanica . based on this analysis and fdm machine capabilities , blades were redesigned . finally , pro e data were transferred to an fdm machine for the manufacture of turbine blades . the objective of this paper is to present the innovative method used to design , modify and manufacture blades in a time and cost effective manner using a concurrent engineering approach"}
{"title": "Evaluation of combined dispatching and routeing strategies for a flexible", "abstract": "manufacturing system This paper deals with the evaluation of combined dispatching and routeing strategies on the performance of a flexible manufacturing system. Three routeing policies - no alternative routings, alternative routeing dynamics and alternative routeing plans - are considered with four dispatching rules with finite buffer capacity. In addition, the effect of changing part mix ratios is also discussed. The performance measures considered are makespan, average machine utilization, average flow time and average delay at local input buffers. Simulation results indicate that the alternative routings dynamic policy gives the best results in three performance measures except for average delay at local input buffers. Further, the effect of changing part mix ratios is not significant", "id": "161", "src": "evaluation of combined dispatching and routeing strategies for a flexible . manufacturing system this paper deals with the evaluation of combined dispatching and routeing strategies on the performance of a flexible manufacturing system . three routeing policies no alternative routings , alternative routeing dynamics and alternative routeing plans are considered with four dispatching rules with finite buffer capacity . in addition , the effect of changing part mix ratios is also discussed . the performance measures considered are makespan , average machine utilization , average flow time and average delay at local input buffers . simulation results indicate that the alternative routings dynamic policy gives the best results in three performance measures except for average delay at local input buffers . further , the effect of changing part mix ratios is not significant"}
{"title": "An intelligent fuzzy decision system for a flexible manufacturing system with", "abstract": "multi-decision points This paper describes an intelligent fuzzy decision support system for real-time scheduling and dispatching of parts in a flexible manufacturing system (FMS), with alternative routing possibilities for all parts. A fuzzy logic approach is developed to improve the system performance by considering multiple performance measures and at multiple decision points. The characteristics of the system status, instead of parts, are fed back to assign priority to the parts waiting to be processed. A simulation model is developed and it is shown that the proposed intelligent fuzzy decision support system keeps all performance measures at a good level. The proposed intelligent system is a promising tool for dealing with scheduling FMSs, in contrast to traditional rules", "id": "162", "src": "an intelligent fuzzy decision system for a flexible manufacturing system with . multi decision points this paper describes an intelligent fuzzy decision support system for real time scheduling and dispatching of parts in a flexible manufacturing system ( fms ) , with alternative routing possibilities for all parts . a fuzzy logic approach is developed to improve the system performance by considering multiple performance measures and at multiple decision points . the characteristics of the system status , instead of parts , are fed back to assign priority to the parts waiting to be processed . a simulation model is developed and it is shown that the proposed intelligent fuzzy decision support system keeps all performance measures at a good level . the proposed intelligent system is a promising tool for dealing with scheduling fmss , in contrast to traditional rules"}
{"title": "A design to cost system for innovative product development", "abstract": "Presents a prototype object-oriented and rule-based system for product cost modelling and design for automation at an early design stage. The developed system comprises a computer aided design (CAD) solid modelling system, a material selection module, a knowledge-based system (KBS), a process optimization module, a design for assembly module, a cost estimation module and a user interface. Two manufacturing processes, namely machining and injection moulding processes, were considered in the developed system. The main function of the system, besides estimating the product cost, is to generate initial process planning, including the generation and selection of machining processes, their sequence and their machining parameters, and to recommend the most economical assembly technique for a product and provide design improvement suggestions based on a design feasibility technique. In addition, a feature-by-feature cost estimation report is generated using the proposed system to highlight the features of high manufacturing cost. Two case studies were used to validate the developed system", "id": "163", "src": "a design to cost system for innovative product development . presents a prototype object oriented and rule based system for product cost modelling and design for automation at an early design stage . the developed system comprises a computer aided design ( cad ) solid modelling system , a material selection module , a knowledge based system ( kbs ) , a process optimization module , a design for assembly module , a cost estimation module and a user interface . two manufacturing processes , namely machining and injection moulding processes , were considered in the developed system . the main function of the system , besides estimating the product cost , is to generate initial process planning , including the generation and selection of machining processes , their sequence and their machining parameters , and to recommend the most economical assembly technique for a product and provide design improvement suggestions based on a design feasibility technique . in addition , a feature by feature cost estimation report is generated using the proposed system to highlight the features of high manufacturing cost . two case studies were used to validate the developed system"}
{"title": "Re-examining the machining frictional boundary conditions using fractals", "abstract": "Presents experimental evidence for the existence of non-Euclidean contact geometry at the tool-chip interface in the machining of aluminium alloy, which challenges conventional assumptions. The geometry of contact at the tool rake face is modelled using fractals and a dimension is computed for its description. The variation in the fractal dimension with the cutting speed is explored", "id": "164", "src": "re examining the machining frictional boundary conditions using fractals . presents experimental evidence for the existence of non euclidean contact geometry at the tool chip interface in the machining of aluminium alloy , which challenges conventional assumptions . the geometry of contact at the tool rake face is modelled using fractals and a dimension is computed for its description . the variation in the fractal dimension with the cutting speed is explored"}
{"title": "Layer-based machining: recent development and support structure design", "abstract": "There is growing interest in additive and subtractive shaping theories that are synthesized to integrate the layered manufacturing process and material removal process. Layer-based machining has emerged as a promising method for integrated additive and subtractive shaping theory. In the paper, major layer-based machining systems are reviewed and compared according to characteristics of stock layers, numerical control machining configurations, stacking operations, input format and raw materials. Support structure, a major issue in machining-based systems which has seldom been addressed in previous research, is investigated in the paper with considerations of four situations: floating overhang, cantilever, vaulted overhang and ceiling. Except for the floating overhang where a support structure should not be overlooked, the necessity for support structures for the other three situations is determined by stress and deflection analysis. This is demonstrated by the machining of a large castle model", "id": "165", "src": "layer based machining recent development and support structure design . there is growing interest in additive and subtractive shaping theories that are synthesized to integrate the layered manufacturing process and material removal process . layer based machining has emerged as a promising method for integrated additive and subtractive shaping theory . in the paper , major layer based machining systems are reviewed and compared according to characteristics of stock layers , numerical control machining configurations , stacking operations , input format and raw materials . support structure , a major issue in machining based systems which has seldom been addressed in previous research , is investigated in the paper with considerations of four situations floating overhang , cantilever , vaulted overhang and ceiling . except for the floating overhang where a support structure should not be overlooked , the necessity for support structures for the other three situations is determined by stress and deflection analysis . this is demonstrated by the machining of a large castle model"}
{"title": "World's biggest battery helps to stabilise Alaska", "abstract": "In this paper, the author describes a battery energy storage system which is under construction to provide voltage compensation in support of Alaska's 138 kV Northern Intertie", "id": "166", "src": "world ' s biggest battery helps to stabilise alaska . in this paper , the author describes a battery energy storage system which is under construction to provide voltage compensation in support of alaska ' s <digit> kv northern intertie"}
{"title": "Information interaction: providing a framework for information architecture", "abstract": "Information interaction is the process that people use in interacting with the content of an information system. Information architecture is a blueprint and navigational aid to the content of information-rich systems. As such information architecture performs an important supporting role in information interactivity. This article elaborates on a model of information interactivity that crosses the \"no-man's land\" between user and computer articulating a model that includes user, content and system, illustrating the context for information architecture", "id": "167", "src": "information interaction providing a framework for information architecture . information interaction is the process that people use in interacting with the content of an information system . information architecture is a blueprint and navigational aid to the content of information rich systems . as such information architecture performs an important supporting role in information interactivity . this article elaborates on a model of information interactivity that crosses the no man ' s land between user and computer articulating a model that includes user , content and system , illustrating the context for information architecture"}
{"title": "All-optical logic NOR gate using two-cascaded semiconductor optical amplifiers", "abstract": "The authors present a novel all-optical logic NOR gate using two-cascaded semiconductor optical. amplifiers (SOAs) in a counterpropagating feedback configuration. This configuration accentuates the gain nonlinearity due to the mutual gain modulation of the two SOAs. The all-optical NOR gate feasibility has been demonstrated delivering an extinction ratio higher than 12 dB over a wide range of wavelength", "id": "168", "src": "all optical logic nor gate using two cascaded semiconductor optical amplifiers . the authors present a novel all optical logic nor gate using two cascaded semiconductor optical . amplifiers ( soas ) in a counterpropagating feedback configuration . this configuration accentuates the gain nonlinearity due to the mutual gain modulation of the two soas . the all optical nor gate feasibility has been demonstrated delivering an extinction ratio higher than <digit> db over a wide range of wavelength"}
{"title": "Prospects for quantitative computed tomography imaging in the presence of", "abstract": "foreign metal bodies using statistical image reconstruction X-ray computed tomography (CT) images of patients bearing metal intracavitary applicators or other metal foreign objects exhibit severe artifacts including streaks and aliasing. We have systematically evaluated via computer simulations the impact of scattered radiation, the polyenergetic spectrum, and measurement noise on the performance of three reconstruction algorithms: conventional filtered backprojection (FBP), deterministic iterative deblurring, and a new iterative algorithm, alternating minimization (AM), based on a CT detector model that includes noise, scatter, and polyenergetic spectra. Contrary to the dominant view of the literature, FBP streaking artifacts are due mostly to mismatches between FBP's simplified model of CT detector response and the physical process of signal acquisition. Artifacts on AM images are significantly mitigated as this algorithm substantially reduces detector-model mismatches. However, metal artifacts are reduced to acceptable levels only when prior knowledge of the metal object in the patient, including its pose, shape, and attenuation map, are used to constrain AM's iterations. AM image reconstruction, in combination with object-constrained CT to estimate the pose of metal objects in the patient, is a promising approach for effectively mitigating metal artifacts and making quantitative estimation of tissue attenuation coefficients a clinical possibility", "id": "169", "src": "prospects for quantitative computed tomography imaging in the presence of . foreign metal bodies using statistical image reconstruction x ray computed tomography ( ct ) images of patients bearing metal intracavitary applicators or other metal foreign objects exhibit severe artifacts including streaks and aliasing . we have systematically evaluated via computer simulations the impact of scattered radiation , the polyenergetic spectrum , and measurement noise on the performance of three reconstruction algorithms conventional filtered backprojection ( fbp ) , deterministic iterative deblurring , and a new iterative algorithm , alternating minimization ( am ) , based on a ct detector model that includes noise , scatter , and polyenergetic spectra . contrary to the dominant view of the literature , fbp streaking artifacts are due mostly to mismatches between fbp ' s simplified model of ct detector response and the physical process of signal acquisition . artifacts on am images are significantly mitigated as this algorithm substantially reduces detector model mismatches . however , metal artifacts are reduced to acceptable levels only when prior knowledge of the metal object in the patient , including its pose , shape , and attenuation map , are used to constrain am ' s iterations . am image reconstruction , in combination with object constrained ct to estimate the pose of metal objects in the patient , is a promising approach for effectively mitigating metal artifacts and making quantitative estimation of tissue attenuation coefficients a clinical possibility"}
{"title": "Matching PET and CT scans of the head and neck area: Development of method and", "abstract": "validation Positron emission tomography (PET) provides important information on tumor biology, but lacks detailed anatomical information. Our aim in the present study was to develop and validate an automatic registration method for matching PET and CT scans of the head and neck. Three difficulties in achieving this goal are (1) nonrigid motions of the neck can hamper the use of automatic ridged body transformations; (2) emission scans contain too little anatomical information to apply standard image fusion methods; and (3) no objective way exists to quantify the quality of the match results. These problems are solved as follows: accurate and reproducible positioning of the patient was achieved by using a radiotherapy treatment mask. The proposed method makes use of the transmission rather than the emission scan. To obtain sufficient (anatomical) information for matching, two bed positions for the transmission scan were included in the protocol. A mutual information-based algorithm was used as a registration technique. PET and CT data were obtained in seven patients. Each patient had two CT scans and one PET scan. The datasets were used to estimate the consistency by matching PET to CT/sub 1/, CT/sub 1/ to CT/sub 2/, and CT/sub 2/ to PET using the full circle consistency test. It was found that using our method, consistency could be obtained of 4 mm and 1.3 degrees on average. The PET voxels used for registration were 5.15 mm, so the errors compared quite favorably with the voxel size. Cropping the images (removing the scanner bed from images) did not improve the consistency of the algorithm. The transmission scan, however, could potentially be reduced to a single position using this approach. In conclusion, the represented algorithm and validation technique has several features that are attractive from both theoretical and practical point of view, it is a user-independent, automatic validation technique for matching CT and PET scans of the head and neck, which gives the opportunity to compare different image enhancements", "id": "170", "src": "matching pet and ct scans of the head and neck area development of method and . validation positron emission tomography ( pet ) provides important information on tumor biology , but lacks detailed anatomical information . our aim in the present study was to develop and validate an automatic registration method for matching pet and ct scans of the head and neck . three difficulties in achieving this goal are ( 1 ) nonrigid motions of the neck can hamper the use of automatic ridged body transformations ( 2 ) emission scans contain too little anatomical information to apply standard image fusion methods and ( 3 ) no objective way exists to quantify the quality of the match results . these problems are solved as follows accurate and reproducible positioning of the patient was achieved by using a radiotherapy treatment mask . the proposed method makes use of the transmission rather than the emission scan . to obtain sufficient ( anatomical ) information for matching , two bed positions for the transmission scan were included in the protocol . a mutual information based algorithm was used as a registration technique . pet and ct data were obtained in seven patients . each patient had two ct scans and one pet scan . the datasets were used to estimate the consistency by matching pet to ct sub 1 , ct sub 1 to ct sub 2 , and ct sub 2 to pet using the full circle consistency test . it was found that using our method , consistency could be obtained of 4 mm and 1 . 3 degrees on average . the pet voxels used for registration were 5 . <digit> mm , so the errors compared quite favorably with the voxel size . cropping the images ( removing the scanner bed from images ) did not improve the consistency of the algorithm . the transmission scan , however , could potentially be reduced to a single position using this approach . in conclusion , the represented algorithm and validation technique has several features that are attractive from both theoretical and practical point of view , it is a user independent , automatic validation technique for matching ct and pet scans of the head and neck , which gives the opportunity to compare different image enhancements"}
{"title": "Fresh tracks [food processing]", "abstract": "Bar code labels and wireless terminals linked to a centralized database accurately track meat products from receiving to customers for Farmland Foods", "id": "171", "src": "fresh tracks food processing . bar code labels and wireless terminals linked to a centralized database accurately track meat products from receiving to customers for farmland foods"}
{"title": "Statistical inference with partial prior information based on a Gauss-type", "abstract": "inequality Potter and Anderson (1983) have developed a Bayesian decision procedure requiring the specification of a class of prior distributions restricted to have a minimal probability content for a given subset of the parameter space. They do not, however, provide a method for the selection of that subset. We show how a generalization of Gauss' inequality can be used to determine the relevant parameter subset", "id": "172", "src": "statistical inference with partial prior information based on a gauss type . inequality potter and anderson ( <digit> ) have developed a bayesian decision procedure requiring the specification of a class of prior distributions restricted to have a minimal probability content for a given subset of the parameter space . they do not , however , provide a method for the selection of that subset . we show how a generalization of gauss ' inequality can be used to determine the relevant parameter subset"}
{"title": "Global stability of the attracting set of an enzyme-catalysed reaction system", "abstract": "The essential feature of enzymatic reactions is a nonlinear dependency of reaction rate on metabolite concentration taking the form of saturation kinetics. Recently, it has been shown that this feature is associated with the phenomenon of \"loss of system coordination\" (Liu, 1999). In this paper, we study a system of ordinary differential equations representing a branched biochemical system of enzyme-mediated reactions. We show that this system can become very sensitive to changes in certain maximum enzyme activities. In particular, we show that the system exhibits three distinct responses: a unique, globally-stable steady-state, large amplitude oscillations, and asymptotically unbounded solutions, with the transition between these states being almost instantaneous. It is shown that the appearance of large amplitude, stable limit cycles occurs due to a \"false\" bifurcation or canard explosion. The subsequent disappearance of limit cycles corresponds to the collapse of the domain of attraction of the attracting set for the system and occurs due to a global bifurcation in the flow, namely, a saddle connection. Subsequently, almost all nonnegative data become unbounded under the action of the dynamical system and correspond exactly to loss of system coordination. We discuss the relevance of these results to the possible consequences of modulating such systems", "id": "173", "src": "global stability of the attracting set of an enzyme catalysed reaction system . the essential feature of enzymatic reactions is a nonlinear dependency of reaction rate on metabolite concentration taking the form of saturation kinetics . recently , it has been shown that this feature is associated with the phenomenon of loss of system coordination ( liu , <digit> ) . in this paper , we study a system of ordinary differential equations representing a branched biochemical system of enzyme mediated reactions . we show that this system can become very sensitive to changes in certain maximum enzyme activities . in particular , we show that the system exhibits three distinct responses a unique , globally stable steady state , large amplitude oscillations , and asymptotically unbounded solutions , with the transition between these states being almost instantaneous . it is shown that the appearance of large amplitude , stable limit cycles occurs due to a false bifurcation or canard explosion . the subsequent disappearance of limit cycles corresponds to the collapse of the domain of attraction of the attracting set for the system and occurs due to a global bifurcation in the flow , namely , a saddle connection . subsequently , almost all nonnegative data become unbounded under the action of the dynamical system and correspond exactly to loss of system coordination . we discuss the relevance of these results to the possible consequences of modulating such systems"}
{"title": "A spatial rainfall simulator for crop production modeling in Southern Africa", "abstract": "This paper describes a methodology for simulating rainfall in dekads across a set of spatial units in areas where long-term meteorological records are available for a small number of sites only. The work forms part of a larger simulation model of the food system in a district of Zimbabwe, which includes a crop production component for yields of maize, small grains and groundnuts. Only a limited number of meteorological stations are available within or surrounding the district that have long time series of rainfall records. Preliminary analysis of rainfall data for these stations suggested that intra-seasonal temporal correlation was negligible, but that rainfall at any given station was correlated with rainfall at neighbouring stations. This spatial correlation structure can be modeled using a multivariate normal distribution consisting of 30 related variables, representing dekadly rainfall in each of the 30 wards. For each ward, log-transformed rainfall for each of the 36 dekads in the year was characterized by a mean and standard deviation, which were interpolated from surrounding meteorological stations. A covariance matrix derived from a distance measure was then used to represent the spatial correlation between wards. Sets of random numbers were then drawn from this distribution to simulate rainfall across the wards in any given dekad. Cross-validation of estimated rainfall parameters against observed parameters for the one meteorological station within the district suggests that the interpolation process works well. The methodology developed is useful in situations where long-term climatic records are scarce and where rainfall shows pronounced spatial correlation, but negligible temporal correlation", "id": "174", "src": "a spatial rainfall simulator for crop production modeling in southern africa . this paper describes a methodology for simulating rainfall in dekads across a set of spatial units in areas where long term meteorological records are available for a small number of sites only . the work forms part of a larger simulation model of the food system in a district of zimbabwe , which includes a crop production component for yields of maize , small grains and groundnuts . only a limited number of meteorological stations are available within or surrounding the district that have long time series of rainfall records . preliminary analysis of rainfall data for these stations suggested that intra seasonal temporal correlation was negligible , but that rainfall at any given station was correlated with rainfall at neighbouring stations . this spatial correlation structure can be modeled using a multivariate normal distribution consisting of <digit> related variables , representing dekadly rainfall in each of the <digit> wards . for each ward , log transformed rainfall for each of the <digit> dekads in the year was characterized by a mean and standard deviation , which were interpolated from surrounding meteorological stations . a covariance matrix derived from a distance measure was then used to represent the spatial correlation between wards . sets of random numbers were then drawn from this distribution to simulate rainfall across the wards in any given dekad . cross validation of estimated rainfall parameters against observed parameters for the one meteorological station within the district suggests that the interpolation process works well . the methodology developed is useful in situations where long term climatic records are scarce and where rainfall shows pronounced spatial correlation , but negligible temporal correlation"}
{"title": "An algorithm to generate all spanning trees with flow", "abstract": "Spanning tree enumeration in undirected graphs is an important issue and task in many problems encountered in computer network and circuit analysis. This paper discusses the spanning tree with flow for the case that there are flow requirements between each node pair. An algorithm based on minimal paths (MPs) is proposed to generate all spanning trees without flow. The proposed algorithm is a structured approach, which splits the system into structural MPs first, and also all steps in it are easy to follow", "id": "175", "src": "an algorithm to generate all spanning trees with flow . spanning tree enumeration in undirected graphs is an important issue and task in many problems encountered in computer network and circuit analysis . this paper discusses the spanning tree with flow for the case that there are flow requirements between each node pair . an algorithm based on minimal paths ( mps ) is proposed to generate all spanning trees without flow . the proposed algorithm is a structured approach , which splits the system into structural mps first , and also all steps in it are easy to follow"}
{"title": "Nonlinear systems arising from nonisothermal, non-Newtonian Hele-Shaw flows in", "abstract": "the presence of body forces and sources In this paper, we first give a formal derivation of several systems of equations for injection moulding. This is done starting from the basic equations for nonisothermal, non-Newtonian flows in a three-dimensional domain. We derive systems for both (T/sup 0/, p/sup 0/) and (T/sup 1/, p/sup 1/) in the presence of body forces and sources. We find that body forces and sources have a nonlinear effect on the systems. We also derive a nonlinear \"Darcy law\". Our formulation includes not only the pressure gradient, but also body forces and sources, which play the role of a nonlinearity. Later, we prove the existence of weak solutions to certain boundary value problems and initial-boundary value problems associated with the resulting equations for (T/sup 0/, p/sup 0/) but in a more general mathematical setting", "id": "176", "src": "nonlinear systems arising from nonisothermal , non newtonian hele shaw flows in . the presence of body forces and sources in this paper , we first give a formal derivation of several systems of equations for injection moulding . this is done starting from the basic equations for nonisothermal , non newtonian flows in a three dimensional domain . we derive systems for both ( t sup 0 , p sup 0 ) and ( t sup 1 , p sup 1 ) in the presence of body forces and sources . we find that body forces and sources have a nonlinear effect on the systems . we also derive a nonlinear darcy law . our formulation includes not only the pressure gradient , but also body forces and sources , which play the role of a nonlinearity . later , we prove the existence of weak solutions to certain boundary value problems and initial boundary value problems associated with the resulting equations for ( t sup 0 , p sup 0 ) but in a more general mathematical setting"}
{"title": "Analyzing the potential of a firm: an operations research approach", "abstract": "An approach to analyzing the potential of a firm, which is understood as the firm's ability to provide goods or (and) services to be supplied to a marketplace under restrictions imposed by a business environment in which the firm functions, is proposed. The approach is based on using linear inequalities and, generally, mixed variables in modelling this ability for a broad spectrum of industrial, transportation, agricultural, and other types of firms and allows one to formulate problems of analyzing the potential of a firm as linear programming problems or mixed programming problems with linear constraints. This approach generalizes a previous one which was proposed for a more narrow class of models, and allows one to effectively employ a widely available software for solving practical problems of the considered kind, especially for firms described by large scale models of mathematical programming", "id": "177", "src": "analyzing the potential of a firm an operations research approach . an approach to analyzing the potential of a firm , which is understood as the firm ' s ability to provide goods or ( and ) services to be supplied to a marketplace under restrictions imposed by a business environment in which the firm functions , is proposed . the approach is based on using linear inequalities and , generally , mixed variables in modelling this ability for a broad spectrum of industrial , transportation , agricultural , and other types of firms and allows one to formulate problems of analyzing the potential of a firm as linear programming problems or mixed programming problems with linear constraints . this approach generalizes a previous one which was proposed for a more narrow class of models , and allows one to effectively employ a widely available software for solving practical problems of the considered kind , especially for firms described by large scale models of mathematical programming"}
{"title": "Discrete output feedback sliding mode control of second order systems - a moving switching line approach", "abstract": "The sliding mode control systems (SMCS) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase. For second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line. In this paper, we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control. The main contributions of this work are such that we do not require to use system states as it makes use of only the output samples for designing the controller. and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase. Simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback", "id": "178", "src": "discrete output feedback sliding mode control of second order systems a moving switching line approach . the sliding mode control systems ( smcs ) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase . for second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line . in this paper , we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control . the main contributions of this work are such that we do not require to use system states as it makes use of only the output samples for designing the controller . and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase . simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback"}
{"title": "When a better interface and easy navigation aren't enough: examining the", "abstract": "information architecture in a law enforcement agency An information architecture that allows users to easily navigate through a system and quickly recover from mistakes is often defined as a highly usable system. But usability in systems design goes beyond a good interface and efficient navigation. In this article we describe two database systems in a law enforcement agency. One system is a legacy, text-based system with cumbersome navigation (RMS); the newer system is a graphical user interface with simplified navigation (CopNet). It is hypothesized that law enforcement users will evaluate CopNet higher than RMS, but experts of the older system will evaluate it higher than others will. We conducted two user studies. One study examined what users thought of RMS and CopNet, and compared RMS experts' evaluations with nonexperts. We found that all users evaluated CopNet as more effective, easier to use, and easier to navigate than RMS, and this was especially noticeable for users who were not experts with the older system. The second, follow-up study examined use behavior after CopNet was deployed some time later. The findings revealed that evaluations of CopNet were not associated with its use. If the newer system had a better interface and was easier to navigate than the older, legacy system, why were law enforcement personnel reluctant to switch? We discuss reasons why switching to a new system is difficult, especially for those who are most adept at using the older system. Implications for system design and usability are also discussed", "id": "179", "src": "when a better interface and easy navigation aren ' t enough examining the . information architecture in a law enforcement agency an information architecture that allows users to easily navigate through a system and quickly recover from mistakes is often defined as a highly usable system . but usability in systems design goes beyond a good interface and efficient navigation . in this article we describe two database systems in a law enforcement agency . one system is a legacy , text based system with cumbersome navigation ( rms ) the newer system is a graphical user interface with simplified navigation ( copnet ) . it is hypothesized that law enforcement users will evaluate copnet higher than rms , but experts of the older system will evaluate it higher than others will . we conducted two user studies . one study examined what users thought of rms and copnet , and compared rms experts ' evaluations with nonexperts . we found that all users evaluated copnet as more effective , easier to use , and easier to navigate than rms , and this was especially noticeable for users who were not experts with the older system . the second , follow up study examined use behavior after copnet was deployed some time later . the findings revealed that evaluations of copnet were not associated with its use . if the newer system had a better interface and was easier to navigate than the older , legacy system , why were law enforcement personnel reluctant to switch we discuss reasons why switching to a new system is difficult , especially for those who are most adept at using the older system . implications for system design and usability are also discussed"}
{"title": "Optimization of planning an advertising campaign of goods and services", "abstract": "A generalization of the mathematical model and operations research problems formulated on its basis, which were presented by Belenky (2001) in the framework of an approach to planning an advertising campaign of goods and services, is considered, and corresponding nonlinear programming problems with linear constraints are formulated", "id": "180", "src": "optimization of planning an advertising campaign of goods and services . a generalization of the mathematical model and operations research problems formulated on its basis , which were presented by belenky ( <digit> ) in the framework of an approach to planning an advertising campaign of goods and services , is considered , and corresponding nonlinear programming problems with linear constraints are formulated"}
{"title": "All-optical XOR gate using semiconductor optical amplifiers without additional", "abstract": "input beam The novel design of an all-optical XOR gate by using cross-gain modulation of semiconductor optical amplifiers has been suggested and demonstrated successfully at 10 Gb/s. Boolean AB and AB of the two input signals A and B have been obtained and combined to achieve the all-optical XOR gate. No additional input beam such as a clock signal or continuous wave light is used in this new design, which is required in other all-optical XOR gates", "id": "181", "src": "all optical xor gate using semiconductor optical amplifiers without additional . input beam the novel design of an all optical xor gate by using cross gain modulation of semiconductor optical amplifiers has been suggested and demonstrated successfully at <digit> gb s . boolean ab and ab of the two input signals a and b have been obtained and combined to achieve the all optical xor gate . no additional input beam such as a clock signal or continuous wave light is used in this new design , which is required in other all optical xor gates"}
{"title": "Trust in online advice", "abstract": "Many people are now influenced by the information and advice they find on the Internet, much of it of dubious quality. This article describes two studies concerned with those factors capable of influencing people's response to online advice. The first study is a qualitative account of a group of house-hunters attempting to find worthwhile information online. The second study describes a survey of more than 2,500 people who had actively sought advice over the Internet. A framework for understanding trust in online advice is proposed in which first impressions are distinguished from more detailed evaluations. Good Web design can influence the first process, but three key factors-source credibility, personalization, and predictability-are shown to predict whether people actually follow the advice given", "id": "182", "src": "trust in online advice . many people are now influenced by the information and advice they find on the internet , much of it of dubious quality . this article describes two studies concerned with those factors capable of influencing people ' s response to online advice . the first study is a qualitative account of a group of house hunters attempting to find worthwhile information online . the second study describes a survey of more than 2 , <digit> people who had actively sought advice over the internet . a framework for understanding trust in online advice is proposed in which first impressions are distinguished from more detailed evaluations . good web design can influence the first process , but three key factors source credibility , personalization , and predictability are shown to predict whether people actually follow the advice given"}
{"title": "The social impact of Internet gambling", "abstract": "Technology has always played a role in the development of gambling practices and continues to provide new market opportunities. One of the fastest growing areas is that of Internet gambling. The effect of such technologies should not be accepted uncritically, particularly as there may be areas of potential concern based on what is known about problem gambling offline. This article has three aims. First, it overviews some of the main social concerns about the rise of Internet gambling. Second, it looks at the limited research that has been carried out in this area. Third, it examines whether Internet gambling is doubly addictive, given research that suggests that the Internet can be addictive itself. It is concluded that technological developments in Internet gambling will increase the potential for problem gambling globally, but that many of the ideas and speculations outlined in this article need to be addressed further by large-scale empirical studies", "id": "183", "src": "the social impact of internet gambling . technology has always played a role in the development of gambling practices and continues to provide new market opportunities . one of the fastest growing areas is that of internet gambling . the effect of such technologies should not be accepted uncritically , particularly as there may be areas of potential concern based on what is known about problem gambling offline . this article has three aims . first , it overviews some of the main social concerns about the rise of internet gambling . second , it looks at the limited research that has been carried out in this area . third , it examines whether internet gambling is doubly addictive , given research that suggests that the internet can be addictive itself . it is concluded that technological developments in internet gambling will increase the potential for problem gambling globally , but that many of the ideas and speculations outlined in this article need to be addressed further by large scale empirical studies"}
{"title": "Computer-mediated communication and remote management: integration or", "abstract": "isolation? The use of intranets and e-mails to communicate with remote staff is increasing rapidly within organizations. For many companies this is viewed as a speedy and cost-effective way of keeping in contact with staff and ensuring their continuing commitment to company goals. This article highlights the problems experienced by staff when managers use intranets and e-mails in an inappropriate fashion for these purposes. Issues of remoteness and isolation are discussed, along with the reports of frustration and disidentification experienced. However, it will be shown that when used appropriately, communication using these technologies can facilitate shared understanding and help remote staff to view their company as alive and exciting. Theoretical aspects are highlighted and the implications of these findings are discussed", "id": "184", "src": "computer mediated communication and remote management integration or . isolation the use of intranets and e mails to communicate with remote staff is increasing rapidly within organizations . for many companies this is viewed as a speedy and cost effective way of keeping in contact with staff and ensuring their continuing commitment to company goals . this article highlights the problems experienced by staff when managers use intranets and e mails in an inappropriate fashion for these purposes . issues of remoteness and isolation are discussed , along with the reports of frustration and disidentification experienced . however , it will be shown that when used appropriately , communication using these technologies can facilitate shared understanding and help remote staff to view their company as alive and exciting . theoretical aspects are highlighted and the implications of these findings are discussed"}
{"title": "Collective action in the age of the Internet: mass communication and online", "abstract": "mobilization This article examines how the Internet transforms collective action. Current practices on the Web bear witness to thriving collective action ranging from persuasive to confrontational, individual to collective, undertakings. Even more influential than direct calls for action is the indirect mobilizing influence of the Internet's powers of mass communication, which is boosted by an antiauthoritarian ideology on the Web. Theoretically, collective action through the otherwise socially isolating computer is possible because people rely on internalized group memberships and social identities to achieve social involvement. Empirical evidence from an online survey among environmental activists and nonactivists confirms that online action is considered an equivalent alternative to offline action by activists and nonactivists alike. However, the Internet may slightly alter the motives underlying collective action and thereby alter the nature of collective action and social movements. Perhaps more fundamental is the reverse influence that successful collective action will have on the nature and function of the Internet", "id": "185", "src": "collective action in the age of the internet mass communication and online . mobilization this article examines how the internet transforms collective action . current practices on the web bear witness to thriving collective action ranging from persuasive to confrontational , individual to collective , undertakings . even more influential than direct calls for action is the indirect mobilizing influence of the internet ' s powers of mass communication , which is boosted by an antiauthoritarian ideology on the web . theoretically , collective action through the otherwise socially isolating computer is possible because people rely on internalized group memberships and social identities to achieve social involvement . empirical evidence from an online survey among environmental activists and nonactivists confirms that online action is considered an equivalent alternative to offline action by activists and nonactivists alike . however , the internet may slightly alter the motives underlying collective action and thereby alter the nature of collective action and social movements . perhaps more fundamental is the reverse influence that successful collective action will have on the nature and function of the internet"}
{"title": "Explanations for the perpetration of and reactions to deception in a virtual", "abstract": "community Cases of identity deception on the Internet are not uncommon. Several cases of a revealed identity deception have been reported in the media. The authors examine a case of deception in an online community composed primarily of information technology professionals. In this case, an established community member (DF) invented a character (Nowheremom) whom he fell in love with and who was eventually killed in a tragic accident. When other members of the community eventually began to question Nowheremom's actual identity, DF admitted that he invented her. The discussion board was flooded with reactions to DF's revelation. The authors propose several explanations for the perpetration of identity deception, including psychiatric illness, identity play, and expressions of true self. They also analyze the reactions of community members and propose three related explanations (social identity, deviance, and norm violation) to account for their reactions. It is argued that virtual communities' reactions to such threatening events provide invaluable clues for the study of group processes on the Internet", "id": "186", "src": "explanations for the perpetration of and reactions to deception in a virtual . community cases of identity deception on the internet are not uncommon . several cases of a revealed identity deception have been reported in the media . the authors examine a case of deception in an online community composed primarily of information technology professionals . in this case , an established community member ( df ) invented a character ( nowheremom ) whom he fell in love with and who was eventually killed in a tragic accident . when other members of the community eventually began to question nowheremom ' s actual identity , df admitted that he invented her . the discussion board was flooded with reactions to df ' s revelation . the authors propose several explanations for the perpetration of identity deception , including psychiatric illness , identity play , and expressions of true self . they also analyze the reactions of community members and propose three related explanations ( social identity , deviance , and norm violation ) to account for their reactions . it is argued that virtual communities ' reactions to such threatening events provide invaluable clues for the study of group processes on the internet"}
{"title": "The effects of asynchronous computer-mediated group interaction on group", "abstract": "processes This article reports a study undertaken to investigate some of the social psychological processes underlying computer-supported group discussion in natural computer-mediated contexts. Based on the concept of deindividuation, it was hypothesized that personal identifiability and group identity would be important factors that affect the perceptions and behavior of members of computer-mediated groups. The degree of personal identifiability and the strength of group identity were manipulated across groups of geographically dispersed computer users who took part in e-mail discussions during a 2-week period. The results do not support the association between deindividuation and uninhibited behavior cited in much previous research. Instead, the data provide some support for a social identity perspective of computer-mediated communication, which explains the higher levels uninhibited in identifiable computer-mediated groups. However, predictions based on social identity theory regarding group polarization and group cohesion were not supported. Possible explanations for this are discussed and further research is suggested to resolve these discrepancies", "id": "187", "src": "the effects of asynchronous computer mediated group interaction on group . processes this article reports a study undertaken to investigate some of the social psychological processes underlying computer supported group discussion in natural computer mediated contexts . based on the concept of deindividuation , it was hypothesized that personal identifiability and group identity would be important factors that affect the perceptions and behavior of members of computer mediated groups . the degree of personal identifiability and the strength of group identity were manipulated across groups of geographically dispersed computer users who took part in e mail discussions during a 2 week period . the results do not support the association between deindividuation and uninhibited behavior cited in much previous research . instead , the data provide some support for a social identity perspective of computer mediated communication , which explains the higher levels uninhibited in identifiable computer mediated groups . however , predictions based on social identity theory regarding group polarization and group cohesion were not supported . possible explanations for this are discussed and further research is suggested to resolve these discrepancies"}
{"title": "Online longitudinal survey research: viability and participation", "abstract": "This article explores the viability of conducting longitudinal survey research using the Internet in samples exposed to trauma. A questionnaire battery assessing psychological adjustment following adverse life experiences was posted online. Participants who signed up to take part in the longitudinal aspect of the study were contacted 3 and 6 months after initial participation to complete the second and third waves of the research. Issues of data screening and sample attrition rates are considered and the demographic profiles and questionnaire scores of those who did and did not take part in the study during successive time points are compared. The results demonstrate that it is possible to conduct repeated measures survey research online and that the similarity in characteristics between those who do and do not take part during successive time points mirrors that found in traditional pencil-and-paper trauma surveys", "id": "188", "src": "online longitudinal survey research viability and participation . this article explores the viability of conducting longitudinal survey research using the internet in samples exposed to trauma . a questionnaire battery assessing psychological adjustment following adverse life experiences was posted online . participants who signed up to take part in the longitudinal aspect of the study were contacted 3 and 6 months after initial participation to complete the second and third waves of the research . issues of data screening and sample attrition rates are considered and the demographic profiles and questionnaire scores of those who did and did not take part in the study during successive time points are compared . the results demonstrate that it is possible to conduct repeated measures survey research online and that the similarity in characteristics between those who do and do not take part during successive time points mirrors that found in traditional pencil and paper trauma surveys"}
{"title": "Internet-based psychological experimenting: five dos and five don'ts", "abstract": "Internet-based psychological experimenting is presented as a method that needs careful consideration of a number of issues-from potential data corruption to revealing confidential information about participants. Ten issues are grouped into five areas of actions to be taken when developing an Internet experiment (dos) and five errors to be avoided (don'ts). Dos include: (a) utilizing dropout as a dependent variable, (b) the use of dropout to detect motivational confounding, (c) placement of questions for personal information, (d) using a collection of techniques, and (e) using Internet-based tools. Don'ts are about: (a) unprotected directories, (b) public access to confidential data, (c) revealing the experiment's structure, (d) ignoring the Internet's technical variance, and (e) improper use of form elements", "id": "189", "src": "internet based psychological experimenting five dos and five don ' ts . internet based psychological experimenting is presented as a method that needs careful consideration of a number of issues from potential data corruption to revealing confidential information about participants . ten issues are grouped into five areas of actions to be taken when developing an internet experiment ( dos ) and five errors to be avoided ( don ' ts ) . dos include ( a ) utilizing dropout as a dependent variable , ( b ) the use of dropout to detect motivational confounding , ( c ) placement of questions for personal information , ( d ) using a collection of techniques , and ( e ) using internet based tools . don ' ts are about ( a ) unprotected directories , ( b ) public access to confidential data , ( c ) revealing the experiment ' s structure , ( d ) ignoring the internet ' s technical variance , and ( e ) improper use of form elements"}
{"title": "Pervasive computing goes to work: interfacing to the enterprise", "abstract": "The paperless office is an idea whose time has come, and come, and come again. To see how pervasive computing applications might bring some substance to this dream, the author spoke recently with key managers and technologists at McKesson Corporation (San Francisco), a healthcare supplier, service, and technology company with US$50 billion in sales last year, and also at AvantGo (Hayward, Calif.), a provider of mobile infrastructure software and services. For the past several years, McKesson has used mobility middleware developed by AvantGo to deploy major supply chain applications with thousands of pervasive clients and multiple servers that replace existing paper-based tracking systems. According to McKesson's managers, their system greatly reduced errors and associated costs caused by redelivery or loss of valuable products, giving McKesson a solid return on its investment", "id": "190", "src": "pervasive computing goes to work interfacing to the enterprise . the paperless office is an idea whose time has come , and come , and come again . to see how pervasive computing applications might bring some substance to this dream , the author spoke recently with key managers and technologists at mckesson corporation ( san francisco ) , a healthcare supplier , service , and technology company with us <digit> billion in sales last year , and also at avantgo ( hayward , calif . ) , a provider of mobile infrastructure software and services . for the past several years , mckesson has used mobility middleware developed by avantgo to deploy major supply chain applications with thousands of pervasive clients and multiple servers that replace existing paper based tracking systems . according to mckesson ' s managers , their system greatly reduced errors and associated costs caused by redelivery or loss of valuable products , giving mckesson a solid return on its investment"}
{"title": "Psychology and the Internet", "abstract": "This article presents an overview of the way that the Internet is being used to assist psychological research and mediate psychological practice. It shows how psychologists are using the Internet to examine the interactions between people and computers, and highlights some of the ways that this research is important to the design and development of useable and acceptable computer systems. In particular, this introduction reviews the research presented at the International Conference on Psychology and the Internet held in the United Kingdom. The final part introduces the eight articles in this special edition. The articles are representative of the breadth of research being conducted on psychology and the Internet: there are two on methodological issues, three on group processes, one on organizational implications, and two on social implications of Internet use", "id": "191", "src": "psychology and the internet . this article presents an overview of the way that the internet is being used to assist psychological research and mediate psychological practice . it shows how psychologists are using the internet to examine the interactions between people and computers , and highlights some of the ways that this research is important to the design and development of useable and acceptable computer systems . in particular , this introduction reviews the research presented at the international conference on psychology and the internet held in the united kingdom . the final part introduces the eight articles in this special edition . the articles are representative of the breadth of research being conducted on psychology and the internet there are two on methodological issues , three on group processes , one on organizational implications , and two on social implications of internet use"}
{"title": "Extended depth-of-focus imaging of chlorophyll fluorescence from intact leaves", "abstract": "Imaging dynamic changes in chlorophyll a fluorescence provides a valuable means with which to examine localised changes in photosynthetic function. Microscope-based systems provide excellent spatial resolution which allows the response of individual cells to be measured. However, such systems have a restricted depth of focus and, as leaves are inherently uneven, only a small proportion of each image at any given focal plane is in focus. In this report we describe the development of algorithms, specifically adapted for imaging chlorophyll fluorescence and photosynthetic function in living plant cells, which allow extended-focus images to be reconstructed from images taken in different focal planes. We describe how these procedures can be used to reconstruct images of chlorophyll fluorescence and calculated photosynthetic parameters, as well as producing a map of leaf topology. The robustness of this procedure is demonstrated using leaves from a number of different plant species", "id": "192", "src": "extended depth of focus imaging of chlorophyll fluorescence from intact leaves . imaging dynamic changes in chlorophyll a fluorescence provides a valuable means with which to examine localised changes in photosynthetic function . microscope based systems provide excellent spatial resolution which allows the response of individual cells to be measured . however , such systems have a restricted depth of focus and , as leaves are inherently uneven , only a small proportion of each image at any given focal plane is in focus . in this report we describe the development of algorithms , specifically adapted for imaging chlorophyll fluorescence and photosynthetic function in living plant cells , which allow extended focus images to be reconstructed from images taken in different focal planes . we describe how these procedures can be used to reconstruct images of chlorophyll fluorescence and calculated photosynthetic parameters , as well as producing a map of leaf topology . the robustness of this procedure is demonstrated using leaves from a number of different plant species"}
{"title": "Allan variance and fractal Brownian motion", "abstract": "Noise filtering is the subject of a voluminous literature in radio engineering. The methods of filtering require knowledge of the frequency response, which is usually unknown. D.W. Allan (see Proc. IEEE, vol.54, no.2, p.221-30, 1966; IEEE Trans. Instr. Measur., vol.IM-36, p.646-54, 1987) proposed a simple method of determining the interval between equally accurate observations which does without this information. In this method, the variances of the increments of noise and signal are equal, so that, in observations with a greater step, the variations caused by noise are smaller than those caused by the signal. This method is the standard accepted by the USA metrology community. The present paper is devoted to a statistical analysis of the Allan method and acquisition of additional information", "id": "193", "src": "allan variance and fractal brownian motion . noise filtering is the subject of a voluminous literature in radio engineering . the methods of filtering require knowledge of the frequency response , which is usually unknown . d . w . allan ( see proc . ieee , vol . <digit> , no . 2 , p . <digit> <digit> , <digit> ieee trans . instr . measur . , vol . im <digit> , p . <digit> <digit> , <digit> ) proposed a simple method of determining the interval between equally accurate observations which does without this information . in this method , the variances of the increments of noise and signal are equal , so that , in observations with a greater step , the variations caused by noise are smaller than those caused by the signal . this method is the standard accepted by the usa metrology community . the present paper is devoted to a statistical analysis of the allan method and acquisition of additional information"}
{"title": "Ideal sliding mode in the problems of convex optimization", "abstract": "The characteristics of the sliding mode that appears with using continuous convex-programming algorithms based on the exact penalty functions were discussed. For the case under study, the ideal sliding mode was shown to occur in the absence of infinite number of switchings", "id": "194", "src": "ideal sliding mode in the problems of convex optimization . the characteristics of the sliding mode that appears with using continuous convex programming algorithms based on the exact penalty functions were discussed . for the case under study , the ideal sliding mode was shown to occur in the absence of infinite number of switchings"}
{"title": "Automation of the recovery of efficiency of complex structure systems", "abstract": "Basic features are set forth of the method for automation of the serviceability recovery of systems of complex structures in real time without the interruption of operation. Specific features of the method are revealed in an important example of the system of control of hardware components of ships", "id": "195", "src": "automation of the recovery of efficiency of complex structure systems . basic features are set forth of the method for automation of the serviceability recovery of systems of complex structures in real time without the interruption of operation . specific features of the method are revealed in an important example of the system of control of hardware components of ships"}
{"title": "Control of combustion processes in an internal combustion engine by", "abstract": "low-temperature plasma A new method of operation of internal combustion engines enhances power and reduces fuel consumption and exhaust toxicity. Low-temperature plasma control combines working processes of thermal engines and steam machines into a single process", "id": "196", "src": "control of combustion processes in an internal combustion engine by . low temperature plasma a new method of operation of internal combustion engines enhances power and reduces fuel consumption and exhaust toxicity . low temperature plasma control combines working processes of thermal engines and steam machines into a single process"}
{"title": "Optimization of the characteristics of computational processes in scalable", "abstract": "resources The scalableness of resources is taken to mean the possibility of the prior change in the obtained dynamic characteristics of computational processes for a certain basic set of processors and the communication medium in an effort to optimize the dynamics of software applications. A method is put forward for the generation of optimal strategies-a set of the versions of the fulfillment of programs on the basis of a vector criterion. The method is urgent for the effective use of resources of computational clusters and metacomputational media and also for dynamic control of processes in real time on the basis of the static scaling", "id": "197", "src": "optimization of the characteristics of computational processes in scalable . resources the scalableness of resources is taken to mean the possibility of the prior change in the obtained dynamic characteristics of computational processes for a certain basic set of processors and the communication medium in an effort to optimize the dynamics of software applications . a method is put forward for the generation of optimal strategies a set of the versions of the fulfillment of programs on the basis of a vector criterion . the method is urgent for the effective use of resources of computational clusters and metacomputational media and also for dynamic control of processes in real time on the basis of the static scaling"}
{"title": "The p-p rearrangement and failure-tolerance of double p-ary multirings and", "abstract": "generalized hypercubes It is shown that an arbitrary grouped p-element permutation can be implemented in a conflict-free way through the commutation of channels on the double p-ary multiring or the double p-ary hypercube. It is revealed that in arbitrary single-element permutations, these commutators display the property of the (p-1)-nodal failure-tolerance and the generalized hypercube displays in addition the property of the (p-1)-channel failure-tolerance", "id": "198", "src": "the p p rearrangement and failure tolerance of double p ary multirings and . generalized hypercubes it is shown that an arbitrary grouped p element permutation can be implemented in a conflict free way through the commutation of channels on the double p ary multiring or the double p ary hypercube . it is revealed that in arbitrary single element permutations , these commutators display the property of the ( p 1 ) nodal failure tolerance and the generalized hypercube displays in addition the property of the ( p 1 ) channel failure tolerance"}
{"title": "Solutions for cooperative games", "abstract": "A new concept of the characteristic function is defined. It matches cooperative games far better than the classical characteristic function and is useful in reducing the number of decisions that can be used as the unique solution of a game", "id": "199", "src": "solutions for cooperative games . a new concept of the characteristic function is defined . it matches cooperative games far better than the classical characteristic function and is useful in reducing the number of decisions that can be used as the unique solution of a game"}
{"title": "Location of transport nets on a heterogeneous territory", "abstract": "The location of transport routes on a heterogeneous territory is studied. The network joins a given set of terminal points and a certain number of additional (branch) points. The problem is formulated, properties of the optimal solution for a. tree-like network, and the number of branch points are studied. A stepwise optimization algorithm for a. network with given adjacency matrix based on an algorithm for constructing minimal-cost routes is designed", "id": "200", "src": "location of transport nets on a heterogeneous territory . the location of transport routes on a heterogeneous territory is studied . the network joins a given set of terminal points and a certain number of additional ( branch ) points . the problem is formulated , properties of the optimal solution for a . tree like network , and the number of branch points are studied . a stepwise optimization algorithm for a . network with given adjacency matrix based on an algorithm for constructing minimal cost routes is designed"}
{"title": "Knowledge management-capturing the skills of key performers in the power", "abstract": "industry The growing pressure to reduce the cost of electrical power in recent years has resulted in an enormous \"brain-drain\" within the power industry. A novel approach has been developed by Eskom to capture these skills before they are lost and to incorporate these into a computer-based programme called \"knowledge management\"", "id": "201", "src": "knowledge management capturing the skills of key performers in the power . industry the growing pressure to reduce the cost of electrical power in recent years has resulted in an enormous brain drain within the power industry . a novel approach has been developed by eskom to capture these skills before they are lost and to incorporate these into a computer based programme called knowledge management"}
{"title": "Control in active systems based on criteria and motivation", "abstract": "For active systems where the principal varies the agents' goal functions by adding to them appropriately weighted goal functions of other agents or a balanced system of inter-agent transfers, the paper formulated and solved the problems of control based on criteria and motivation. Linear active systems were considered by way of example", "id": "202", "src": "control in active systems based on criteria and motivation . for active systems where the principal varies the agents ' goal functions by adding to them appropriately weighted goal functions of other agents or a balanced system of inter agent transfers , the paper formulated and solved the problems of control based on criteria and motivation . linear active systems were considered by way of example"}
{"title": "Flexibility analysis of complex technical systems under uncertainty", "abstract": "An important problem in designing technical systems under partial uncertainty of the initial physical, chemical, and technological data is the determination of a design in which the technical system is flexible, i.e., its control system is capable of guaranteeing that the constraints hold even under changes in external and internal factors and application of fuzzy mathematical models in its design. Three flexibility problems, viz., the flexibility of a technical system of given structure, structural flexibility of a technical system, and the optimal design guaranteeing the flexibility of a technical system, are studied. Two approaches to these problems are elaborated. Results of a computation experiment are given", "id": "203", "src": "flexibility analysis of complex technical systems under uncertainty . an important problem in designing technical systems under partial uncertainty of the initial physical , chemical , and technological data is the determination of a design in which the technical system is flexible , i . e . , its control system is capable of guaranteeing that the constraints hold even under changes in external and internal factors and application of fuzzy mathematical models in its design . three flexibility problems , viz . , the flexibility of a technical system of given structure , structural flexibility of a technical system , and the optimal design guaranteeing the flexibility of a technical system , are studied . two approaches to these problems are elaborated . results of a computation experiment are given"}
{"title": "A fuzzy logic adaptation circuit for control systems of deformable space", "abstract": "vehicles: its design A fuzzy-logic adaptation algorithm is designed for adjusting the discreteness period of a control system for ensuring the stability and quality of control process with regard to the elastic structural vibrations of a deformable space vehicle. Its performance is verified by digital modeling of a discrete control system with two objects", "id": "204", "src": "a fuzzy logic adaptation circuit for control systems of deformable space . vehicles its design a fuzzy logic adaptation algorithm is designed for adjusting the discreteness period of a control system for ensuring the stability and quality of control process with regard to the elastic structural vibrations of a deformable space vehicle . its performance is verified by digital modeling of a discrete control system with two objects"}
{"title": "\"Hidden convexity\" of finite-dimensional stationary linear discrete-time", "abstract": "systems under conical constraints New properties of finite-dimensional linear discrete-time systems under conical control constraints that are similar to the \"hidden convexity\" of continuous-time systems are studied", "id": "205", "src": "hidden convexity of finite dimensional stationary linear discrete time . systems under conical constraints new properties of finite dimensional linear discrete time systems under conical control constraints that are similar to the hidden convexity of continuous time systems are studied"}
{"title": "The set of stable polynomials of linear discrete systems: its geometry", "abstract": "The multidimensional stability domain of linear discrete systems is studied. Its configuration is determined from the parameters of its intersection with coordinate axes, coordinate planes, and certain auxiliary planes. Counterexamples for the discrete variant of the Kharitonov theorem are given", "id": "206", "src": "the set of stable polynomials of linear discrete systems its geometry . the multidimensional stability domain of linear discrete systems is studied . its configuration is determined from the parameters of its intersection with coordinate axes , coordinate planes , and certain auxiliary planes . counterexamples for the discrete variant of the kharitonov theorem are given"}
{"title": "Stochastic systems with a random jump in phase trajectory: stability of their", "abstract": "motions The probabilistic stability of the perturbed motion of a system with parameters under the action of a general Markov process is studied. The phase vector is assumed to experience random jumps when the structure the system suffers random jumps. Such a situation is encountered, for example, in the motion of a solid with random jumps in its mass. The mean-square stability of random-structure linear systems and stability. of nonlinear systems in the first approximation are studied. The applied approach is helpful in studying the asymptotic probabilistic stability and mean-square exponential stability of stochastic systems through the stability of the respective deterministic systems", "id": "207", "src": "stochastic systems with a random jump in phase trajectory stability of their . motions the probabilistic stability of the perturbed motion of a system with parameters under the action of a general markov process is studied . the phase vector is assumed to experience random jumps when the structure the system suffers random jumps . such a situation is encountered , for example , in the motion of a solid with random jumps in its mass . the mean square stability of random structure linear systems and stability . of nonlinear systems in the first approximation are studied . the applied approach is helpful in studying the asymptotic probabilistic stability and mean square exponential stability of stochastic systems through the stability of the respective deterministic systems"}
{"title": "A nonlinear time-optimal control problem", "abstract": "Sufficient conditions for the existence of an optimal control in a time-optimal control problem with fixed ends for a smooth nonlinear control system are formulated. The properties of this system for characterizing the optimal control switching points are studied", "id": "208", "src": "a nonlinear time optimal control problem . sufficient conditions for the existence of an optimal control in a time optimal control problem with fixed ends for a smooth nonlinear control system are formulated . the properties of this system for characterizing the optimal control switching points are studied"}
{"title": "System embedding. Polynomial equations", "abstract": "The class of solutions of the polynomial equations including their generalizations in the form of the Bezout matrix identities was constructed analytically using the technology of constructive system embedding. The structure of a solution depends on the number of steps of the Euclidean algorithm and is obtained explicitly by appropriate substitutions. Illustrative and descriptive examples are presented", "id": "209", "src": "system embedding . polynomial equations . the class of solutions of the polynomial equations including their generalizations in the form of the bezout matrix identities was constructed analytically using the technology of constructive system embedding . the structure of a solution depends on the number of steps of the euclidean algorithm and is obtained explicitly by appropriate substitutions . illustrative and descriptive examples are presented"}
{"title": "An optimal control algorithm based on reachability set approximation and", "abstract": "linearization The terminal functional of a general control system is refined by studying an analogous problem for a variational system and regularization. A sequential refinement method is designed by combining the local approximation of the reachability set and reduction. The corresponding algorithm has relaxation properties. An illustrative example is given", "id": "210", "src": "an optimal control algorithm based on reachability set approximation and . linearization the terminal functional of a general control system is refined by studying an analogous problem for a variational system and regularization . a sequential refinement method is designed by combining the local approximation of the reachability set and reduction . the corresponding algorithm has relaxation properties . an illustrative example is given"}
{"title": "A universal decomposition of the integration range for exponential functions", "abstract": "The problem of determining the independent constants for decomposition of the integration range of exponential functions was solved on the basis of a similar approach to polynomials. The constants obtained enable one to decompose the integration range in two so that the integrals over them are equal independently of the function parameters. For the nontrigonometrical polynomials of even functions, an alternative approach was presented", "id": "211", "src": "a universal decomposition of the integration range for exponential functions . the problem of determining the independent constants for decomposition of the integration range of exponential functions was solved on the basis of a similar approach to polynomials . the constants obtained enable one to decompose the integration range in two so that the integrals over them are equal independently of the function parameters . for the nontrigonometrical polynomials of even functions , an alternative approach was presented"}
{"title": "An application of fuzzy linear regression to the information technology in", "abstract": "Turkey Fuzzy set theory deals with the vagueness of human thought. A major contribution of fuzzy set theory is its capability of representing vague knowledge. Fuzzy set theory is very practical when sufficient and reliable data isn't available. Information technology (IT) is the acquisition, processing, storage and dissemination of information in all its forms (auditory, pictorial, textual and numerical) through a combination of computers, telecommunication, networks and electronic devices. IT includes matters concerned with the furtherance of computer science and technology, design, development, installation and implementation of information systems and applications. In the paper, assuming that there are n independent variables and the regression function is linear, the possible levels of information technology (the sale levels of computer equipment) in Turkey are forecasted by using fuzzy linear regression. The independent variables assumed are the import level and the export level of computer equipment", "id": "212", "src": "an application of fuzzy linear regression to the information technology in . turkey fuzzy set theory deals with the vagueness of human thought . a major contribution of fuzzy set theory is its capability of representing vague knowledge . fuzzy set theory is very practical when sufficient and reliable data isn ' t available . information technology ( it ) is the acquisition , processing , storage and dissemination of information in all its forms ( auditory , pictorial , textual and numerical ) through a combination of computers , telecommunication , networks and electronic devices . it includes matters concerned with the furtherance of computer science and technology , design , development , installation and implementation of information systems and applications . in the paper , assuming that there are n independent variables and the regression function is linear , the possible levels of information technology ( the sale levels of computer equipment ) in turkey are forecasted by using fuzzy linear regression . the independent variables assumed are the import level and the export level of computer equipment"}
{"title": "Synchronizing experiments with linear interval systems", "abstract": "Concerns generalized control problems without exact information. <P>A method of constructing a minimal synchronizing sequence for a linear interval system over the field of real numbers is developed. This problem is reduced to a system of linear inequalities", "id": "213", "src": "synchronizing experiments with linear interval systems . concerns generalized control problems without exact information . < p > a method of constructing a minimal synchronizing sequence for a linear interval system over the field of real numbers is developed . this problem is reduced to a system of linear inequalities"}
{"title": "Diagnosis of the technical state of heat systems", "abstract": "A step-by-step approach to the diagnosis of the technical state of heat systems is stated. The class of physical defects is supplemented by the behavioral defects of objects, which are related to the disturbance of the modes of their operation. The implementation of the approach is illustrated by an example of the solution of a specific problem of the diagnosis of a closed heat consumption system", "id": "214", "src": "diagnosis of the technical state of heat systems . a step by step approach to the diagnosis of the technical state of heat systems is stated . the class of physical defects is supplemented by the behavioral defects of objects , which are related to the disturbance of the modes of their operation . the implementation of the approach is illustrated by an example of the solution of a specific problem of the diagnosis of a closed heat consumption system"}
{"title": "Fault-tolerant computer-aided control systems with multiversion-threshold", "abstract": "adaptation: adaptation methods, reliability estimation, and choice of an architecture For multiversion majority-redundant computer-aided control systems, systematization of adaptation methods that are stable to hardware and software failures, a method for estimating their reliability from an event graph model, and a method for selecting a standard architecture with regard for reliability requirements are studied", "id": "215", "src": "fault tolerant computer aided control systems with multiversion threshold . adaptation adaptation methods , reliability estimation , and choice of an architecture for multiversion majority redundant computer aided control systems , systematization of adaptation methods that are stable to hardware and software failures , a method for estimating their reliability from an event graph model , and a method for selecting a standard architecture with regard for reliability requirements are studied"}
{"title": "Nonlockability in multirings and hypercubes at serial transmission of data", "abstract": "blocks For the multiring and hypercube, a method of conflictless realization of an arbitrary permutation of \"large\" data items that can be divided into many \"smaller\" data blocks was considered, and its high efficiency was demonstrated", "id": "216", "src": "nonlockability in multirings and hypercubes at serial transmission of data . blocks for the multiring and hypercube , a method of conflictless realization of an arbitrary permutation of large data items that can be divided into many smaller data blocks was considered , and its high efficiency was demonstrated"}
{"title": "Linear models of circuits based on the multivalued components", "abstract": "Linearization and planarization of the circuit models is pivotal to the submicron technologies. On the other hand, the characteristics of the VLSI circuits can be sometimes improved by using the multivalued components. It was shown that any l-level circuit based on the multivalued components is representable as an algebraic model based on l linear arithmetic polynomials mapped correspondingly into l decision diagrams that are linear and planar by nature. Complexity of representing a circuit as the linear decision diagram was estimated as O(G) with G for the number of multivalued components in the circuit. The results of testing the LinearDesignMV algorithm on circuits of more than 8000 LGSynth 93 multivalued components were presented", "id": "217", "src": "linear models of circuits based on the multivalued components . linearization and planarization of the circuit models is pivotal to the submicron technologies . on the other hand , the characteristics of the vlsi circuits can be sometimes improved by using the multivalued components . it was shown that any l level circuit based on the multivalued components is representable as an algebraic model based on l linear arithmetic polynomials mapped correspondingly into l decision diagrams that are linear and planar by nature . complexity of representing a circuit as the linear decision diagram was estimated as o ( g ) with g for the number of multivalued components in the circuit . the results of testing the lineardesignmv algorithm on circuits of more than <digit> lgsynth <digit> multivalued components were presented"}
{"title": "A new approach to the problem of structural identification. II", "abstract": "The subject under discussion is a new approach to the problem of structural identification, which relies on the recognition of a decisive role of the human factor in the process of structural identification. Potential possibilities of the suggested approach are illustrated by the statement of a new mathematical problem of structural identification", "id": "218", "src": "a new approach to the problem of structural identification . ii . the subject under discussion is a new approach to the problem of structural identification , which relies on the recognition of a decisive role of the human factor in the process of structural identification . potential possibilities of the suggested approach are illustrated by the statement of a new mathematical problem of structural identification"}
{"title": "A method of determining a sequence of the best solutions to the problems of", "abstract": "optimization on finite sets and the problem of network reconstruction A method of determining a sequence of the best solutions to the problems of optimization on finite sets was proposed. Its complexity was estimated by a polynomial of the dimension of problem input, given number of sequence terms, and complexity of completing the design of the original extremal problem. The technique developed was applied to the typical problem of network reconstruction with the aim of increasing its throughput under restricted reconstruction costs", "id": "219", "src": "a method of determining a sequence of the best solutions to the problems of . optimization on finite sets and the problem of network reconstruction a method of determining a sequence of the best solutions to the problems of optimization on finite sets was proposed . its complexity was estimated by a polynomial of the dimension of problem input , given number of sequence terms , and complexity of completing the design of the original extremal problem . the technique developed was applied to the typical problem of network reconstruction with the aim of increasing its throughput under restricted reconstruction costs"}
{"title": "Stabilization of a linear object by frequency-modulated pulsed signals", "abstract": "A control system consisting of an unstable continuous linear part and a pulse-frequency modulator in the feedback circuit is studied. Conditions for the boundedness of the solutions of the system under any initial data are determined", "id": "220", "src": "stabilization of a linear object by frequency modulated pulsed signals . a control system consisting of an unstable continuous linear part and a pulse frequency modulator in the feedback circuit is studied . conditions for the boundedness of the solutions of the system under any initial data are determined"}
{"title": "Reachability sets of a class of multistep control processes: their design", "abstract": "An upper estimate and an iterative \"restriction\" algorithm for the reachability set for determining the optimal control for a class of multistep control processes are designed", "id": "221", "src": "reachability sets of a class of multistep control processes their design . an upper estimate and an iterative restriction algorithm for the reachability set for determining the optimal control for a class of multistep control processes are designed"}
{"title": "Generalized confidence sets for a statistically indeterminate random vector", "abstract": "A problem is considered for the construction of confidence sets for a random vector, the information on distribution parameters of which is incomplete. To obtain exact estimates and a detailed analysis of the problem, the notion is introduced of a generalized confidence set for a statistically indeterminate random vector. Properties of generalized confidence sets are studied. It is shown that the standard method of estimation, which relies on the unification of confidence sets, leads in many cases to wider confidence estimates. For a normally distributed random vector with an inaccurately known mean value, generalized confidence sets are built tip and the dependence of sizes of a generalized confidence set on the forms and parameters of a set of possible mean values is examined", "id": "222", "src": "generalized confidence sets for a statistically indeterminate random vector . a problem is considered for the construction of confidence sets for a random vector , the information on distribution parameters of which is incomplete . to obtain exact estimates and a detailed analysis of the problem , the notion is introduced of a generalized confidence set for a statistically indeterminate random vector . properties of generalized confidence sets are studied . it is shown that the standard method of estimation , which relies on the unification of confidence sets , leads in many cases to wider confidence estimates . for a normally distributed random vector with an inaccurately known mean value , generalized confidence sets are built tip and the dependence of sizes of a generalized confidence set on the forms and parameters of a set of possible mean values is examined"}
{"title": "Evolution of the high-end computing market in the USA", "abstract": "This paper focuses on the technological change in the high-end computing market. The discussion combines historical analysis with strategic analysis to provide a framework to analyse a key component of the computer industry. This analysis begins from the perspective of government research and development spending; then examines the confusion around the evolution of the high-end computing market in the context of standard theories of technology strategy and new product innovation. Rather than the high-end market being 'dead', one should view the market as changing due to increased capability and competition from the low-end personal computer market. The high-end market is also responding to new product innovation from the introduction of new parallel computing architectures. In the conclusion, key leverage points in the market are identified and the trends in high-end computing are highlighted with implications", "id": "223", "src": "evolution of the high end computing market in the usa . this paper focuses on the technological change in the high end computing market . the discussion combines historical analysis with strategic analysis to provide a framework to analyse a key component of the computer industry . this analysis begins from the perspective of government research and development spending then examines the confusion around the evolution of the high end computing market in the context of standard theories of technology strategy and new product innovation . rather than the high end market being ' dead ' , one should view the market as changing due to increased capability and competition from the low end personal computer market . the high end market is also responding to new product innovation from the introduction of new parallel computing architectures . in the conclusion , key leverage points in the market are identified and the trends in high end computing are highlighted with implications"}
{"title": "Strong active solution in non-cooperative games", "abstract": "For the non-cooperative games and the problems of accepting or rejecting a proposal, a new notion of equilibrium was proposed, its place among the known basic equilibria was established, and its application to the static and dynamic game problems was demonstrated", "id": "224", "src": "strong active solution in non cooperative games . for the non cooperative games and the problems of accepting or rejecting a proposal , a new notion of equilibrium was proposed , its place among the known basic equilibria was established , and its application to the static and dynamic game problems was demonstrated"}
{"title": "System embedding. Control with reduced observer", "abstract": "Two interrelated problems-design of the reduced observer of plant state separately and together with its control system-were considered from the standpoint of designing the multivariable linear systems from the desired matrix transfer functions. The matrix equations defining the entire constructive class of solutions of the posed problems were obtained using the system embedding technology. As was demonstrated, control based on the reduced observer is capable to provide the desired response to the control input, as well as the response to the nonzero initial conditions, only for the directly measurable part of the components of the state vector. An illustrative example was presented", "id": "225", "src": "system embedding . control with reduced observer . two interrelated problems design of the reduced observer of plant state separately and together with its control system were considered from the standpoint of designing the multivariable linear systems from the desired matrix transfer functions . the matrix equations defining the entire constructive class of solutions of the posed problems were obtained using the system embedding technology . as was demonstrated , control based on the reduced observer is capable to provide the desired response to the control input , as well as the response to the nonzero initial conditions , only for the directly measurable part of the components of the state vector . an illustrative example was presented"}
{"title": "Spectral characteristics of the linear systems over a bounded time interval", "abstract": "Consideration was given to the spectral characteristics of the linear dynamic systems over a bounded time interval. Singular characteristics of standard dynamic blocks, transcendental characteristic equations, and partial spectra of the singular functions were studied. Relationship between the spectra under study and the classical frequency characteristic was demonstrated", "id": "226", "src": "spectral characteristics of the linear systems over a bounded time interval . consideration was given to the spectral characteristics of the linear dynamic systems over a bounded time interval . singular characteristics of standard dynamic blocks , transcendental characteristic equations , and partial spectra of the singular functions were studied . relationship between the spectra under study and the classical frequency characteristic was demonstrated"}
{"title": "Quantum computing with solids", "abstract": "Science and technology could be revolutionized by quantum computers, but building them from solid-state devices will not be easy. The author outlines the challenges in scaling up the technology from lab experiments to practical devices", "id": "227", "src": "quantum computing with solids . science and technology could be revolutionized by quantum computers , but building them from solid state devices will not be easy . the author outlines the challenges in scaling up the technology from lab experiments to practical devices"}
{"title": "The perils of privacy", "abstract": "The recent string of failures among dotcom companies has heightened fears of privacy abuse. What should happen to the names and addresses on a customer list if these details were obtained under a privacy policy which specified no disclosure to any third party? Should the personal data in the list be deemed to be an asset of a failing company which can be transferred to any future (third party) purchaser for its purposes? Or should the privacy policy take precedence over the commercial concerns of the purchaser?", "id": "228", "src": "the perils of privacy . the recent string of failures among dotcom companies has heightened fears of privacy abuse . what should happen to the names and addresses on a customer list if these details were obtained under a privacy policy which specified no disclosure to any third party should the personal data in the list be deemed to be an asset of a failing company which can be transferred to any future ( third party ) purchaser for its purposes or should the privacy policy take precedence over the commercial concerns of the purchaser"}
{"title": "Enterprise in focus at NetSec 2002", "abstract": "NetSec 2002 took place in San Francisco, amid industry reflection on the balance to be struck between combatting cyber-terrorism and safeguarding civil liberties post-9.11. The author reports on the punditry and the pedagogy at the CSI event, focusing on security in the enterprise", "id": "229", "src": "enterprise in focus at netsec <digit> . netsec <digit> took place in san francisco , amid industry reflection on the balance to be struck between combatting cyber terrorism and safeguarding civil liberties post 9 . <digit> . the author reports on the punditry and the pedagogy at the csi event , focusing on security in the enterprise"}
{"title": "Trusted...or...trustworthy: the search for a new paradigm for computer and", "abstract": "network security This paper sets out a number of major questions and challenges which include: (a) just what is meant by `trusted' or `trustworthy' systems after 20 years of experience, or more likely, lack of business level experience, with the 'trusted computer system' criteria anyway; (b) does anyone really care about the adoption of international standards for computer system security evaluation by IT product and system manufacturers and suppliers (IS 15408) and, if so, how does it all relate to business risk management anyway (IS 17799); (c) with the explosion of adoption of the microcomputer and personal computer some 20 years ago, has the industry abandoned all that it learnt about security during the `mainframe era'; or - `whatever happened to MULTICS' and its lessons; (d) has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases, particularly where Web or Internet-based information services are being proposed as the major `next best thing' in the IT industry; (e) has the `fourth generation' of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last `generation', the professionals who developed and created the applications for shared mainframe and minicomputer systems?", "id": "230", "src": "trusted . . . or . . . trustworthy the search for a new paradigm for computer and . network security this paper sets out a number of major questions and challenges which include ( a ) just what is meant by trusted ' or trustworthy ' systems after <digit> years of experience , or more likely , lack of business level experience , with the ' trusted computer system ' criteria anyway ( b ) does anyone really care about the adoption of international standards for computer system security evaluation by it product and system manufacturers and suppliers ( is <digit> ) and , if so , how does it all relate to business risk management anyway ( is <digit> ) ( c ) with the explosion of adoption of the microcomputer and personal computer some <digit> years ago , has the industry abandoned all that it learnt about security during the mainframe era ' or whatever happened to multics ' and its lessons ( d ) has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases , particularly where web or internet based information services are being proposed as the major next best thing ' in the it industry ( e ) has the fourth generation ' of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last generation ' , the professionals who developed and created the applications for shared mainframe and minicomputer systems"}
{"title": "Much ado about nothing: Win32.Perrun", "abstract": "JPEG files do not contain any executable code and it is impossible to infect such files. The author takes a look at the details surrounding the Win32.Perrun virus and make clear exactly what it does. The main virus feature is its ability to affect JPEG image files (compressed graphic images) and to spread via affected JPEG files. The virus affects, or modifies, or alters JPEG files but does not \"infect\" them", "id": "231", "src": "much ado about nothing win32 . perrun . jpeg files do not contain any executable code and it is impossible to infect such files . the author takes a look at the details surrounding the win32 . perrun virus and make clear exactly what it does . the main virus feature is its ability to affect jpeg image files ( compressed graphic images ) and to spread via affected jpeg files . the virus affects , or modifies , or alters jpeg files but does not infect them"}
{"title": "Information security policy - what do international information security", "abstract": "standards say? One of the most important information security controls, is the information security policy. This vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. This results in the policy authors turning to existing sources for guidance. One of these sources is the various international information security standards. These standards are a good starting point for determining what the information security policy should consist of, but should not be relied upon exclusively for guidance. Firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. It is far more important the information security policy must fit in with the organisation's culture and must therefore be developed with this in mind", "id": "232", "src": "information security policy what do international information security . standards say one of the most important information security controls , is the information security policy . this vital direction giving document is , however , not always easy to develop and the authors thereof battle with questions such as what constitutes a policy . this results in the policy authors turning to existing sources for guidance . one of these sources is the various international information security standards . these standards are a good starting point for determining what the information security policy should consist of , but should not be relied upon exclusively for guidance . firstly , they are not comprehensive in their coverage and furthermore , tending to rather address the processes needed for successfully implementing the information security policy . it is far more important the information security policy must fit in with the organisation ' s culture and must therefore be developed with this in mind"}
{"title": "Security crisis management - the basics", "abstract": "Of the more pervasive problems in any kind of security event is how the security event is managed from the inception to the end. There's a lot written about how to manage a specific incident or how to deal with a point problem such as a firewall log, but little tends to be written about how to deal with the management of a security event as part of corporate crisis management. This article discusses the basics of security crisis management and of the logical steps required to ensure that a security crisis does not get out of hand", "id": "233", "src": "security crisis management the basics . of the more pervasive problems in any kind of security event is how the security event is managed from the inception to the end . there ' s a lot written about how to manage a specific incident or how to deal with a point problem such as a firewall log , but little tends to be written about how to deal with the management of a security event as part of corporate crisis management . this article discusses the basics of security crisis management and of the logical steps required to ensure that a security crisis does not get out of hand"}
{"title": "A conceptual framework for evaluation of information technology investments", "abstract": "The decision to acquire a new information technology poses a number of serious evaluation and selection problems to technology managers, because the new system must not only meet current information requirements of the organisation, but also the needs for future expansion. Tangible and intangible benefits factors, as well as risks factors, must be identified and evaluated. The paper provides a review of ten major evaluation categories and available models, which fall under each category, showing their advantages and disadvantages in handling the above difficulties. This paper describes strategic implications involved in the selection decision, and the inherent difficulties in: (1) choosing or developing a model, (2) obtaining realistic inputs for the model, and (3) making tradeoffs among the conflicting factors. It proposes a conceptual framework to help the decision maker in choosing the most appropriate methodology in the evaluation process. It also offers a new model, called GAHP, for the evaluation problem combining integer goal linear programming and analytic hierarchy process (AHP) in a single hybrid multiple objective multi-criteria model. A goal programming methodology, with zero-one integer variables and mixed integer constraints, is used to set goal target values against which information technology alternatives are evaluated and selected. AHP is used to structure the evaluation process providing pairwise comparison mechanisms to quantify subjective, nonmonetary, intangible benefits and risks factors, in deriving data for the model. A case illustration is provided showing how GAHP can be formulated and solved", "id": "234", "src": "a conceptual framework for evaluation of information technology investments . the decision to acquire a new information technology poses a number of serious evaluation and selection problems to technology managers , because the new system must not only meet current information requirements of the organisation , but also the needs for future expansion . tangible and intangible benefits factors , as well as risks factors , must be identified and evaluated . the paper provides a review of ten major evaluation categories and available models , which fall under each category , showing their advantages and disadvantages in handling the above difficulties . this paper describes strategic implications involved in the selection decision , and the inherent difficulties in ( 1 ) choosing or developing a model , ( 2 ) obtaining realistic inputs for the model , and ( 3 ) making tradeoffs among the conflicting factors . it proposes a conceptual framework to help the decision maker in choosing the most appropriate methodology in the evaluation process . it also offers a new model , called gahp , for the evaluation problem combining integer goal linear programming and analytic hierarchy process ( ahp ) in a single hybrid multiple objective multi criteria model . a goal programming methodology , with zero one integer variables and mixed integer constraints , is used to set goal target values against which information technology alternatives are evaluated and selected . ahp is used to structure the evaluation process providing pairwise comparison mechanisms to quantify subjective , nonmonetary , intangible benefits and risks factors , in deriving data for the model . a case illustration is provided showing how gahp can be formulated and solved"}
{"title": "Data storage: re-format. Closely tracking a fast-moving sector", "abstract": "In the past few years the data center market has changed dramatically, forcing many companies into consolidation or bankruptcy. Gone are the days when companies raised millions of dollars to acquire large industrial buildings and transform them into glittering, high-tech palaces filled with the latest telecommunication and data technology. Whereas manufacturers of communication technology deliver the racked equipment in these, often mission-critical, facilities, ABB focuses mainly on the building infrastructure. Besides the very important redundant power supply, ABB also provides the redundant air conditioning and the security system", "id": "235", "src": "data storage re format . closely tracking a fast moving sector . in the past few years the data center market has changed dramatically , forcing many companies into consolidation or bankruptcy . gone are the days when companies raised millions of dollars to acquire large industrial buildings and transform them into glittering , high tech palaces filled with the latest telecommunication and data technology . whereas manufacturers of communication technology deliver the racked equipment in these , often mission critical , facilities , abb focuses mainly on the building infrastructure . besides the very important redundant power supply , abb also provides the redundant air conditioning and the security system"}
{"title": "Industrial/sup IT/ for performance buildings", "abstract": "ABB has taken a close look at how buildings are used and has come up with a radical solution for the technical infrastructure that places the end-user's processes at the center and integrates all the building's systems around their needs. The new solution is based on the realization that tasks like setting up an office meeting, registering a hotel guest or moving a patient in a hospital, can all benefit from the same Industrial IT concepts employed by ABB to optimize manufacturing, for example in the automotive industry", "id": "236", "src": "industrial sup it for performance buildings . abb has taken a close look at how buildings are used and has come up with a radical solution for the technical infrastructure that places the end user ' s processes at the center and integrates all the building ' s systems around their needs . the new solution is based on the realization that tasks like setting up an office meeting , registering a hotel guest or moving a patient in a hospital , can all benefit from the same industrial it concepts employed by abb to optimize manufacturing , for example in the automotive industry"}
{"title": "Virtual engineering office: a state-of-the-art platform for engineering", "abstract": "collaboration A sales force in Latin America, the design department in Europe, and production in Asia? Arrangements of this kind are the new business reality for today's global manufacturing companies. But how are such global operations to be effectively coordinated? ABB's answer was to develop and implement a new platform for high-performance, real-time collaboration. Globally distributed engineering teams can now work together, regardless of time, location or the CAD system they use, making ABB easier to do business with, for customers as well as suppliers", "id": "237", "src": "virtual engineering office a state of the art platform for engineering . collaboration a sales force in latin america , the design department in europe , and production in asia arrangements of this kind are the new business reality for today ' s global manufacturing companies . but how are such global operations to be effectively coordinated abb ' s answer was to develop and implement a new platform for high performance , real time collaboration . globally distributed engineering teams can now work together , regardless of time , location or the cad system they use , making abb easier to do business with , for customers as well as suppliers"}
{"title": "Post-haste. 100th robotic containerization system installed in US mail sorting", "abstract": "center Spot welding, machine tending, material handling, picking, packing, painting, palletizing, assembly...the list of tasks being performed by ABB robots keeps on growing. Adding to this portfolio is a new robot containerization system (RCS) that ABB developed specifically for the United States Postal Service (USPS). The RCS has brought new levels of speed, accuracy, efficiency and productivity to the process of sorting and containerizing mail and packages. Recently, the 100th ABB RCS was installed at the USPS processing and distribution center in Columbus, Ohio", "id": "238", "src": "post haste . 100th robotic containerization system installed in us mail sorting . center spot welding , machine tending , material handling , picking , packing , painting , palletizing , assembly . . . the list of tasks being performed by abb robots keeps on growing . adding to this portfolio is a new robot containerization system ( rcs ) that abb developed specifically for the united states postal service ( usps ) . the rcs has brought new levels of speed , accuracy , efficiency and productivity to the process of sorting and containerizing mail and packages . recently , the 100th abb rcs was installed at the usps processing and distribution center in columbus , ohio"}
{"title": "Optimize/sup IT/ robot condition monitoring tool", "abstract": "As robots have gained more and more 'humanlike' capability, users have looked increasingly to their builders for ways to measure the critical variables-the robotic equivalent of a physical check-up-in order to monitor their condition and schedule maintenance more effectively. This is all the more essential considering the tremendous pressure there is to improve productivity in today's global markets. Developed for ABB robots with an S4-family controller and based on the company's broad process know-how, Optimize/sup IT/ robot condition monitoring offers maintenance routines with embedded checklists that give a clear indication of a robot's operating condition. It performs semi-automatic measurements that support engineers during trouble-shooting and enable action to be taken to prevent unplanned stops. By comparing these measurements with reference data, negative trends can be detected early and potential breakdowns predicted. Armed with all these features, Optimize/sup IT/ robot condition monitoring provides the ideal basis for reliability-centered maintenance (RCM) for robots", "id": "239", "src": "optimize sup it robot condition monitoring tool . as robots have gained more and more ' humanlike ' capability , users have looked increasingly to their builders for ways to measure the critical variables the robotic equivalent of a physical check up in order to monitor their condition and schedule maintenance more effectively . this is all the more essential considering the tremendous pressure there is to improve productivity in today ' s global markets . developed for abb robots with an s4 family controller and based on the company ' s broad process know how , optimize sup it robot condition monitoring offers maintenance routines with embedded checklists that give a clear indication of a robot ' s operating condition . it performs semi automatic measurements that support engineers during trouble shooting and enable action to be taken to prevent unplanned stops . by comparing these measurements with reference data , negative trends can be detected early and potential breakdowns predicted . armed with all these features , optimize sup it robot condition monitoring provides the ideal basis for reliability centered maintenance ( rcm ) for robots"}
{"title": "Pane relief. Robotic solutions for car windshield assembly", "abstract": "Just looking through a car's windshield doesn't give us much reason to wonder about how it's made. The idea that special manufacturing expertise might be required can hardly occur to anyone, but that's exactly what is needed to ensure crystal-clear visibility, not to mention a perfect fit every time one is pressed into place on a car production line. Comprising two thin glass sheets joined by a vinyl interlayer, windshields are assembled-usually manually-to very precise product and environmental specifications. To make sure this is done as perfectly as possible, the industry invests heavily in the equipment used for their fabrication. ABB has now developed a robot-based Compact Assembling System for the automatic assembly of laminated windshields that speeds up production and increases cost efficiency", "id": "240", "src": "pane relief . robotic solutions for car windshield assembly . just looking through a car ' s windshield doesn ' t give us much reason to wonder about how it ' s made . the idea that special manufacturing expertise might be required can hardly occur to anyone , but that ' s exactly what is needed to ensure crystal clear visibility , not to mention a perfect fit every time one is pressed into place on a car production line . comprising two thin glass sheets joined by a vinyl interlayer , windshields are assembled usually manually to very precise product and environmental specifications . to make sure this is done as perfectly as possible , the industry invests heavily in the equipment used for their fabrication . abb has now developed a robot based compact assembling system for the automatic assembly of laminated windshields that speeds up production and increases cost efficiency"}
{"title": "Shaping the future. BendWizard: a tool for off-line programming of robotic", "abstract": "tending systems Setting up a robot to make metal cabinets or cases for desktop computers can be a complex operation. For instance, one expert might be required to carry out a feasibility study, and then another to actually program the robot. Understandably, the need for so much expertise, and the time that's required, generally limits the usefulness of automation to high-volume production. Workshops producing parts in batches smaller than 50 or so, or which rely heavily on semiskilled operators, are therefore often discouraged from investing in automation, and so miss out on its many advantages. What is needed is a software tool that operators without special knowledge of robotics, or with no more than rudimentary CAD skills, can use. One which allows easy offline programming and simulation of the work cell on a PC", "id": "241", "src": "shaping the future . bendwizard a tool for off line programming of robotic . tending systems setting up a robot to make metal cabinets or cases for desktop computers can be a complex operation . for instance , one expert might be required to carry out a feasibility study , and then another to actually program the robot . understandably , the need for so much expertise , and the time that ' s required , generally limits the usefulness of automation to high volume production . workshops producing parts in batches smaller than <digit> or so , or which rely heavily on semiskilled operators , are therefore often discouraged from investing in automation , and so miss out on its many advantages . what is needed is a software tool that operators without special knowledge of robotics , or with no more than rudimentary cad skills , can use . one which allows easy offline programming and simulation of the work cell on a pc"}
{"title": "Press shop. Industrial IT solutions for the press shop", "abstract": "Globalization of the world's markets is challenging the traditional limits of manufacturing efficiency. The competitive advantage belongs to those who understand the new requirements and opportunities, and who commit to integrated solutions that span the value chain all the way from demand to production. ABB's automation and IT expertise and the process know-how gained from its long involvement with the automotive industry, have been brought together in new, state-of-the-art software solutions for press shops. Integrated into Industrial IT architecture, they allow the full potential of the shops to be realized, with advantages at every step in the supply chain", "id": "242", "src": "press shop . industrial it solutions for the press shop . globalization of the world ' s markets is challenging the traditional limits of manufacturing efficiency . the competitive advantage belongs to those who understand the new requirements and opportunities , and who commit to integrated solutions that span the value chain all the way from demand to production . abb ' s automation and it expertise and the process know how gained from its long involvement with the automotive industry , have been brought together in new , state of the art software solutions for press shops . integrated into industrial it architecture , they allow the full potential of the shops to be realized , with advantages at every step in the supply chain"}
{"title": "Real-time enterprise solutions for discrete manufacturing and consumer goods", "abstract": "Customer satisfaction and a focus on core competencies have dominated the thinking of a whole host of industries in recent years. However, one outcome, the outsourcing of noncore activities, has made the production of goods-from order entry to final delivery-more and more complex. Suppliers, subsuppliers, producers and customers are therefore busy adopting a new, more collaborative approach. This is mainly taking the form of order-driven planning and scheduling of production, but it is also being steered by a need to reduce inventories and working capital as well as a desire to increase throughput and optimize production", "id": "243", "src": "real time enterprise solutions for discrete manufacturing and consumer goods . customer satisfaction and a focus on core competencies have dominated the thinking of a whole host of industries in recent years . however , one outcome , the outsourcing of noncore activities , has made the production of goods from order entry to final delivery more and more complex . suppliers , subsuppliers , producers and customers are therefore busy adopting a new , more collaborative approach . this is mainly taking the form of order driven planning and scheduling of production , but it is also being steered by a need to reduce inventories and working capital as well as a desire to increase throughput and optimize production"}
{"title": "Extinction cross sections of realistic raindrops: data-bank established using", "abstract": "T-matrix method and nonlinear fitting technique A new computer program is developed based on the T-matrix method to generate a large number of total (extinction) cross sections (TCS) values of the realistic raindrops that are deformed due to a balance of the forces that act on a drop failing under gravity, and were described in shape by Pruppacher and Pitter (1971). These data for various dimensions of the raindrops (mean effective radius from 0 to 3.25 mm), frequencies (10 to 80 GHz), (horizontal and vertical) polarizations, and temperatures (0, 10 and 20 degrees C) are stored to establish a data bank. Furthermore, a curve fitting technique, i.e., interpolation of order 3, is implemented for the TCS values in the data bank. Therefore, the interpolated TCS results can be obtained readily from the interpolation process with negligible or even null computational time and efforts. Error analysis is carried out to show the high accuracy of the present analysis and applicability of the interpolation. At three operating frequencies of 15, 21.225, and 38 GHz locally used in Singapore, some new TCS values are obtained from the new fast and efficient interpolation with a good accuracy", "id": "244", "src": "extinction cross sections of realistic raindrops data bank established using . t matrix method and nonlinear fitting technique a new computer program is developed based on the t matrix method to generate a large number of total ( extinction ) cross sections ( tcs ) values of the realistic raindrops that are deformed due to a balance of the forces that act on a drop failing under gravity , and were described in shape by pruppacher and pitter ( <digit> ) . these data for various dimensions of the raindrops ( mean effective radius from 0 to 3 . <digit> mm ) , frequencies ( <digit> to <digit> ghz ) , ( horizontal and vertical ) polarizations , and temperatures ( 0 , <digit> and <digit> degrees c ) are stored to establish a data bank . furthermore , a curve fitting technique , i . e . , interpolation of order 3 , is implemented for the tcs values in the data bank . therefore , the interpolated tcs results can be obtained readily from the interpolation process with negligible or even null computational time and efforts . error analysis is carried out to show the high accuracy of the present analysis and applicability of the interpolation . at three operating frequencies of <digit> , <digit> . <digit> , and <digit> ghz locally used in singapore , some new tcs values are obtained from the new fast and efficient interpolation with a good accuracy"}
{"title": "Challenges and trends in discrete manufacturing", "abstract": "Over 50 years ago, the 100,000 workers at Ford's Rouge automobile factory turned out 1200 cars per day. Nowadays, Ford's plant on that same site still produces 800 cars each day but with just 3000 workers. Similar stories abound in the manufacturing industries; technology revolution and evolution; a shift from vertical integration, better business and production practices and improved industrial relations-all have changed manufacturing beyond recognition. So what are the current challenges and trends in manufacturing? Certainly, the relentless advance of technology will continue, as will user pressure for more customized design or improved environmental friendliness. Some trends are already with us and more, as yet indiscernible, will come. But one major, fundamental shift now resounding throughout industry is the way in which information involving every single aspect of the manufacturing process is being integrated into one seamless system", "id": "245", "src": "challenges and trends in discrete manufacturing . over <digit> years ago , the <digit> , <digit> workers at ford ' s rouge automobile factory turned out <digit> cars per day . nowadays , ford ' s plant on that same site still produces <digit> cars each day but with just <digit> workers . similar stories abound in the manufacturing industries technology revolution and evolution a shift from vertical integration , better business and production practices and improved industrial relations all have changed manufacturing beyond recognition . so what are the current challenges and trends in manufacturing certainly , the relentless advance of technology will continue , as will user pressure for more customized design or improved environmental friendliness . some trends are already with us and more , as yet indiscernible , will come . but one major , fundamental shift now resounding throughout industry is the way in which information involving every single aspect of the manufacturing process is being integrated into one seamless system"}
{"title": "On the Beth properties of some intuitionistic modal logics", "abstract": "Let L be one of the intuitionistic modal logics. As in the classical modal case, we define two different forms of the Beth property for L, which are denoted by B1 and B2; in this paper we study the relation among B1, B2 and the interpolation properties C1 and C2. It turns out that C1 implies B1, but contrary to the boolean case, is not equivalent to B1. It is shown that B2 and C2 are independent, and moreover it comes out that, in contrast to classical case, there exists an extension of the intuitionistic modal logic of S/sub 4/-type, that has not the property B2. Finally we give two algebraic properties, that characterize respectively B1 and B2", "id": "246", "src": "on the beth properties of some intuitionistic modal logics . let l be one of the intuitionistic modal logics . as in the classical modal case , we define two different forms of the beth property for l , which are denoted by b1 and b2 in this paper we study the relation among b1 , b2 and the interpolation properties c1 and c2 . it turns out that c1 implies b1 , but contrary to the boolean case , is not equivalent to b1 . it is shown that b2 and c2 are independent , and moreover it comes out that , in contrast to classical case , there exists an extension of the intuitionistic modal logic of s sub 4 type , that has not the property b2 . finally we give two algebraic properties , that characterize respectively b1 and b2"}
{"title": "More constructions for Boolean algebras", "abstract": "We construct Boolean algebras with prescribed behaviour concerning depth for the free product of two Boolean algebras over a third, in ZFC using pcf; assuming squares we get results on ultraproducts. We also deal with the family of cardinalities and topological density of homomorphic images of Boolean algebras (you can translate it to topology-on the cardinalities of closed subspaces); and lastly we deal with inequalities between cardinal invariants, mainly d(B)/sup kappa /<|B| implies ind(B)>/sup kappa /V Depth(B)>or=log(|B|)", "id": "247", "src": "more constructions for boolean algebras . we construct boolean algebras with prescribed behaviour concerning depth for the free product of two boolean algebras over a third , in zfc using pcf assuming squares we get results on ultraproducts . we also deal with the family of cardinalities and topological density of homomorphic images of boolean algebras ( you can translate it to topology on the cardinalities of closed subspaces ) and lastly we deal with inequalities between cardinal invariants , mainly d ( b ) sup kappa < b implies ind ( b ) > sup kappa v depth ( b ) > or log ( b )"}
{"title": "IT as a key enabler to law firm competitiveness", "abstract": "Professional services firms have traditionally been able to thrive in virtually any market conditions. They have been consistently successful for several decades without ever needing to reexamine or change their basic operating model. However, gradual but inexorable change in client expectations and the business environment over recent years now means that more of the same is no longer enough. In future, law firms will increasingly need to exploit IT more effectively in order to remain competitive. To do this, they will need to ensure that all their information systems function as an integrated whole and are available to their staff, clients and business partners. The authors set out the lessons to be learned for law firms in the light of the recent PA Consulting survey", "id": "248", "src": "it as a key enabler to law firm competitiveness . professional services firms have traditionally been able to thrive in virtually any market conditions . they have been consistently successful for several decades without ever needing to reexamine or change their basic operating model . however , gradual but inexorable change in client expectations and the business environment over recent years now means that more of the same is no longer enough . in future , law firms will increasingly need to exploit it more effectively in order to remain competitive . to do this , they will need to ensure that all their information systems function as an integrated whole and are available to their staff , clients and business partners . the authors set out the lessons to be learned for law firms in the light of the recent pa consulting survey"}
{"title": "Electronic signatures - much ado?", "abstract": "Whilst the market may be having a crisis of confidence regarding the prospects for e-commerce, the EU and the Government continue apace to develop the legal framework. Most recently, this has resulted in the Electronic Signatures Regulations 2002. These Regulations were made on 13 February 2002 and came into force on 8 March 2002. The Regulations implement the European Electronic Signatures Directive (1999/93/EC). Critics may say that the Regulations were implemented too late (they were due to have been implemented by 19 July 2001), with too short a consultation period (25 January 2002 to 12 February 2002) and with an unconvincing case as to what they add to English law (as to which, read on). The author explains the latest development on e-signatures and the significance of Certification Service Providers (CSPs)", "id": "249", "src": "electronic signatures much ado . whilst the market may be having a crisis of confidence regarding the prospects for e commerce , the eu and the government continue apace to develop the legal framework . most recently , this has resulted in the electronic signatures regulations <digit> . these regulations were made on <digit> february <digit> and came into force on 8 march <digit> . the regulations implement the european electronic signatures directive ( <digit> <digit> ec ) . critics may say that the regulations were implemented too late ( they were due to have been implemented by <digit> july <digit> ) , with too short a consultation period ( <digit> january <digit> to <digit> february <digit> ) and with an unconvincing case as to what they add to english law ( as to which , read on ) . the author explains the latest development on e signatures and the significance of certification service providers ( csps )"}
{"title": "Naomi Campbell: drugs, distress and the Data Protection Act", "abstract": "In the first case of its kind, Naomi Campbell successfully sued Mirror Group Newspapers for damage and distress caused by breach of the Data Protection Act 1998. Partner N. Wildish and assistant M. Turle of City law firm Field Fisher Waterhouse discuss the case and the legal implications of which online publishers should be aware", "id": "250", "src": "naomi campbell drugs , distress and the data protection act . in the first case of its kind , naomi campbell successfully sued mirror group newspapers for damage and distress caused by breach of the data protection act <digit> . partner n . wildish and assistant m . turle of city law firm field fisher waterhouse discuss the case and the legal implications of which online publishers should be aware"}
{"title": "Don't always believe what you Reed [optimisation techniques for Web sites and", "abstract": "trade mark infringement] On 20 May 2002, Mr Justice Pumfrey gave judgment in the case of (1) Reed Executive Plc (2) Reed Solutions Plc versus (1) Reed Business Information Limited (2) Reed Elsevier (UK) Limited (3) totaljobs.com Limited. The case explored for the first time in any detail the extent to which the use of various optimisation techniques for Web sites could give rise to new forms of trade mark infringement and passing off. The author reports on the case and offers his comments", "id": "251", "src": "don ' t always believe what you reed optimisation techniques for web sites and . trade mark infringement on <digit> may <digit> , mr justice pumfrey gave judgment in the case of ( 1 ) reed executive plc ( 2 ) reed solutions plc versus ( 1 ) reed business information limited ( 2 ) reed elsevier ( uk ) limited ( 3 ) totaljobs . com limited . the case explored for the first time in any detail the extent to which the use of various optimisation techniques for web sites could give rise to new forms of trade mark infringement and passing off . the author reports on the case and offers his comments"}
{"title": "Finally! some sensible European legislation on software", "abstract": "The European Commission has formally tabled a draft Directive on the Protection by Patents of Computer-Implemented Inventions. The aim of this very important Directive is to harmonise national patent laws relating to inventions using software. It follows an extensive consultation launched by the Commission in October 2000. The impetus behind the Directive was the recognition at EU level of a total lack of unity between the European Patent Office and European national courts in deciding what was or was not deemed patentable when it came to the subject of computer programs", "id": "252", "src": "finally some sensible european legislation on software . the european commission has formally tabled a draft directive on the protection by patents of computer implemented inventions . the aim of this very important directive is to harmonise national patent laws relating to inventions using software . it follows an extensive consultation launched by the commission in october <digit> . the impetus behind the directive was the recognition at eu level of a total lack of unity between the european patent office and european national courts in deciding what was or was not deemed patentable when it came to the subject of computer programs"}
{"title": "Cyberobscenity and the ambit of English criminal law", "abstract": "The author looks at a recent case and questions the Court of Appeal's approach. In the author's submission, the Court of Appeal's decision in Perrin was wrong. P published no material in England and Wales, and should not have been convicted of any offence under English law, even if it were proved that he sought to attract English subscribers to his site. That may be an unpalatable conclusion but, if the content of foreign-hosted Internet sites is to be controlled, the only sensible way forward is through international agreement and cooperation. The Council of Europe's Cybercrime Convention provides some indication of the limited areas over which widespread international agreement might be achieved", "id": "253", "src": "cyberobscenity and the ambit of english criminal law . the author looks at a recent case and questions the court of appeal ' s approach . in the author ' s submission , the court of appeal ' s decision in perrin was wrong . p published no material in england and wales , and should not have been convicted of any offence under english law , even if it were proved that he sought to attract english subscribers to his site . that may be an unpalatable conclusion but , if the content of foreign hosted internet sites is to be controlled , the only sensible way forward is through international agreement and cooperation . the council of europe ' s cybercrime convention provides some indication of the limited areas over which widespread international agreement might be achieved"}
{"title": "E-government", "abstract": "The author provides an introduction to the main issues surrounding E-government modernisation and electronic delivery of all public services by 2005. The author makes it clear that E-government is about transformation, not computers and hints at the special legal issues which may arise", "id": "254", "src": "e government . the author provides an introduction to the main issues surrounding e government modernisation and electronic delivery of all public services by <digit> . the author makes it clear that e government is about transformation , not computers and hints at the special legal issues which may arise"}
{"title": "Vendor qualifications for IT staff and networking", "abstract": "In some cases, vendor-run accreditation schemes can offer an objective measure of a job applicant's skills, but they do not always indicate the true extent of practical abilities", "id": "255", "src": "vendor qualifications for it staff and networking . in some cases , vendor run accreditation schemes can offer an objective measure of a job applicant ' s skills , but they do not always indicate the true extent of practical abilities"}
{"title": "Evolution of litigation support systems", "abstract": "For original paper see ibid., vol. 12, no. 6: \"The E-mail of the Species\". The author responds to that paper and argues that printing, scanning and imaging E-mails or other electronic (rather than paper) documents prior to listing and disclosure seems to be unnecessary, not 'proportionate' (from a costs point of view) and not particularly helpful, to either side. He asks how litigation support systems might evolve to help and support the legal team in their task", "id": "256", "src": "evolution of litigation support systems . for original paper see ibid . , vol . <digit> , no . 6 the e mail of the species . the author responds to that paper and argues that printing , scanning and imaging e mails or other electronic ( rather than paper ) documents prior to listing and disclosure seems to be unnecessary , not ' proportionate ' ( from a costs point of view ) and not particularly helpful , to either side . he asks how litigation support systems might evolve to help and support the legal team in their task"}
{"title": "Evicting orang utans from the office [electronic storage of legal files]", "abstract": "Having espoused the principle of the paperless office some time ago, we decided to apply it to our stored files. First we consulted the Law Society rules governing storage of files on electronic media. The next step was for us to draw up a protocol for scanning the files. The benefits of the exercise have been significant. The area previously used for storage has been freed for other use. Files are now available online, instantaneously. When we have needed to send out files to the client or following a change of solicitor, we have been able to do so almost immediately, by E-mail, retaining a copy for our future reference. The files are protected from loss or deterioration, back-up copies having been taken which are stored off site. The complete stored file archive can be put in your pocket (in CD-ROM format) or on a laptop, facilitating remote working", "id": "257", "src": "evicting orang utans from the office electronic storage of legal files . having espoused the principle of the paperless office some time ago , we decided to apply it to our stored files . first we consulted the law society rules governing storage of files on electronic media . the next step was for us to draw up a protocol for scanning the files . the benefits of the exercise have been significant . the area previously used for storage has been freed for other use . files are now available online , instantaneously . when we have needed to send out files to the client or following a change of solicitor , we have been able to do so almost immediately , by e mail , retaining a copy for our future reference . the files are protected from loss or deterioration , back up copies having been taken which are stored off site . the complete stored file archive can be put in your pocket ( in cd rom format ) or on a laptop , facilitating remote working"}
{"title": "Electronic data exchange for real estate", "abstract": "With HM Land Registry's consultation now underway, no one denies that the property industry is facing a period of unprecedented change. PISCES (Property Information Systems Common Exchange) is a property-focused electronic data exchange standard. The standard is a set of definitions and rules to facilitate electronic transfer of data between key business areas and between different types of software packages that are used regularly by the property industry. It is not itself a piece of software but an enabling technology that allows software providers to prepare solutions within their own packages to transfer data between databases. This provides the attractive prospect of seamless transfer of data within and between systems and organisations", "id": "258", "src": "electronic data exchange for real estate . with hm land registry ' s consultation now underway , no one denies that the property industry is facing a period of unprecedented change . pisces ( property information systems common exchange ) is a property focused electronic data exchange standard . the standard is a set of definitions and rules to facilitate electronic transfer of data between key business areas and between different types of software packages that are used regularly by the property industry . it is not itself a piece of software but an enabling technology that allows software providers to prepare solutions within their own packages to transfer data between databases . this provides the attractive prospect of seamless transfer of data within and between systems and organisations"}
{"title": "E-mail and the legal profession", "abstract": "The widespread use of E-mail can be found in all areas of commerce, and the legal profession is one that has embraced this new medium of communication. E-mail is not without its drawbacks, however. Due to the nature of the technologies behind the medium, it is a less secure form of communication than many of those traditionally used by the legal profession, including DX, facsimile, and standard and registered post. There are a number of ways in which E-mails originating from the practice may be protected, including software encryption, hardware encryption and various methods of controlling and administering access to the E-mails", "id": "259", "src": "e mail and the legal profession . the widespread use of e mail can be found in all areas of commerce , and the legal profession is one that has embraced this new medium of communication . e mail is not without its drawbacks , however . due to the nature of the technologies behind the medium , it is a less secure form of communication than many of those traditionally used by the legal profession , including dx , facsimile , and standard and registered post . there are a number of ways in which e mails originating from the practice may be protected , including software encryption , hardware encryption and various methods of controlling and administering access to the e mails"}
{"title": "Spam solution?", "abstract": "The author describes a solution to spam E-mails: disposable E-mail addresses (DEA). Mailshell's free trial Web-based E-mail service allows you, if you start getting spammed on that DEA, just to delete the DEA in Mailshell, and all E-mail thereafter sent to that address will automatically be junked (though you can later restore that address if you want). Mailshell allows any number of DEA", "id": "260", "src": "spam solution . the author describes a solution to spam e mails disposable e mail addresses ( dea ) . mailshell ' s free trial web based e mail service allows you , if you start getting spammed on that dea , just to delete the dea in mailshell , and all e mail thereafter sent to that address will automatically be junked ( though you can later restore that address if you want ) . mailshell allows any number of dea"}
{"title": "7 key tests in choosing your Web site firm", "abstract": "Most legal firms now have a Web site and are starting to evaluate the return on their investment. The paper looks at factors involved when choosing a firm to help set up or improve a Web site. (1) Look for a company that combines technical skills and business experience. (2) Look for a company that offers excellent customer service. (3) Check that the Web site firm is committed to developing and proactively updating the Web site. (4) Make sure the firm has a proven track record and a good portfolio. (5) Look for a company with both a breadth as well as depth of skills. (6) Make sure the firm can deliver work on target, in budget and to specification. (7) Ensure that you will enjoy working and feel comfortable with the Web site firm staff", "id": "261", "src": "7 key tests in choosing your web site firm . most legal firms now have a web site and are starting to evaluate the return on their investment . the paper looks at factors involved when choosing a firm to help set up or improve a web site . ( 1 ) look for a company that combines technical skills and business experience . ( 2 ) look for a company that offers excellent customer service . ( 3 ) check that the web site firm is committed to developing and proactively updating the web site . ( 4 ) make sure the firm has a proven track record and a good portfolio . ( 5 ) look for a company with both a breadth as well as depth of skills . ( 6 ) make sure the firm can deliver work on target , in budget and to specification . ( 7 ) ensure that you will enjoy working and feel comfortable with the web site firm staff"}
{"title": "Why your Web strategy is, err, wrong", "abstract": "An awkward look at a few standard views from the author, who thinks that most people have got it, err, wrong. Like every other investment, when the time comes to sign the contract, the question that should be asked is not whether it is a good investment, but whether it is the best investment the firm can make with the money. the author argues that he would be surprised if any law firm Web site he has seen yet would jump that particular hurdle", "id": "262", "src": "why your web strategy is , err , wrong . an awkward look at a few standard views from the author , who thinks that most people have got it , err , wrong . like every other investment , when the time comes to sign the contract , the question that should be asked is not whether it is a good investment , but whether it is the best investment the firm can make with the money . the author argues that he would be surprised if any law firm web site he has seen yet would jump that particular hurdle"}
{"title": "A humanist's legacy in medical informatics: visions and accomplishments of", "abstract": "Professor Jean-Raoul Scherrer The objective is to report on the work of Prof. Jean-Raoul Scherrer, and show how his humanist vision, medical skills and scientific background have enabled and shaped the development of medical informatics over the last 30 years. Starting with the mainframe-based patient-centred hospital information system DIOGENE in the 70s, Prof. Scherrer developed, implemented and evolved innovative concepts of man-machine interfaces, distributed and federated environments, leading the way with information systems that obstinately focused on the support of care providers and patients. Through a rigorous design of terminologies and ontologies, the DIOGENE data would then serve as a basis for the development of clinical research, data mining, and lead to innovative natural language processing techniques. In parallel, Prof. Scherrer supported the development of medical image management, ranging from a distributed picture archiving and communication systems (PACS) to molecular imaging of protein electrophoreses. Recognizing the need for improving the quality and trustworthiness of medical information of the Web, Prof. Scherrer created the Health-On-the Net (HON) foundation. These achievements, made possible thanks to his visionary mind, deep humanism, creativity, generosity and determination, have made of Prof. Scherrer a true pioneer and leader of the human-centered, patient-oriented application of information technology for improving healthcare", "id": "263", "src": "a humanist ' s legacy in medical informatics visions and accomplishments of . professor jean raoul scherrer the objective is to report on the work of prof . jean raoul scherrer , and show how his humanist vision , medical skills and scientific background have enabled and shaped the development of medical informatics over the last <digit> years . starting with the mainframe based patient centred hospital information system diogene in the 70s , prof . scherrer developed , implemented and evolved innovative concepts of man machine interfaces , distributed and federated environments , leading the way with information systems that obstinately focused on the support of care providers and patients . through a rigorous design of terminologies and ontologies , the diogene data would then serve as a basis for the development of clinical research , data mining , and lead to innovative natural language processing techniques . in parallel , prof . scherrer supported the development of medical image management , ranging from a distributed picture archiving and communication systems ( pacs ) to molecular imaging of protein electrophoreses . recognizing the need for improving the quality and trustworthiness of medical information of the web , prof . scherrer created the health on the net ( hon ) foundation . these achievements , made possible thanks to his visionary mind , deep humanism , creativity , generosity and determination , have made of prof . scherrer a true pioneer and leader of the human centered , patient oriented application of information technology for improving healthcare"}
{"title": "Medicine in the 21 st century: global problems, global solutions", "abstract": "The objectives are to discuss application areas of information, technology in medicine and health care on the occasion of the opening of the Private Universitat fur Medizinische Informatik and Technik Tirol/University for Health Informatics and Technology Tyrol (LIMIT) at Innsbruck, Tyrol, Austria. Important application areas of information technology in medicine and health are appropriate individual access to medical knowledge, new engineering developments such as new radiant imaging methods and the implantable pacemaker/defibrillator devices, mathematical modeling for understanding the workings of the human body, the computer-based patient record, as well as new knowledge in molecular biology, human genetics, and biotechnology. Challenges and responsibilities for medical informatics research include medical data privacy and intellectual property rights inherent in the content of the information systems", "id": "264", "src": "medicine in the <digit> st century global problems , global solutions . the objectives are to discuss application areas of information , technology in medicine and health care on the occasion of the opening of the private universitat fur medizinische informatik and technik tirol university for health informatics and technology tyrol ( limit ) at innsbruck , tyrol , austria . important application areas of information technology in medicine and health are appropriate individual access to medical knowledge , new engineering developments such as new radiant imaging methods and the implantable pacemaker defibrillator devices , mathematical modeling for understanding the workings of the human body , the computer based patient record , as well as new knowledge in molecular biology , human genetics , and biotechnology . challenges and responsibilities for medical informatics research include medical data privacy and intellectual property rights inherent in the content of the information systems"}
{"title": "Guidelines, the Internet, and personal health: insights from the Canadian", "abstract": "HEALNet experience The objectives are to summarize the insights gained in collaborative research in a Canadian Network of Centres of Excellence, devoted to the promotion of evidence-based practice, and to relate this experience to Internet support of health promotion and consumer health informatics. A subjective review of insights is undertaken. Work directed the development of systems incorporating guidelines, care maps, etc., for use by professionals met with limited acceptance. Evidence-based tools for health care consumers are a desirable complement but require radically different content and delivery modes. In addition to evidence-based material offered by professionals, a wide array of Internet-based products and services provided by consumers for consumers emerged and proved a beneficial complement. The consumer-driven products and services provided via the Internet are a potentially important and beneficial complement of traditional health services. They affect the health consumer-provider roles and require changes in healthcare practices", "id": "265", "src": "guidelines , the internet , and personal health insights from the canadian . healnet experience the objectives are to summarize the insights gained in collaborative research in a canadian network of centres of excellence , devoted to the promotion of evidence based practice , and to relate this experience to internet support of health promotion and consumer health informatics . a subjective review of insights is undertaken . work directed the development of systems incorporating guidelines , care maps , etc . , for use by professionals met with limited acceptance . evidence based tools for health care consumers are a desirable complement but require radically different content and delivery modes . in addition to evidence based material offered by professionals , a wide array of internet based products and services provided by consumers for consumers emerged and proved a beneficial complement . the consumer driven products and services provided via the internet are a potentially important and beneficial complement of traditional health services . they affect the health consumer provider roles and require changes in healthcare practices"}
{"title": "ISCSI poised to lower SAN costs", "abstract": "IT managers building storage area networks or expanding their capacity may be able to save money by using iSCSI and IP systems rather than Fibre Channel technologies", "id": "266", "src": "iscsi poised to lower san costs . it managers building storage area networks or expanding their capacity may be able to save money by using iscsi and ip systems rather than fibre channel technologies"}
{"title": "Standard protocol for exchange of health-checkup data based on SGML: the", "abstract": "Health-checkup Data Markup Language (HDML) The objectives are to develop a health/medical data interchange model for efficient electronic exchange of data among health-checkup facilities. A Health-checkup Data Markup Language (HDML) was developed on the basis of the Standard Generalized Markup Language (SGML), and a feasibility study carried out, involving data exchange between two health checkup facilities. The structure of HDML is described. The transfer of numerical lab data, summary findings and health status assessment was successful. HDML is an improvement to laboratory data exchange. Further work has to address the exchange of qualitative and textual data", "id": "267", "src": "standard protocol for exchange of health checkup data based on sgml the . health checkup data markup language ( hdml ) the objectives are to develop a health medical data interchange model for efficient electronic exchange of data among health checkup facilities . a health checkup data markup language ( hdml ) was developed on the basis of the standard generalized markup language ( sgml ) , and a feasibility study carried out , involving data exchange between two health checkup facilities . the structure of hdml is described . the transfer of numerical lab data , summary findings and health status assessment was successful . hdml is an improvement to laboratory data exchange . further work has to address the exchange of qualitative and textual data"}
{"title": "Development of a health guidance support system for lifestyle improvement", "abstract": "The objective is to provide automated advice for lifestyle adjustment based on an assessment of the results of a questionnaire and medical examination or health checkup data. A system was developed that gathers data based on questions regarding weight gain, exercise, smoking, sleep, eating habits, salt intake, animal fat intake, snacks, alcohol, and oral hygiene, body mass index, resting blood pressure, fasting blood sugar, total cholesterol, triglycerides, uric acid and liver function tests. Based on the relationships between the lifestyle data and the health checkup data, a health assessment sheet was generated for persons being allocated to a multiple-risk factor syndrome group. Health assessment and useful advice for lifestyle improvement were automatically extracted with the system, toward the high risk group for life style related diseases. The system is operational. In comparison with conventional, limited advice methods, we developed a practical system that defined the necessity for lifestyle improvement more clearly, and made giving advice easier", "id": "268", "src": "development of a health guidance support system for lifestyle improvement . the objective is to provide automated advice for lifestyle adjustment based on an assessment of the results of a questionnaire and medical examination or health checkup data . a system was developed that gathers data based on questions regarding weight gain , exercise , smoking , sleep , eating habits , salt intake , animal fat intake , snacks , alcohol , and oral hygiene , body mass index , resting blood pressure , fasting blood sugar , total cholesterol , triglycerides , uric acid and liver function tests . based on the relationships between the lifestyle data and the health checkup data , a health assessment sheet was generated for persons being allocated to a multiple risk factor syndrome group . health assessment and useful advice for lifestyle improvement were automatically extracted with the system , toward the high risk group for life style related diseases . the system is operational . in comparison with conventional , limited advice methods , we developed a practical system that defined the necessity for lifestyle improvement more clearly , and made giving advice easier"}
{"title": "Organization design: The continuing influence of information technology", "abstract": "Drawing from an information processing perspective, this paper examines how information technology (IT) has been a catalyst in the development of new forms of organizational structures. The article draws a historical linkage between the relative stability of an organization's task environment starting after the Second World War to the present environmental instability that now characterizes many industries. Specifically, the authors suggest that advances in IT have enabled managers to adapt existing forms and create new models for organizational design that better fit requirements of an unstable environment. Time has seemingly borne out this hypothesis as the bureaucratic structure evolved to the matrix to the network and now to the emerging shadow structure. IT has gone from a support mechanism to a substitute for organizational structures in the form of the shadow structure. The article suggests that the evolving and expanding role of IT will continue for organizations that face unstable environments", "id": "269", "src": "organization design the continuing influence of information technology . drawing from an information processing perspective , this paper examines how information technology ( it ) has been a catalyst in the development of new forms of organizational structures . the article draws a historical linkage between the relative stability of an organization ' s task environment starting after the second world war to the present environmental instability that now characterizes many industries . specifically , the authors suggest that advances in it have enabled managers to adapt existing forms and create new models for organizational design that better fit requirements of an unstable environment . time has seemingly borne out this hypothesis as the bureaucratic structure evolved to the matrix to the network and now to the emerging shadow structure . it has gone from a support mechanism to a substitute for organizational structures in the form of the shadow structure . the article suggests that the evolving and expanding role of it will continue for organizations that face unstable environments"}
{"title": "Knowledge-based structures and organisational commitment", "abstract": "Organisational commitment, the emotional attachment of an employee to the employing organisation, has attracted a substantial body of literature, relating the concept to various antecedents, including organisational structure, and to a range of consequences, including financially important performance factors such as productivity and staff turnover. The new areas of knowledge management and learning organisations offer substantial promise as imperatives for the organisation of business enterprises. As organisations in the contemporary environment adopt knowledge-based structures to improve their competitive position, there is value in examining these structures against other performance related factors. Theoretical knowledge-based structures put forward by R. Miles et al. (1997) and J. Quinn et al. (1996) and an existing implementation are examined to determine common features inherent in these approaches. These features are posited as a typical form and their impact on organisational commitment and hence on individual and organisational performance is examined", "id": "270", "src": "knowledge based structures and organisational commitment . organisational commitment , the emotional attachment of an employee to the employing organisation , has attracted a substantial body of literature , relating the concept to various antecedents , including organisational structure , and to a range of consequences , including financially important performance factors such as productivity and staff turnover . the new areas of knowledge management and learning organisations offer substantial promise as imperatives for the organisation of business enterprises . as organisations in the contemporary environment adopt knowledge based structures to improve their competitive position , there is value in examining these structures against other performance related factors . theoretical knowledge based structures put forward by r . miles et al . ( <digit> ) and j . quinn et al . ( <digit> ) and an existing implementation are examined to determine common features inherent in these approaches . these features are posited as a typical form and their impact on organisational commitment and hence on individual and organisational performance is examined"}
{"title": "The evolution of information systems: Their impact on organizations and", "abstract": "structures Information systems and organization structures have been highly interconnected with each other. Over the years, information systems architectures as well as organization structures have evolved from centralized to more decentralized forms. This research looks at the evolution of both information systems and organization structures. In the process, it looks into the impact of computers on organizations, and examines the ways organization structures have changed, in association with changes in information system architectures. It also suggests logical linkages between information system architectures and their \"fit\" with certain organization structures and strategies. It concludes with some implications for emerging and future organizational forms, and provides a quick review of the effect of the Internet on small businesses traditionally using stand-alone computers", "id": "271", "src": "the evolution of information systems their impact on organizations and . structures information systems and organization structures have been highly interconnected with each other . over the years , information systems architectures as well as organization structures have evolved from centralized to more decentralized forms . this research looks at the evolution of both information systems and organization structures . in the process , it looks into the impact of computers on organizations , and examines the ways organization structures have changed , in association with changes in information system architectures . it also suggests logical linkages between information system architectures and their fit with certain organization structures and strategies . it concludes with some implications for emerging and future organizational forms , and provides a quick review of the effect of the internet on small businesses traditionally using stand alone computers"}
{"title": "In search of a general enterprise model", "abstract": "Many organisations, particularly SMEs, are reluctant to invest time and money in models to support decision making. Such reluctance could be overcome if a model could be used for several purposes rather than using a traditional \"single perspective\" model. This requires the development of a \"general enterprise model\" (GEM), which can be applied to a wide range of problem domains with unlimited scope. Current enterprise modelling frameworks only deal effectively with nondynamic modelling issues whilst dynamic modelling issues have traditionally only been addressed at the operational level. Although the majority of research in this area relates to manufacturing companies, the framework for a GEM must be equally applicable to service and public sector organisations. The paper identifies five key design issues that need to be considered when constructing a GEM. A framework for such a GEM is presented based on a \"plug and play\" methodology and demonstrated by a simple case study", "id": "272", "src": "in search of a general enterprise model . many organisations , particularly smes , are reluctant to invest time and money in models to support decision making . such reluctance could be overcome if a model could be used for several purposes rather than using a traditional single perspective model . this requires the development of a general enterprise model ( gem ) , which can be applied to a wide range of problem domains with unlimited scope . current enterprise modelling frameworks only deal effectively with nondynamic modelling issues whilst dynamic modelling issues have traditionally only been addressed at the operational level . although the majority of research in this area relates to manufacturing companies , the framework for a gem must be equally applicable to service and public sector organisations . the paper identifies five key design issues that need to be considered when constructing a gem . a framework for such a gem is presented based on a plug and play methodology and demonstrated by a simple case study"}
{"title": "Strategies for high throughput, templated zeolite synthesis", "abstract": "The design and redesign of high throughput experiments for zeolite synthesis are addressed. A model that relates materials function to the chemical composition of the zeolite and the structure directing agent is introduced. Using this model, several Monte Carlo-like design protocols are evaluated. Multi-round protocols are bound to be effective, and strategies that use a priori information about the structure-directing libraries are found to be the best", "id": "273", "src": "strategies for high throughput , templated zeolite synthesis . the design and redesign of high throughput experiments for zeolite synthesis are addressed . a model that relates materials function to the chemical composition of the zeolite and the structure directing agent is introduced . using this model , several monte carlo like design protocols are evaluated . multi round protocols are bound to be effective , and strategies that use a priori information about the structure directing libraries are found to be the best"}
{"title": "Variable structure intelligent control for PM synchronous servo motor drive", "abstract": "The variable structure control (VSC) of discrete time systems based on intelligent control is presented in this paper. A novel approach is proposed for the state estimation. A linear observer is firstly designed. Then a neural network is used for compensating uncertainty. The parameter of the VSC scheme is adjusted online by a neural network. Practical operating results from a PM synchronous motor (PMSM) illustrate the effectiveness and practicability of the proposed approach", "id": "274", "src": "variable structure intelligent control for pm synchronous servo motor drive . the variable structure control ( vsc ) of discrete time systems based on intelligent control is presented in this paper . a novel approach is proposed for the state estimation . a linear observer is firstly designed . then a neural network is used for compensating uncertainty . the parameter of the vsc scheme is adjusted online by a neural network . practical operating results from a pm synchronous motor ( pmsm ) illustrate the effectiveness and practicability of the proposed approach"}
{"title": "A nonlinear modulation strategy for hybrid AC/DC power systems", "abstract": "A nonlinear control strategy to improve transient stability of a multi-machine AC power system with several DC links terminated in the presence of large disturbances is presented. The approach proposed in this paper is based on differential geometric theory, and the HVDC systems are taken as a variable admittance connected at the inverter or rectifier AC bus. After deriving the analytical description of the relationship between the variable admittance and active power flows of each generator, the traditional generator dynamic equations can thus be expressed with the variable admittance of HVDC systems as an additional state variable and changed to an affine form, which is suitable for global linearization method being used to determine its control variable. An important feature of the proposed method is that, the modulated DC power is an adaptive and non-linear function of AC system states, and it can be realized by local feedback and less transmitted data from, adjacent generators. The design procedure is tested on a dual-infeed hybrid AC/DC system", "id": "275", "src": "a nonlinear modulation strategy for hybrid ac dc power systems . a nonlinear control strategy to improve transient stability of a multi machine ac power system with several dc links terminated in the presence of large disturbances is presented . the approach proposed in this paper is based on differential geometric theory , and the hvdc systems are taken as a variable admittance connected at the inverter or rectifier ac bus . after deriving the analytical description of the relationship between the variable admittance and active power flows of each generator , the traditional generator dynamic equations can thus be expressed with the variable admittance of hvdc systems as an additional state variable and changed to an affine form , which is suitable for global linearization method being used to determine its control variable . an important feature of the proposed method is that , the modulated dc power is an adaptive and non linear function of ac system states , and it can be realized by local feedback and less transmitted data from , adjacent generators . the design procedure is tested on a dual infeed hybrid ac dc system"}
{"title": "Mobile computing \"Killer app\" competition", "abstract": "Design competitions offer students an excellent way to gain hands-on experience in engineering and computer science courses. The University of Florida, in partnership with Motorola, has held two mobile computing design competitions. In Spring and Fall 2001, students in Abdelsalam Helal's Mobile Computing class designed killer apps for a Motorola smart phone", "id": "276", "src": "mobile computing killer app competition . design competitions offer students an excellent way to gain hands on experience in engineering and computer science courses . the university of florida , in partnership with motorola , has held two mobile computing design competitions . in spring and fall <digit> , students in abdelsalam helal ' s mobile computing class designed killer apps for a motorola smart phone"}
{"title": "Firewall card shields data", "abstract": "The SlotShield 3000 firewall on a PCI card saves power and space, but might not offer enough security for large networks", "id": "277", "src": "firewall card shields data . the slotshield <digit> firewall on a pci card saves power and space , but might not offer enough security for large networks"}
{"title": "Standards for service discovery and delivery", "abstract": "For the past five years, competing industries and standards developers have been hotly pursuing automatic configuration, now coined the broader term service discovery. Jini, Universal Plug and Play (UPnP), Salutation, and Service Location Protocol are among the front-runners in this new race. However, choosing service discovery as the topic of the hour goes beyond the need for plug-and-play solutions or support for the SOHO (small office/home office) user. Service discovery's potential in mobile and pervasive computing environments motivated my choice", "id": "278", "src": "standards for service discovery and delivery . for the past five years , competing industries and standards developers have been hotly pursuing automatic configuration , now coined the broader term service discovery . jini , universal plug and play ( upnp ) , salutation , and service location protocol are among the front runners in this new race . however , choosing service discovery as the topic of the hour goes beyond the need for plug and play solutions or support for the soho ( small office home office ) user . service discovery ' s potential in mobile and pervasive computing environments motivated my choice"}
{"title": "The role of speech input in wearable computing", "abstract": "Speech recognition seems like an attractive input mechanism for wearable computers, and as we saw in this magazine's first issue, several companies are promoting products that use limited speech interfaces for specific tasks. However, we must overcome several challenges to using speech recognition in more general contexts, and interface designers must be wary of applying the technology to situations where speech is inappropriate", "id": "279", "src": "the role of speech input in wearable computing . speech recognition seems like an attractive input mechanism for wearable computers , and as we saw in this magazine ' s first issue , several companies are promoting products that use limited speech interfaces for specific tasks . however , we must overcome several challenges to using speech recognition in more general contexts , and interface designers must be wary of applying the technology to situations where speech is inappropriate"}
{"title": "The ubiquitous provisioning of internet services to portable devices", "abstract": "Advances in mobile telecommunications and device miniaturization call for providing both standard and novel location- and context-dependent Internet services to mobile clients. Mobile agents are dynamic, asynchronous, and autonomous, making the MA programming paradigm suitable for developing novel middleware for mobility-enabled services", "id": "280", "src": "the ubiquitous provisioning of internet services to portable devices . advances in mobile telecommunications and device miniaturization call for providing both standard and novel location and context dependent internet services to mobile clients . mobile agents are dynamic , asynchronous , and autonomous , making the ma programming paradigm suitable for developing novel middleware for mobility enabled services"}
{"title": "Integrating virtual and physical context to support knowledge workers", "abstract": "The Kimura system augments and integrates independent tools into a pervasive computing system that monitors a user's interactions with the computer, an electronic whiteboard, and a variety of networked peripheral devices and data sources", "id": "281", "src": "integrating virtual and physical context to support knowledge workers . the kimura system augments and integrates independent tools into a pervasive computing system that monitors a user ' s interactions with the computer , an electronic whiteboard , and a variety of networked peripheral devices and data sources"}
{"title": "Data management in location-dependent information services", "abstract": "Location-dependent information services have great promise for mobile and pervasive computing environments. They can provide local and nonlocal news, weather, and traffic reports as well as directory services. Before they can be implemented on a large scale, however, several research issues must be addressed", "id": "282", "src": "data management in location dependent information services . location dependent information services have great promise for mobile and pervasive computing environments . they can provide local and nonlocal news , weather , and traffic reports as well as directory services . before they can be implemented on a large scale , however , several research issues must be addressed"}
{"title": "Modeling privacy control in context-aware systems", "abstract": "Significant complexity issues challenge designers of context-aware systems with privacy control. Information spaces provide a way to organize information, resources, and services around important privacy-relevant contextual factors. In this article, we describe a theoretical model for privacy control in context-aware systems based on a core abstraction of information spaces. We have previously focused on deriving socially based privacy objectives in pervasive computing environments. Building on Ravi Sandhu's four-layer OM-AM (objectives, models, architectures, and mechanisms) idea, we aim to use information spaces to construct a model for privacy control that supports our socially based privacy objectives. We also discuss how we can introduce decentralization, a desirable property for many pervasive computing systems, into our information space model, using unified privacy tagging", "id": "283", "src": "modeling privacy control in context aware systems . significant complexity issues challenge designers of context aware systems with privacy control . information spaces provide a way to organize information , resources , and services around important privacy relevant contextual factors . in this article , we describe a theoretical model for privacy control in context aware systems based on a core abstraction of information spaces . we have previously focused on deriving socially based privacy objectives in pervasive computing environments . building on ravi sandhu ' s four layer om am ( objectives , models , architectures , and mechanisms ) idea , we aim to use information spaces to construct a model for privacy control that supports our socially based privacy objectives . we also discuss how we can introduce decentralization , a desirable property for many pervasive computing systems , into our information space model , using unified privacy tagging"}
{"title": "ConChat: a context-aware chat program", "abstract": "ConChat is a context-aware chat program that enriches electronic communication by providing contextual information and resolving potential semantic conflicts between users.ConChat uses contextual information to improve electronic communication. Using contextual cues, users can infer during a conversation what the other person is doing and what is happening in his or her immediate surroundings. For example, if a user learns that the other person is talking with somebody else or is involved in some urgent activity, he or she knows to expect a slower response. Conversely, if the user learns that the other person is sitting in a meeting directly related to the conversation, he or she then knows to respond more quickly. Also, by informing users about the other person's context and tagging potentially ambiguous chat messages, ConChat explores how context can improve electronic communication by reducing semantic conflicts", "id": "284", "src": "conchat a context aware chat program . conchat is a context aware chat program that enriches electronic communication by providing contextual information and resolving potential semantic conflicts between users . conchat uses contextual information to improve electronic communication . using contextual cues , users can infer during a conversation what the other person is doing and what is happening in his or her immediate surroundings . for example , if a user learns that the other person is talking with somebody else or is involved in some urgent activity , he or she knows to expect a slower response . conversely , if the user learns that the other person is sitting in a meeting directly related to the conversation , he or she then knows to respond more quickly . also , by informing users about the other person ' s context and tagging potentially ambiguous chat messages , conchat explores how context can improve electronic communication by reducing semantic conflicts"}
{"title": "A context-aware decision engine for content adaptation", "abstract": "Building a good content adaptation service for mobile devices poses many challenges. To meet these challenges, this quality-of-service-aware decision engine automatically negotiates for the appropriate adaptation decision for synthesizing an optimal content version", "id": "285", "src": "a context aware decision engine for content adaptation . building a good content adaptation service for mobile devices poses many challenges . to meet these challenges , this quality of service aware decision engine automatically negotiates for the appropriate adaptation decision for synthesizing an optimal content version"}
{"title": "Reconfigurable context-sensitive middleware for pervasive computing", "abstract": "Context-sensitive applications need data from sensors, devices, and user actions, and might need ad hoc communication support to dynamically discover new devices and engage in spontaneous information exchange. Reconfigurable Context-Sensitive Middleware facilitates the development and runtime operations of context-sensitive pervasive computing software", "id": "286", "src": "reconfigurable context sensitive middleware for pervasive computing . context sensitive applications need data from sensors , devices , and user actions , and might need ad hoc communication support to dynamically discover new devices and engage in spontaneous information exchange . reconfigurable context sensitive middleware facilitates the development and runtime operations of context sensitive pervasive computing software"}
{"title": "Activity and location recognition using wearable sensors", "abstract": "Using measured acceleration and angular velocity data gathered through inexpensive, wearable sensors, this dead-reckoning method can determine a user's location, detect transitions between preselected locations, and recognize and classify sitting, standing, and walking behaviors. Experiments demonstrate the proposed method's effectiveness", "id": "287", "src": "activity and location recognition using wearable sensors . using measured acceleration and angular velocity data gathered through inexpensive , wearable sensors , this dead reckoning method can determine a user ' s location , detect transitions between preselected locations , and recognize and classify sitting , standing , and walking behaviors . experiments demonstrate the proposed method ' s effectiveness"}
{"title": "Analyzing the benefits of 300 mm conveyor-based AMHS", "abstract": "While the need for automation in 300 mm fabs is not debated, the form and performance of such automation is still in question. Software simulation that compares conveyor-based continuous flow transport technology to conventional car-based wafer-lot delivery has detailed delivery time and throughput advantages to the former", "id": "288", "src": "analyzing the benefits of <digit> mm conveyor based amhs . while the need for automation in <digit> mm fabs is not debated , the form and performance of such automation is still in question . software simulation that compares conveyor based continuous flow transport technology to conventional car based wafer lot delivery has detailed delivery time and throughput advantages to the former"}
{"title": "How to avoid merger pitfalls", "abstract": "Paul Diamond of consultancy KPMG explains why careful IT asset management is crucial to the success of mergers", "id": "289", "src": "how to avoid merger pitfalls . paul diamond of consultancy kpmg explains why careful it asset management is crucial to the success of mergers"}
{"title": "Labscape: a smart environment for the cell biology laboratory", "abstract": "Labscape is a smart environment that we designed to improve the experience of people who work in a cell biology laboratory. Our goal in creating it was to simplify, laboratory work by making information available where it is needed and by collecting and organizing data where and when it is created into a formal representation that others can understand and process. By helping biologists produce a more complete record of their work with less effort, Labscape is designed to foster improved collaboration in conjunction with increased individual efficiency and satisfaction. A user-driven system, although technologically conservative, embraces a central goal of ubiquitous computing: to enhance the ability to perform domain tasks through fluid interaction with computational resources. Smart environments could soon replace the pen and paper commonly used in the laboratory setting", "id": "290", "src": "labscape a smart environment for the cell biology laboratory . labscape is a smart environment that we designed to improve the experience of people who work in a cell biology laboratory . our goal in creating it was to simplify , laboratory work by making information available where it is needed and by collecting and organizing data where and when it is created into a formal representation that others can understand and process . by helping biologists produce a more complete record of their work with less effort , labscape is designed to foster improved collaboration in conjunction with increased individual efficiency and satisfaction . a user driven system , although technologically conservative , embraces a central goal of ubiquitous computing to enhance the ability to perform domain tasks through fluid interaction with computational resources . smart environments could soon replace the pen and paper commonly used in the laboratory setting"}
{"title": "Broadcasts keep staff in picture [intranets]", "abstract": "Mark Hawkins, chief operating officer at UK-based streaming media specialist Twofourtv, explains how firms can benefit by linking their corporate intranets to broadcasting technology", "id": "291", "src": "broadcasts keep staff in picture intranets . mark hawkins , chief operating officer at uk based streaming media specialist twofourtv , explains how firms can benefit by linking their corporate intranets to broadcasting technology"}
{"title": "Java portability put to the test", "abstract": "Sun Microsystems' recently launched Java Verification Program aims to enable companies to assess the cross-platform portability of applications written in Java, and to help software vendors ensure that their solutions can run in heterogenous J2EE application server environments", "id": "292", "src": "java portability put to the test . sun microsystems ' recently launched java verification program aims to enable companies to assess the cross platform portability of applications written in java , and to help software vendors ensure that their solutions can run in heterogenous j2ee application server environments"}
{"title": "The eyes have it [hotel security]", "abstract": "CCTV systems can help lodging establishments accomplish a range of objectives, from deterring criminals to observing staff interactions with clientele. But pitfalls can arise if the CCTV system has not been properly integrated into the overall hotel security plan. CCTV system designs at new hotel properties are often too sophisticated, too complicated, and too costly, and do not take into consideration the security realities of site management. These problems arise when the professionals designing or installing the system, including architects, construction engineers, integrators, and consultants, are not familiar with a hotel's operating strategies or security standards", "id": "293", "src": "the eyes have it hotel security . cctv systems can help lodging establishments accomplish a range of objectives , from deterring criminals to observing staff interactions with clientele . but pitfalls can arise if the cctv system has not been properly integrated into the overall hotel security plan . cctv system designs at new hotel properties are often too sophisticated , too complicated , and too costly , and do not take into consideration the security realities of site management . these problems arise when the professionals designing or installing the system , including architects , construction engineers , integrators , and consultants , are not familiar with a hotel ' s operating strategies or security standards"}
{"title": "Online masquerade: whose e-mail is it?", "abstract": "E-mails carrying viruses like the recent Klez worm use deceptively simple techniques and known vulnerabilities to spread from one computer to another with ease", "id": "294", "src": "online masquerade whose e mail is it . e mails carrying viruses like the recent klez worm use deceptively simple techniques and known vulnerabilities to spread from one computer to another with ease"}
{"title": "Relativistic constraints on the distinguishability of orthogonal quantum states", "abstract": "The constraints imposed by special relativity on the distinguishability of quantum states are discussed. An explicit expression relating the probability of an error in distinguishing two orthogonal single-photon states to their structure, the time t at which a measurement starts, and the interval of time T elapsed from the start of the measurement until the time at which the outcome is obtained by an observer is given as an example", "id": "295", "src": "relativistic constraints on the distinguishability of orthogonal quantum states . the constraints imposed by special relativity on the distinguishability of quantum states are discussed . an explicit expression relating the probability of an error in distinguishing two orthogonal single photon states to their structure , the time t at which a measurement starts , and the interval of time t elapsed from the start of the measurement until the time at which the outcome is obtained by an observer is given as an example"}
{"title": "Rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel", "abstract": "electrophoresis of DNA Large-scale genotyping, mapping and expression profiling require affordable, fully automated high-throughput devices enabling rapid, high-performance analysis using minute quantities of reagents. In this paper, we describe a new combination of microwell polymerase chain reaction (PCR) based DNA amplification technique with automated ultrathin-layer gel electrophoresis analysis of the resulting products. This technique decreases the reagent consumption (total reaction volume 0.75-1 mu L), the time requirement of the PCR (15-20 min) and subsequent ultrathin-layer gel electrophoresis based fragment analysis (5 min) by automating the current manual procedure and reducing the human intervention using sample loading robots and computerized real time data analysis. Small aliquots (0.2 mu L) of the submicroliter size PCR reaction were transferred onto loading membranes and analyzed by ultrathin-layer gel electrophoresis which is a novel, high-performance and automated microseparation technique. This system employs integrated scanning laser-induced fluorescence-avalanche photodiode detection and combines the advantages of conventional slab and capillary gel electrophoresis. Visualization of the DNA fragments was accomplished by \"in migratio\" complexation with ethidium bromide during the electrophoresis process also enabling real time imaging and data analysis", "id": "296", "src": "rapid microwell polymerase chain reaction with subsequent ultrathin layer gel . electrophoresis of dna large scale genotyping , mapping and expression profiling require affordable , fully automated high throughput devices enabling rapid , high performance analysis using minute quantities of reagents . in this paper , we describe a new combination of microwell polymerase chain reaction ( pcr ) based dna amplification technique with automated ultrathin layer gel electrophoresis analysis of the resulting products . this technique decreases the reagent consumption ( total reaction volume 0 . <digit> 1 mu l ) , the time requirement of the pcr ( <digit> <digit> min ) and subsequent ultrathin layer gel electrophoresis based fragment analysis ( 5 min ) by automating the current manual procedure and reducing the human intervention using sample loading robots and computerized real time data analysis . small aliquots ( 0 . 2 mu l ) of the submicroliter size pcr reaction were transferred onto loading membranes and analyzed by ultrathin layer gel electrophoresis which is a novel , high performance and automated microseparation technique . this system employs integrated scanning laser induced fluorescence avalanche photodiode detection and combines the advantages of conventional slab and capillary gel electrophoresis . visualization of the dna fragments was accomplished by in migratio complexation with ethidium bromide during the electrophoresis process also enabling real time imaging and data analysis"}
{"title": "Simple minds [health care IT]", "abstract": "A few things done properly, and soon, is the short-term strategy for the UK NHS IT programme. Can it deliver this time?", "id": "297", "src": "simple minds health care it . a few things done properly , and soon , is the short term strategy for the uk nhs it programme . can it deliver this time"}
{"title": "Absorption of long waves by nonresonant parametric microstructures", "abstract": "Using simple acoustical and mechanical models, we consider the conceptual possibility of designing an active absorbing (nonreflecting) coating in the form of a thin layer with small-scale stratification and fast time modulation of parameters. Algorithms for space-time modulation of the controlled-layer structure are studied in detail for a one-dimensional boundary-value problem. These algorithms do not require wave-field measurements, which eliminates the self-excitation problem that is characteristic of active systems. The majority of the considered algorithms of parametric control transform the low-frequency incident wave to high-frequency waves of the technological band for which the waveguiding medium inside the layer is assumed to be opaque (absorbing). The efficient use conditions are found for all the algorithms. It is shown that the absorbing layer can be as thin as desired with respect to the minimum spatial scale of the incident wave and ensures efficient absorption in a wide frequency interval (starting from zero frequency) that is bounded from above only by a finite space-time resolution of the parameter-control operations. The structure of a three-dimensional parametric \"'black\" coating whose efficiency is independent of the angle of incidence of an incoming wave is developed on the basis of the studied one-dimensional problems. The general solution of the problem of diffraction of incident waves from such a coating is obtained. This solution is analyzed in detail for the case of a disk-shaped element", "id": "298", "src": "absorption of long waves by nonresonant parametric microstructures . using simple acoustical and mechanical models , we consider the conceptual possibility of designing an active absorbing ( nonreflecting ) coating in the form of a thin layer with small scale stratification and fast time modulation of parameters . algorithms for space time modulation of the controlled layer structure are studied in detail for a one dimensional boundary value problem . these algorithms do not require wave field measurements , which eliminates the self excitation problem that is characteristic of active systems . the majority of the considered algorithms of parametric control transform the low frequency incident wave to high frequency waves of the technological band for which the waveguiding medium inside the layer is assumed to be opaque ( absorbing ) . the efficient use conditions are found for all the algorithms . it is shown that the absorbing layer can be as thin as desired with respect to the minimum spatial scale of the incident wave and ensures efficient absorption in a wide frequency interval ( starting from zero frequency ) that is bounded from above only by a finite space time resolution of the parameter control operations . the structure of a three dimensional parametric ' black coating whose efficiency is independent of the angle of incidence of an incoming wave is developed on the basis of the studied one dimensional problems . the general solution of the problem of diffraction of incident waves from such a coating is obtained . this solution is analyzed in detail for the case of a disk shaped element"}
{"title": "2002 in-house fulfillment systems report [publishing]", "abstract": "CM's 13th annual survey of in-house fulfillment system suppliers brings you up to date on the current capabilities of the leading publication software packages", "id": "299", "src": "<digit> in house fulfillment systems report publishing . cm ' s 13th annual survey of in house fulfillment system suppliers brings you up to date on the current capabilities of the leading publication software packages"}
{"title": "Writing the fulfillment RFP [publishing]", "abstract": "For the uninitiated, writing a request for proposal can seem both mysterious and daunting. Here's a format that will make you look like a pro the first time out", "id": "300", "src": "writing the fulfillment rfp publishing . for the uninitiated , writing a request for proposal can seem both mysterious and daunting . here ' s a format that will make you look like a pro the first time out"}
{"title": "Library services today and tomorrow: lessons from iLumina, a digital library", "abstract": "for creating and sharing teaching resources This article is based on the emerging experience associated with a digital library of instructional resources, iLumina, in which the contributors of resources and the users of those resources are the same-an open community of instructors in science, mathematics, engineering, and technology. Moreover, it is not the resources, most of which will be distributed across the Internet, but metadata about the resources that is the focus of the central iLumina repository and its support services for resource contributors and users. The distributed iLumina library is a community-sharing library for repurposing and adding value to potentially useful, mostly non-commercial instructional resources that are typically more granular in nature than commercially developed course materials. The experience of developing iLumina is raising a range of issues that have nothing to do with the place and time characteristics of the instructional context in which iLumina instructional resources are created or used. The issues instead have their locus in the democratization of both the professional roles of librarians and the quality assurance mechanisms associated with traditional peer review", "id": "301", "src": "library services today and tomorrow lessons from ilumina , a digital library . for creating and sharing teaching resources this article is based on the emerging experience associated with a digital library of instructional resources , ilumina , in which the contributors of resources and the users of those resources are the same an open community of instructors in science , mathematics , engineering , and technology . moreover , it is not the resources , most of which will be distributed across the internet , but metadata about the resources that is the focus of the central ilumina repository and its support services for resource contributors and users . the distributed ilumina library is a community sharing library for repurposing and adding value to potentially useful , mostly non commercial instructional resources that are typically more granular in nature than commercially developed course materials . the experience of developing ilumina is raising a range of issues that have nothing to do with the place and time characteristics of the instructional context in which ilumina instructional resources are created or used . the issues instead have their locus in the democratization of both the professional roles of librarians and the quality assurance mechanisms associated with traditional peer review"}
{"title": "The Canadian National Site Licensing Project", "abstract": "In January 2000, a consortium of 64 universities in Canada signed a historic inter-institutional agreement that launched the Canadian National Site Licensing Project (CNSLP), a three-year pilot project aimed at bolstering the research and innovation capacity of the country's universities. CNSLP tests the feasibility of licensing, on a national scale, electronic versions of scholarly publications; in its initial phases the project is focused on full-text electronic journals and research databases in science, engineering, health and environmental disciplines. This article provides an overview of the CNSLP initiative, summarizes organizational and licensing accomplishments to date, and offers preliminary observations on challenges and opportunities for subsequent phases of the project", "id": "302", "src": "the canadian national site licensing project . in january <digit> , a consortium of <digit> universities in canada signed a historic inter institutional agreement that launched the canadian national site licensing project ( cnslp ) , a three year pilot project aimed at bolstering the research and innovation capacity of the country ' s universities . cnslp tests the feasibility of licensing , on a national scale , electronic versions of scholarly publications in its initial phases the project is focused on full text electronic journals and research databases in science , engineering , health and environmental disciplines . this article provides an overview of the cnslp initiative , summarizes organizational and licensing accomplishments to date , and offers preliminary observations on challenges and opportunities for subsequent phases of the project"}
{"title": "The UK's National Electronic Site Licensing Initiative (NESLI)", "abstract": "In 1998 the UK created the National Electronic Site Licensing Initiative (NESLI) to increase and improve access to electronic journals and to negotiate license agreements on behalf of academic libraries. The use of a model license agreement and the success of site licensing is discussed. Highlights from an interim evaluation by the Joint Information Systems Committee (JISC) are noted and key issues and questions arising from the evaluation are identified", "id": "303", "src": "the uk ' s national electronic site licensing initiative ( nesli ) . in <digit> the uk created the national electronic site licensing initiative ( nesli ) to increase and improve access to electronic journals and to negotiate license agreements on behalf of academic libraries . the use of a model license agreement and the success of site licensing is discussed . highlights from an interim evaluation by the joint information systems committee ( jisc ) are noted and key issues and questions arising from the evaluation are identified"}
{"title": "The role of CAUL (Council of Australian Libraries) in consortial purchasing", "abstract": "The Council of Australian University Librarians, constituted in 1965 for the purposes of cooperative action and the sharing of information, assumed the role of consortial purchasing agent in 1996 on behalf of its members and associate organisations in Australia and New Zealand. This role continues to grow in tandem with the burgeoning of electronic publication and the acceptance of publishers of the advantages of dealing with consortia. The needs of the Australian university community overlap significantly with consortia in North America and Europe, but important differences are highlighted", "id": "304", "src": "the role of caul ( council of australian libraries ) in consortial purchasing . the council of australian university librarians , constituted in <digit> for the purposes of cooperative action and the sharing of information , assumed the role of consortial purchasing agent in <digit> on behalf of its members and associate organisations in australia and new zealand . this role continues to grow in tandem with the burgeoning of electronic publication and the acceptance of publishers of the advantages of dealing with consortia . the needs of the australian university community overlap significantly with consortia in north america and europe , but important differences are highlighted"}
{"title": "Licensing experiences in the Netherlands", "abstract": "The licensing strategy of university libraries in the Netherlands is closely connected with university policies to develop document servers and to make research publications available on the Web. National agreements have been made with major publishers, such as Elsevier Science and Kluwer Academic, to provide access to a wide range of scientific information and to experiment with new ways of providing information and new business models", "id": "305", "src": "licensing experiences in the netherlands . the licensing strategy of university libraries in the netherlands is closely connected with university policies to develop document servers and to make research publications available on the web . national agreements have been made with major publishers , such as elsevier science and kluwer academic , to provide access to a wide range of scientific information and to experiment with new ways of providing information and new business models"}
{"title": "International library consortia: positive starts, promising futures", "abstract": "Library consortia have grown substantially over the past ten years, both within North America and globally. As this resurgent consortial movement has begun to mature, and as publishers and vendors have begun to adapt to consortial purchasing models, consortia have expanded their agendas for action. The movement to globalize consortia is traced (including the development and current work of the International Coalition of Library Consortia-ICOLC). A methodology is explored to classify library consortia by articulating the key factors that affect and distinguish consortia as organizations within three major areas: strategic, tactical, and practical (or managerial) concerns. Common consortial values are examined, and a list of known international library consortia is presented", "id": "306", "src": "international library consortia positive starts , promising futures . library consortia have grown substantially over the past ten years , both within north america and globally . as this resurgent consortial movement has begun to mature , and as publishers and vendors have begun to adapt to consortial purchasing models , consortia have expanded their agendas for action . the movement to globalize consortia is traced ( including the development and current work of the international coalition of library consortia icolc ) . a methodology is explored to classify library consortia by articulating the key factors that affect and distinguish consortia as organizations within three major areas strategic , tactical , and practical ( or managerial ) concerns . common consortial values are examined , and a list of known international library consortia is presented"}
{"title": "The Open Archives Initiative: realizing simple and effective digital library", "abstract": "interoperability The Open Archives Initiative (OAI) is dedicated to solving problems of digital library interoperability. Its focus has been on defining simple protocols, most recently for the exchange of metadata from archives. The OAI evolved out of a need to increase access to scholarly publications by supporting the creation of interoperable digital libraries. As a first step towards such interoperability, a metadata harvesting protocol was developed to support the streaming of metadata from one repository to another, ultimately to a provider of user services such as browsing, searching, or annotation. This article provides an overview of the mission, philosophy, and technical framework of the OAI", "id": "307", "src": "the open archives initiative realizing simple and effective digital library . interoperability the open archives initiative ( oai ) is dedicated to solving problems of digital library interoperability . its focus has been on defining simple protocols , most recently for the exchange of metadata from archives . the oai evolved out of a need to increase access to scholarly publications by supporting the creation of interoperable digital libraries . as a first step towards such interoperability , a metadata harvesting protocol was developed to support the streaming of metadata from one repository to another , ultimately to a provider of user services such as browsing , searching , or annotation . this article provides an overview of the mission , philosophy , and technical framework of the oai"}
{"title": "Content standards for electronic books: the OEBF publication structure and the", "abstract": "role of public interest participation In the emerging world of electronic publishing how we create, distribute, and read books will be in a large part determined by an underlying framework of content standards that establishes the range of technological opportunities and constraints for publishing and reading systems. But efforts to develop content standards based on sound engineering models must skillfully negotiate competing and sometimes apparently irreconcilable objectives if they are to produce results relevant to the rapidly changing course of technology. The Open eBook Forum's Publication Structure, an XML-based specification for electronic books, is an example of the sort of timely and innovative problem solving required for successful real-world standards development. As a result of this effort, the electronic book industry will not only happen sooner and on a larger scale than it would have otherwise, but the electronic books it produces will be more functional, more interoperable, and more accessible to all readers. Public interest participants have a critical role in this process", "id": "308", "src": "content standards for electronic books the oebf publication structure and the . role of public interest participation in the emerging world of electronic publishing how we create , distribute , and read books will be in a large part determined by an underlying framework of content standards that establishes the range of technological opportunities and constraints for publishing and reading systems . but efforts to develop content standards based on sound engineering models must skillfully negotiate competing and sometimes apparently irreconcilable objectives if they are to produce results relevant to the rapidly changing course of technology . the open ebook forum ' s publication structure , an xml based specification for electronic books , is an example of the sort of timely and innovative problem solving required for successful real world standards development . as a result of this effort , the electronic book industry will not only happen sooner and on a larger scale than it would have otherwise , but the electronic books it produces will be more functional , more interoperable , and more accessible to all readers . public interest participants have a critical role in this process"}
{"title": "Fuzzy modeling based on generalized conjunction operations", "abstract": "An approach to fuzzy modeling based on the tuning of parametric conjunction operations is proposed. First, some methods for the construction of parametric generalized conjunction operations simpler than the known parametric classes of conjunctions are considered and discussed. Second, several examples of function approximation by fuzzy models, based on the tuning of the parameters of the new conjunction operations, are given and their approximation performances are compared with the approaches based on a tuning of membership functions and other approaches proposed in the literature. It is seen that the tuning of the conjunction operations can be used for obtaining fuzzy models with a sufficiently good performance when the tuning of membership functions is not possible or not desirable", "id": "309", "src": "fuzzy modeling based on generalized conjunction operations . an approach to fuzzy modeling based on the tuning of parametric conjunction operations is proposed . first , some methods for the construction of parametric generalized conjunction operations simpler than the known parametric classes of conjunctions are considered and discussed . second , several examples of function approximation by fuzzy models , based on the tuning of the parameters of the new conjunction operations , are given and their approximation performances are compared with the approaches based on a tuning of membership functions and other approaches proposed in the literature . it is seen that the tuning of the conjunction operations can be used for obtaining fuzzy models with a sufficiently good performance when the tuning of membership functions is not possible or not desirable"}
{"title": "Project Euclid and the role of research libraries in scholarly publishing", "abstract": "Project Euclid, a joint electronic journal publishing initiative of Cornell University Library and Duke University Press is discussed in the broader contexts of the changing patterns of scholarly communication and the publishing scene of mathematics. Specific aspects of the project such as partnerships and the creation of an economic model are presented as well as what it takes to be a publisher. Libraries have gained important and relevant experience through the creation and management of digital libraries, but they need to develop further skills if they want to adopt a new role in the life cycle of scholarly communication", "id": "310", "src": "project euclid and the role of research libraries in scholarly publishing . project euclid , a joint electronic journal publishing initiative of cornell university library and duke university press is discussed in the broader contexts of the changing patterns of scholarly communication and the publishing scene of mathematics . specific aspects of the project such as partnerships and the creation of an economic model are presented as well as what it takes to be a publisher . libraries have gained important and relevant experience through the creation and management of digital libraries , but they need to develop further skills if they want to adopt a new role in the life cycle of scholarly communication"}
{"title": "Perspectives on scholarly online books: the Columbia University Online Books", "abstract": "Evaluation Project The Online Books Evaluation Project at Columbia University studied the potential for scholarly online books from 1995 to 1999. Issues included scholars' interest in using online books, the role they might play in scholarly life, features that scholars and librarians sought in online books, the costs of producing and owning print and online books, and potential marketplace arrangements. Scholars see potential for online books to make their research, learning, and teaching more efficient and effective. Librarians see potential to serve their scholars better. Librarians may face lower costs if they can serve their scholars with online books instead of print books. Publishers may be able to offer scholars greater opportunities to use their books while enhancing their own profitability", "id": "311", "src": "perspectives on scholarly online books the columbia university online books . evaluation project the online books evaluation project at columbia university studied the potential for scholarly online books from <digit> to <digit> . issues included scholars ' interest in using online books , the role they might play in scholarly life , features that scholars and librarians sought in online books , the costs of producing and owning print and online books , and potential marketplace arrangements . scholars see potential for online books to make their research , learning , and teaching more efficient and effective . librarians see potential to serve their scholars better . librarians may face lower costs if they can serve their scholars with online books instead of print books . publishers may be able to offer scholars greater opportunities to use their books while enhancing their own profitability"}
{"title": "The California Digital Library and the eScholarship program", "abstract": "The eScholarship program was launched in 2000 to foster faculty-led innovation in scholarly publishing. An initiative of the University of California (UC) and a program of the California Digital Library, the eScholarship program has stimulated significant interest in its short life. Its modest but visible accomplishments garner praise from many quarters, within and beyond the University of California. In perhaps the best indication of its timeliness and momentum, there are more proposals submitted to eScholarship today than the CDL can manage. This early success is due in part to the sheer power of an idea whose time has come, but also to the unique approach on which CDL was founded and the eScholarship initiative was first launched", "id": "312", "src": "the california digital library and the escholarship program . the escholarship program was launched in <digit> to foster faculty led innovation in scholarly publishing . an initiative of the university of california ( uc ) and a program of the california digital library , the escholarship program has stimulated significant interest in its short life . its modest but visible accomplishments garner praise from many quarters , within and beyond the university of california . in perhaps the best indication of its timeliness and momentum , there are more proposals submitted to escholarship today than the cdl can manage . this early success is due in part to the sheer power of an idea whose time has come , but also to the unique approach on which cdl was founded and the escholarship initiative was first launched"}
{"title": "BioOne: a new model for scholarly publishing", "abstract": "This article describes a unique electronic journal publishing project involving the University of Kansas, the Big 12 Plus Libraries Consortium, the American Institute of Biological Sciences, Allen Press, and SPARC, the Scholarly Publishing and Academic Resources Coalition. This partnership has created BioOne, a database of 40 full-text society journals in the biological and environmental sciences, which was launched in April, 2001. The genesis and development of the project is described and financial, technical, and intellectual property models for the project are discussed. Collaborative strategies for the project are described", "id": "313", "src": "bioone a new model for scholarly publishing . this article describes a unique electronic journal publishing project involving the university of kansas , the big <digit> plus libraries consortium , the american institute of biological sciences , allen press , and sparc , the scholarly publishing and academic resources coalition . this partnership has created bioone , a database of <digit> full text society journals in the biological and environmental sciences , which was launched in april , <digit> . the genesis and development of the project is described and financial , technical , and intellectual property models for the project are discussed . collaborative strategies for the project are described"}
{"title": "Symbiosis or alienation: advancing the university press/research library", "abstract": "relationship through electronic scholarly communication University presses and research libraries have a long tradition of collaboration. The rapidly expanding electronic scholarly communication environment offers important new opportunities for cooperation and for innovative new models of publishing. The economics of libraries and scholarly publishers have strained the working relationship and promoted debates on important information policy issues. This article explores the context for advancing the partnership, cites examples of joint efforts in electronic publishing, and presents an action plan for working together", "id": "314", "src": "symbiosis or alienation advancing the university press research library . relationship through electronic scholarly communication university presses and research libraries have a long tradition of collaboration . the rapidly expanding electronic scholarly communication environment offers important new opportunities for cooperation and for innovative new models of publishing . the economics of libraries and scholarly publishers have strained the working relationship and promoted debates on important information policy issues . this article explores the context for advancing the partnership , cites examples of joint efforts in electronic publishing , and presents an action plan for working together"}
{"title": "Support vector machines model for classification of thermal error in machine", "abstract": "tools This paper addresses a change in the concept of machine tool thermal error prediction which has been hitherto carried out by directly mapping them with the temperature of critical elements on the machine. The model developed herein using support vector machines, a powerful data-training algorithm, seeks to account for the impact of specific operating conditions, in addition to temperature variation, on the effective prediction of thermal errors. Several experiments were conducted to study the error pattern, which was found to change significantly with variation in operating conditions. This model attempts to classify the error based on operating conditions. Once classified, the error is then predicted based on the temperature states. This paper also briefly describes the concept of the implementation of such a comprehensive model along with an on-line error assessment and calibration system in a PC-based open-architecture controller environment, so that it could be employed in regular production for the purpose of periodic calibration of machine tools", "id": "315", "src": "support vector machines model for classification of thermal error in machine . tools this paper addresses a change in the concept of machine tool thermal error prediction which has been hitherto carried out by directly mapping them with the temperature of critical elements on the machine . the model developed herein using support vector machines , a powerful data training algorithm , seeks to account for the impact of specific operating conditions , in addition to temperature variation , on the effective prediction of thermal errors . several experiments were conducted to study the error pattern , which was found to change significantly with variation in operating conditions . this model attempts to classify the error based on operating conditions . once classified , the error is then predicted based on the temperature states . this paper also briefly describes the concept of the implementation of such a comprehensive model along with an on line error assessment and calibration system in a pc based open architecture controller environment , so that it could be employed in regular production for the purpose of periodic calibration of machine tools"}
{"title": "Adaptive and efficient mutual exclusion", "abstract": "The paper presents adaptive algorithms for mutual exclusion using only read and write operations; the performance of the algorithms depends only on the point contention, i.e., the number of processes that are concurrently active during algorithm execution (and not on n, the total number of processes). Our algorithm has O(k) remote step complexity and O(log k) system response time, where k is the point contention. The remote step complexity is the maximal number of steps performed by a process where a wait is counted as one step. The system response time is the time interval between subsequent entries to the critical section, where one time unit is the minimal interval in which every active process performs at least one step. The space complexity of this algorithm is O(N log n), where N is the range of process names. We show how to make the space complexity of our algorithm depend solely on n, while preserving the other performance measures of the algorithm", "id": "316", "src": "adaptive and efficient mutual exclusion . the paper presents adaptive algorithms for mutual exclusion using only read and write operations the performance of the algorithms depends only on the point contention , i . e . , the number of processes that are concurrently active during algorithm execution ( and not on n , the total number of processes ) . our algorithm has o ( k ) remote step complexity and o ( log k ) system response time , where k is the point contention . the remote step complexity is the maximal number of steps performed by a process where a wait is counted as one step . the system response time is the time interval between subsequent entries to the critical section , where one time unit is the minimal interval in which every active process performs at least one step . the space complexity of this algorithm is o ( n log n ) , where n is the range of process names . we show how to make the space complexity of our algorithm depend solely on n , while preserving the other performance measures of the algorithm"}
{"title": "The congenial talking philosophers problem in computer networks", "abstract": "Group mutual exclusion occurs naturally in situations where a resource can be shared by processes of the same group, but not by processes of different groups. For example, suppose data is stored in a CD-jukebox. Then, when a disc is loaded for access, users that need data on the disc can concurrently access the disc, while users that need data on a different disc have to wait until the current disc is unloaded. The design issues for group mutual exclusion have been modeled as the Congenial Talking Philosophers problem, and solutions for shared memory models have been proposed (Y.-J. Young, 2000; P. Keane and M. Moir, 1999). As in ordinary mutual exclusion and many other problems in distributed systems, however, techniques developed for shared memory do not necessarily apply to message passing (and vice versa). We investigate solutions for Congenial Talking Philosophers in computer networks where processes communicate by asynchronous message passing. We first present a solution that is a straightforward adaptation from G. Ricart and A.K. Agrawala's (1981) algorithm for ordinary mutual exclusion. Then we show that the simple modification suffers a severe performance degradation that could cause the system to behave as though only one process of a group can be in the critical section at a time. We then present a more efficient and highly concurrent distributed algorithm for the problem, the first such solution in computer networks", "id": "317", "src": "the congenial talking philosophers problem in computer networks . group mutual exclusion occurs naturally in situations where a resource can be shared by processes of the same group , but not by processes of different groups . for example , suppose data is stored in a cd jukebox . then , when a disc is loaded for access , users that need data on the disc can concurrently access the disc , while users that need data on a different disc have to wait until the current disc is unloaded . the design issues for group mutual exclusion have been modeled as the congenial talking philosophers problem , and solutions for shared memory models have been proposed ( y . j . young , <digit> p . keane and m . moir , <digit> ) . as in ordinary mutual exclusion and many other problems in distributed systems , however , techniques developed for shared memory do not necessarily apply to message passing ( and vice versa ) . we investigate solutions for congenial talking philosophers in computer networks where processes communicate by asynchronous message passing . we first present a solution that is a straightforward adaptation from g . ricart and a . k . agrawala ' s ( <digit> ) algorithm for ordinary mutual exclusion . then we show that the simple modification suffers a severe performance degradation that could cause the system to behave as though only one process of a group can be in the critical section at a time . we then present a more efficient and highly concurrent distributed algorithm for the problem , the first such solution in computer networks"}
{"title": "Universal dynamic synchronous self-stabilization", "abstract": "We prove the existence of a \"universal\" synchronous self-stabilizing protocol, that is, a protocol that allows a distributed system to stabilize to a desired nonreactive behaviour (as long as a protocol stabilizing to that behaviour exists). Previous proposals required drastic increases in asymmetry and knowledge to work, whereas our protocol does not use any additional knowledge, and does not require more symmetry-breaking conditions than available; thus, it is also stabilizing with respect to dynamic changes in the topology. We prove an optimal quiescence time n + D for a synchronous network of n processors and diameter D; the protocol can be made finite state with a negligible loss in quiescence time. Moreover, an optimal D + 1 protocol is given for the case of unique identifiers. As a consequence, we provide an effective proof technique that allows one to show whether self-stabilization to a certain behaviour is possible under a wide range of models", "id": "318", "src": "universal dynamic synchronous self stabilization . we prove the existence of a universal synchronous self stabilizing protocol , that is , a protocol that allows a distributed system to stabilize to a desired nonreactive behaviour ( as long as a protocol stabilizing to that behaviour exists ) . previous proposals required drastic increases in asymmetry and knowledge to work , whereas our protocol does not use any additional knowledge , and does not require more symmetry breaking conditions than available thus , it is also stabilizing with respect to dynamic changes in the topology . we prove an optimal quiescence time n + d for a synchronous network of n processors and diameter d the protocol can be made finite state with a negligible loss in quiescence time . moreover , an optimal d + 1 protocol is given for the case of unique identifiers . as a consequence , we provide an effective proof technique that allows one to show whether self stabilization to a certain behaviour is possible under a wide range of models"}
{"title": "Randomized two-process wait-free test-and-set", "abstract": "We present the first explicit, and currently simplest, randomized algorithm for two-process wait-free test-and-set. It is implemented with two 4-valued single writer single reader atomic variables. A test-and-set takes at most 11 expected elementary steps, while a reset takes exactly 1 elementary step. Based on a finite-state analysis, the proofs of correctness and expected length are compressed into one table", "id": "319", "src": "randomized two process wait free test and set . we present the first explicit , and currently simplest , randomized algorithm for two process wait free test and set . it is implemented with two 4 valued single writer single reader atomic variables . a test and set takes at most <digit> expected elementary steps , while a reset takes exactly 1 elementary step . based on a finite state analysis , the proofs of correctness and expected length are compressed into one table"}
{"title": "Identification of evolving fuzzy rule-based models", "abstract": "An approach to identification of evolving fuzzy rule-based (eR) models is proposed. eR models implement a method for the noniterative update of both the rule-base structure and parameters by incremental unsupervised learning. The rule-base evolves by adding more informative rules than those that previously formed the model. In addition, existing rules can be replaced with new rules based on ranking using the informative potential of the data. In this way, the rule-base structure is inherited and updated when new informative data become available, rather than being completely retrained. The adaptive nature of these evolving rule-based models, in combination with the highly transparent and compact form of fuzzy rules, makes them a promising candidate for modeling and control of complex processes, competitive to neural networks. The approach has been tested on a benchmark problem and on an air-conditioning component modeling application using data from an installation serving a real building. The results illustrate the viability and efficiency of the approach", "id": "320", "src": "identification of evolving fuzzy rule based models . an approach to identification of evolving fuzzy rule based ( er ) models is proposed . er models implement a method for the noniterative update of both the rule base structure and parameters by incremental unsupervised learning . the rule base evolves by adding more informative rules than those that previously formed the model . in addition , existing rules can be replaced with new rules based on ranking using the informative potential of the data . in this way , the rule base structure is inherited and updated when new informative data become available , rather than being completely retrained . the adaptive nature of these evolving rule based models , in combination with the highly transparent and compact form of fuzzy rules , makes them a promising candidate for modeling and control of complex processes , competitive to neural networks . the approach has been tested on a benchmark problem and on an air conditioning component modeling application using data from an installation serving a real building . the results illustrate the viability and efficiency of the approach"}
{"title": "Aim for the enterprise: Microsoft Project 2002", "abstract": "A long-time favorite of project managers, Microsoft Project 2002 is making its enterprise debut. Its new Web-based collaboration tools and improved scalability with OLAP support make it much easier to manage multiple Web projects with disparate workgroups and budgets", "id": "321", "src": "aim for the enterprise microsoft project <digit> . a long time favorite of project managers , microsoft project <digit> is making its enterprise debut . its new web based collaboration tools and improved scalability with olap support make it much easier to manage multiple web projects with disparate workgroups and budgets"}
{"title": "Central hub for design assets: Adobe GoLive 6.0", "abstract": "Adobe GoLive is a strong contender for Web authoring and publishing. Version 6.0 features a flexible GUI environment combined with a comprehensive workgroup and collaboration server, plus tight integration with leading design tools", "id": "322", "src": "central hub for design assets adobe golive 6 . 0 . adobe golive is a strong contender for web authoring and publishing . version 6 . 0 features a flexible gui environment combined with a comprehensive workgroup and collaboration server , plus tight integration with leading design tools"}
{"title": "Reaching for five nines: ActiveWatch and SiteSeer", "abstract": "Every Web admin's dream is achieving the fabled five nines-99.999 percent uptime. To attain such availability, your Web site must be down no more than about five minutes per year. Technologies like RAID, clustering, and load balancing make this easier, but to actually track uptime, maintain auditable records, and discover patterns in failures to prevent downtime in the future, you'll need to set up external monitoring. Because your Internet connection is a key factor in measuring uptime, you must monitor your site from the Internet itself, beyond your firewall. You could monitor with custom software on remote hosts, or you could use one of the two reasonably priced services available: Mercury Interactive's ActiveWatch and Freshwater Software's SiteSeer. (Freshwater Software has been a subsidiary of Mercury Interactive for about a year now.) The two services offer a slightly different mix of features and target different markets. Both services offer availability and performance monitoring from several remote locations, alerts to email or pager, and periodic reports. They differ in what's most easily monitored, and in the way you interact with the services", "id": "323", "src": "reaching for five nines activewatch and siteseer . every web admin ' s dream is achieving the fabled five nines <digit> . <digit> percent uptime . to attain such availability , your web site must be down no more than about five minutes per year . technologies like raid , clustering , and load balancing make this easier , but to actually track uptime , maintain auditable records , and discover patterns in failures to prevent downtime in the future , you ' ll need to set up external monitoring . because your internet connection is a key factor in measuring uptime , you must monitor your site from the internet itself , beyond your firewall . you could monitor with custom software on remote hosts , or you could use one of the two reasonably priced services available mercury interactive ' s activewatch and freshwater software ' s siteseer . ( freshwater software has been a subsidiary of mercury interactive for about a year now . ) the two services offer a slightly different mix of features and target different markets . both services offer availability and performance monitoring from several remote locations , alerts to email or pager , and periodic reports . they differ in what ' s most easily monitored , and in the way you interact with the services"}
{"title": "Accessible streaming content", "abstract": "Make sure your Web site is offering quality service to all your users. The article provides some tips and tactics for making your streaming media accessible. Accessibility of streaming content for people with disabilities is often not part of the spec for multimedia projects, but it certainly affects your quality of service. Most of the resources available on Web accessibility deal with HTML. Fortunately, rich media and streaming content developers have a growing number of experts to turn to for information and assistance. The essentials of providing accessible streaming content are simple: blind and visually impaired people need audio to discern important visual detail and interface elements, while deaf and hard-of-hearing people need text to access sound effects and dialog. Actually implementing these principles is quite a challenge, though. Now due to a relatively new law in the US, known as Section 508, dealing with accessibility issues is becoming an essential part of publishing on the Web", "id": "324", "src": "accessible streaming content . make sure your web site is offering quality service to all your users . the article provides some tips and tactics for making your streaming media accessible . accessibility of streaming content for people with disabilities is often not part of the spec for multimedia projects , but it certainly affects your quality of service . most of the resources available on web accessibility deal with html . fortunately , rich media and streaming content developers have a growing number of experts to turn to for information and assistance . the essentials of providing accessible streaming content are simple blind and visually impaired people need audio to discern important visual detail and interface elements , while deaf and hard of hearing people need text to access sound effects and dialog . actually implementing these principles is quite a challenge , though . now due to a relatively new law in the us , known as section <digit> , dealing with accessibility issues is becoming an essential part of publishing on the web"}
{"title": "What you get is what you see [Web performance monitoring]", "abstract": "To get the best possible performance from your Web infrastructure, you'll need a complete view. Don't neglect the big picture because you're too busy concentrating on details. The increasing complexity of Web sites and the content they provide has consequently increased the complexity of the infrastructure that supports them. But with some knowledge of networking, a handful of useful tools, and the insight that those tools provide, designing and operating for optimal performance and reliability is within your grasp", "id": "325", "src": "what you get is what you see web performance monitoring . to get the best possible performance from your web infrastructure , you ' ll need a complete view . don ' t neglect the big picture because you ' re too busy concentrating on details . the increasing complexity of web sites and the content they provide has consequently increased the complexity of the infrastructure that supports them . but with some knowledge of networking , a handful of useful tools , and the insight that those tools provide , designing and operating for optimal performance and reliability is within your grasp"}
{"title": "The culture of usability", "abstract": "Now that most of us agree that usability testing is an integral investment in site development, it's time to recognize that the standard approach falls short. It is possible to do less work and get better results while spending less money. By bringing usability testing in-house and breaking tests into more manageable sessions, you can vastly improve your online offering without affecting your profit margin", "id": "326", "src": "the culture of usability . now that most of us agree that usability testing is an integral investment in site development , it ' s time to recognize that the standard approach falls short . it is possible to do less work and get better results while spending less money . by bringing usability testing in house and breaking tests into more manageable sessions , you can vastly improve your online offering without affecting your profit margin"}
{"title": "Debugging Web applications", "abstract": "The author considers how one can save time tracking down bugs in Web-based applications by arming yourself with the right tools and programming practices. A wide variety of debugging tools have been written with Web developers in mind", "id": "327", "src": "debugging web applications . the author considers how one can save time tracking down bugs in web based applications by arming yourself with the right tools and programming practices . a wide variety of debugging tools have been written with web developers in mind"}
{"title": "Unsafe at any speed?", "abstract": "While Sun prides itself on Java's secure sandbox programming model, Microsoft takes a looser approach. Its C# language incorporates C-like concepts, including pointers and memory management. But is unsafe code really a boon to programmers, or is it a step backward?", "id": "328", "src": "unsafe at any speed . while sun prides itself on java ' s secure sandbox programming model , microsoft takes a looser approach . its c# language incorporates c like concepts , including pointers and memory management . but is unsafe code really a boon to programmers , or is it a step backward"}
{"title": "Building digital collections at the OAC: current strategies with a view to", "abstract": "future uses Providing a context for the exploration of user defined virtual collections, the article describes the history and recent development of the Online Archive of California (OAC). Stating that usability and user needs are primary factors in digital resource development, issues explored include collaborations to build digital collections, reliance upon professional standards for description and encoding, system architecture, interface design, the need for user tools, and the role of archivists as interpreters in the digital environment", "id": "329", "src": "building digital collections at the oac current strategies with a view to . future uses providing a context for the exploration of user defined virtual collections , the article describes the history and recent development of the online archive of california ( oac ) . stating that usability and user needs are primary factors in digital resource development , issues explored include collaborations to build digital collections , reliance upon professional standards for description and encoding , system architecture , interface design , the need for user tools , and the role of archivists as interpreters in the digital environment"}
{"title": "Nuts and bolts: implementing descriptive standards to enable virtual", "abstract": "collections To date, online archival information systems have relied heavily on legacy finding aids for data to encode and provide to end users, despite fairly strong indications in the archival literature that such legacy data is problematic even as a mediated access tool. Archivists have only just begun to study the utility of archival descriptive data for end users in unmediated settings such as via the Web. The ability of future archival information systems to respond to the expectations and needs of end users is inextricably linked to archivists getting their collective data house in order. The General International Standard Archival Description (ISAD(G)) offers the profession a place from which to start extricating ourselves from the idiosyncracies of our legacy data and description practices", "id": "330", "src": "nuts and bolts implementing descriptive standards to enable virtual . collections to date , online archival information systems have relied heavily on legacy finding aids for data to encode and provide to end users , despite fairly strong indications in the archival literature that such legacy data is problematic even as a mediated access tool . archivists have only just begun to study the utility of archival descriptive data for end users in unmediated settings such as via the web . the ability of future archival information systems to respond to the expectations and needs of end users is inextricably linked to archivists getting their collective data house in order . the general international standard archival description ( isad ( g ) ) offers the profession a place from which to start extricating ourselves from the idiosyncracies of our legacy data and description practices"}
{"title": "Learning weights for the quasi-weighted means", "abstract": "We study the determination of weights for quasi-weighted means (also called quasi-linear means) when a set of examples is given. We consider first a simple case, the learning of weights for weighted means, and then we extend the approach to the more general case of a quasi-weighted mean. We consider the case of a known arbitrary generator f. The paper finishes considering the use of parametric functions that are suitable when the values to aggregate are measure values or ratio", "id": "331", "src": "learning weights for the quasi weighted means . we study the determination of weights for quasi weighted means ( also called quasi linear means ) when a set of examples is given . we consider first a simple case , the learning of weights for weighted means , and then we extend the approach to the more general case of a quasi weighted mean . we consider the case of a known arbitrary generator f . the paper finishes considering the use of parametric functions that are suitable when the values to aggregate are measure values or ratio"}
{"title": "Prospecting virtual collections", "abstract": "Virtual collections are a distinct sub-species of digital collections and digital archives. Archivists and curators as archivists and curators do not construct virtual collections; rather they enable virtual collections through the application of descriptive and other standards. Virtual collections are constructed by end users", "id": "332", "src": "prospecting virtual collections . virtual collections are a distinct sub species of digital collections and digital archives . archivists and curators as archivists and curators do not construct virtual collections rather they enable virtual collections through the application of descriptive and other standards . virtual collections are constructed by end users"}
{"title": "Union outreach - a pilgrim's progress", "abstract": "As the American labor movement continues on its path toward reorganization and rejuvenation, archivists are challenged to ensure that the organizational, political, and cultural changes labor unions are experiencing are fully documented. The article examines the need for labor archivists to reach out actively to unions and the problems they face in getting their message across, not only to union leadership but also to union members. Outreach by labor archivists is vital on three critical fronts: the need to secure union funding in support of labor archival programs; obtaining union cooperation in reviewing and amending obsolete deposit agreements; and coordinating efforts with unions to save the records of closing district and local union offices. Attempting to resolve these outstanding issues, labor archivists are pulled between two distinct institutional cultures (one academic in nature, the other enmeshed in a union bureaucracy) and often have their own labor archival programs compromised by the internal dynamics and politics inherent in administering large academic libraries and unions. If labor archivists are to be successful, they must find their collective voice within the labor movement and establish their relevancy to unions during a period of momentous change and restructuring. Moreover, archivists need to give greater thought to designing and implementing outreach programs that bridge the fundamental \"disconnect\" between union bureaucracies and the rank and file, and unions and the public", "id": "333", "src": "union outreach a pilgrim ' s progress . as the american labor movement continues on its path toward reorganization and rejuvenation , archivists are challenged to ensure that the organizational , political , and cultural changes labor unions are experiencing are fully documented . the article examines the need for labor archivists to reach out actively to unions and the problems they face in getting their message across , not only to union leadership but also to union members . outreach by labor archivists is vital on three critical fronts the need to secure union funding in support of labor archival programs obtaining union cooperation in reviewing and amending obsolete deposit agreements and coordinating efforts with unions to save the records of closing district and local union offices . attempting to resolve these outstanding issues , labor archivists are pulled between two distinct institutional cultures ( one academic in nature , the other enmeshed in a union bureaucracy ) and often have their own labor archival programs compromised by the internal dynamics and politics inherent in administering large academic libraries and unions . if labor archivists are to be successful , they must find their collective voice within the labor movement and establish their relevancy to unions during a period of momentous change and restructuring . moreover , archivists need to give greater thought to designing and implementing outreach programs that bridge the fundamental disconnect between union bureaucracies and the rank and file , and unions and the public"}
{"title": "The impact of EAD adoption on archival programs: a pilot survey of early", "abstract": "implementers The article reports the results of a survey conducted to assess the impact that the implementation of Encoded Archival Description (EAD) has on archival programs. By gathering data related to the funding, staffing, and evaluation of EAD programs and about institutional goals for EAD implementation, the study explored how EAD has affected the operations of the institutions which are utilizing it and the extent to which EAD has become a part of regular repository functions", "id": "334", "src": "the impact of ead adoption on archival programs a pilot survey of early . implementers the article reports the results of a survey conducted to assess the impact that the implementation of encoded archival description ( ead ) has on archival programs . by gathering data related to the funding , staffing , and evaluation of ead programs and about institutional goals for ead implementation , the study explored how ead has affected the operations of the institutions which are utilizing it and the extent to which ead has become a part of regular repository functions"}
{"title": "K-12 instruction and digital access to archival materials", "abstract": "Providing K-12 schools with digital access to archival materials can strengthen both student learning and archival practice, although it cannot replace direct physical access to records. The article compares a variety of electronic and nonelectronic projects to promote teaching with primary source materials. The article also examines some of the different historiographical and pedagogical approaches used in archival Web sites geared for K-12 instruction, focusing on differences between the educational sites sponsored by the Library of Congress and the National Archives and Records Administration", "id": "335", "src": "k <digit> instruction and digital access to archival materials . providing k <digit> schools with digital access to archival materials can strengthen both student learning and archival practice , although it cannot replace direct physical access to records . the article compares a variety of electronic and nonelectronic projects to promote teaching with primary source materials . the article also examines some of the different historiographical and pedagogical approaches used in archival web sites geared for k <digit> instruction , focusing on differences between the educational sites sponsored by the library of congress and the national archives and records administration"}
{"title": "The archival imagination of David Bearman, revisited", "abstract": "Many archivists regard the archival imagination evidenced in the writings of David Bearman as avant-garde. Archivist L. Henry (1998) has sharply criticized Bearman for being irreverent toward the archival theory and practice outlined by classical American archivist T. R. Schellenberg. Although Bearman is sometimes credited (and sometimes berated) for establishing \"a new paradigm\" centered on the archival management of electronic records, his methods and strategies are intended to encompass all forms of record keeping. The article provides general observations on Bearman's archival imagination, lists some of its components, and addresses elements of Henry's critique. Although the long lasting impact of Bearman's imagination upon the archival profession might be questioned, it nonetheless deserves continued consideration by archivists and inclusion as a component of graduate archival education", "id": "336", "src": "the archival imagination of david bearman , revisited . many archivists regard the archival imagination evidenced in the writings of david bearman as avant garde . archivist l . henry ( <digit> ) has sharply criticized bearman for being irreverent toward the archival theory and practice outlined by classical american archivist t . r . schellenberg . although bearman is sometimes credited ( and sometimes berated ) for establishing a new paradigm centered on the archival management of electronic records , his methods and strategies are intended to encompass all forms of record keeping . the article provides general observations on bearman ' s archival imagination , lists some of its components , and addresses elements of henry ' s critique . although the long lasting impact of bearman ' s imagination upon the archival profession might be questioned , it nonetheless deserves continued consideration by archivists and inclusion as a component of graduate archival education"}
{"title": "Pattern recognition strategies for molecular surfaces. II. Surface", "abstract": "complementarity For pt.I see ibid., vol.23, p.1176-87 (2002). Fuzzy logic based algorithms for the quantitative treatment of complementarity of molecular surfaces are presented. Therein, the overlapping surface patches defined in part I of this series are used. The identification of complementary surface patches can be considered as a first step for the solution of molecular docking problems. Standard technologies can then be used for further optimization of the resulting complex structures. The algorithms are applied to 33 biomolecular complexes. After the optimization with a downhill simplex method, for all these complexes one structure was found, which is in very good agreement with the experimental results", "id": "337", "src": "pattern recognition strategies for molecular surfaces . ii . surface . complementarity for pt . i see ibid . , vol . <digit> , p . <digit> <digit> ( <digit> ) . fuzzy logic based algorithms for the quantitative treatment of complementarity of molecular surfaces are presented . therein , the overlapping surface patches defined in part i of this series are used . the identification of complementary surface patches can be considered as a first step for the solution of molecular docking problems . standard technologies can then be used for further optimization of the resulting complex structures . the algorithms are applied to <digit> biomolecular complexes . after the optimization with a downhill simplex method , for all these complexes one structure was found , which is in very good agreement with the experimental results"}
{"title": "Pattern recognition strategies for molecular surfaces. I. Pattern generation", "abstract": "using fuzzy set theory A new method for the characterization of molecules based on the model approach of molecular surfaces is presented. We use the topographical properties of the surface as well as the electrostatic potential, the local lipophilicity/hydrophilicity, and the hydrogen bond density on the surface for characterization. The definition and the calculation method for these properties are reviewed. The surface is segmented into overlapping patches with similar molecular properties. These patches can be used to represent the characteristic local features of the molecule in a way that is beyond the atomistic resolution but can nevertheless be applied for the analysis of partial similarities of different molecules as well as for the identification of molecular complementarity in a very general sense. The patch representation can be used for different applications, which will be demonstrated in subsequent articles", "id": "338", "src": "pattern recognition strategies for molecular surfaces . i . pattern generation . using fuzzy set theory a new method for the characterization of molecules based on the model approach of molecular surfaces is presented . we use the topographical properties of the surface as well as the electrostatic potential , the local lipophilicity hydrophilicity , and the hydrogen bond density on the surface for characterization . the definition and the calculation method for these properties are reviewed . the surface is segmented into overlapping patches with similar molecular properties . these patches can be used to represent the characteristic local features of the molecule in a way that is beyond the atomistic resolution but can nevertheless be applied for the analysis of partial similarities of different molecules as well as for the identification of molecular complementarity in a very general sense . the patch representation can be used for different applications , which will be demonstrated in subsequent articles"}
{"title": "An efficient parallel algorithm for the calculation of canonical MP2 energies", "abstract": "We present the parallel version of a previous serial algorithm for the efficient calculation of canonical MP2 energies. It is based on the Saebo-Almlof direct-integral transformation, coupled with an efficient prescreening of the AO integrals. The parallel algorithm avoids synchronization delays by spawning a second set of slaves during the bin-sort prior to the second half-transformation. Results are presented for systems with up to 2000 basis functions. MP2 energies for molecules with 400-500 basis functions can be routinely calculated to microhartree accuracy on a small number of processors (6-8) in a matter of minutes with modern PC-based parallel computers", "id": "339", "src": "an efficient parallel algorithm for the calculation of canonical mp2 energies . we present the parallel version of a previous serial algorithm for the efficient calculation of canonical mp2 energies . it is based on the saebo almlof direct integral transformation , coupled with an efficient prescreening of the ao integrals . the parallel algorithm avoids synchronization delays by spawning a second set of slaves during the bin sort prior to the second half transformation . results are presented for systems with up to <digit> basis functions . mp2 energies for molecules with <digit> <digit> basis functions can be routinely calculated to microhartree accuracy on a small number of processors ( 6 8 ) in a matter of minutes with modern pc based parallel computers"}
{"title": "A method for correlations analysis of coordinates: applications for molecular", "abstract": "conformations We describe a new method to analyze multiple correlations between subsets of coordinates that represent a sample. The correlation is established only between specific regions of interest at the coordinates. First, the region(s) of interest are selected at each molecular coordinate. Next, a correlation matrix is constructed for the selected regions. The matrix is subject to further analysis, illuminating the multidimensional structural characteristics that exist in the conformational space. The method's abilities are demonstrated in several examples: it is used to analyze the conformational space of complex molecules, it is successfully applied to compare related conformational spaces, and it is used to analyze a diverse set of protein folding trajectories", "id": "340", "src": "a method for correlations analysis of coordinates applications for molecular . conformations we describe a new method to analyze multiple correlations between subsets of coordinates that represent a sample . the correlation is established only between specific regions of interest at the coordinates . first , the region ( s ) of interest are selected at each molecular coordinate . next , a correlation matrix is constructed for the selected regions . the matrix is subject to further analysis , illuminating the multidimensional structural characteristics that exist in the conformational space . the method ' s abilities are demonstrated in several examples it is used to analyze the conformational space of complex molecules , it is successfully applied to compare related conformational spaces , and it is used to analyze a diverse set of protein folding trajectories"}
{"title": "Genetic algorithm guided selection: variable selection and subset selection", "abstract": "A novel genetic algorithm guided selection method, GAS, has been described. The method utilizes a simple encoding scheme which can represent both compounds and variables used to construct a QSAR/QSPR model. A genetic algorithm is then utilized to simultaneously optimize the encoded variables that include both descriptors and compound subsets. The GAS method generates multiple models each applying to a subset of the compounds. Typically the subsets represent clusters with different chemotypes. Also a procedure based on molecular similarity is presented to determine which model should be applied to a given test set compound. The variable selection method implemented in GAS has been tested and compared using the Selwood data set (n = 31 compounds; nu = 53 descriptors). The results showed that the method is comparable to other published methods. The subset selection method implemented in GAS has been first tested using an artificial data set (n = 100 points; nu = 1 descriptor) to examine its ability to subset data points and second applied to analyze the XLOGP data set (n = 1831 compounds; nu = 126 descriptors). The method is able to correctly identify artificial data points belonging to various subsets. The analysis of the XLOGP data set shows that the subset selection method can be useful in improving a QSAR/QSPR model when the variable selection method fails", "id": "341", "src": "genetic algorithm guided selection variable selection and subset selection . a novel genetic algorithm guided selection method , gas , has been described . the method utilizes a simple encoding scheme which can represent both compounds and variables used to construct a qsar qspr model . a genetic algorithm is then utilized to simultaneously optimize the encoded variables that include both descriptors and compound subsets . the gas method generates multiple models each applying to a subset of the compounds . typically the subsets represent clusters with different chemotypes . also a procedure based on molecular similarity is presented to determine which model should be applied to a given test set compound . the variable selection method implemented in gas has been tested and compared using the selwood data set ( n <digit> compounds nu <digit> descriptors ) . the results showed that the method is comparable to other published methods . the subset selection method implemented in gas has been first tested using an artificial data set ( n <digit> points nu 1 descriptor ) to examine its ability to subset data points and second applied to analyze the xlogp data set ( n <digit> compounds nu <digit> descriptors ) . the method is able to correctly identify artificial data points belonging to various subsets . the analysis of the xlogp data set shows that the subset selection method can be useful in improving a qsar qspr model when the variable selection method fails"}
{"title": "A formal model of computing with words", "abstract": "Classical automata are formal models of computing with values. Fuzzy automata are generalizations of classical automata where the knowledge about the system's next state is vague or uncertain. It is worth noting that like classical automata, fuzzy automata can only process strings of input symbols. Therefore, such fuzzy automata are still (abstract) devices for computing with values, although a certain vagueness or uncertainty are involved in the process of computation. We introduce a new kind of fuzzy automata whose inputs are instead strings of fuzzy subsets of the input alphabet. These new fuzzy automata may serve as formal models of computing with words. We establish an extension principle from computing with values to computing with words. This principle indicates that computing with words can be implemented with computing with values with the price of a big amount of extra computations", "id": "342", "src": "a formal model of computing with words . classical automata are formal models of computing with values . fuzzy automata are generalizations of classical automata where the knowledge about the system ' s next state is vague or uncertain . it is worth noting that like classical automata , fuzzy automata can only process strings of input symbols . therefore , such fuzzy automata are still ( abstract ) devices for computing with values , although a certain vagueness or uncertainty are involved in the process of computation . we introduce a new kind of fuzzy automata whose inputs are instead strings of fuzzy subsets of the input alphabet . these new fuzzy automata may serve as formal models of computing with words . we establish an extension principle from computing with values to computing with words . this principle indicates that computing with words can be implemented with computing with values with the price of a big amount of extra computations"}
{"title": "Using molecular equivalence numbers to visually explore structural features", "abstract": "that distinguish chemical libraries A molecular equivalence number (meqnum) classifies a molecule with respect to a class of structural features or topological shapes such as its cyclic system or its set of functional groups. Meqnums can be used to organize molecular structures into nonoverlapping, yet highly relatable classes. We illustrate the construction of some different types of meqnums and present via examples some methods of comparing diverse chemical libraries based on meqnums. In the examples we compare a library which is a random sample from the MDL Drug Data Report (MDDR) with a library which is a random sample from the Available Chemical Directory (ACD). In our analyses, we discover some interesting features of the topological shape of a molecule and its set of functional groups that are strongly linked with compounds occurring in the MDDR but not in the ACD. We also illustrate the utility of molecular equivalence indices in delineating the structural domain over which an SAR conclusion is valid", "id": "343", "src": "using molecular equivalence numbers to visually explore structural features . that distinguish chemical libraries a molecular equivalence number ( meqnum ) classifies a molecule with respect to a class of structural features or topological shapes such as its cyclic system or its set of functional groups . meqnums can be used to organize molecular structures into nonoverlapping , yet highly relatable classes . we illustrate the construction of some different types of meqnums and present via examples some methods of comparing diverse chemical libraries based on meqnums . in the examples we compare a library which is a random sample from the mdl drug data report ( mddr ) with a library which is a random sample from the available chemical directory ( acd ) . in our analyses , we discover some interesting features of the topological shape of a molecule and its set of functional groups that are strongly linked with compounds occurring in the mddr but not in the acd . we also illustrate the utility of molecular equivalence indices in delineating the structural domain over which an sar conclusion is valid"}
{"title": "On the use of neural network ensembles in QSAR and QSPR", "abstract": "Despite their growing popularity among neural network practitioners, ensemble methods have not been widely adopted in structure-activity and structure-property correlation. Neural networks are inherently unstable, in that small changes in the training set and/or training parameters can lead to large changes in their generalization performance. Recent research has shown that by capitalizing on the diversity of the individual models, ensemble techniques can minimize uncertainty and produce more stable and accurate predictors. In this work, we present a critical assessment of the most common ensemble technique known as bootstrap aggregation, or bagging, as applied to QSAR and QSPR. Although aggregation does offer definitive advantages, we demonstrate that bagging may not be the best possible choice and that simpler techniques such as retraining with the full sample can often produce superior results. These findings are rationalized using Krogh and Vedelsby's (1995) decomposition of the generalization error into a term that measures the average generalization performance of the individual networks and a term that measures the diversity among them. For networks that are designed to resist over-fitting, the benefits of aggregation are clear but not overwhelming", "id": "344", "src": "on the use of neural network ensembles in qsar and qspr . despite their growing popularity among neural network practitioners , ensemble methods have not been widely adopted in structure activity and structure property correlation . neural networks are inherently unstable , in that small changes in the training set and or training parameters can lead to large changes in their generalization performance . recent research has shown that by capitalizing on the diversity of the individual models , ensemble techniques can minimize uncertainty and produce more stable and accurate predictors . in this work , we present a critical assessment of the most common ensemble technique known as bootstrap aggregation , or bagging , as applied to qsar and qspr . although aggregation does offer definitive advantages , we demonstrate that bagging may not be the best possible choice and that simpler techniques such as retraining with the full sample can often produce superior results . these findings are rationalized using krogh and vedelsby ' s ( <digit> ) decomposition of the generalization error into a term that measures the average generalization performance of the individual networks and a term that measures the diversity among them . for networks that are designed to resist over fitting , the benefits of aggregation are clear but not overwhelming"}
{"title": "Median partitioning: a novel method for the selection of representative subsets", "abstract": "from large compound pools A method termed median partitioning (MP) has been developed to select diverse sets of molecules from large compound pools. Unlike many other methods for subset selection, the MP approach does not depend on pairwise comparison of molecules and can therefore be applied to very large compound collections. The only time limiting step is the calculation of molecular descriptors for database compounds. MP employs arrays of property descriptors with little correlation to divide large compound pools into partitions from which representative molecules can be selected. In each of n subsequent steps, a population of molecules is divided into subpopulations above and below the median value of a property descriptor until a desired number of 2/sup n/ partitions are obtained. For descriptor evaluation and selection, an entropy formulation was embedded in a genetic algorithm. MP has been applied to generate a subset of the Available Chemicals Directory, and the results have been compared with cell-based partitioning", "id": "345", "src": "median partitioning a novel method for the selection of representative subsets . from large compound pools a method termed median partitioning ( mp ) has been developed to select diverse sets of molecules from large compound pools . unlike many other methods for subset selection , the mp approach does not depend on pairwise comparison of molecules and can therefore be applied to very large compound collections . the only time limiting step is the calculation of molecular descriptors for database compounds . mp employs arrays of property descriptors with little correlation to divide large compound pools into partitions from which representative molecules can be selected . in each of n subsequent steps , a population of molecules is divided into subpopulations above and below the median value of a property descriptor until a desired number of 2 sup n partitions are obtained . for descriptor evaluation and selection , an entropy formulation was embedded in a genetic algorithm . mp has been applied to generate a subset of the available chemicals directory , and the results have been compared with cell based partitioning"}
{"title": "Chemical information based scaling of molecular descriptors: a universal", "abstract": "chemical scale for library design and analysis Scaling is a difficult issue for any analysis of chemical properties or molecular topology when disparate descriptors are involved. To compare properties across different data sets, a common scale must be defined. Using several publicly available databases (ACD, CMC, MDDR, and NCI) as a basis, we propose to define chemically meaningful scales for a number of molecular properties and topology descriptors. These chemically derived scaling functions have several advantages. First, it is possible to define chemically relevant scales, greatly simplifying similarity and diversity analyses across data sets. Second, this approach provides a convenient method for setting descriptor boundaries that define chemically reasonable topology spaces. For example, descriptors can be scaled so that compounds with little potential for biological activity, bioavailability, or other drug-like characteristics are easily identified as outliers. We have compiled scaling values for 314 molecular descriptors. In addition the 10th and 90th percentile values for each descriptor have been calculated for use in outlier filtering", "id": "346", "src": "chemical information based scaling of molecular descriptors a universal . chemical scale for library design and analysis scaling is a difficult issue for any analysis of chemical properties or molecular topology when disparate descriptors are involved . to compare properties across different data sets , a common scale must be defined . using several publicly available databases ( acd , cmc , mddr , and nci ) as a basis , we propose to define chemically meaningful scales for a number of molecular properties and topology descriptors . these chemically derived scaling functions have several advantages . first , it is possible to define chemically relevant scales , greatly simplifying similarity and diversity analyses across data sets . second , this approach provides a convenient method for setting descriptor boundaries that define chemically reasonable topology spaces . for example , descriptors can be scaled so that compounds with little potential for biological activity , bioavailability , or other drug like characteristics are easily identified as outliers . we have compiled scaling values for <digit> molecular descriptors . in addition the 10th and 90th percentile values for each descriptor have been calculated for use in outlier filtering"}
{"title": "MTD-PLS: a PLS-based variant of the MTD method. II. Mapping ligand-receptor", "abstract": "interactions. Enzymatic acetic acid esters hydrolysis The PLS variant of the MTD method (T.I. Oprea et al., SAR QSAR Environ. Res. 2001, 12, 75-92) was applied to a series of 25 acetylcholinesterase hydrolysis substrates. Statistically significant MTD-PLS models (q/sup 2/ between 0.7 and 0.8) are in agreement with previous MTD models, with the advantage that local contributions are understood beyond the occupancy/nonoccupancy interpretation in MTD. A \"chemically intuitive\" approach further forces MTD-PLS coefficients to assume only negative (or zero) values for fragmental volume descriptors and positive (or zero) values for fragmental hydrophobicity descriptors. This further separates the various kinds of local interactions at each vertex of the MTD hypermolecule, making this method suitable for medicinal chemistry synthesis planning", "id": "347", "src": "mtd pls a pls based variant of the mtd method . ii . mapping ligand receptor . interactions . enzymatic acetic acid esters hydrolysis the pls variant of the mtd method ( t . i . oprea et al . , sar qsar environ . res . <digit> , <digit> , <digit> <digit> ) was applied to a series of <digit> acetylcholinesterase hydrolysis substrates . statistically significant mtd pls models ( q sup 2 between 0 . 7 and 0 . 8 ) are in agreement with previous mtd models , with the advantage that local contributions are understood beyond the occupancy nonoccupancy interpretation in mtd . a chemically intuitive approach further forces mtd pls coefficients to assume only negative ( or zero ) values for fragmental volume descriptors and positive ( or zero ) values for fragmental hydrophobicity descriptors . this further separates the various kinds of local interactions at each vertex of the mtd hypermolecule , making this method suitable for medicinal chemistry synthesis planning"}
{"title": "Prediction of ultraviolet spectral absorbance using quantitative", "abstract": "structure-property relationships High performance liquid chromatography (HPLC) with ultraviolet (UV) spectrophotometric detection is a common method for analyzing reaction products in organic chemistry. This procedure would benefit from a computational model for predicting the relative response of organic molecules. Models are now reported for the prediction of the integrated UV absorbance for a diverse set of organic compounds using a quantitative structure-property relationship (QSPR) approach. A seven-descriptor linear correlation with a squared correlation coefficient (R/sup 2/) of 0.815 is reported for a data set of 521.compounds. Using the sum of ZINDO oscillator strengths in the integration range as an additional descriptor allowed reduction in the number of descriptors producing a robust model for 460 compounds with five descriptors and a squared correlation coefficient 0.857. The descriptors used in the models are discussed with respect to the physical nature of the UV absorption process", "id": "348", "src": "prediction of ultraviolet spectral absorbance using quantitative . structure property relationships high performance liquid chromatography ( hplc ) with ultraviolet ( uv ) spectrophotometric detection is a common method for analyzing reaction products in organic chemistry . this procedure would benefit from a computational model for predicting the relative response of organic molecules . models are now reported for the prediction of the integrated uv absorbance for a diverse set of organic compounds using a quantitative structure property relationship ( qspr ) approach . a seven descriptor linear correlation with a squared correlation coefficient ( r sup 2 ) of 0 . <digit> is reported for a data set of <digit> . compounds . using the sum of zindo oscillator strengths in the integration range as an additional descriptor allowed reduction in the number of descriptors producing a robust model for <digit> compounds with five descriptors and a squared correlation coefficient 0 . <digit> . the descriptors used in the models are discussed with respect to the physical nature of the uv absorption process"}
{"title": "Assessment of the macrocyclic effect for the complexation of crown-ethers with", "abstract": "alkali cations using the substructural molecular fragments method The Substructural Molecular Fragments method (Solov'ev, V. P.; Varnek, A. A.; Wipff, G. J. Chem. Inf. Comput. Sci. 2000, 40, 847-858) was applied to assess stability constants (logK) of the complexes of crown-ethers, polyethers, and glymes with Na/sup +/, K/sup +/, and Cs/sup +/ in methanol. One hundred forty-seven computational models including different fragment sets coupled with linear or nonlinear fitting equations were applied for the data sets containing 69 (Na/sup +/), 123 (K/sup +/), and 31 (Cs/sup +/) compounds. To account for the \"macrocyclic effect\" for crown-ethers, an additional \"cyclicity\" descriptor was used. \"Predicted\" stability constants both for macrocyclic compounds and for their open-chain analogues are in good agreement with the experimental data reported earlier and with those studied experimentally in this work. The macrocyclic effect as a function of cation and ligand is quantitatively estimated for all studied crown-ethers", "id": "349", "src": "assessment of the macrocyclic effect for the complexation of crown ethers with . alkali cations using the substructural molecular fragments method the substructural molecular fragments method ( solov ' ev , v . p . varnek , a . a . wipff , g . j . chem . inf . comput . sci . <digit> , <digit> , <digit> <digit> ) was applied to assess stability constants ( logk ) of the complexes of crown ethers , polyethers , and glymes with na sup + , k sup + , and cs sup + in methanol . one hundred forty seven computational models including different fragment sets coupled with linear or nonlinear fitting equations were applied for the data sets containing <digit> ( na sup + ) , <digit> ( k sup + ) , and <digit> ( cs sup + ) compounds . to account for the macrocyclic effect for crown ethers , an additional cyclicity descriptor was used . predicted stability constants both for macrocyclic compounds and for their open chain analogues are in good agreement with the experimental data reported earlier and with those studied experimentally in this work . the macrocyclic effect as a function of cation and ligand is quantitatively estimated for all studied crown ethers"}
{"title": "Improving the predicting power of partial order based QSARs through linear", "abstract": "extensions Partial order theory (POT) is an attractive and operationally simple method that allows ordering of compounds, based on selected structural and/or electronic descriptors (modeled order), or based on their end points, e.g., solubility (experimental order). If the modeled order resembles the experimental order, compounds that are not experimentally investigated can be assigned a position in the model that eventually might lead to a prediction of an end-point value. However, in the application of POT in quantitative structure-activity relationship modeling, only the compounds directly comparable to the noninvestigated compounds are applied. To explore the possibilities of improving the methodology, the theory is extended by application of the so-called linear extensions of the model order. The study show that partial ordering combined with linear extensions appears as a promising tool providing probability distribution curves in the range of possible end-point values for compounds not being experimentally investigated", "id": "350", "src": "improving the predicting power of partial order based qsars through linear . extensions partial order theory ( pot ) is an attractive and operationally simple method that allows ordering of compounds , based on selected structural and or electronic descriptors ( modeled order ) , or based on their end points , e . g . , solubility ( experimental order ) . if the modeled order resembles the experimental order , compounds that are not experimentally investigated can be assigned a position in the model that eventually might lead to a prediction of an end point value . however , in the application of pot in quantitative structure activity relationship modeling , only the compounds directly comparable to the noninvestigated compounds are applied . to explore the possibilities of improving the methodology , the theory is extended by application of the so called linear extensions of the model order . the study show that partial ordering combined with linear extensions appears as a promising tool providing probability distribution curves in the range of possible end point values for compounds not being experimentally investigated"}
{"title": "Novel ZE-isomerism descriptors derived from molecular topology and their", "abstract": "application to QSAR analysis We introduce several series of novel ZE-isomerism descriptors derived directly from two-dimensional molecular topology. These descriptors make use of a quantity named ZE-isomerism correction, which is added to the vertex degrees of atoms connected by double bonds in Z and E configurations. This approach is similar to the one described previously for topological chirality descriptors (Golbraikh, A., et al. J. Chem. Inf. Comput. Sci. 2001, 41, 147-158). The ZE-isomerism descriptors include modified molecular connectivity indices, overall Zagreb indices, extended connectivity, overall connectivity, and topological charge indices. They can be either real or complex numbers. Mathematical properties of different subgroups of ZE-isomerism descriptors are discussed. These descriptors circumvent the inability of conventional topological indices to distinguish between Z and E isomers. The applicability of ZE-isomerism descriptors to QSAR analysis is demonstrated in the studies of a series of 131 anticancer agents inhibiting tubulin polymerization", "id": "351", "src": "novel ze isomerism descriptors derived from molecular topology and their . application to qsar analysis we introduce several series of novel ze isomerism descriptors derived directly from two dimensional molecular topology . these descriptors make use of a quantity named ze isomerism correction , which is added to the vertex degrees of atoms connected by double bonds in z and e configurations . this approach is similar to the one described previously for topological chirality descriptors ( golbraikh , a . , et al . j . chem . inf . comput . sci . <digit> , <digit> , <digit> <digit> ) . the ze isomerism descriptors include modified molecular connectivity indices , overall zagreb indices , extended connectivity , overall connectivity , and topological charge indices . they can be either real or complex numbers . mathematical properties of different subgroups of ze isomerism descriptors are discussed . these descriptors circumvent the inability of conventional topological indices to distinguish between z and e isomers . the applicability of ze isomerism descriptors to qsar analysis is demonstrated in the studies of a series of <digit> anticancer agents inhibiting tubulin polymerization"}
{"title": "Computer mediated communication and university international students", "abstract": "The design for the preliminary study presented was based on the experiences of the international students and faculty members of a small southwest university being surveyed and interviewed. The data collection procedure blends qualitative and quantitative data. A strong consensus was found that supports the study's premise that there is an association between the use of computer mediated communication (CMC) and teaching and learning performance of international students. Both groups believe CMC to be an effective teaching and learning tool by: increasing the frequency and quality of communication between students and instructors; improving language skills through increased writing and communication opportunities; allowing students and instructors to stay current and to compete effectively; providing alternative teaching and learning methods to increase students' confidence in their ability to communicate effectively with peers and instructors; and improving the instructors' pedagogical focus and questioning techniques", "id": "352", "src": "computer mediated communication and university international students . the design for the preliminary study presented was based on the experiences of the international students and faculty members of a small southwest university being surveyed and interviewed . the data collection procedure blends qualitative and quantitative data . a strong consensus was found that supports the study ' s premise that there is an association between the use of computer mediated communication ( cmc ) and teaching and learning performance of international students . both groups believe cmc to be an effective teaching and learning tool by increasing the frequency and quality of communication between students and instructors improving language skills through increased writing and communication opportunities allowing students and instructors to stay current and to compete effectively providing alternative teaching and learning methods to increase students ' confidence in their ability to communicate effectively with peers and instructors and improving the instructors ' pedagogical focus and questioning techniques"}
{"title": "Uncertainty bounds and their use in the design of interval type-2 fuzzy logic", "abstract": "systems We derive inner- and outer-bound sets for the type-reduced set of an interval type-2 fuzzy logic system (FLS), based on a new mathematical interpretation of the Karnik-Mendel iterative procedure for computing the type-reduced set. The bound sets can not only provide estimates about the uncertainty contained in the output of an interval type-2 FLS, but can also be used to design an interval type-2 FLS. We demonstrate, by means of a simulation experiment, that the resulting system can operate without type-reduction and can achieve similar performance to one that uses type-reduction. Therefore, our new design method, based on the bound sets, can relieve the computation burden of an interval type-2 FLS during its operation, which makes an interval type-2 FLS useful for real-time applications", "id": "353", "src": "uncertainty bounds and their use in the design of interval type 2 fuzzy logic . systems we derive inner and outer bound sets for the type reduced set of an interval type 2 fuzzy logic system ( fls ) , based on a new mathematical interpretation of the karnik mendel iterative procedure for computing the type reduced set . the bound sets can not only provide estimates about the uncertainty contained in the output of an interval type 2 fls , but can also be used to design an interval type 2 fls . we demonstrate , by means of a simulation experiment , that the resulting system can operate without type reduction and can achieve similar performance to one that uses type reduction . therefore , our new design method , based on the bound sets , can relieve the computation burden of an interval type 2 fls during its operation , which makes an interval type 2 fls useful for real time applications"}
{"title": "Entrepreneurs in Action: a Web-case model", "abstract": "Much of the traditional schooling in America is built around systems of compliance and control, characteristics which stifle the creative and entrepreneurial instincts of the children who are subjected to these tactics. The article explores a different approach to education, one that involves capturing the interest of the student through the use of problem and project-based instruction delivered via the Internet. Called Entrepreneurs in Action, this program seeks to involve students in a problem at the outset and to promote the learning of traditional subject areas as a process of the problem-solving activities that are undertaken. The program's details are explained, from elementary school through university level courses, and the authors outline their plans to test the efficacy of the program at each level", "id": "354", "src": "entrepreneurs in action a web case model . much of the traditional schooling in america is built around systems of compliance and control , characteristics which stifle the creative and entrepreneurial instincts of the children who are subjected to these tactics . the article explores a different approach to education , one that involves capturing the interest of the student through the use of problem and project based instruction delivered via the internet . called entrepreneurs in action , this program seeks to involve students in a problem at the outset and to promote the learning of traditional subject areas as a process of the problem solving activities that are undertaken . the program ' s details are explained , from elementary school through university level courses , and the authors outline their plans to test the efficacy of the program at each level"}
{"title": "Factors contributing to preservice teachers' discomfort in a Web-based course", "abstract": "structured as an inquiry A report is given of a qualitative emergent design study of a Science, Technology, Society Interaction (STS) Web-enhanced course. Students' discomfort during the pilot test provided insight into the intellectual scaffolding that preservice secondary science teachers needed to optimize their performance when required to develop understanding through open-ended inquiry in a Web environment. Eight factors identified contributed to student discomfort: computer skills, paradigm shifts, trust, time management, thinking about their own thinking, systematic inquiry, self-assessment, and scientific discourse. These factors suggested developing understanding through inquiry by conducting a self-designed, open-ended, systematic inquiry required autonomous learning involving metacognitive skills and time management skills. To the extent in which students either came into the course with this scaffolding, or developed it during the course, they were successful in learning about STS and its relationship to science teaching. Changes in the Web site made to accommodate learners' needs as they surfaced are described", "id": "355", "src": "factors contributing to preservice teachers ' discomfort in a web based course . structured as an inquiry a report is given of a qualitative emergent design study of a science , technology , society interaction ( sts ) web enhanced course . students ' discomfort during the pilot test provided insight into the intellectual scaffolding that preservice secondary science teachers needed to optimize their performance when required to develop understanding through open ended inquiry in a web environment . eight factors identified contributed to student discomfort computer skills , paradigm shifts , trust , time management , thinking about their own thinking , systematic inquiry , self assessment , and scientific discourse . these factors suggested developing understanding through inquiry by conducting a self designed , open ended , systematic inquiry required autonomous learning involving metacognitive skills and time management skills . to the extent in which students either came into the course with this scaffolding , or developed it during the course , they were successful in learning about sts and its relationship to science teaching . changes in the web site made to accommodate learners ' needs as they surfaced are described"}
{"title": "Recommendations for implementing Internet inquiry projects", "abstract": "The purpose of the study presented was to provide recommendations to teachers who are interested in implementing Internet inquiry projects. Four classes of ninth- and tenth-grade honors students (N = 100) participated in an Internet inquiry project in which they were presented with an ecology question that required them to make a decision based on information that they gathered, analyzed, and synthesized from the Internet and their textbook. Students then composed papers with a rationale for their decision. Students in one group had access to pre-selected relevant Web sites, access to the entire Internet, and were provided with less online support. Students in the other group had access to only pre-selected relevant Web sites, but were provided with more online support. Two of the most important recommendations were: 1) to provide students with more online support; and 2) to provide students with pre-selected relevant Web sites and allow them to search the Internet for information", "id": "356", "src": "recommendations for implementing internet inquiry projects . the purpose of the study presented was to provide recommendations to teachers who are interested in implementing internet inquiry projects . four classes of ninth and tenth grade honors students ( n <digit> ) participated in an internet inquiry project in which they were presented with an ecology question that required them to make a decision based on information that they gathered , analyzed , and synthesized from the internet and their textbook . students then composed papers with a rationale for their decision . students in one group had access to pre selected relevant web sites , access to the entire internet , and were provided with less online support . students in the other group had access to only pre selected relevant web sites , but were provided with more online support . two of the most important recommendations were 1 ) to provide students with more online support and 2 ) to provide students with pre selected relevant web sites and allow them to search the internet for information"}
{"title": "Alien Rescue: a problem-based hypermedia learning environment for middle school", "abstract": "science The article describes an innovative hypermedia product for sixth graders in space science: Alien Rescue. Using a problem-based learning approach that is highly interactive, Alien Rescue engages students in scientific investigations aimed at finding solutions to complex and meaningful problems. Problem-based learning (PBL) is an instructional strategy proven to be effective in medical and business fields, and it is increasingly popular in education. However, using PBL in K-12 classrooms is challenging and requires access to rich knowledge bases and cognitive tools. Alien Rescue is designed to provide such cognitive support for successful use of PBL in sixth-grade classrooms. The design and development of Alien Rescue is guided by current educational research. Research is an integral part of this project. Results of formative evaluation and research studies are being integrated into the development and improvement of the program. Alien Rescue is designed in accordance with the National Science Standards and the Texas Essential Knowledge and Skills (TEKS) for science. So far Alien Rescue has been field-tested by approximately 1400 sixth graders. More use in middle schools is in progress and more research on its use is planned", "id": "357", "src": "alien rescue a problem based hypermedia learning environment for middle school . science the article describes an innovative hypermedia product for sixth graders in space science alien rescue . using a problem based learning approach that is highly interactive , alien rescue engages students in scientific investigations aimed at finding solutions to complex and meaningful problems . problem based learning ( pbl ) is an instructional strategy proven to be effective in medical and business fields , and it is increasingly popular in education . however , using pbl in k <digit> classrooms is challenging and requires access to rich knowledge bases and cognitive tools . alien rescue is designed to provide such cognitive support for successful use of pbl in sixth grade classrooms . the design and development of alien rescue is guided by current educational research . research is an integral part of this project . results of formative evaluation and research studies are being integrated into the development and improvement of the program . alien rescue is designed in accordance with the national science standards and the texas essential knowledge and skills ( teks ) for science . so far alien rescue has been field tested by approximately <digit> sixth graders . more use in middle schools is in progress and more research on its use is planned"}
{"title": "Project-based learning: teachers learning and using high-tech to preserve Cajun", "abstract": "culture Using project-based learning pedagogy in EdTc 658 Advances in Educational Technology, the author has trained inservice teachers in Southwestern Louisiana with an advanced computer multimedia program called Director(R) (Macromedia, Inc.). The content of this course focused on modeling the project-based learning pedagogy and researching Acadian's traditions and legacy. With the multi-functions of microcomputers, new technologies were used to preserve and celebrate the local culture with superiority of text, graphics, animation, sound, and video. The article describes how several groups of school teachers in the surrounding areas of a regional state university of Louisiana learned computer multimedia using project-based learning and integrated their learning into local cultural heritage", "id": "358", "src": "project based learning teachers learning and using high tech to preserve cajun . culture using project based learning pedagogy in edtc <digit> advances in educational technology , the author has trained inservice teachers in southwestern louisiana with an advanced computer multimedia program called director ( r ) ( macromedia , inc . ) . the content of this course focused on modeling the project based learning pedagogy and researching acadian ' s traditions and legacy . with the multi functions of microcomputers , new technologies were used to preserve and celebrate the local culture with superiority of text , graphics , animation , sound , and video . the article describes how several groups of school teachers in the surrounding areas of a regional state university of louisiana learned computer multimedia using project based learning and integrated their learning into local cultural heritage"}
{"title": "Presentation media, information complexity, and learning outcomes", "abstract": "Multimedia computing provides a variety of information presentation modality combinations. Educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text-only and text with static pictures in facilitating optimal human information processing and, therefore, comprehension. The article reports the findings from a 3 (text-only, overhead slides, and multimedia presentation)*2 (high and low information complexity) factorial experiment. Subjects read a text script, viewed an acetate overhead slide presentation, or viewed a multimedia presentation depicting the greenhouse effect (low complexity) or photocopier operation (high complexity). Multimedia was superior to text-only and overhead slides for comprehension. Information complexity diminished comprehension and perceived presentation quality. Multimedia was able to reduce the negative impact of information complexity on comprehension and increase the extent of sustained attention to the presentation. These findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity. Moreover, multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention; the quality and effectiveness of information processing attained (i.e., use of verbal and visual working memory) is also significant", "id": "359", "src": "presentation media , information complexity , and learning outcomes . multimedia computing provides a variety of information presentation modality combinations . educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text only and text with static pictures in facilitating optimal human information processing and , therefore , comprehension . the article reports the findings from a 3 ( text only , overhead slides , and multimedia presentation ) *2 ( high and low information complexity ) factorial experiment . subjects read a text script , viewed an acetate overhead slide presentation , or viewed a multimedia presentation depicting the greenhouse effect ( low complexity ) or photocopier operation ( high complexity ) . multimedia was superior to text only and overhead slides for comprehension . information complexity diminished comprehension and perceived presentation quality . multimedia was able to reduce the negative impact of information complexity on comprehension and increase the extent of sustained attention to the presentation . these findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity . moreover , multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention the quality and effectiveness of information processing attained ( i . e . , use of verbal and visual working memory ) is also significant"}
{"title": "Real-time tissue characterization on the basis of in vivo Raman spectra", "abstract": "The application of in vivo Raman spectroscopy for clinical diagnosis demands dedicated software that can perform the necessary signal processing and subsequent (multivariate) data analysis, enabling clinically relevant parameters to be extracted and made available in real time. Here we describe the design and implementation of a software package that allows for real-time signal processing and data analysis of Raman spectra. The design is based on automatic data exchange between Grams, a spectroscopic data acquisition and analysis program, and Matlab, a program designed for array-based calculations. The data analysis software has a modular design providing great flexibility in developing custom data analysis routines for different applications. The implementation is illustrated by a computationally demanding application for the classification of skin spectra using principal component analysis and linear discriminant analysis", "id": "360", "src": "real time tissue characterization on the basis of in vivo raman spectra . the application of in vivo raman spectroscopy for clinical diagnosis demands dedicated software that can perform the necessary signal processing and subsequent ( multivariate ) data analysis , enabling clinically relevant parameters to be extracted and made available in real time . here we describe the design and implementation of a software package that allows for real time signal processing and data analysis of raman spectra . the design is based on automatic data exchange between grams , a spectroscopic data acquisition and analysis program , and matlab , a program designed for array based calculations . the data analysis software has a modular design providing great flexibility in developing custom data analysis routines for different applications . the implementation is illustrated by a computationally demanding application for the classification of skin spectra using principal component analysis and linear discriminant analysis"}
{"title": "Loudspeaker voice-coil inductance losses: circuit models, parameter estimation,", "abstract": "and effect on frequency response When the series resistance is separated and treated as a separate element, it is shown that losses in an inductor require the ratio of the flux to MMF in the core to be frequency dependent. For small-signal operation, this dependence leads to a circuit model composed of a lossless inductor and a resistor in parallel, both of which are frequency dependent. Mathematical expressions for these elements are derived under the assumption that the ratio of core flux to MMF varies as omega /sup n-1/, where n is a constant. A linear regression technique is described for extracting the model parameters from measured data. Experimental data are presented to justify the model for the lossy inductance of a loudspeaker voice-coil. A SPICE example is presented to illustrate the effects of voice-coil inductor losses on the frequency response of a typical driver", "id": "361", "src": "loudspeaker voice coil inductance losses circuit models , parameter estimation , . and effect on frequency response when the series resistance is separated and treated as a separate element , it is shown that losses in an inductor require the ratio of the flux to mmf in the core to be frequency dependent . for small signal operation , this dependence leads to a circuit model composed of a lossless inductor and a resistor in parallel , both of which are frequency dependent . mathematical expressions for these elements are derived under the assumption that the ratio of core flux to mmf varies as omega sup n 1 , where n is a constant . a linear regression technique is described for extracting the model parameters from measured data . experimental data are presented to justify the model for the lossy inductance of a loudspeaker voice coil . a spice example is presented to illustrate the effects of voice coil inductor losses on the frequency response of a typical driver"}
{"title": "Complexity transitions in global algorithms for sparse linear systems over", "abstract": "finite fields We study the computational complexity of a very basic problem, namely that of finding solutions to a very large set of random linear equations in a finite Galois field modulo q. Using tools from statistical mechanics we are able to identify phase transitions in the structure of the solution space and to connect them to the changes in the performance of a global algorithm, namely Gaussian elimination. Crossing phase boundaries produces a dramatic increase in memory and CPU requirements necessary for the algorithms. In turn, this causes the saturation of the upper bounds for the running time. We illustrate the results on the specific problem of integer factorization, which is of central interest for deciphering messages encrypted with the RSA cryptosystem", "id": "362", "src": "complexity transitions in global algorithms for sparse linear systems over . finite fields we study the computational complexity of a very basic problem , namely that of finding solutions to a very large set of random linear equations in a finite galois field modulo q . using tools from statistical mechanics we are able to identify phase transitions in the structure of the solution space and to connect them to the changes in the performance of a global algorithm , namely gaussian elimination . crossing phase boundaries produces a dramatic increase in memory and cpu requirements necessary for the algorithms . in turn , this causes the saturation of the upper bounds for the running time . we illustrate the results on the specific problem of integer factorization , which is of central interest for deciphering messages encrypted with the rsa cryptosystem"}
{"title": "Noise effect on memory recall in dynamical neural network model of hippocampus", "abstract": "We investigate some noise effect on a neural network model proposed by Araki and Aihara (1998) for the memory recall of dynamical patterns in the hippocampus and the entorhinal cortex; the noise effect is important since the release of transmitters at synaptic clefts, the operation of gate of ion channels and so on are known as stochastic phenomena. We consider two kinds of noise effect due to a deterministic noise and a stochastic noise. By numerical simulations, we find that reasonable values of noise give better performance on the memory recall of dynamical patterns. Furthermore we investigate the effect of the strength of external inputs on the memory recall", "id": "363", "src": "noise effect on memory recall in dynamical neural network model of hippocampus . we investigate some noise effect on a neural network model proposed by araki and aihara ( <digit> ) for the memory recall of dynamical patterns in the hippocampus and the entorhinal cortex the noise effect is important since the release of transmitters at synaptic clefts , the operation of gate of ion channels and so on are known as stochastic phenomena . we consider two kinds of noise effect due to a deterministic noise and a stochastic noise . by numerical simulations , we find that reasonable values of noise give better performance on the memory recall of dynamical patterns . furthermore we investigate the effect of the strength of external inputs on the memory recall"}
{"title": "Fuzzy polynomial neural networks: hybrid architectures of fuzzy modeling", "abstract": "We introduce a concept of fuzzy polynomial neural networks (FPNNs), a hybrid modeling architecture combining polynomial neural networks (PNNs) and fuzzy neural networks (FNNs). The development of the FPNNs dwells on the technologies of computational intelligence (CI), namely fuzzy sets, neural networks, and genetic algorithms. The structure of the FPNN results from a synergistic usage of FNN and PNN. FNNs contribute to the formation of the premise part of the rule-based structure of the FPNN. The consequence part of the FPNN is designed using PNNs. The structure of the PNN is not fixed in advance as it usually takes place in the case of conventional neural networks, but becomes organized dynamically to meet the required approximation error. We exploit a group method of data handling (GMDH) to produce this dynamic topology of the network. The performance of the FPNN is quantified through experimentation that exploits standard data already used in fuzzy modeling. The obtained experimental results reveal that the proposed networks exhibit high accuracy and generalization capabilities in comparison to other similar fuzzy models", "id": "364", "src": "fuzzy polynomial neural networks hybrid architectures of fuzzy modeling . we introduce a concept of fuzzy polynomial neural networks ( fpnns ) , a hybrid modeling architecture combining polynomial neural networks ( pnns ) and fuzzy neural networks ( fnns ) . the development of the fpnns dwells on the technologies of computational intelligence ( ci ) , namely fuzzy sets , neural networks , and genetic algorithms . the structure of the fpnn results from a synergistic usage of fnn and pnn . fnns contribute to the formation of the premise part of the rule based structure of the fpnn . the consequence part of the fpnn is designed using pnns . the structure of the pnn is not fixed in advance as it usually takes place in the case of conventional neural networks , but becomes organized dynamically to meet the required approximation error . we exploit a group method of data handling ( gmdh ) to produce this dynamic topology of the network . the performance of the fpnn is quantified through experimentation that exploits standard data already used in fuzzy modeling . the obtained experimental results reveal that the proposed networks exhibit high accuracy and generalization capabilities in comparison to other similar fuzzy models"}
{"title": "MEMS applications in computer disk drive dual-stage servo systems", "abstract": "We present a decoupled discrete time pole placement design method, which can be combined with a self-tuning scheme to compensate variations in the microactuator's (MA's) resonance mode. Section I of the paper describes the design and fabrication of a prototype microactuator with an integrated gimbal structure. Section II presents a decoupled track-following controller design and a self-tuning control scheme to compensate for the MA's resonance mode variations", "id": "365", "src": "mems applications in computer disk drive dual stage servo systems . we present a decoupled discrete time pole placement design method , which can be combined with a self tuning scheme to compensate variations in the microactuator ' s ( ma ' s ) resonance mode . section i of the paper describes the design and fabrication of a prototype microactuator with an integrated gimbal structure . section ii presents a decoupled track following controller design and a self tuning control scheme to compensate for the ma ' s resonance mode variations"}
{"title": "Nuclear magnetic resonance molecular photography", "abstract": "A procedure is described for storing a two-dimensional (2D) pattern consisting of 32*32=1024 bits in a spin state of a molecular system and then retrieving the stored information as a stack of nuclear magnetic resonance spectra. The system used is a nematic liquid crystal, the protons of which act as spin clusters with strong intramolecular interactions. The technique used is a programmable multifrequency irradiation with low amplitude. When it is applied to the liquid crystal, a large number of coherent long-lived /sup 1/H response signals can be excited, resulting in a spectrum showing many sharp peaks with controllable frequencies and amplitudes. The spectral resolution is enhanced by using a second weak pulse with a 90 degrees phase shift, so that the 1024 bits of information can be retrieved as a set of well-resolved pseudo-2D spectra reproducing the input pattern", "id": "366", "src": "nuclear magnetic resonance molecular photography . a procedure is described for storing a two dimensional ( 2d ) pattern consisting of 32*32 <digit> bits in a spin state of a molecular system and then retrieving the stored information as a stack of nuclear magnetic resonance spectra . the system used is a nematic liquid crystal , the protons of which act as spin clusters with strong intramolecular interactions . the technique used is a programmable multifrequency irradiation with low amplitude . when it is applied to the liquid crystal , a large number of coherent long lived sup 1 h response signals can be excited , resulting in a spectrum showing many sharp peaks with controllable frequencies and amplitudes . the spectral resolution is enhanced by using a second weak pulse with a <digit> degrees phase shift , so that the <digit> bits of information can be retrieved as a set of well resolved pseudo 2d spectra reproducing the input pattern"}
{"title": "Novel active noise-reducing headset using earshell vibration control", "abstract": "Active noise-reducing (ANR) headsets are available commercially in applications varying from aviation communication to consumer audio. Current ANR systems use passive attenuation at high frequencies and loudspeaker-based active noise control at low frequencies to achieve broadband noise reduction. This paper presents a novel ANR headset in which the external noise transmitted to the user's ear via earshell vibration is reduced by controlling the vibration of the earshell using force actuators acting against an inertial mass or the earshell headband. Model-based theoretical analysis using velocity feedback control showed that current piezoelectric actuators provide sufficient force but require lower stiffness for improved low-frequency performance. Control simulations based on experimental data from a laboratory headset showed that good performance can potentially be achieved in practice by a robust feedback controller, while a single-frequency real-time control experiment verified that noise reduction can be achieved using earshell vibration control", "id": "367", "src": "novel active noise reducing headset using earshell vibration control . active noise reducing ( anr ) headsets are available commercially in applications varying from aviation communication to consumer audio . current anr systems use passive attenuation at high frequencies and loudspeaker based active noise control at low frequencies to achieve broadband noise reduction . this paper presents a novel anr headset in which the external noise transmitted to the user ' s ear via earshell vibration is reduced by controlling the vibration of the earshell using force actuators acting against an inertial mass or the earshell headband . model based theoretical analysis using velocity feedback control showed that current piezoelectric actuators provide sufficient force but require lower stiffness for improved low frequency performance . control simulations based on experimental data from a laboratory headset showed that good performance can potentially be achieved in practice by a robust feedback controller , while a single frequency real time control experiment verified that noise reduction can be achieved using earshell vibration control"}
{"title": "Theoretical and experimental investigations on coherence of traffic noise", "abstract": "transmission through an open window into a rectangular room in high-rise buildings A method for theoretically calculating the coherence between sound pressure inside a rectangular room in a high-rise building and that outside the open window of the room is proposed. The traffic noise transmitted into a room is generally dominated by low-frequency components, to which active noise control (ANC) technology may find an application. However, good coherence between reference and error signals is essential for an effective noise reduction and should be checked first. Based on traffic noise prediction methods, wave theory, and mode coupling theory, the results of this paper enabled one to determine the potentials and limitations of ANC used to reduce such a transmission. Experimental coherence results are shown for two similar, empty rectangular rooms located on the 17th and 30th floors of a 34 floor high-rise building. The calculated results with the proposed method are generally in good agreement with the experimental results and demonstrate the usefulness of the method for predicting the coherence", "id": "368", "src": "theoretical and experimental investigations on coherence of traffic noise . transmission through an open window into a rectangular room in high rise buildings a method for theoretically calculating the coherence between sound pressure inside a rectangular room in a high rise building and that outside the open window of the room is proposed . the traffic noise transmitted into a room is generally dominated by low frequency components , to which active noise control ( anc ) technology may find an application . however , good coherence between reference and error signals is essential for an effective noise reduction and should be checked first . based on traffic noise prediction methods , wave theory , and mode coupling theory , the results of this paper enabled one to determine the potentials and limitations of anc used to reduce such a transmission . experimental coherence results are shown for two similar , empty rectangular rooms located on the 17th and 30th floors of a <digit> floor high rise building . the calculated results with the proposed method are generally in good agreement with the experimental results and demonstrate the usefulness of the method for predicting the coherence"}
{"title": "High-density remote storage: the Ohio State University Libraries depository", "abstract": "The article describes a high-density off-site book storage facility operated by the Ohio State University Libraries. Opened in 1995, it has the capacity to house nearly 1.5 million items in only 9000 square feet by shelving books by size on 30-foot tall shelving. A sophisticated climate control system extends the life of stored materials up to 12 times. An online catalog record for each item informs patrons that the item is located in a remote location. Regular courier deliveries from the storage facility bring requested materials to patrons with minimal delay", "id": "369", "src": "high density remote storage the ohio state university libraries depository . the article describes a high density off site book storage facility operated by the ohio state university libraries . opened in <digit> , it has the capacity to house nearly 1 . 5 million items in only <digit> square feet by shelving books by size on <digit> foot tall shelving . a sophisticated climate control system extends the life of stored materials up to <digit> times . an online catalog record for each item informs patrons that the item is located in a remote location . regular courier deliveries from the storage facility bring requested materials to patrons with minimal delay"}
{"title": "Hours of operation and service in academic libraries: toward a national", "abstract": "standard In an effort toward establishing a standard for academic library hours, the article surveys and compares hours of operation and service for ARL libraries and IPEDS survey respondents. The article ranks the ARL (Association for Research Libraries) libraries according to hours of operation and reference hours and then briefly discusses such issues as libraries offering twenty-four access and factors affecting service hour decisions", "id": "370", "src": "hours of operation and service in academic libraries toward a national . standard in an effort toward establishing a standard for academic library hours , the article surveys and compares hours of operation and service for arl libraries and ipeds survey respondents . the article ranks the arl ( association for research libraries ) libraries according to hours of operation and reference hours and then briefly discusses such issues as libraries offering twenty four access and factors affecting service hour decisions"}
{"title": "Using the Web to answer legal reference questions", "abstract": "In an effort to help non-law librarians with basic legal reference questions, the author highlights three basic legal Web sites and outlines useful subject-specific Web sites that focus on statutes and regulations, case law and attorney directories", "id": "371", "src": "using the web to answer legal reference questions . in an effort to help non law librarians with basic legal reference questions , the author highlights three basic legal web sites and outlines useful subject specific web sites that focus on statutes and regulations , case law and attorney directories"}
{"title": "The service side of systems librarianship", "abstract": "Describes the role of a systems librarian at a small academic library. Although online catalogs and the Internet are making library accessibility more convenient, the need for library buildings and professionals has not diminished. Typical duties of a systems librarian and the effects of new technology on librarianship are discussed. Services provided to other constituencies on campus and the blurring relationship between the library and computer services are also presented", "id": "372", "src": "the service side of systems librarianship . describes the role of a systems librarian at a small academic library . although online catalogs and the internet are making library accessibility more convenient , the need for library buildings and professionals has not diminished . typical duties of a systems librarian and the effects of new technology on librarianship are discussed . services provided to other constituencies on campus and the blurring relationship between the library and computer services are also presented"}
{"title": "Defining electronic librarianship: a content analysis of job advertisements", "abstract": "Advances in technology create dramatic changes within libraries. The complex issues surrounding this new electronic, end-user environment have major ramifications and require expert knowledge. Electronic services librarians and electronic resources librarians are two specialized titles that have recently emerged within the field of librarianship to fill this niche. Job advertisements listed in American Libraries from January 1989 to December 1998 were examined to identify responsibilities, qualifications, organizational and salary information relating to the newly emerging role of electronic librarian", "id": "373", "src": "defining electronic librarianship a content analysis of job advertisements . advances in technology create dramatic changes within libraries . the complex issues surrounding this new electronic , end user environment have major ramifications and require expert knowledge . electronic services librarians and electronic resources librarians are two specialized titles that have recently emerged within the field of librarianship to fill this niche . job advertisements listed in american libraries from january <digit> to december <digit> were examined to identify responsibilities , qualifications , organizational and salary information relating to the newly emerging role of electronic librarian"}
{"title": "Customer in-reach and library strategic systems: the case of ILLiad", "abstract": "Libraries have walls. Recognizing this fact, the Interlibrary Loan Department at Virginia Tech is creating systems and services that enable our customers to reach past our walls at anytime from anywhere. Customer in-reach enables Virginia Tech faculty, students, and staff anywhere in the world to obtain information and services heretofore available only to our on-campus customers. ILLiad, Virginia Tech's interlibrary borrowing system, is the library strategic system that attains this goal. The principles that guided development of ILLiad are widely applicable", "id": "374", "src": "customer in reach and library strategic systems the case of illiad . libraries have walls . recognizing this fact , the interlibrary loan department at virginia tech is creating systems and services that enable our customers to reach past our walls at anytime from anywhere . customer in reach enables virginia tech faculty , students , and staff anywhere in the world to obtain information and services heretofore available only to our on campus customers . illiad , virginia tech ' s interlibrary borrowing system , is the library strategic system that attains this goal . the principles that guided development of illiad are widely applicable"}
{"title": "NuVox shows staying power with new cash, new market", "abstract": "Who says you can't raise cash in today's telecom market? NuVox Communications positions itself for the long run with $78.5 million in funding and a new credit facility", "id": "375", "src": "nuvox shows staying power with new cash , new market . who says you can ' t raise cash in today ' s telecom market nuvox communications positions itself for the long run with <digit> . 5 million in funding and a new credit facility"}
{"title": "Improvements and critique on Sugeno's and Yasukawa's qualitative modeling", "abstract": "Investigates Sugeno's and Yasukawa's (1993) qualitative fuzzy modeling approach. We propose some easily implementable solutions for the unclear details of the original paper, such as trapezoid approximation of membership functions, rule creation from sample data points, and selection of important variables. We further suggest an improved parameter identification algorithm to be applied instead of the original one. These details are crucial concerning the method's performance as it is shown in a comparative analysis and helps to improve the accuracy of the built-up model. Finally, we propose a possible further rule base reduction which can be applied successfully in certain cases. This improvement reduces the time requirement of the method by up to 16% in our experiments", "id": "376", "src": "improvements and critique on sugeno ' s and yasukawa ' s qualitative modeling . investigates sugeno ' s and yasukawa ' s ( <digit> ) qualitative fuzzy modeling approach . we propose some easily implementable solutions for the unclear details of the original paper , such as trapezoid approximation of membership functions , rule creation from sample data points , and selection of important variables . we further suggest an improved parameter identification algorithm to be applied instead of the original one . these details are crucial concerning the method ' s performance as it is shown in a comparative analysis and helps to improve the accuracy of the built up model . finally , we propose a possible further rule base reduction which can be applied successfully in certain cases . this improvement reduces the time requirement of the method by up to <digit> in our experiments"}
{"title": "The plot thins: thin-client computer systems and academic libraries", "abstract": "The few libraries that have tried thin client architectures have noted a number of compelling reasons to do so. For starters, thin client devices are far less expensive than most PCs. More importantly, thin client computing devices are believed to be far less expensive to manage and support than traditional PCs", "id": "377", "src": "the plot thins thin client computer systems and academic libraries . the few libraries that have tried thin client architectures have noted a number of compelling reasons to do so . for starters , thin client devices are far less expensive than most pcs . more importantly , thin client computing devices are believed to be far less expensive to manage and support than traditional pcs"}
{"title": "Academic libraries and community: making the connection", "abstract": "I explore the theme of academic libraries serving and reaching out to the broader community. I highlight interesting projects reported on in the literature (such as the Through Our Parents' Eyes project) and report on others. I look at challenges to community partnerships and recommendations for making them succeed. Although I focus on links with the broader community, I also took at methods for increasing cooperation among various units on campus, so that the needs of campus community groups-such as distance education students or disabled students-are effectively addressed. Though academic libraries are my focus, we can learn a lot from the community building efforts of public libraries", "id": "378", "src": "academic libraries and community making the connection . i explore the theme of academic libraries serving and reaching out to the broader community . i highlight interesting projects reported on in the literature ( such as the through our parents ' eyes project ) and report on others . i look at challenges to community partnerships and recommendations for making them succeed . although i focus on links with the broader community , i also took at methods for increasing cooperation among various units on campus , so that the needs of campus community groups such as distance education students or disabled students are effectively addressed . though academic libraries are my focus , we can learn a lot from the community building efforts of public libraries"}
{"title": "Using Internet search engines to estimate word frequency", "abstract": "The present research investigated Internet search engines as a rapid, cost-effective alternative for estimating word frequencies. Frequency estimates for 382 words were obtained and compared across four methods: (1) Internet search engines, (2) the Kucera and Francis (1967) analysis of a traditional linguistic corpus, (3) the CELEX English linguistic database (Baayen et al., 1995), and (4) participant ratings of familiarity. The results showed that Internet search engines produced frequency estimates that were highly consistent with those reported by Kucera and Francis and those calculated from CELEX, highly consistent across search engines, and very reliable over a 6 month period of time. Additional results suggested that Internet search engines are an excellent option when traditional word frequency analyses do not contain the necessary data (e.g., estimates for forenames and slang). In contrast, participants' familiarity judgments did not correspond well with the more objective estimates of word frequency. Researchers are advised to use search engines with large databases (e.g., AltaVista) to ensure the greatest representativeness of the frequency estimates", "id": "379", "src": "using internet search engines to estimate word frequency . the present research investigated internet search engines as a rapid , cost effective alternative for estimating word frequencies . frequency estimates for <digit> words were obtained and compared across four methods ( 1 ) internet search engines , ( 2 ) the kucera and francis ( <digit> ) analysis of a traditional linguistic corpus , ( 3 ) the celex english linguistic database ( baayen et al . , <digit> ) , and ( 4 ) participant ratings of familiarity . the results showed that internet search engines produced frequency estimates that were highly consistent with those reported by kucera and francis and those calculated from celex , highly consistent across search engines , and very reliable over a 6 month period of time . additional results suggested that internet search engines are an excellent option when traditional word frequency analyses do not contain the necessary data ( e . g . , estimates for forenames and slang ) . in contrast , participants ' familiarity judgments did not correspond well with the more objective estimates of word frequency . researchers are advised to use search engines with large databases ( e . g . , altavista ) to ensure the greatest representativeness of the frequency estimates"}
{"title": "Visual-word identification thresholds for the 260 fragmented words of the", "abstract": "Snodgrass and Vanderwart pictures in Spanish Word difficulty varies from language to language; therefore, normative data of verbal stimuli cannot be imported directly from another language. We present mean identification thresholds for the 260 screen-fragmented words corresponding to the total set of Snodgrass and Vanderwart (1980) pictures. Individual words were fragmented in eight levels using Turbo Pascal, and the resulting program was implemented on a PC microcomputer. The words were presented individually to a group of 40 Spanish observers, using a controlled time procedure. An unspecific learning effect was found showing that performance improved due to practice with the task. Finally, of the 11 psycholinguistic variables that previous researchers have shown to affect word identification, only imagery accounted for a significant amount of variance in the threshold values", "id": "380", "src": "visual word identification thresholds for the <digit> fragmented words of the . snodgrass and vanderwart pictures in spanish word difficulty varies from language to language therefore , normative data of verbal stimuli cannot be imported directly from another language . we present mean identification thresholds for the <digit> screen fragmented words corresponding to the total set of snodgrass and vanderwart ( <digit> ) pictures . individual words were fragmented in eight levels using turbo pascal , and the resulting program was implemented on a pc microcomputer . the words were presented individually to a group of <digit> spanish observers , using a controlled time procedure . an unspecific learning effect was found showing that performance improved due to practice with the task . finally , of the <digit> psycholinguistic variables that previous researchers have shown to affect word identification , only imagery accounted for a significant amount of variance in the threshold values"}
{"title": "A Web-accessible database of characteristics of the 1,945 basic Japanese kanji", "abstract": "In 1981, the Japanese government published a list of the 1,945 basic Japanese kanji (Jooyoo Kanji-hyo), including specifications of pronunciation. This list was established as the standard for kanji usage in print. The database for 1,945 basic Japanese kanji provides 30 cells that explain in detail the various characteristics of kanji. Means, standard deviations, distributions, and information related to previous research concerning these kanji are provided in this paper. The database is saved as a Microsoft Excel 2000 file for Windows. This kanji database is accessible on the Web site of the Oxford Text Archive, Oxford University (http://ota.ahds.ac.uk). Using this database, researchers and educators will be able to conduct planned experiments and organize classroom instruction on the basis of the known characteristics of selected kanji", "id": "381", "src": "a web accessible database of characteristics of the 1 , <digit> basic japanese kanji . in <digit> , the japanese government published a list of the 1 , <digit> basic japanese kanji ( jooyoo kanji hyo ) , including specifications of pronunciation . this list was established as the standard for kanji usage in print . the database for 1 , <digit> basic japanese kanji provides <digit> cells that explain in detail the various characteristics of kanji . means , standard deviations , distributions , and information related to previous research concerning these kanji are provided in this paper . the database is saved as a microsoft excel <digit> file for windows . this kanji database is accessible on the web site of the oxford text archive , oxford university ( http ota . ahds . ac . uk ) . using this database , researchers and educators will be able to conduct planned experiments and organize classroom instruction on the basis of the known characteristics of selected kanji"}
{"title": "Full-screen ultrafast video modes over-clocked by simple VESA routines and", "abstract": "registers reprogramming under MS-DOS Fast full-screen presentation of stimuli is necessary in psychological research. Although Spitczok von Brisinski (1994) introduced a method that achieved ultrafast display by reprogramming the registers, he could not produce an acceptable full-screen display. In this report, the author introduces a new method combining VESA routine calling with register reprogramming that can yield a display at 640 * 480 resolution, with a refresh rate of about 150 Hz", "id": "382", "src": "full screen ultrafast video modes over clocked by simple vesa routines and . registers reprogramming under ms dos fast full screen presentation of stimuli is necessary in psychological research . although spitczok von brisinski ( <digit> ) introduced a method that achieved ultrafast display by reprogramming the registers , he could not produce an acceptable full screen display . in this report , the author introduces a new method combining vesa routine calling with register reprogramming that can yield a display at <digit> * <digit> resolution , with a refresh rate of about <digit> hz"}
{"title": "Measuring keyboard response delays by comparing keyboard and joystick inputs", "abstract": "The response characteristics of PC keyboards have to be identified when they are used as response devices in psychological experiments. In the past, the proposed method has been to check the characteristics independently by means of external measurement equipment. However, with the availability of different PC models and the rapid pace of model change, there is an urgent need for the development of convenient and accurate methods of checking. The method proposed consists of raising the precision of the PC's clock to the microsecond level and using a joystick connected to the MIDI terminal of a sound board to give the PC an independent timing function. Statistical processing of the data provided by this method makes it possible to estimate accurately the keyboard scanning interval time and the average keyboard delay time. The results showed that measured keyboard delay times varied from 11 to 73 msec, depending on the keyboard model, with most values being less than 30 msec", "id": "383", "src": "measuring keyboard response delays by comparing keyboard and joystick inputs . the response characteristics of pc keyboards have to be identified when they are used as response devices in psychological experiments . in the past , the proposed method has been to check the characteristics independently by means of external measurement equipment . however , with the availability of different pc models and the rapid pace of model change , there is an urgent need for the development of convenient and accurate methods of checking . the method proposed consists of raising the precision of the pc ' s clock to the microsecond level and using a joystick connected to the midi terminal of a sound board to give the pc an independent timing function . statistical processing of the data provided by this method makes it possible to estimate accurately the keyboard scanning interval time and the average keyboard delay time . the results showed that measured keyboard delay times varied from <digit> to <digit> msec , depending on the keyboard model , with most values being less than <digit> msec"}
{"title": "Computer program to generate operant schedules", "abstract": "A computer program for programming schedules of reinforcement is described. Students can use the program to experience schedules of reinforcement that are typically used with nonhuman subjects. Accumulative recording of a student's response can be shown on the screen and/or printed with the computer's printer. The program can also be used to program operant schedules for animal subjects. The program was tested with human subjects experiencing fixed ratio, variable ratio, fixed interval, and variable interval schedules. Performance for human subjects on a given schedule was similar to performance for nonhuman subjects on the same schedule", "id": "384", "src": "computer program to generate operant schedules . a computer program for programming schedules of reinforcement is described . students can use the program to experience schedules of reinforcement that are typically used with nonhuman subjects . accumulative recording of a student ' s response can be shown on the screen and or printed with the computer ' s printer . the program can also be used to program operant schedules for animal subjects . the program was tested with human subjects experiencing fixed ratio , variable ratio , fixed interval , and variable interval schedules . performance for human subjects on a given schedule was similar to performance for nonhuman subjects on the same schedule"}
{"title": "On-line Homework/Quiz/Exam applet: freely available Java software for", "abstract": "evaluating performance on line The Homework/Quiz/Exam applet is a freely available Java program that can be used to evaluate student performance on line for any content authored by a teacher. It has database connectivity so that student scores are automatically recorded. It allows several different types of questions. Each question can be linked to images and detailed story problems. Three levels of feedback are provided to student responses. It allows teachers to randomize the sequence of questions and to randomize which of several options is the correct answer in multiple-choice questions. The creation and editing of questions involves menu selections, button presses, and the typing of content; no programming knowledge is required. The code is open source in order to encourage modifications that will meet individual pedagogical needs", "id": "385", "src": "on line homework quiz exam applet freely available java software for . evaluating performance on line the homework quiz exam applet is a freely available java program that can be used to evaluate student performance on line for any content authored by a teacher . it has database connectivity so that student scores are automatically recorded . it allows several different types of questions . each question can be linked to images and detailed story problems . three levels of feedback are provided to student responses . it allows teachers to randomize the sequence of questions and to randomize which of several options is the correct answer in multiple choice questions . the creation and editing of questions involves menu selections , button presses , and the typing of content no programming knowledge is required . the code is open source in order to encourage modifications that will meet individual pedagogical needs"}
{"title": "WEXTOR: a Web-based tool for generating and visualizing experimental designs", "abstract": "and procedures WEXTOR is a Javascript-based experiment generator and teaching tool on the World Wide Web that can be used to design laboratory and Web experiments in a guided step-by-step process. It dynamically creates the customized Web pages and Javascripts needed for the experimental procedure and provides experimenters with a print-ready visual display of their experimental design. WEXTOR flexibly supports complete and incomplete factorial designs with between-subjects, within-subjects, and quasi-experimental factors, as well as mixed designs. The software implements client-side response time measurement and contains a content wizard for creating interactive materials, as well as dependent measures (graphical scales, multiple-choice items, etc.), on the experiment pages. However, it does not aim to replace a full-fledged HTML editor. Several methodological features specifically needed in Web experimental design have been implemented in the Web-based tool and are described in this paper. WEXTOR is platform independent. The created Web pages can be uploaded to any type of Web server in which data may be recorded in logfiles or via a database. The current version of WEXTOR is freely available for educational and noncommercial purposes. Its Web address is http://www.genpsylab.unizh.ch/wextor/index.html", "id": "386", "src": "wextor a web based tool for generating and visualizing experimental designs . and procedures wextor is a javascript based experiment generator and teaching tool on the world wide web that can be used to design laboratory and web experiments in a guided step by step process . it dynamically creates the customized web pages and javascripts needed for the experimental procedure and provides experimenters with a print ready visual display of their experimental design . wextor flexibly supports complete and incomplete factorial designs with between subjects , within subjects , and quasi experimental factors , as well as mixed designs . the software implements client side response time measurement and contains a content wizard for creating interactive materials , as well as dependent measures ( graphical scales , multiple choice items , etc . ) , on the experiment pages . however , it does not aim to replace a full fledged html editor . several methodological features specifically needed in web experimental design have been implemented in the web based tool and are described in this paper . wextor is platform independent . the created web pages can be uploaded to any type of web server in which data may be recorded in logfiles or via a database . the current version of wextor is freely available for educational and noncommercial purposes . its web address is http www . genpsylab . unizh . ch wextor index . html"}
{"title": "Adaptive neural/fuzzy control for interpolated nonlinear systems", "abstract": "Adaptive control for nonlinear time-varying systems is of both theoretical and practical importance. We propose an adaptive control methodology for a class of nonlinear systems with a time-varying structure. This class of systems is composed of interpolations of nonlinear subsystems which are input-output feedback linearizable. Both indirect and direct adaptive control methods are developed, where the spatially localized models (in the form of Takagi-Sugeno fuzzy systems or radial basis function neural networks) are used as online approximators to learn the unknown dynamics of the system. Without assumptions on rate of change of system dynamics, the proposed adaptive control methods guarantee that all internal signals of the system are bounded and the tracking error is asymptotically stable. The performance of the adaptive controller is demonstrated using a jet engine control problem", "id": "387", "src": "adaptive neural fuzzy control for interpolated nonlinear systems . adaptive control for nonlinear time varying systems is of both theoretical and practical importance . we propose an adaptive control methodology for a class of nonlinear systems with a time varying structure . this class of systems is composed of interpolations of nonlinear subsystems which are input output feedback linearizable . both indirect and direct adaptive control methods are developed , where the spatially localized models ( in the form of takagi sugeno fuzzy systems or radial basis function neural networks ) are used as online approximators to learn the unknown dynamics of the system . without assumptions on rate of change of system dynamics , the proposed adaptive control methods guarantee that all internal signals of the system are bounded and the tracking error is asymptotically stable . the performance of the adaptive controller is demonstrated using a jet engine control problem"}
{"title": "ePsych: interactive demonstrations and experiments in psychology", "abstract": "ePsych (http://epsych.msstate.edu), a new Web site currently under active development, is intended to teach students about the discipline of psychology. The site presumes little prior knowledge about the field and so may be used in introductory classes, but it incorporates sufficient depth of coverage to be useful in more advanced classes as well. Numerous interactive and dynamic elements are incorporated into various modules, orientations, and guidebooks. These elements include Java-based experiments and demonstrations, video clips, and animated diagrams. Rapid access to all material is provided through a layer-based navigation system that allows users to visit various \"Worlds of the Mind.\" Active learning is encouraged, by challenging students with puzzles and problems and by providing the opportunity to \"dig deeper\" to learn more about the phenomena at hand", "id": "388", "src": "epsych interactive demonstrations and experiments in psychology . epsych ( http epsych . msstate . edu ) , a new web site currently under active development , is intended to teach students about the discipline of psychology . the site presumes little prior knowledge about the field and so may be used in introductory classes , but it incorporates sufficient depth of coverage to be useful in more advanced classes as well . numerous interactive and dynamic elements are incorporated into various modules , orientations , and guidebooks . these elements include java based experiments and demonstrations , video clips , and animated diagrams . rapid access to all material is provided through a layer based navigation system that allows users to visit various worlds of the mind . active learning is encouraged , by challenging students with puzzles and problems and by providing the opportunity to dig deeper to learn more about the phenomena at hand"}
{"title": "Information architecture without internal theory: an inductive design process", "abstract": "This article suggests that Information Architecture (IA) design is primarily an inductive process. Although top-level goals, user attributes and available content are periodically considered, the process involves bottom-up design activities. IA is inductive partly because it lacks internal theory, and partly because it is an activity that supports emergent phenomena (user experiences) from basic design components. The nature of IA design is well described by Constructive Induction (CI), a design process that involves locating the best representational framework for the design problem, identifying a solution within that framework and translating it back to the design problem at hand. The future of IA, if it remains inductive or develops a body of theory (or both), is considered", "id": "389", "src": "information architecture without internal theory an inductive design process . this article suggests that information architecture ( ia ) design is primarily an inductive process . although top level goals , user attributes and available content are periodically considered , the process involves bottom up design activities . ia is inductive partly because it lacks internal theory , and partly because it is an activity that supports emergent phenomena ( user experiences ) from basic design components . the nature of ia design is well described by constructive induction ( ci ) , a design process that involves locating the best representational framework for the design problem , identifying a solution within that framework and translating it back to the design problem at hand . the future of ia , if it remains inductive or develops a body of theory ( or both ) , is considered"}
{"title": "Information architecture for the Web: The IA matrix approach to designing", "abstract": "children's portals The article presents a matrix that can serve as a tool for designing the information architecture of a Web portal in a logical and systematic manner. The information architect begins by inputting the portal's objective, target user, and target content. The matrix then determines the most appropriate information architecture attributes for the portal by filling in the Applied Information Architecture portion of the matrix. The article discusses how the matrix works using the example of a children's Web portal to provide access to museum information", "id": "390", "src": "information architecture for the web the ia matrix approach to designing . children ' s portals the article presents a matrix that can serve as a tool for designing the information architecture of a web portal in a logical and systematic manner . the information architect begins by inputting the portal ' s objective , target user , and target content . the matrix then determines the most appropriate information architecture attributes for the portal by filling in the applied information architecture portion of the matrix . the article discusses how the matrix works using the example of a children ' s web portal to provide access to museum information"}
{"title": "Information architecture: notes toward a new curriculum", "abstract": "There are signs that information architecture is coalescing into a field of professional practice. However, if it is to become a profession, it must develop a means of educating new information architects. Lessons from other fields suggest that professional education typically evolves along a predictable path, from apprenticeships to trade schools to college- and university-level education. Information architecture education may develop more quickly to meet the growing demands of the information society. Several pedagogical approaches employed in other fields may be adopted for information architecture education, as long as the resulting curricula provide an interdisciplinary approach and balance instruction in technical and design skills with consideration of theoretical concepts. Key content areas are information organization, graphic. design, computer science, user and usability studies, and communication. Certain logistics must be worked out, including where information architecture studies should be housed and what kinds of degrees should be offered and at what levels. The successful information architecture curriculum will be flexible and adaptable in order to meet the changing needs of students and the marketplace", "id": "391", "src": "information architecture notes toward a new curriculum . there are signs that information architecture is coalescing into a field of professional practice . however , if it is to become a profession , it must develop a means of educating new information architects . lessons from other fields suggest that professional education typically evolves along a predictable path , from apprenticeships to trade schools to college and university level education . information architecture education may develop more quickly to meet the growing demands of the information society . several pedagogical approaches employed in other fields may be adopted for information architecture education , as long as the resulting curricula provide an interdisciplinary approach and balance instruction in technical and design skills with consideration of theoretical concepts . key content areas are information organization , graphic . design , computer science , user and usability studies , and communication . certain logistics must be worked out , including where information architecture studies should be housed and what kinds of degrees should be offered and at what levels . the successful information architecture curriculum will be flexible and adaptable in order to meet the changing needs of students and the marketplace"}
{"title": "Information architecture in JASIST: just where did we come from?", "abstract": "The emergence of Information Architecture within the information systems world has been simultaneously drawn out yet rapid. Those with an eye on history are quick to point to Wurman's 1976 use of the term \"architecture of information,\" but it has only been in the last 2 years that IA has become the source of sufficient interest for people to label themselves professionally as Information Architects. The impetus for this recent emergence of IA can be traced to a historical summit, supported by ASIS&T in May 2000 at Boston. It was here that several hundred of us gathered to thrash out the questions of just what IA was and what this new field might become. At the time of the summit, invited to present a short talk on my return journey from the annual ACM SIGCHI conference, I entered the summit expecting little and convinced that IA was nothing new. I left 2 days later refreshed, not just by the enthusiasm of the attendees for this term but by IA's potential to unify the disparate perspectives and orientations of professionals from a range of disciplines. It was at this summit that the idea for the special issue took root. I proposed the idea to Don Kraft, hoping he would find someone else to run with it. AS luck would have it, I ended up taking charge of it myself, with initial support from David Blair. From the suggestion to the finished product-has been the best part of 2 years, and in that time more than 50 volunteers reviewed over 20 submissions", "id": "392", "src": "information architecture in jasist just where did we come from . the emergence of information architecture within the information systems world has been simultaneously drawn out yet rapid . those with an eye on history are quick to point to wurman ' s <digit> use of the term architecture of information , but it has only been in the last 2 years that ia has become the source of sufficient interest for people to label themselves professionally as information architects . the impetus for this recent emergence of ia can be traced to a historical summit , supported by asis&t in may <digit> at boston . it was here that several hundred of us gathered to thrash out the questions of just what ia was and what this new field might become . at the time of the summit , invited to present a short talk on my return journey from the annual acm sigchi conference , i entered the summit expecting little and convinced that ia was nothing new . i left 2 days later refreshed , not just by the enthusiasm of the attendees for this term but by ia ' s potential to unify the disparate perspectives and orientations of professionals from a range of disciplines . it was at this summit that the idea for the special issue took root . i proposed the idea to don kraft , hoping he would find someone else to run with it . as luck would have it , i ended up taking charge of it myself , with initial support from david blair . from the suggestion to the finished product has been the best part of 2 years , and in that time more than <digit> volunteers reviewed over <digit> submissions"}
{"title": "The impact of the Internet on public library use: an analysis of the current", "abstract": "consumer market for library and Internet services The potential impact of the Internet on the public's demand for the services and resources of public libraries is an issue of critical importance. The research reported in this article provides baseline data concerning the evolving relationship between the public's use of the library and its use of the Internet. The authors developed a consumer model of the American adult market for information services and resources, segmented by use (or nonuse) of the public library and by access (or lack of access) to, and use (or nonuse) of, the Internet. A national Random Digit Dialing telephone survey collected data to estimate the size of each of six market segments, and to describe their usage choices between the public library and the Internet. The analyses presented in this article provide estimates of the size and demographics of each of the market segments; describe why people are currently using the public library and the Internet; identify the decision criteria people use in their choices of which provider to use; identify areas in which libraries and the Internet appear to be competing and areas in which they appear to be complementary; and identify reasons why people choose not to use the public library and/or the Internet. The data suggest that some differentiation between the library and the Internet is taking place, which may very well have an impact on consumer choices between the two. Longitudinal research is necessary to fully reveal trends in these usage choices, which have implications for all types of libraries in planning and policy development", "id": "393", "src": "the impact of the internet on public library use an analysis of the current . consumer market for library and internet services the potential impact of the internet on the public ' s demand for the services and resources of public libraries is an issue of critical importance . the research reported in this article provides baseline data concerning the evolving relationship between the public ' s use of the library and its use of the internet . the authors developed a consumer model of the american adult market for information services and resources , segmented by use ( or nonuse ) of the public library and by access ( or lack of access ) to , and use ( or nonuse ) of , the internet . a national random digit dialing telephone survey collected data to estimate the size of each of six market segments , and to describe their usage choices between the public library and the internet . the analyses presented in this article provide estimates of the size and demographics of each of the market segments describe why people are currently using the public library and the internet identify the decision criteria people use in their choices of which provider to use identify areas in which libraries and the internet appear to be competing and areas in which they appear to be complementary and identify reasons why people choose not to use the public library and or the internet . the data suggest that some differentiation between the library and the internet is taking place , which may very well have an impact on consumer choices between the two . longitudinal research is necessary to fully reveal trends in these usage choices , which have implications for all types of libraries in planning and policy development"}
{"title": "Duality revisited: construction of fractional frequency distributions based on", "abstract": "two dual Lotka laws Fractional frequency distributions of, for example, authors with a certain (fractional) number of papers are very irregular, and therefore not easy to model or to explain. The article gives a first attempt to this by as suming two simple Lotka laws (with exponent 2): one for the number of authors with n papers (total count here) and one for the number of papers with n authors, n in N. Based on an earlier made convolution model of Egghe, interpreted and reworked now for discrete scores, we are able to produce theoretical fractional frequency distributions with only one parameter, which are in very close agreement with the practical ones as found in a large dataset produced earlier by Rao (1995). The article also shows that (irregular) fractional frequency distributions are a consequence of Lotka's law, and are not examples of breakdowns of this famous historical law", "id": "394", "src": "duality revisited construction of fractional frequency distributions based on . two dual lotka laws fractional frequency distributions of , for example , authors with a certain ( fractional ) number of papers are very irregular , and therefore not easy to model or to explain . the article gives a first attempt to this by as suming two simple lotka laws ( with exponent 2 ) one for the number of authors with n papers ( total count here ) and one for the number of papers with n authors , n in n . based on an earlier made convolution model of egghe , interpreted and reworked now for discrete scores , we are able to produce theoretical fractional frequency distributions with only one parameter , which are in very close agreement with the practical ones as found in a large dataset produced earlier by rao ( <digit> ) . the article also shows that ( irregular ) fractional frequency distributions are a consequence of lotka ' s law , and are not examples of breakdowns of this famous historical law"}
{"title": "Relevance of Web documents: ghosts consensus method", "abstract": "The dominant method currently used to improve the quality of Internet search systems is often called \"digital democracy.\" Such an approach implies the utilization of the majority opinion of Internet users to determine the most relevant documents: for example, citation index usage for sorting of search results (google.com) or an enrichment of a query with terms that are asked frequently in relation with the query's theme. \"Digital democracy\" is an effective instrument in many cases, but it has an unavoidable shortcoming, which is a matter of principle: the average intellectual and cultural level of Internet users is very low; everyone knows what kind of information is dominant in Internet query statistics. Therefore, when one searches the Internet by means of \"digital democracy\" systems, one gets answers that reflect an underlying assumption that the user's mind potential is very low, and that his cultural interests are not demanding. Thus, it is more correct to use the term \"digital ochlocracy\" to refer to Internet search systems with \"digital democracy.\" Based on the well-known mathematical mechanism of linear programming, we propose a method to solve the indicated problem", "id": "395", "src": "relevance of web documents ghosts consensus method . the dominant method currently used to improve the quality of internet search systems is often called digital democracy . such an approach implies the utilization of the majority opinion of internet users to determine the most relevant documents for example , citation index usage for sorting of search results ( google . com ) or an enrichment of a query with terms that are asked frequently in relation with the query ' s theme . digital democracy is an effective instrument in many cases , but it has an unavoidable shortcoming , which is a matter of principle the average intellectual and cultural level of internet users is very low everyone knows what kind of information is dominant in internet query statistics . therefore , when one searches the internet by means of digital democracy systems , one gets answers that reflect an underlying assumption that the user ' s mind potential is very low , and that his cultural interests are not demanding . thus , it is more correct to use the term digital ochlocracy to refer to internet search systems with digital democracy . based on the well known mathematical mechanism of linear programming , we propose a method to solve the indicated problem"}
{"title": "Note on \"Deterministic inventory lot-size models under inflation with shortages", "abstract": "and deterioration for fluctuating demand\" by Yang et al For original paper see H.-L. Yang et al., ibid., vol.48, p.144-58 (2001). Yang et al. extended the lot-size models to allow for inflation and fluctuating demand. For this model they proved that the optimal replenishment schedule exists and is unique. They also proposed an algorithm to find the optimal policy. The present paper provides examples, which show that the optimal replenishment schedule and consequently the overall optimal policy may not exist", "id": "396", "src": "note on deterministic inventory lot size models under inflation with shortages . and deterioration for fluctuating demand by yang et al for original paper see h . l . yang et al . , ibid . , vol . <digit> , p . <digit> <digit> ( <digit> ) . yang et al . extended the lot size models to allow for inflation and fluctuating demand . for this model they proved that the optimal replenishment schedule exists and is unique . they also proposed an algorithm to find the optimal policy . the present paper provides examples , which show that the optimal replenishment schedule and consequently the overall optimal policy may not exist"}
{"title": "Designing a screening experiment for highly reliable products", "abstract": "Within a reasonable life-testing time, how to improve the reliability of highly reliable products is one of the great challenges. By using a resolution III experiment together with degradation test, Tseng et al. (1995) presented a case study of improving the reliability of fluorescent lamps. However, in conducting such an experiment, they did not address the problem of how to choose the optimal settings of variables, such as sample size, inspection frequency, and termination time for each run, which are influential to the correct identification of significant factors and the experimental cost. Assuming that the product's degradation paths satisfy Wiener processes, this paper proposes a systematic approach to the aforementioned problem. First, an identification rule is proposed. Next, under the constraints of a minimum probability of correct decision and a maximum probability of incorrect decision of the proposed identification rule, the optimum test plan can be obtained by minimizing the total experimental cost. An example is provided to illustrate the proposed method", "id": "397", "src": "designing a screening experiment for highly reliable products . within a reasonable life testing time , how to improve the reliability of highly reliable products is one of the great challenges . by using a resolution iii experiment together with degradation test , tseng et al . ( <digit> ) presented a case study of improving the reliability of fluorescent lamps . however , in conducting such an experiment , they did not address the problem of how to choose the optimal settings of variables , such as sample size , inspection frequency , and termination time for each run , which are influential to the correct identification of significant factors and the experimental cost . assuming that the product ' s degradation paths satisfy wiener processes , this paper proposes a systematic approach to the aforementioned problem . first , an identification rule is proposed . next , under the constraints of a minimum probability of correct decision and a maximum probability of incorrect decision of the proposed identification rule , the optimum test plan can be obtained by minimizing the total experimental cost . an example is provided to illustrate the proposed method"}
{"title": "Analysis and efficient implementation of a linguistic fuzzy c-means", "abstract": "The paper is concerned with a linguistic fuzzy c-means (FCM) algorithm with vectors of fuzzy numbers as inputs. This algorithm is based on the extension principle and the decomposition theorem. It turns out that using the extension principle to extend the capability of the standard membership update equation to deal with a linguistic vector has a huge computational complexity. In order to cope with this problem, an efficient method based on fuzzy arithmetic and optimization has been developed and analyzed. We also carefully examine and prove that the algorithm behaves in a way similar to the FCM in the degenerate linguistic case. Synthetic data sets and the iris data set have been used to illustrate the behavior of this linguistic version of the FCM", "id": "398", "src": "analysis and efficient implementation of a linguistic fuzzy c means . the paper is concerned with a linguistic fuzzy c means ( fcm ) algorithm with vectors of fuzzy numbers as inputs . this algorithm is based on the extension principle and the decomposition theorem . it turns out that using the extension principle to extend the capability of the standard membership update equation to deal with a linguistic vector has a huge computational complexity . in order to cope with this problem , an efficient method based on fuzzy arithmetic and optimization has been developed and analyzed . we also carefully examine and prove that the algorithm behaves in a way similar to the fcm in the degenerate linguistic case . synthetic data sets and the iris data set have been used to illustrate the behavior of this linguistic version of the fcm"}
{"title": "Warranty reserves for nonstationary sales processes", "abstract": "Estimation of warranty costs, in the event of product failure within the warranty period, is of importance to the manufacturer. Costs associated with replacement or repair of the product are usually drawn from a warranty reserve fund created by the manufacturer. Considering a stochastic sales process, first and second moments (and thereby the variance) are derived for the manufacturer's total discounted warranty cost of a single sale for single-component items under four different warranty policies from a manufacturer's point of view. These servicing strategies represent a renewable free-replacement, nonrenewable free-replacement, renewable pro-rata, and a nonrenewable minimal-repair warranty plans. The results are extended to determine the mean and variance of total discounted warranty costs for the total sales over the life cycle of the product. Furthermore, using a normal approximation, warranty reserves necessary for a certain protection level, so that reserves are not completely depleted, are found. Results and their managerial implications are studied through an extensive example", "id": "399", "src": "warranty reserves for nonstationary sales processes . estimation of warranty costs , in the event of product failure within the warranty period , is of importance to the manufacturer . costs associated with replacement or repair of the product are usually drawn from a warranty reserve fund created by the manufacturer . considering a stochastic sales process , first and second moments ( and thereby the variance ) are derived for the manufacturer ' s total discounted warranty cost of a single sale for single component items under four different warranty policies from a manufacturer ' s point of view . these servicing strategies represent a renewable free replacement , nonrenewable free replacement , renewable pro rata , and a nonrenewable minimal repair warranty plans . the results are extended to determine the mean and variance of total discounted warranty costs for the total sales over the life cycle of the product . furthermore , using a normal approximation , warranty reserves necessary for a certain protection level , so that reserves are not completely depleted , are found . results and their managerial implications are studied through an extensive example"}
{"title": "A multimodal data collection tool using REALbasic and Mac OS X", "abstract": "This project uses REALbasic 3.5 in the Mac OS X environment for development of a configuration tool that builds a data collection procedure for investigating the effectiveness of sonified graphs. The advantage of using REALbasic with the Mac OS X system is that it provides rapid development of stimulus presentation, direct recording of data to files, and control over other procedural issues. The program can be made to run natively on the new Mac OS X system, older Mac OS systems, and Windows (98SE, ME, 2000 PRO). With modification, similar programs could be used to present any number of visual/auditory stimulus combinations, complete with questions for each stimulus", "id": "400", "src": "a multimodal data collection tool using realbasic and mac os x . this project uses realbasic 3 . 5 in the mac os x environment for development of a configuration tool that builds a data collection procedure for investigating the effectiveness of sonified graphs . the advantage of using realbasic with the mac os x system is that it provides rapid development of stimulus presentation , direct recording of data to files , and control over other procedural issues . the program can be made to run natively on the new mac os x system , older mac os systems , and windows ( 98se , me , <digit> pro ) . with modification , similar programs could be used to present any number of visual auditory stimulus combinations , complete with questions for each stimulus"}
{"title": "Toward an Experimental Timing Standards Lab: benchmarking precision in the real", "abstract": "world Much discussion has taken place over the relative merits of various platforms and operating systems for real-time data collection. Most would agree that, provided great care is taken, many are capable of millisecond timing precision. However, to date, much of this work has focused on the theoretical aspects of raw performance. It is our belief that researchers would be better informed if they could place confidence limits on their own specific paradigms in situ and without modification. To this end, we have developed a millisecond precision test rig that can control and time experiments on a second presentation machine. We report on the specialist hardware and software used. We elucidate the importance of the approach in relation to real-world experimentation", "id": "401", "src": "toward an experimental timing standards lab benchmarking precision in the real . world much discussion has taken place over the relative merits of various platforms and operating systems for real time data collection . most would agree that , provided great care is taken , many are capable of millisecond timing precision . however , to date , much of this work has focused on the theoretical aspects of raw performance . it is our belief that researchers would be better informed if they could place confidence limits on their own specific paradigms in situ and without modification . to this end , we have developed a millisecond precision test rig that can control and time experiments on a second presentation machine . we report on the specialist hardware and software used . we elucidate the importance of the approach in relation to real world experimentation"}
{"title": "A server-side program for delivering experiments with animations", "abstract": "A server-side program for animation experiments is presented. The program is capable of delivering an experiment composed of discrete animation sequences in various file formats, collecting a discrete or continuous response from the observer, evaluating the appropriateness of the response, and ensuring that the user is not proceeding at an unreasonable rate. Most parameters of the program are controllable by experimenter-edited text files or simple switches in the program code, thereby minimizing the need for programming to create new experiments. A simple demonstration experiment is discussed and is freely available", "id": "402", "src": "a server side program for delivering experiments with animations . a server side program for animation experiments is presented . the program is capable of delivering an experiment composed of discrete animation sequences in various file formats , collecting a discrete or continuous response from the observer , evaluating the appropriateness of the response , and ensuring that the user is not proceeding at an unreasonable rate . most parameters of the program are controllable by experimenter edited text files or simple switches in the program code , thereby minimizing the need for programming to create new experiments . a simple demonstration experiment is discussed and is freely available"}
{"title": "Using NetCloak to develop server-side Web-based experiments without writing CGI", "abstract": "programs Server-side experiments use the Web server, rather than the participant's browser, to handle tasks such as random assignment, eliminating inconsistencies with Java and other client-side applications. Heretofore, experimenters wishing to create server-side experiments have had to write programs to create common gateway interface (CGI) scripts in programming languages such as Perl and C++. NetCloak uses simple, HTML-like commands to create CGIs. We used NetCloak to implement an experiment on probability estimation. Measurements of time on task and participants' IP addresses assisted quality control. Without prior training, in less than 1 month, we were able to use NetCloak to design and create a Web-based experiment and to help graduate students create three Web-based experiments of their own", "id": "403", "src": "using netcloak to develop server side web based experiments without writing cgi . programs server side experiments use the web server , rather than the participant ' s browser , to handle tasks such as random assignment , eliminating inconsistencies with java and other client side applications . heretofore , experimenters wishing to create server side experiments have had to write programs to create common gateway interface ( cgi ) scripts in programming languages such as perl and c++ . netcloak uses simple , html like commands to create cgis . we used netcloak to implement an experiment on probability estimation . measurements of time on task and participants ' ip addresses assisted quality control . without prior training , in less than 1 month , we were able to use netcloak to design and create a web based experiment and to help graduate students create three web based experiments of their own"}
{"title": "Open courseware and shared knowledge in higher education", "abstract": "Most college and university campuses in the United States and much of the developed world today maintain one, two, or several learning management systems (LMSs), which are courseware products that provide students and faculty with Web-based tools to manage course-related applications. Since the mid-1990s, two predominant models of Web courseware management systems have emerged: commercial and noncommercial. Some of the commercial products available today were created in academia as noncommercial but have since become commercially encumbered. Other products remain noncommercial but are struggling to survive in a world of fierce commercial competition. This article argues for an ethics of pedagogy in higher education that would be based on the guiding assumptions of the non-proprietary, peer-to-peer, open-source software movement", "id": "404", "src": "open courseware and shared knowledge in higher education . most college and university campuses in the united states and much of the developed world today maintain one , two , or several learning management systems ( lmss ) , which are courseware products that provide students and faculty with web based tools to manage course related applications . since the mid 1990s , two predominant models of web courseware management systems have emerged commercial and noncommercial . some of the commercial products available today were created in academia as noncommercial but have since become commercially encumbered . other products remain noncommercial but are struggling to survive in a world of fierce commercial competition . this article argues for an ethics of pedagogy in higher education that would be based on the guiding assumptions of the non proprietary , peer to peer , open source software movement"}
{"title": "Web-based experiments controlled by JavaScript: an example from probability", "abstract": "learning JavaScript programs can be used to control Web experiments. This technique is illustrated by an experiment that tested the effects of advice on performance in the classic probability-learning paradigm. Previous research reported that people tested via the Web or in the lab tended to match the probabilities of their responses to the probabilities that those responses would be reinforced. The optimal strategy, however, is to consistently choose the more frequent event; probability matching produces suboptimal performance. We investigated manipulations we reasoned should improve performance. A horse race scenario in which participants predicted the winner in each of a series of races between two horses was compared with an abstract scenario used previously. Ten groups of learners received different amounts of advice, including all combinations of (1) explicit instructions concerning the optimal strategy, (2) explicit instructions concerning a monetary sum to maximize, and (3) accurate information concerning the probabilities of events. The results showed minimal effects of horse race versus abstract scenario. Both advice concerning the optimal strategy and probability information contributed significantly to performance in the task. This paper includes a brief tutorial on JavaScript, explaining with simple examples how to assemble a browser-based experiment", "id": "405", "src": "web based experiments controlled by javascript an example from probability . learning javascript programs can be used to control web experiments . this technique is illustrated by an experiment that tested the effects of advice on performance in the classic probability learning paradigm . previous research reported that people tested via the web or in the lab tended to match the probabilities of their responses to the probabilities that those responses would be reinforced . the optimal strategy , however , is to consistently choose the more frequent event probability matching produces suboptimal performance . we investigated manipulations we reasoned should improve performance . a horse race scenario in which participants predicted the winner in each of a series of races between two horses was compared with an abstract scenario used previously . ten groups of learners received different amounts of advice , including all combinations of ( 1 ) explicit instructions concerning the optimal strategy , ( 2 ) explicit instructions concerning a monetary sum to maximize , and ( 3 ) accurate information concerning the probabilities of events . the results showed minimal effects of horse race versus abstract scenario . both advice concerning the optimal strategy and probability information contributed significantly to performance in the task . this paper includes a brief tutorial on javascript , explaining with simple examples how to assemble a browser based experiment"}
{"title": "Using latent semantic analysis to assess reader strategies", "abstract": "We tested a computer-based procedure for assessing reader strategies that was based on verbal protocols that utilized latent semantic analysis (LSA). Students were given self-explanation-reading training (SERT), which teaches strategies that facilitate self-explanation during reading, such as elaboration based on world knowledge and bridging between text sentences. During a computerized version of SERT practice, students read texts and typed self-explanations into a computer after each sentence. The use of SERT strategies during this practice was assessed by determining the extent to which students used the information in the current sentence versus the prior text or world knowledge in their self-explanations. This assessment was made on the basis of human judgments and LSA. Both human judgments and LSA were remarkably similar and indicated that students who were not complying with SERT tended to paraphrase the text sentences, whereas students who were compliant with SERT tended to explain the sentences in terms of what they knew about the world and of information provided in the prior text context. The similarity between human judgments and LSA indicates that LSA will be useful in accounting for reading strategies in a Web-based version of SERT", "id": "406", "src": "using latent semantic analysis to assess reader strategies . we tested a computer based procedure for assessing reader strategies that was based on verbal protocols that utilized latent semantic analysis ( lsa ) . students were given self explanation reading training ( sert ) , which teaches strategies that facilitate self explanation during reading , such as elaboration based on world knowledge and bridging between text sentences . during a computerized version of sert practice , students read texts and typed self explanations into a computer after each sentence . the use of sert strategies during this practice was assessed by determining the extent to which students used the information in the current sentence versus the prior text or world knowledge in their self explanations . this assessment was made on the basis of human judgments and lsa . both human judgments and lsa were remarkably similar and indicated that students who were not complying with sert tended to paraphrase the text sentences , whereas students who were compliant with sert tended to explain the sentences in terms of what they knew about the world and of information provided in the prior text context . the similarity between human judgments and lsa indicates that lsa will be useful in accounting for reading strategies in a web based version of sert"}
{"title": "Personality research on the Internet: a comparison of Web-based and traditional", "abstract": "instruments in take-home and in-class settings Students, faculty, and researchers have become increasingly comfortable with the Internet, and many of them are interested in using the Web to collect data. Few published studies have investigated the differences between Web-based data and data collected with more traditional methods. In order to investigate these potential differences, two important factors were crossed in this study: whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent's choosing. The Visions of Morality scale (Shelton and McAdams, 1990) was used, and the participants were assigned to one of four conditions: in-class Web survey, in-class paper-and-pencil survey; take-home Web survey, and take-home paper-and-pencil survey. No significant differences in scores were found for any condition; however, response rates were affected by the type of survey administered, with the take-home Web-based instrument having the lowest response rate. Therefore, researchers need to be aware that different modes of administration may affect subject attrition and may, therefore, confound investigations of other independent variables", "id": "407", "src": "personality research on the internet a comparison of web based and traditional . instruments in take home and in class settings students , faculty , and researchers have become increasingly comfortable with the internet , and many of them are interested in using the web to collect data . few published studies have investigated the differences between web based data and data collected with more traditional methods . in order to investigate these potential differences , two important factors were crossed in this study whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent ' s choosing . the visions of morality scale ( shelton and mcadams , <digit> ) was used , and the participants were assigned to one of four conditions in class web survey , in class paper and pencil survey take home web survey , and take home paper and pencil survey . no significant differences in scores were found for any condition however , response rates were affected by the type of survey administered , with the take home web based instrument having the lowest response rate . therefore , researchers need to be aware that different modes of administration may affect subject attrition and may , therefore , confound investigations of other independent variables"}
{"title": "Implications of document-level literacy skills for Web site design", "abstract": "The proliferation of World Wide Web (Web) sites and the low cost of publishing information on the Web have placed a tremendous amount of information at the fingertips of millions of people. Although most of this information is at least intended to be accurate, there is much that is rumor, innuendo, urban legend, and outright falsehood. This raises problems especially for students (of all ages) trying to do research or learn about some topic. Finding accurate, credible information requires document level literacy skills, such as integration, sourcing, corroboration, and search. This paper discusses these skills and offers a list of simple ways that designers of educational Web sites can help their visitors utilize these skills", "id": "408", "src": "implications of document level literacy skills for web site design . the proliferation of world wide web ( web ) sites and the low cost of publishing information on the web have placed a tremendous amount of information at the fingertips of millions of people . although most of this information is at least intended to be accurate , there is much that is rumor , innuendo , urban legend , and outright falsehood . this raises problems especially for students ( of all ages ) trying to do research or learn about some topic . finding accurate , credible information requires document level literacy skills , such as integration , sourcing , corroboration , and search . this paper discusses these skills and offers a list of simple ways that designers of educational web sites can help their visitors utilize these skills"}
{"title": "Fuzzy control of multivariable process by modified error decoupling", "abstract": "In this paper, a control concept for the squared (equal number of inputs and outputs) multivariable process systems is given. The proposed control system consists of two parts, single loop fuzzy controllers in each loop and a centralized decoupling unit. The fuzzy control system uses feedback control to minimize the error in the loop and the decoupler uses an adaptive technique to mitigate loop interactions. The decoupler predicts the interacting loop changes and modifies the input (error) of the loop controller. The controller was tested on the simulation model of \"single component vaporizer\" process", "id": "409", "src": "fuzzy control of multivariable process by modified error decoupling . in this paper , a control concept for the squared ( equal number of inputs and outputs ) multivariable process systems is given . the proposed control system consists of two parts , single loop fuzzy controllers in each loop and a centralized decoupling unit . the fuzzy control system uses feedback control to minimize the error in the loop and the decoupler uses an adaptive technique to mitigate loop interactions . the decoupler predicts the interacting loop changes and modifies the input ( error ) of the loop controller . the controller was tested on the simulation model of single component vaporizer process"}
{"title": "Improving computer security for authentication of users: influence of proactive", "abstract": "password restrictions Entering a user name-password combination is a widely used procedure for identification and authentication in computer systems. However, it is a notoriously weak method, in that the passwords adopted by many users are easy to crack. In an attempt to, improve security, proactive password checking may be used, in which passwords must meet several criteria to be more resistant to cracking. In two experiments, we examined the influence of proactive password restrictions on the time that it took to generate an acceptable password and to use it subsequently to log in. The required length was a minimum of five characters in experiment I and eight characters in experiment 2. In both experiments, one condition had only the length restriction, and the other had additional restrictions. The additional restrictions greatly increased the time it took to generate the password but had only a small effect on the time it took to use it subsequently to log in. For the five-character passwords, 75% were cracked when no other restrictions were imposed, and this was reduced to 33% with the additional restrictions. For the eight-character passwords, 17% were cracked with no other restrictions, and 12.5% with restrictions. The results indicate that increasing the minimum character length reduces crackability and increases security, regardless of whether additional restrictions are imposed", "id": "410", "src": "improving computer security for authentication of users influence of proactive . password restrictions entering a user name password combination is a widely used procedure for identification and authentication in computer systems . however , it is a notoriously weak method , in that the passwords adopted by many users are easy to crack . in an attempt to , improve security , proactive password checking may be used , in which passwords must meet several criteria to be more resistant to cracking . in two experiments , we examined the influence of proactive password restrictions on the time that it took to generate an acceptable password and to use it subsequently to log in . the required length was a minimum of five characters in experiment i and eight characters in experiment 2 . in both experiments , one condition had only the length restriction , and the other had additional restrictions . the additional restrictions greatly increased the time it took to generate the password but had only a small effect on the time it took to use it subsequently to log in . for the five character passwords , <digit> were cracked when no other restrictions were imposed , and this was reduced to <digit> with the additional restrictions . for the eight character passwords , <digit> were cracked with no other restrictions , and <digit> . 5 with restrictions . the results indicate that increasing the minimum character length reduces crackability and increases security , regardless of whether additional restrictions are imposed"}
{"title": "Multidimensional data visualization", "abstract": "Historically, data visualization has been limited primarily to two dimensions (e.g., histograms or scatter plots). Available software packages (e.g., Data Desk 6.1, MatLab 6.1, SAS-JMP 4.04, SPSS 10.0) are capable of producing three-dimensional scatter plots with (varying degrees of) user interactivity. We constructed our own data visualization application with the Visualization Toolkit (Schroeder et al., 1998) and Tcl/Tk to display multivariate data through the application of glyphs (Ware, 2000). A glyph is a visual object onto which many data parameters may be mapped, each with a different visual attribute (e.g., size or color). We used our multi-dimensional data viewer to explore data from several psycholinguistic experiments. The graphical interface provides flexibility when users dynamically explore the multidimensional image rendered from raw experimental data. We highlight advantages of multidimensional data visualization and consider some potential limitations", "id": "411", "src": "multidimensional data visualization . historically , data visualization has been limited primarily to two dimensions ( e . g . , histograms or scatter plots ) . available software packages ( e . g . , data desk 6 . 1 , matlab 6 . 1 , sas jmp 4 . <digit> , spss <digit> . 0 ) are capable of producing three dimensional scatter plots with ( varying degrees of ) user interactivity . we constructed our own data visualization application with the visualization toolkit ( schroeder et al . , <digit> ) and tcl tk to display multivariate data through the application of glyphs ( ware , <digit> ) . a glyph is a visual object onto which many data parameters may be mapped , each with a different visual attribute ( e . g . , size or color ) . we used our multi dimensional data viewer to explore data from several psycholinguistic experiments . the graphical interface provides flexibility when users dynamically explore the multidimensional image rendered from raw experimental data . we highlight advantages of multidimensional data visualization and consider some potential limitations"}
{"title": "Fitting mixed-effects models for repeated ordinal outcomes with the NLMIXED", "abstract": "procedure This paper presents an analysis of repeated ordinal outcomes arising from two psychological studies. The first case is a repeated measures analysis of variance; the second is a mixed-effects regression. in a longitudinal design. In both, the subject-specific variation is modeled by including random effects in the linear predictor (inside a link function) of a generalized linear model. The NLMIXED procedure in SAS is used to fit the mixed-effects models for the categorical response data. The presentation emphasizes the parallel between the model. specifications and the SAS statements. The purpose of this paper is to facilitate the use of mixed-effects models in the analysis of repeated ordinal outcomes", "id": "412", "src": "fitting mixed effects models for repeated ordinal outcomes with the nlmixed . procedure this paper presents an analysis of repeated ordinal outcomes arising from two psychological studies . the first case is a repeated measures analysis of variance the second is a mixed effects regression . in a longitudinal design . in both , the subject specific variation is modeled by including random effects in the linear predictor ( inside a link function ) of a generalized linear model . the nlmixed procedure in sas is used to fit the mixed effects models for the categorical response data . the presentation emphasizes the parallel between the model . specifications and the sas statements . the purpose of this paper is to facilitate the use of mixed effects models in the analysis of repeated ordinal outcomes"}
{"title": "Teaching psychology as a laboratory science in the age of the Internet", "abstract": "For over 30 years, psychologists have relied on computers to teach experimental psychology. With the advent of experiment generators, students can create well-designed experiments and can test sophisticated hypotheses from the start of their undergraduate training. Characteristics of new Net-based experiment generators are discussed and compared with traditional stand-alone generators. A call is made to formally evaluate the instructional effectiveness of the wide range of experiment generators now available. Specifically, software should be evaluated in terms of known learning outcomes, using appropriate control groups. The many inherent differences between any two software programs should be made clear. The teacher's instructional method should be fully described and held constant between comparisons. Finally, the often complex interaction between the teacher's instructional method and the pedagogical details of the software must be considered", "id": "413", "src": "teaching psychology as a laboratory science in the age of the internet . for over <digit> years , psychologists have relied on computers to teach experimental psychology . with the advent of experiment generators , students can create well designed experiments and can test sophisticated hypotheses from the start of their undergraduate training . characteristics of new net based experiment generators are discussed and compared with traditional stand alone generators . a call is made to formally evaluate the instructional effectiveness of the wide range of experiment generators now available . specifically , software should be evaluated in terms of known learning outcomes , using appropriate control groups . the many inherent differences between any two software programs should be made clear . the teacher ' s instructional method should be fully described and held constant between comparisons . finally , the often complex interaction between the teacher ' s instructional method and the pedagogical details of the software must be considered"}
{"title": "Capturing niche markets with copper", "abstract": "For \"last-mile access\" in niche applications, twisted copper pair may be the cable of best option to gain access and deliver desired services. The article discusses how operators can use network edge devices to serve new customers. Niche market segments represent a significant opportunity for cable TV delivery of television and high-speed Internet signals. But the existing telecommunications infrastructure in those developments frequently presents unique challenges for the service provider to overcome", "id": "414", "src": "capturing niche markets with copper . for last mile access in niche applications , twisted copper pair may be the cable of best option to gain access and deliver desired services . the article discusses how operators can use network edge devices to serve new customers . niche market segments represent a significant opportunity for cable tv delivery of television and high speed internet signals . but the existing telecommunications infrastructure in those developments frequently presents unique challenges for the service provider to overcome"}
{"title": "Fresh voices, big ideas [IBM internship program]", "abstract": "IBM is matching up computer-science and MBA students with its business managers in an 11-week summer internship program and challenging them to develop innovative technology ideas", "id": "415", "src": "fresh voices , big ideas ibm internship program . ibm is matching up computer science and mba students with its business managers in an <digit> week summer internship program and challenging them to develop innovative technology ideas"}
{"title": "Down up [IT projects]", "abstract": "Despite the second quarter's gloomy GDP report, savvy CIOs are forging ahead with big IT projects that will position their companies to succeed when the economy soars again", "id": "416", "src": "down up it projects . despite the second quarter ' s gloomy gdp report , savvy cios are forging ahead with big it projects that will position their companies to succeed when the economy soars again"}
{"title": "An automated parallel image registration technique based on the correlation of", "abstract": "wavelet features With the increasing importance of multiple multiplatform remote sensing missions, fast and automatic integration of digital data from disparate sources has become critical to the success of these endeavors. Our work utilizes maxima of wavelet coefficients to form the basic features of a correlation-based automatic registration algorithm. Our wavelet-based registration algorithm is tested successfully with data from the National Oceanic and Atmospheric Administration (NOAA) Advanced Very High Resolution Radiometer (AVHRR) and the Landsat Thematic Mapper (TM), which differ by translation and/or rotation. By the choice of high-frequency wavelet features, this method is similar to an edge-based correlation method, but by exploiting the multiresolution nature of a wavelet decomposition, our method achieves higher computational speeds for comparable accuracies. This algorithm has been implemented on a single-instruction multiple-data (SIMD) massively parallel computer, the MasPar MP-2, as well as on the CrayT3D, the Cray T3E, and a Beowulf cluster of Pentium workstations", "id": "417", "src": "an automated parallel image registration technique based on the correlation of . wavelet features with the increasing importance of multiple multiplatform remote sensing missions , fast and automatic integration of digital data from disparate sources has become critical to the success of these endeavors . our work utilizes maxima of wavelet coefficients to form the basic features of a correlation based automatic registration algorithm . our wavelet based registration algorithm is tested successfully with data from the national oceanic and atmospheric administration ( noaa ) advanced very high resolution radiometer ( avhrr ) and the landsat thematic mapper ( tm ) , which differ by translation and or rotation . by the choice of high frequency wavelet features , this method is similar to an edge based correlation method , but by exploiting the multiresolution nature of a wavelet decomposition , our method achieves higher computational speeds for comparable accuracies . this algorithm has been implemented on a single instruction multiple data ( simd ) massively parallel computer , the maspar mp 2 , as well as on the crayt3d , the cray t3e , and a beowulf cluster of pentium workstations"}
{"title": "Design of PID-type controllers using multiobjective genetic algorithms", "abstract": "The design of a PID controller is a multiobjective problem. A plant and a set of specifications to be satisfied are given. The designer has to adjust the parameters of the PID controller such that the feedback interconnection of the plant and the controller satisfies the specifications. These specifications are usually competitive and any acceptable solution requires a tradeoff among them. An approach for adjusting the parameters of a PID controller based on multiobjective optimization and genetic algorithms is presented in this paper. The MRCD (multiobjective robust control design) genetic algorithm has been employed. The approach can be easily generalized to design multivariable coupled and decentralized PID loops and has been successfully validated for a large number of experimental cases", "id": "418", "src": "design of pid type controllers using multiobjective genetic algorithms . the design of a pid controller is a multiobjective problem . a plant and a set of specifications to be satisfied are given . the designer has to adjust the parameters of the pid controller such that the feedback interconnection of the plant and the controller satisfies the specifications . these specifications are usually competitive and any acceptable solution requires a tradeoff among them . an approach for adjusting the parameters of a pid controller based on multiobjective optimization and genetic algorithms is presented in this paper . the mrcd ( multiobjective robust control design ) genetic algorithm has been employed . the approach can be easily generalized to design multivariable coupled and decentralized pid loops and has been successfully validated for a large number of experimental cases"}
{"title": "Temelin casts its shadow [nuclear power plant]", "abstract": "Reservations about Temelin nuclear plant in the Czech Republic are political rather than technical. This paper discusses the problems of turbogenerator vibrations and how they were diagnosed. The paper also discusses some of the other problems of commissioning the power plant. The simulator used for training new staff is also mentioned", "id": "419", "src": "temelin casts its shadow nuclear power plant . reservations about temelin nuclear plant in the czech republic are political rather than technical . this paper discusses the problems of turbogenerator vibrations and how they were diagnosed . the paper also discusses some of the other problems of commissioning the power plant . the simulator used for training new staff is also mentioned"}
{"title": "How should team captains order golfers on the final day of the Ryder Cup", "abstract": "matches? I used game theory to examine how team captains should select their slates for the final day of the Ryder Cup matches. Under the assumption that golfers have different abilities and are not influenced by pressure or momentum, I found that drawing names from a hat will do no worse than any other strategy", "id": "420", "src": "how should team captains order golfers on the final day of the ryder cup . matches i used game theory to examine how team captains should select their slates for the final day of the ryder cup matches . under the assumption that golfers have different abilities and are not influenced by pressure or momentum , i found that drawing names from a hat will do no worse than any other strategy"}
{"title": "Mount Sinai Hospital uses integer programming to allocate operating room time", "abstract": "An integer-programming model and a post-solution heuristic allocates operating room time to the five surgical divisions at Toronto's Mount Sinai Hospital. The hospital has used this approach for several years and credits it with both administrative savings and the ability to produce quickly an equitable master surgical schedule", "id": "421", "src": "mount sinai hospital uses integer programming to allocate operating room time . an integer programming model and a post solution heuristic allocates operating room time to the five surgical divisions at toronto ' s mount sinai hospital . the hospital has used this approach for several years and credits it with both administrative savings and the ability to produce quickly an equitable master surgical schedule"}
{"title": "Using the Small Business Innovation Research Program to turn your ideas into", "abstract": "products The US Government's Small Business Innovation Research Program helps small businesses transform new ideas into commercial products. The program provides an ideal means for businesses and universities to obtaining funding for cooperative projects. Rules and information for the program are readily available, and I will give a few helpful hints to provide guidance", "id": "422", "src": "using the small business innovation research program to turn your ideas into . products the us government ' s small business innovation research program helps small businesses transform new ideas into commercial products . the program provides an ideal means for businesses and universities to obtaining funding for cooperative projects . rules and information for the program are readily available , and i will give a few helpful hints to provide guidance"}
{"title": "Student consulting projects benefit faculty and industry", "abstract": "Student consulting projects require students to apply OR/MS tools to obtain insight into the activities of firms in the community. These projects benefit faculty by providing clear feedback on the real capabilities of students, a broad connection to local industry, and material for case studies and research. They benefit companies by stimulating new thinking regarding their activities and delivering results they can use. Projects provide insights into the end-user modeling mode of OR/MS practice. Projects support continuous improvement as the lessons gained from a crop of projects enable better teaching during the next course offering, which in turn leads to better projects and further insights into teaching", "id": "423", "src": "student consulting projects benefit faculty and industry . student consulting projects require students to apply or ms tools to obtain insight into the activities of firms in the community . these projects benefit faculty by providing clear feedback on the real capabilities of students , a broad connection to local industry , and material for case studies and research . they benefit companies by stimulating new thinking regarding their activities and delivering results they can use . projects provide insights into the end user modeling mode of or ms practice . projects support continuous improvement as the lessons gained from a crop of projects enable better teaching during the next course offering , which in turn leads to better projects and further insights into teaching"}
{"title": "In search of strategic operations research/management science", "abstract": "We define strategic OR/MS as \"OR/MS work that leads to a sustainable competitive advantage.\" We found evidence of strategic OR/MS in the literature of strategic information systems (SIS) and OR/MS. We examined 30 early examples of SIS, many of which contained OR/MS work. Many of the most successful had high OR/MS content, while the least successful contained none. The inclusion of OR/MS work may be a key to sustaining an advantage from information technology. We also examined the Edelman Prize finalist articles published between 1990 and 1999. We found that 13 of the 42 private sector applications meet our definition of strategic OR/MS", "id": "424", "src": "in search of strategic operations research management science . we define strategic or ms as or ms work that leads to a sustainable competitive advantage . we found evidence of strategic or ms in the literature of strategic information systems ( sis ) and or ms . we examined <digit> early examples of sis , many of which contained or ms work . many of the most successful had high or ms content , while the least successful contained none . the inclusion of or ms work may be a key to sustaining an advantage from information technology . we also examined the edelman prize finalist articles published between <digit> and <digit> . we found that <digit> of the <digit> private sector applications meet our definition of strategic or ms"}
{"title": "Baseball, optimization, and the World Wide Web", "abstract": "The competition for baseball play-off spots-the fabled pennant race-is one of the most closely watched American sports traditions. While play-off race statistics, such as games back and magic number, are informative, they are overly conservative and do not account for the remaining schedule of games. Using optimization techniques, one can model schedule effects explicitly and determine precisely when a team has secured a play-off spot or has been eliminated from contention. The RIOT Baseball Play-off Races Web site developed at the University of California, Berkeley, provides automatic updates of new, optimization-based play-off race statistics each day of the major league baseball season. In developing the site, we found that we could determine the first-place elimination status of all teams in a division using a single linear-programming formulation, since a minimum win threshold for teams finishing in first place applies to all teams in a division. We identified a similar (but weaker) result for the problem of play-off elimination with wildcard teams", "id": "425", "src": "baseball , optimization , and the world wide web . the competition for baseball play off spots the fabled pennant race is one of the most closely watched american sports traditions . while play off race statistics , such as games back and magic number , are informative , they are overly conservative and do not account for the remaining schedule of games . using optimization techniques , one can model schedule effects explicitly and determine precisely when a team has secured a play off spot or has been eliminated from contention . the riot baseball play off races web site developed at the university of california , berkeley , provides automatic updates of new , optimization based play off race statistics each day of the major league baseball season . in developing the site , we found that we could determine the first place elimination status of all teams in a division using a single linear programming formulation , since a minimum win threshold for teams finishing in first place applies to all teams in a division . we identified a similar ( but weaker ) result for the problem of play off elimination with wildcard teams"}
{"title": "From revenue management concepts to software systems", "abstract": "In 1999, after developing and installing over 170 revenue management (RM) systems for more than 70 airlines, PROS Revenue Management, Inc. had the opportunity to develop RM systems for three companies in nonairline industries. PROS research and design department designed the opportunity analysis study (OAS), a mix of OR/MS, consulting, and software development practices to determine the applicability of RM in new business situations. PROS executed OASs with the three companies. In all three cases, the OAS supported the value of RM and led to contracts for implementation of RM systems", "id": "426", "src": "from revenue management concepts to software systems . in <digit> , after developing and installing over <digit> revenue management ( rm ) systems for more than <digit> airlines , pros revenue management , inc . had the opportunity to develop rm systems for three companies in nonairline industries . pros research and design department designed the opportunity analysis study ( oas ) , a mix of or ms , consulting , and software development practices to determine the applicability of rm in new business situations . pros executed oass with the three companies . in all three cases , the oas supported the value of rm and led to contracts for implementation of rm systems"}
{"title": "Lower bounds on the information rate of secret sharing schemes with homogeneous", "abstract": "access structure We present some new lower bounds on the optimal information rate and on the optimal average information rate of secret sharing schemes with homogeneous access structure. These bounds are found by using some covering constructions and a new parameter, the k-degree of a participant, that is introduced in this paper. Our bounds improve the previous ones in almost all cases", "id": "427", "src": "lower bounds on the information rate of secret sharing schemes with homogeneous . access structure we present some new lower bounds on the optimal information rate and on the optimal average information rate of secret sharing schemes with homogeneous access structure . these bounds are found by using some covering constructions and a new parameter , the k degree of a participant , that is introduced in this paper . our bounds improve the previous ones in almost all cases"}
{"title": "A self-adjusting quality of service control scheme", "abstract": "We propose and analyze a self-adjusting Quality of Service (QoS) control scheme with the goal of optimizing the system reward as a result of servicing different priority clients with varying workload, QoS and reward/penalty requirements. Our scheme is based on resource partitioning and designated \"degrade QoS areas\" such that system resources are partitioned into priority areas each of which is reserved specifically to serve only clients in a corresponding class with no QoS degradation, plus one \"degraded QoS area\" into which all clients can be admitted with QoS adjustment being applied only to the lowest priority clients. We show that the best partition is dictated by the workload and the reward/penalty characteristics of clients in difference priority classes. The analysis results can be used by a QoS manager to optimize the system total reward dynamically in response to changing workloads at run time. We demonstrate the validity of our scheme by means of simulation and comparing the proposed QoS self-adjusting scheme with those that do not use resource partitioning or designated degraded QoS areas", "id": "428", "src": "a self adjusting quality of service control scheme . we propose and analyze a self adjusting quality of service ( qos ) control scheme with the goal of optimizing the system reward as a result of servicing different priority clients with varying workload , qos and reward penalty requirements . our scheme is based on resource partitioning and designated degrade qos areas such that system resources are partitioned into priority areas each of which is reserved specifically to serve only clients in a corresponding class with no qos degradation , plus one degraded qos area into which all clients can be admitted with qos adjustment being applied only to the lowest priority clients . we show that the best partition is dictated by the workload and the reward penalty characteristics of clients in difference priority classes . the analysis results can be used by a qos manager to optimize the system total reward dynamically in response to changing workloads at run time . we demonstrate the validity of our scheme by means of simulation and comparing the proposed qos self adjusting scheme with those that do not use resource partitioning or designated degraded qos areas"}
{"title": "Fusion of qualitative bond graph and genetic algorithms: A fault diagnosis", "abstract": "application In this paper, the problem of fault diagnosis via integration of genetic algorithms (GA's) and qualitative bond graphs (QBG's) is addressed. We suggest that GA's can be used to search for possible fault components among a system of qualitative equations. The QBG is adopted as the modeling scheme to generate a set of qualitative equations. The qualitative bond graph provides a unified approach for modeling engineering systems, in particular, mechatronic systems. In order to demonstrate the performance of the proposed algorithm, we have tested the proposed algorithm on an in-house designed and built floating disc experimental setup. Results from fault diagnosis in the floating disc system are presented and discussed. Additional measurements will be required to localize the fault when more than one fault candidate is inferred. Fault diagnosis is activated by a fault detection mechanism when a discrepancy between measured abnormal behavior and predicted system behavior is observed. The fault detection mechanism is not presented here", "id": "429", "src": "fusion of qualitative bond graph and genetic algorithms a fault diagnosis . application in this paper , the problem of fault diagnosis via integration of genetic algorithms ( ga ' s ) and qualitative bond graphs ( qbg ' s ) is addressed . we suggest that ga ' s can be used to search for possible fault components among a system of qualitative equations . the qbg is adopted as the modeling scheme to generate a set of qualitative equations . the qualitative bond graph provides a unified approach for modeling engineering systems , in particular , mechatronic systems . in order to demonstrate the performance of the proposed algorithm , we have tested the proposed algorithm on an in house designed and built floating disc experimental setup . results from fault diagnosis in the floating disc system are presented and discussed . additional measurements will be required to localize the fault when more than one fault candidate is inferred . fault diagnosis is activated by a fault detection mechanism when a discrepancy between measured abnormal behavior and predicted system behavior is observed . the fault detection mechanism is not presented here"}
{"title": "There is no optimal routing policy for the torus", "abstract": "A routing policy is the method used to select a specific output channel for a message from among a number of acceptable output channels. An optimal routing policy is a policy that maximizes the probability of a message reaching its destination without delays. Optimal routing policies have been proposed for several regular networks, including the mesh and the hypercube. An open problem in interconnection network research has been the identification of an optimal routing policy for the torus. In this paper, we show that there is no optimal routing policy for the torus. Our result is demonstrated by presenting a detailed example in which the best choice of output channel is dependent on the probability of each channel being available. This result settles, in the negative, a conjecture by J. Wu (1996) concerning an optimal routing policy for the torus", "id": "430", "src": "there is no optimal routing policy for the torus . a routing policy is the method used to select a specific output channel for a message from among a number of acceptable output channels . an optimal routing policy is a policy that maximizes the probability of a message reaching its destination without delays . optimal routing policies have been proposed for several regular networks , including the mesh and the hypercube . an open problem in interconnection network research has been the identification of an optimal routing policy for the torus . in this paper , we show that there is no optimal routing policy for the torus . our result is demonstrated by presenting a detailed example in which the best choice of output channel is dependent on the probability of each channel being available . this result settles , in the negative , a conjecture by j . wu ( <digit> ) concerning an optimal routing policy for the torus"}
{"title": "Optimal online algorithm for scheduling on two identical machines with machine", "abstract": "availability constraints This paper considers the online scheduling on two identical machines with machine availability constraints for minimizing makespan. We assume that machine M/sub j/ is unavailable during period from s/sub j/ to t/sub j/ (0 <or= s/sub j/ < t/sub j/), j = 1, 2, and the unavailable periods of two machines do not overlap. We show that the competitive ratio of list scheduling is 3. We further give an optimal algorithm with a competitive ratio 5/2", "id": "431", "src": "optimal online algorithm for scheduling on two identical machines with machine . availability constraints this paper considers the online scheduling on two identical machines with machine availability constraints for minimizing makespan . we assume that machine m sub j is unavailable during period from s sub j to t sub j ( 0 < or s sub j < t sub j ) , j 1 , 2 , and the unavailable periods of two machines do not overlap . we show that the competitive ratio of list scheduling is 3 . we further give an optimal algorithm with a competitive ratio 5 2"}
{"title": "An efficient retrieval selection algorithm for video servers with random", "abstract": "duplicated assignment storage technique Random duplicated assignment (RDA) is an approach in which video data is stored by assigning a number of copies of each data block to different, randomly chosen disks. It has been shown that this approach results in smaller response times and lower disk and RAM costs compared to the well-known disk stripping techniques. Based on this storage approach, one has to determine, for each given batch of data blocks, from which disk each of the data blocks is to be retrieved. This is to be done in such a way that the maximum load of the disks is minimized. The problem is called the retrieval selection problem (RSP). In this paper, we propose a new efficient algorithm for RSP. This algorithm is based on the breadth-first search approach and is able to guarantee optimal solutions for RSP in O(n/sup 2/+mn), where m and n correspond to the number of data blocks and the number of disks, respectively. We show that our proposed algorithm has a lower time complexity than an existing algorithm, called the MFS algorithm", "id": "432", "src": "an efficient retrieval selection algorithm for video servers with random . duplicated assignment storage technique random duplicated assignment ( rda ) is an approach in which video data is stored by assigning a number of copies of each data block to different , randomly chosen disks . it has been shown that this approach results in smaller response times and lower disk and ram costs compared to the well known disk stripping techniques . based on this storage approach , one has to determine , for each given batch of data blocks , from which disk each of the data blocks is to be retrieved . this is to be done in such a way that the maximum load of the disks is minimized . the problem is called the retrieval selection problem ( rsp ) . in this paper , we propose a new efficient algorithm for rsp . this algorithm is based on the breadth first search approach and is able to guarantee optimal solutions for rsp in o ( n sup 2 +mn ) , where m and n correspond to the number of data blocks and the number of disks , respectively . we show that our proposed algorithm has a lower time complexity than an existing algorithm , called the mfs algorithm"}
{"title": "Edit distance of run-length encoded strings", "abstract": "Let X and Y be two run-length encoded strings, of encoded lengths k and l, respectively. We present a simple O(|X|l+|Y|k) time algorithm that computes their edit distance", "id": "433", "src": "edit distance of run length encoded strings . let x and y be two run length encoded strings , of encoded lengths k and l , respectively . we present a simple o ( x l+ y k ) time algorithm that computes their edit distance"}
{"title": "Fault-tolerant Hamiltonian laceability of hypercubes", "abstract": "It is known that every hypercube Q/sub n/ is a bipartite graph. Assume that n>or=2 and F is a subset of edges with |F|<or=n-2. We prove that there exists a Hamiltonian path in Q/sub n/-F between any two vertices of different partite sets. Moreover, there exists a path of length 2/sup n/-2 between any two vertices of the same partite set. Assume that n>or=3 and F is a subset of edges with |F|<or=n-3. We prove that there exists a Hamiltonian path in Q/sub n/-{v}-F between any two vertices in the partite set without v. Furthermore, all bounds are tight", "id": "434", "src": "fault tolerant hamiltonian laceability of hypercubes . it is known that every hypercube q sub n is a bipartite graph . assume that n > or 2 and f is a subset of edges with f < or n 2 . we prove that there exists a hamiltonian path in q sub n f between any two vertices of different partite sets . moreover , there exists a path of length 2 sup n 2 between any two vertices of the same partite set . assume that n > or 3 and f is a subset of edges with f < or n 3 . we prove that there exists a hamiltonian path in q sub n v f between any two vertices in the partite set without v . furthermore , all bounds are tight"}
{"title": "An identity-based society oriented signature scheme with anonymous signers", "abstract": "In this paper, we propose a new society oriented scheme, based on the Guillou-Quisquater (1989) signature scheme. The scheme is identity-based and the signatures are verified with respect to only one identity. That is, the verifier does not have to know the identity of the co-signers, but just that of the organization they represent", "id": "435", "src": "an identity based society oriented signature scheme with anonymous signers . in this paper , we propose a new society oriented scheme , based on the guillou quisquater ( <digit> ) signature scheme . the scheme is identity based and the signatures are verified with respect to only one identity . that is , the verifier does not have to know the identity of the co signers , but just that of the organization they represent"}
{"title": "Operational phase-space probability distribution in quantum communication", "abstract": "theory Operational phase-space probability distributions are useful tools for describing quantum mechanical systems, including quantum communication and quantum information processing systems. It is shown that quantum communication channels with Gaussian noise and quantum teleportation of continuous variables are described by operational phase-space probability distributions. The relation of operational phase-space probability distribution to the extended phase-space formalism proposed by Chountasis and Vourdas (1998) is discussed", "id": "436", "src": "operational phase space probability distribution in quantum communication . theory operational phase space probability distributions are useful tools for describing quantum mechanical systems , including quantum communication and quantum information processing systems . it is shown that quantum communication channels with gaussian noise and quantum teleportation of continuous variables are described by operational phase space probability distributions . the relation of operational phase space probability distribution to the extended phase space formalism proposed by chountasis and vourdas ( <digit> ) is discussed"}
{"title": "Embedding of level-continuous fuzzy sets on Banach spaces", "abstract": "In this paper we present an extension of the Minkowski embedding theorem, showing the existence of an isometric embedding between the classF/sub c/(X) of compact-convex and level-continuous fuzzy sets on a real separable Banach space X and C([0, 1] * B(X*)), the Banach space of real continuous functions defined on the cartesian product between [0, 1] and the unit ball B(X*) in the dual space X*. Also, by using this embedding, we give some applications to the characterization of relatively compact subsets of F/sub c/(X). In particular, an Ascoli-Arzela type theorem is proved and applied to solving the Cauchy problem x(t) = f(t, x(t)), x(t/sub 0/) = x/sub 0/ on F/sub c/(X)", "id": "437", "src": "embedding of level continuous fuzzy sets on banach spaces . in this paper we present an extension of the minkowski embedding theorem , showing the existence of an isometric embedding between the classf sub c ( x ) of compact convex and level continuous fuzzy sets on a real separable banach space x and c ( 0 , 1 * b ( x* ) ) , the banach space of real continuous functions defined on the cartesian product between 0 , 1 and the unit ball b ( x* ) in the dual space x* . also , by using this embedding , we give some applications to the characterization of relatively compact subsets of f sub c ( x ) . in particular , an ascoli arzela type theorem is proved and applied to solving the cauchy problem x ( t ) f ( t , x ( t ) ) , x ( t sub 0 ) x sub 0 on f sub c ( x )"}
{"title": "Correlation of intuitionistic fuzzy sets by centroid method", "abstract": "In this paper, we propose a method to calculate the correlation coefficient of intuitionistic fuzzy sets by means of \"centroid\". This value obtained from our formula tell us not only the strength of relationship between the intuitionistic fuzzy sets, but also whether the intuitionistic fuzzy sets are positively or negatively related. This approach looks better than previous methods which only evaluate the strength of the relation. Furthermore, we extend the \"centroid\" method to interval-valued intuitionistic fuzzy sets. The value of the correlation coefficient between interval-valued intuitionistic fuzzy sets lies in the interval [-1, 1], as computed from our formula", "id": "438", "src": "correlation of intuitionistic fuzzy sets by centroid method . in this paper , we propose a method to calculate the correlation coefficient of intuitionistic fuzzy sets by means of centroid . this value obtained from our formula tell us not only the strength of relationship between the intuitionistic fuzzy sets , but also whether the intuitionistic fuzzy sets are positively or negatively related . this approach looks better than previous methods which only evaluate the strength of the relation . furthermore , we extend the centroid method to interval valued intuitionistic fuzzy sets . the value of the correlation coefficient between interval valued intuitionistic fuzzy sets lies in the interval 1 , 1 , as computed from our formula"}
{"title": "Neighborhood operator systems and approximations", "abstract": "This paper presents a framework for the study of generalizing the standard notion of equivalence relation in rough set approximation space with various categories of k-step neighborhood systems. Based on a binary relation on a finite universe, six families of binary relations are obtained, and the corresponding six classes of k-step neighborhood systems are derived. Extensions of Pawlak's (1982) rough set approximation operators based on such neighborhood systems are proposed. Properties of neighborhood operator systems and rough set approximation operators are investigated, and their connections are examined", "id": "439", "src": "neighborhood operator systems and approximations . this paper presents a framework for the study of generalizing the standard notion of equivalence relation in rough set approximation space with various categories of k step neighborhood systems . based on a binary relation on a finite universe , six families of binary relations are obtained , and the corresponding six classes of k step neighborhood systems are derived . extensions of pawlak ' s ( <digit> ) rough set approximation operators based on such neighborhood systems are proposed . properties of neighborhood operator systems and rough set approximation operators are investigated , and their connections are examined"}
{"title": "Model predictive control helps to regulate slow processes-robust barrel", "abstract": "temperature control Slow temperature control is a challenging control problem. The problem becomes even more challenging when multiple zones are involved, such as in barrel temperature control for extruders. Often, strict closed-loop performance requirements (such as fast startup with no overshoot and maintaining tight temperature control during production) are given for such applications. When characteristics of the system are examined, it becomes clear that a commonly used proportional plus integral plus derivative (PID) controller cannot meet such performance specifications for this kind of system. The system either will overshoot or not maintain the temperature within the specified range during the production run. In order to achieve the required performance, a control strategy that utilizes techniques such as model predictive control, autotuning, and multiple parameter PID is formulated. This control strategy proves to be very effective in achieving the desired specifications, and is very robust", "id": "440", "src": "model predictive control helps to regulate slow processes robust barrel . temperature control slow temperature control is a challenging control problem . the problem becomes even more challenging when multiple zones are involved , such as in barrel temperature control for extruders . often , strict closed loop performance requirements ( such as fast startup with no overshoot and maintaining tight temperature control during production ) are given for such applications . when characteristics of the system are examined , it becomes clear that a commonly used proportional plus integral plus derivative ( pid ) controller cannot meet such performance specifications for this kind of system . the system either will overshoot or not maintain the temperature within the specified range during the production run . in order to achieve the required performance , a control strategy that utilizes techniques such as model predictive control , autotuning , and multiple parameter pid is formulated . this control strategy proves to be very effective in achieving the desired specifications , and is very robust"}
{"title": "Numerical representation of binary relations with a multiplicative error", "abstract": "function This paper studies the case of the representation of a binary relation via a numerical function with threshold (error) depending on both compared alternatives. The error is considered to be multiplicative, its value being either directly or inversely proportional to the values of the numerical function. For the first case, it is proved that a binary relation is a semiorder. Moreover, any semiorder can be represented in this form. In the second case, the corresponding binary relation is an interval order", "id": "441", "src": "numerical representation of binary relations with a multiplicative error . function this paper studies the case of the representation of a binary relation via a numerical function with threshold ( error ) depending on both compared alternatives . the error is considered to be multiplicative , its value being either directly or inversely proportional to the values of the numerical function . for the first case , it is proved that a binary relation is a semiorder . moreover , any semiorder can be represented in this form . in the second case , the corresponding binary relation is an interval order"}
{"title": "A pretopological approach for structural analysis", "abstract": "The aim of this paper is to present a methodological approach for problems encountered in structural analysis. This approach is based upon the pretopological concepts of pseudoclosure and minimal closed subsets. The advantage of this approach is that it provides a framework which is general enough to model and formulate different types of connections that exist between the elements of a population. In addition, it has enabled us to develop a new structural analysis algorithm. An explanation of the definitions and properties of the pretopological concepts applied in this work is first shown and illustrated in sample settings. The structural analysis algorithm is then described and the results obtained in an economic study of the impact of geographic proximity on scientific collaborations are presented", "id": "442", "src": "a pretopological approach for structural analysis . the aim of this paper is to present a methodological approach for problems encountered in structural analysis . this approach is based upon the pretopological concepts of pseudoclosure and minimal closed subsets . the advantage of this approach is that it provides a framework which is general enough to model and formulate different types of connections that exist between the elements of a population . in addition , it has enabled us to develop a new structural analysis algorithm . an explanation of the definitions and properties of the pretopological concepts applied in this work is first shown and illustrated in sample settings . the structural analysis algorithm is then described and the results obtained in an economic study of the impact of geographic proximity on scientific collaborations are presented"}
{"title": "On batch-constructing B/sup +/-trees: algorithm and its performance evaluation", "abstract": "Efficient construction of indexes is very important in bulk-loading a database or adding a new index to an existing database since both of them should handle an enormous volume of data. In this paper, we propose an algorithm for batch-constructing the B/sup +/-tree, the most widely used index structure in database systems. The main characteristic of our algorithm is to simultaneously process all the key values to be placed on each B+-tree page when accessing the page. This avoids the overhead due to accessing the same page multiple times, which results from applying the B+-tree insertion algorithm repeatedly. For performance evaluation, we have analyzed our algorithm in terms of the number of disk accesses. The results show that the number of disk accesses excluding those in the relocation process is identical to the number of pages belonging to the B/sup +/-tree. Considering that the relocation process is an unavoidable preprocessing step for batch-constructing of B/sup +/-trees, our algorithm requires just one disk access per B+-tree page, and therefore turns out to be optimal. We also present the performance tendency in relation with different parameter values via simulation. Finally, we show the performance enhancement effect of our algorithm, compared with the one using repeated insertions through experiments", "id": "443", "src": "on batch constructing b sup + trees algorithm and its performance evaluation . efficient construction of indexes is very important in bulk loading a database or adding a new index to an existing database since both of them should handle an enormous volume of data . in this paper , we propose an algorithm for batch constructing the b sup + tree , the most widely used index structure in database systems . the main characteristic of our algorithm is to simultaneously process all the key values to be placed on each b+ tree page when accessing the page . this avoids the overhead due to accessing the same page multiple times , which results from applying the b+ tree insertion algorithm repeatedly . for performance evaluation , we have analyzed our algorithm in terms of the number of disk accesses . the results show that the number of disk accesses excluding those in the relocation process is identical to the number of pages belonging to the b sup + tree . considering that the relocation process is an unavoidable preprocessing step for batch constructing of b sup + trees , our algorithm requires just one disk access per b+ tree page , and therefore turns out to be optimal . we also present the performance tendency in relation with different parameter values via simulation . finally , we show the performance enhancement effect of our algorithm , compared with the one using repeated insertions through experiments"}
{"title": "Synthetic simultaneity - natural and artificial", "abstract": "In control loops, each element introduces time delays. If those time delays are larger than the critical times for control of the system, a problem exists. I show a simple approach to mitigating this problem by basing the controller's decisions not on the observations themselves but on our projections as to what the observations will be at the time our controls reach the controlled system. Finally, I argue that synthetic simultaneity explains Libet's (1993) results better than Libet's explanation", "id": "444", "src": "synthetic simultaneity natural and artificial . in control loops , each element introduces time delays . if those time delays are larger than the critical times for control of the system , a problem exists . i show a simple approach to mitigating this problem by basing the controller ' s decisions not on the observations themselves but on our projections as to what the observations will be at the time our controls reach the controlled system . finally , i argue that synthetic simultaneity explains libet ' s ( <digit> ) results better than libet ' s explanation"}
{"title": "MACLP: multi agent constraint logic programming", "abstract": "Multi agent systems (MAS) have become the key technology for decomposing complex problems in order to solve them more efficiently, or for problems distributed in nature. However, many industrial applications, besides their distributed nature, also involve a large number of parameters and constraints, i.e. they are combinatorial. Solving such particularly hard problems efficiently requires programming tools that combine MAS technology with a programming schema that facilitates the modeling and solution of constraints. This paper presents MACLP (multi agent constraint logic programming), a logic programming platform for building, in a declarative way, multi agent systems with constraint-solving capabilities. MACLP extends CSPCONS, a logic programming system that permits distributed program execution through communicating sequential Prolog processes with constraints, by providing all the necessary facilities for communication between agents. These facilities abstract from the programmer all the low-level details of the communication and allow him to focus on the development of the agent itself", "id": "445", "src": "maclp multi agent constraint logic programming . multi agent systems ( mas ) have become the key technology for decomposing complex problems in order to solve them more efficiently , or for problems distributed in nature . however , many industrial applications , besides their distributed nature , also involve a large number of parameters and constraints , i . e . they are combinatorial . solving such particularly hard problems efficiently requires programming tools that combine mas technology with a programming schema that facilitates the modeling and solution of constraints . this paper presents maclp ( multi agent constraint logic programming ) , a logic programming platform for building , in a declarative way , multi agent systems with constraint solving capabilities . maclp extends cspcons , a logic programming system that permits distributed program execution through communicating sequential prolog processes with constraints , by providing all the necessary facilities for communication between agents . these facilities abstract from the programmer all the low level details of the communication and allow him to focus on the development of the agent itself"}
{"title": "Self-organizing feature maps predicting sea levels", "abstract": "In this paper, a new method for predicting sea levels employing self-organizing feature maps is introduced. For that purpose the maps are transformed from an unsupervised learning procedure to a supervised one. Two concepts, originally developed to solve the problems of convergence of other network types, are proposed to be applied to Kohonen networks: a functional relationship between the number of neurons and the number of learning examples and a criterion to break off learning. The latter one can be shown to be conform with the process of self-organization by using U-matrices for visualization of the learning procedure. The predictions made using these neural models are compared for accuracy with observations and with the prognoses prepared using six models: two hydrodynamic models, a statistical model, a nearest neighbor model, the persistence model, and the verbal forecasts that are broadcast and kept on record by the Sea Level Forecast Service of the Federal Maritime and Hydrography Agency (BSH) in Hamburg. Before training the maps, the meteorological and oceanographic situation has to be condensed as well as possible, and the weight and learning vectors have to be made as small as possible. The self-organizing feature maps predict sea levels better than all six models of comparison", "id": "446", "src": "self organizing feature maps predicting sea levels . in this paper , a new method for predicting sea levels employing self organizing feature maps is introduced . for that purpose the maps are transformed from an unsupervised learning procedure to a supervised one . two concepts , originally developed to solve the problems of convergence of other network types , are proposed to be applied to kohonen networks a functional relationship between the number of neurons and the number of learning examples and a criterion to break off learning . the latter one can be shown to be conform with the process of self organization by using u matrices for visualization of the learning procedure . the predictions made using these neural models are compared for accuracy with observations and with the prognoses prepared using six models two hydrodynamic models , a statistical model , a nearest neighbor model , the persistence model , and the verbal forecasts that are broadcast and kept on record by the sea level forecast service of the federal maritime and hydrography agency ( bsh ) in hamburg . before training the maps , the meteorological and oceanographic situation has to be condensed as well as possible , and the weight and learning vectors have to be made as small as possible . the self organizing feature maps predict sea levels better than all six models of comparison"}
{"title": "Selecting rail grade crossing investments with a decision support system", "abstract": "The Federal Railroad Administration (FRA) has developed a series of rail and rail-related analysis tools that assist FRA officials, Metropolitan Planning Organizations (MPOs), state Department of Transportation (DOT), and other constituents in evaluating the cost and benefits of potential infrastructure projects. To meet agency objectives, the FRA wants to add a high-speed rail grade crossing analysis tool to its package of rail and rail-related intermodal software products. This paper presents a conceptual decision support system (DSS) that can assist officials in achieving this goal. The paper first introduces the FRA's objectives and the role of cost benefit analysis in achieving these objectives. Next, there is a discussion of the models needed to assess the feasibility of proposed high-speed rail grade crossing investments and the presentation of a decision support system (DSS) that can deliver these models transparently to users. Then, the paper illustrates a system session and examines the potential benefits from system use", "id": "447", "src": "selecting rail grade crossing investments with a decision support system . the federal railroad administration ( fra ) has developed a series of rail and rail related analysis tools that assist fra officials , metropolitan planning organizations ( mpos ) , state department of transportation ( dot ) , and other constituents in evaluating the cost and benefits of potential infrastructure projects . to meet agency objectives , the fra wants to add a high speed rail grade crossing analysis tool to its package of rail and rail related intermodal software products . this paper presents a conceptual decision support system ( dss ) that can assist officials in achieving this goal . the paper first introduces the fra ' s objectives and the role of cost benefit analysis in achieving these objectives . next , there is a discussion of the models needed to assess the feasibility of proposed high speed rail grade crossing investments and the presentation of a decision support system ( dss ) that can deliver these models transparently to users . then , the paper illustrates a system session and examines the potential benefits from system use"}
{"title": "A study on meaning processing of dialogue with an example of development of", "abstract": "travel consultation system This paper describes an approach to processing meaning instead of processing information in computing. Human intellectual activity is supported by linguistic activities in the brain. Therefore, processing the meaning of language instead of processing information should allow us to realize human intelligence on a computer. As an example of the proposed framework for processing meaning, we build a travel consultation dialogue system which can understand utterance by a user and retrieve information through dialogue. Through a simulation example of the system, we show that both information processing and language processing are integrated", "id": "448", "src": "a study on meaning processing of dialogue with an example of development of . travel consultation system this paper describes an approach to processing meaning instead of processing information in computing . human intellectual activity is supported by linguistic activities in the brain . therefore , processing the meaning of language instead of processing information should allow us to realize human intelligence on a computer . as an example of the proposed framework for processing meaning , we build a travel consultation dialogue system which can understand utterance by a user and retrieve information through dialogue . through a simulation example of the system , we show that both information processing and language processing are integrated"}
{"title": "From a biological to a computational model for the autonomous behavior of an", "abstract": "animat Endowing an autonomous system like a robot with intelligent behavior is difficult for several reasons. First, behavior is such a wide topic that a general framework paradigm of inspiration must be chosen in order to obtain a consistent model. Such a framework can be, for example, biological modeling or an artificial intelligence approach. Second, a general framework is not sufficient to determine a fully specified program to be implemented in a robot. Many choices, tuning and tests must be carried out before obtaining a robust system. A biological model is presented, based on the definition of cortex-like automata, representing elementary functions in the perceptive, motor or associative domain. These automata are connected in a network whose architecture, functioning and learning rules are described in a cortical framework. Second, the computational model derived from that biological model is specified. The way units exchange and compute variables through links is explained, with reference to corresponding biological elements. It is then easier to report experiments allowing an autonomous system to learn regularities of a simple environment and to exploit them to satisfy some internal drives. Even if additional biological hints can be added, this model allow us to better understand how a biological model can be implemented and how biological properties can emerge from a distributed set of units", "id": "449", "src": "from a biological to a computational model for the autonomous behavior of an . animat endowing an autonomous system like a robot with intelligent behavior is difficult for several reasons . first , behavior is such a wide topic that a general framework paradigm of inspiration must be chosen in order to obtain a consistent model . such a framework can be , for example , biological modeling or an artificial intelligence approach . second , a general framework is not sufficient to determine a fully specified program to be implemented in a robot . many choices , tuning and tests must be carried out before obtaining a robust system . a biological model is presented , based on the definition of cortex like automata , representing elementary functions in the perceptive , motor or associative domain . these automata are connected in a network whose architecture , functioning and learning rules are described in a cortical framework . second , the computational model derived from that biological model is specified . the way units exchange and compute variables through links is explained , with reference to corresponding biological elements . it is then easier to report experiments allowing an autonomous system to learn regularities of a simple environment and to exploit them to satisfy some internal drives . even if additional biological hints can be added , this model allow us to better understand how a biological model can be implemented and how biological properties can emerge from a distributed set of units"}
{"title": "Nissan v. Nissan [trademark dispute]", "abstract": "Is a trademark dispute a case of David v. Goliath or a corporation fending off a greedy opportunist? This paper discusses the case of Uzi Nissan, who is locked in a multimillion-dollar legal battle over whether or not his use of the nissan.com Internet domain name infringes upon Japan's Nissan Motor Co.'s trademark. At the heart of the matter is the impact of the global Internet on trademark law, which traditionally has been strongly influenced by geographic considerations. The paper discusses the background to the case from both sides and the issues involved", "id": "450", "src": "nissan v . nissan trademark dispute . is a trademark dispute a case of david v . goliath or a corporation fending off a greedy opportunist this paper discusses the case of uzi nissan , who is locked in a multimillion dollar legal battle over whether or not his use of the nissan . com internet domain name infringes upon japan ' s nissan motor co . ' s trademark . at the heart of the matter is the impact of the global internet on trademark law , which traditionally has been strongly influenced by geographic considerations . the paper discusses the background to the case from both sides and the issues involved"}
{"title": "Design PID controllers for desired time-domain or frequency-domain response", "abstract": "Practical requirements on the design of control systems, especially process control systems, are usually specified in terms of time-domain response, such as overshoot and rise time, or frequency-domain response, such as resonance peak and stability margin. Although numerous methods have been developed for the design of the proportional-integral-derivative (PID) controller, little work has been done in relation to the quantitative time-domain and frequency-domain responses. In this paper, we study the following problem: Given a nominal stable process with time delay, we design a suboptimal PID controller to achieve the required time-domain response or frequency-domain response for the nominal system or the uncertain system. An H/sub infinity / PID controller is developed based on optimal control theory and the parameters are derived analytically. Its properties are investigated and compared with that of two developed suboptimal controllers: an H/sub 2/ PID controller and a Maclaurin PID controller", "id": "451", "src": "design pid controllers for desired time domain or frequency domain response . practical requirements on the design of control systems , especially process control systems , are usually specified in terms of time domain response , such as overshoot and rise time , or frequency domain response , such as resonance peak and stability margin . although numerous methods have been developed for the design of the proportional integral derivative ( pid ) controller , little work has been done in relation to the quantitative time domain and frequency domain responses . in this paper , we study the following problem given a nominal stable process with time delay , we design a suboptimal pid controller to achieve the required time domain response or frequency domain response for the nominal system or the uncertain system . an h sub infinity pid controller is developed based on optimal control theory and the parameters are derived analytically . its properties are investigated and compared with that of two developed suboptimal controllers an h sub 2 pid controller and a maclaurin pid controller"}
{"title": "Virtual borders, real laws [Internet activity and treaties]", "abstract": "National governments are working to tame activity on the Internet. They have worked steadily to extend control over online activities that they believe affect their interests, even when the activities occur outside their borders. These usually involve what governments regard as their domain: protecting public order, enforcing commercial laws, and, occasionally, protecting consumer interests. Methods have included assertions or legal jurisdiction based on where material is accessible instead of where it originates, and the blocking of sites, service providers, or entire high level domains from access by citizens. Such instances are mentioned in this article. Whilst larger companies are able to defend themselves against overseas lawsuits, individuals and smaller organizations lack the resources to defend what are often normal business activities at home, but could violate the laws of local jurisdictions in countries around the world. The problems of libel are discussed as are the blocking of certain sites by certain countries. Efforts to draw up Internet treaties are also mentioned", "id": "452", "src": "virtual borders , real laws internet activity and treaties . national governments are working to tame activity on the internet . they have worked steadily to extend control over online activities that they believe affect their interests , even when the activities occur outside their borders . these usually involve what governments regard as their domain protecting public order , enforcing commercial laws , and , occasionally , protecting consumer interests . methods have included assertions or legal jurisdiction based on where material is accessible instead of where it originates , and the blocking of sites , service providers , or entire high level domains from access by citizens . such instances are mentioned in this article . whilst larger companies are able to defend themselves against overseas lawsuits , individuals and smaller organizations lack the resources to defend what are often normal business activities at home , but could violate the laws of local jurisdictions in countries around the world . the problems of libel are discussed as are the blocking of certain sites by certain countries . efforts to draw up internet treaties are also mentioned"}
{"title": "A better ballot box?", "abstract": "Election officials are examining technologies to address a wide range of voting issues. The problems observed in the November 2000 US election accelerated existing trends to get rid of lever machines, punch-cards, and hand-counted paper ballots and replace them with mark-sense balloting, Internet, and automatic teller machine (ATM) kiosk style computer-based systems. An estimated US $2-$4 billion will be spent in the United States and Canada to update voting systems during the next decade. Voting online might enable citizens to vote even if they are unable to get to the polls. Yet making these methods work right turns out to be considerably more difficult than originally thought. New electronic voting systems pose risks as well as solutions. As it turns out, many of the voting products currently for sale provide less accountability, poorer reliability, and greater opportunity for widespread fraud than those already in use. This paper discusses the technology available and how to ensure accurate ballots", "id": "453", "src": "a better ballot box . election officials are examining technologies to address a wide range of voting issues . the problems observed in the november <digit> us election accelerated existing trends to get rid of lever machines , punch cards , and hand counted paper ballots and replace them with mark sense balloting , internet , and automatic teller machine ( atm ) kiosk style computer based systems . an estimated us 2 4 billion will be spent in the united states and canada to update voting systems during the next decade . voting online might enable citizens to vote even if they are unable to get to the polls . yet making these methods work right turns out to be considerably more difficult than originally thought . new electronic voting systems pose risks as well as solutions . as it turns out , many of the voting products currently for sale provide less accountability , poorer reliability , and greater opportunity for widespread fraud than those already in use . this paper discusses the technology available and how to ensure accurate ballots"}
{"title": "Putting pen to screen on Tablet PCs", "abstract": "With the release of the first Tablet PCs produced to Microsoft Corp.'s general specifications, handheld computers may be about to leap into the ring with today's laptops. They will be about the size of the smaller laptops, will be at least as powerful, and maybe their biggest selling point-will be able to handle handwritten text. The Tablet PCs will be amply configured, general-purpose machines with more than enough power to run the full-blown Windows XP operating system. In particular, they will allow handwritten text to be entered onto a digitizing tablet and recognized, a functionality that's called pen-based computing. The Tablet PC will far outpace the computing power of existing small devices such as PDAs (personal digital assistants), including those variants based on Microsoft's own Pocket PC operating system", "id": "454", "src": "putting pen to screen on tablet pcs . with the release of the first tablet pcs produced to microsoft corp . ' s general specifications , handheld computers may be about to leap into the ring with today ' s laptops . they will be about the size of the smaller laptops , will be at least as powerful , and maybe their biggest selling point will be able to handle handwritten text . the tablet pcs will be amply configured , general purpose machines with more than enough power to run the full blown windows xp operating system . in particular , they will allow handwritten text to be entered onto a digitizing tablet and recognized , a functionality that ' s called pen based computing . the tablet pc will far outpace the computing power of existing small devices such as pdas ( personal digital assistants ) , including those variants based on microsoft ' s own pocket pc operating system"}
{"title": "Horizontal waypoint guidance design using optimal control", "abstract": "A horizontal waypoint guidance algorithm is proposed by applying line-following guidance to waypoint line segments in sequence. The line-following guidance is designed using an LQR (linear quadratic regulator). Then, the optimal waypoint changing points are derived by minimizing the accelerations required for changing the waypoint line segments. Also derived is a sufficient condition for the stability bound of ground speed changes based on the Lyapunov stability theorem. Simulation results show that the proposed algorithm can effectively guide a vehicle along the sequence of waypoint line segments", "id": "455", "src": "horizontal waypoint guidance design using optimal control . a horizontal waypoint guidance algorithm is proposed by applying line following guidance to waypoint line segments in sequence . the line following guidance is designed using an lqr ( linear quadratic regulator ) . then , the optimal waypoint changing points are derived by minimizing the accelerations required for changing the waypoint line segments . also derived is a sufficient condition for the stability bound of ground speed changes based on the lyapunov stability theorem . simulation results show that the proposed algorithm can effectively guide a vehicle along the sequence of waypoint line segments"}
{"title": "Separation and tracking of multiple broadband sources with one electromagnetic", "abstract": "vector sensor A structure for adaptively separating, enhancing and tracking uncorrelated sources with an electromagnetic vector sensor (EMVS) is presented. The structure consists of a set of parallel spatial processors, one for each individual source. Two stages of processing are involved in each spatial processor. The first preprocessing stage rejects all other sources except the one of interest, while the second stage is an adaptive one for maximizing the signal-to-noise ratio (SNR) and tracking the desired source. The preprocessings are designed using the latest source parameter estimates obtained from the source trackers, and a redesign is activated periodically or whenever any source has been detected by the source trackers to have made significant movement. Compared with conventional adaptive beamforming, the algorithm has the advantage that no a priori information on any desired signal location is needed, the sources are separated at maximum SNR, and their locations are available. The structure is also well suited for parallel implementation. Numerical examples are included to illustrate the capability and performance of the algorithm", "id": "456", "src": "separation and tracking of multiple broadband sources with one electromagnetic . vector sensor a structure for adaptively separating , enhancing and tracking uncorrelated sources with an electromagnetic vector sensor ( emvs ) is presented . the structure consists of a set of parallel spatial processors , one for each individual source . two stages of processing are involved in each spatial processor . the first preprocessing stage rejects all other sources except the one of interest , while the second stage is an adaptive one for maximizing the signal to noise ratio ( snr ) and tracking the desired source . the preprocessings are designed using the latest source parameter estimates obtained from the source trackers , and a redesign is activated periodically or whenever any source has been detected by the source trackers to have made significant movement . compared with conventional adaptive beamforming , the algorithm has the advantage that no a priori information on any desired signal location is needed , the sources are separated at maximum snr , and their locations are available . the structure is also well suited for parallel implementation . numerical examples are included to illustrate the capability and performance of the algorithm"}
{"title": "Recursive state estimation for multiple switching models with unknown", "abstract": "transition probabilities This work considers hybrid systems with continuous-valued target states and discrete-valued regime variable. The changes (switches) of the regime variable are modeled by a finite state Markov chain with unknown and random transition probabilities following Dirichlet distributions. Our work analytically derives the marginal posterior distribution of the states and regime variables, the transition probabilities being integrated out. This leads to a variety of recursive hybrid state estimation schemes which are an appealing intuitive and straightforward extension of standard algorithms. Their performance is illustrated by a maneuvering target tracking example", "id": "457", "src": "recursive state estimation for multiple switching models with unknown . transition probabilities this work considers hybrid systems with continuous valued target states and discrete valued regime variable . the changes ( switches ) of the regime variable are modeled by a finite state markov chain with unknown and random transition probabilities following dirichlet distributions . our work analytically derives the marginal posterior distribution of the states and regime variables , the transition probabilities being integrated out . this leads to a variety of recursive hybrid state estimation schemes which are an appealing intuitive and straightforward extension of standard algorithms . their performance is illustrated by a maneuvering target tracking example"}
{"title": "MATLAB code for plotting ambiguity functions", "abstract": "A MATLAB code capable of plotting ambiguity functions of many different radar signals is presented. The program makes use of MATLAB's sparse matrix operations, and avoids loops. The program could be useful as a pedagogical tool in radar courses teaching pulse compression", "id": "458", "src": "matlab code for plotting ambiguity functions . a matlab code capable of plotting ambiguity functions of many different radar signals is presented . the program makes use of matlab ' s sparse matrix operations , and avoids loops . the program could be useful as a pedagogical tool in radar courses teaching pulse compression"}
{"title": "Incremental motion control of linear synchronous motor", "abstract": "In this study a particular incremental motion control problem, which is specified by the trapezoidal velocity profile using multisegment sliding mode control (MSSMC), is proposed to control a permanent magnet linear synchronous motor (PMLSM) servo drive system. First, the structure and operating principle of the PMLSM are described in detail. Second, a field-oriented control PMLSM servo drive is introduced. Then, each segment of the multisegment switching surfaces is designed to match the corresponding part of the trapezoidal velocity profile, thus the motor dynamics on the specified-segment switching surface have the desired velocity or acceleration corresponding part of the trapezoidal velocity profile. In addition, the proposed control system is implemented in a PC-based computer control system. Finally, the effectiveness of the proposed PMLSM servo drive system is demonstrated by some simulated and experimental results", "id": "459", "src": "incremental motion control of linear synchronous motor . in this study a particular incremental motion control problem , which is specified by the trapezoidal velocity profile using multisegment sliding mode control ( mssmc ) , is proposed to control a permanent magnet linear synchronous motor ( pmlsm ) servo drive system . first , the structure and operating principle of the pmlsm are described in detail . second , a field oriented control pmlsm servo drive is introduced . then , each segment of the multisegment switching surfaces is designed to match the corresponding part of the trapezoidal velocity profile , thus the motor dynamics on the specified segment switching surface have the desired velocity or acceleration corresponding part of the trapezoidal velocity profile . in addition , the proposed control system is implemented in a pc based computer control system . finally , the effectiveness of the proposed pmlsm servo drive system is demonstrated by some simulated and experimental results"}
{"title": "Feedforward maximum power point tracking of PV systems using fuzzy controller", "abstract": "A feedforward maximum power (MP) point tracking scheme is developed for the interleaved dual boost (IDB) converter fed photovoltaic (PV) system using fuzzy controller. The tracking algorithm changes the duty ratio of the converter such that the solar cell array (SCA) voltage equals the voltage corresponding to the MP point at that solar insolation. This is done by the feedforward loop, which generates an error signal by comparing the instantaneous array voltage and reference voltage. The reference voltage for the feedforward loop, corresponding to the MP point, is obtained by an off-line trained neural network. Experimental data is used for off-line training of the neural network, which employs back-propagation algorithm. The proposed fuzzy feedforward peak power tracking effectiveness is demonstrated through the simulation and experimental results, and compared with the conventional proportional plus integral (PI) controller based system. Finally, a comparative study of interleaved boost and conventional boost converter for the PV applications is given and their suitability is discussed", "id": "460", "src": "feedforward maximum power point tracking of pv systems using fuzzy controller . a feedforward maximum power ( mp ) point tracking scheme is developed for the interleaved dual boost ( idb ) converter fed photovoltaic ( pv ) system using fuzzy controller . the tracking algorithm changes the duty ratio of the converter such that the solar cell array ( sca ) voltage equals the voltage corresponding to the mp point at that solar insolation . this is done by the feedforward loop , which generates an error signal by comparing the instantaneous array voltage and reference voltage . the reference voltage for the feedforward loop , corresponding to the mp point , is obtained by an off line trained neural network . experimental data is used for off line training of the neural network , which employs back propagation algorithm . the proposed fuzzy feedforward peak power tracking effectiveness is demonstrated through the simulation and experimental results , and compared with the conventional proportional plus integral ( pi ) controller based system . finally , a comparative study of interleaved boost and conventional boost converter for the pv applications is given and their suitability is discussed"}
{"title": "A PID standard: What, why, how?", "abstract": "The paper is written for all who develop and use P&IDs. It will aid in solving the long existing and continuing problem of confusing information on P&IDs. The acronym P&ID is widely understood to mean the principal document used to define the details of how a process works and how it is controlled. The ISA Dictionary definition for P&ID tells what they do, \"show the interconnection of process equipment and the instrumentation used to control the process. In the process industry a standard set of symbols is used to prepare drawings of processes. The instrument symbols used in these drawings are generally based on ISA-S5.1.\" In the paper the ISA standard is referred to as ISA-5.1. The article develops the concept of the \"standard\" and poses some of the questions that the \"standard\" can answer", "id": "461", "src": "a pid standard what , why , how . the paper is written for all who develop and use p&ids . it will aid in solving the long existing and continuing problem of confusing information on p&ids . the acronym p&id is widely understood to mean the principal document used to define the details of how a process works and how it is controlled . the isa dictionary definition for p&id tells what they do , show the interconnection of process equipment and the instrumentation used to control the process . in the process industry a standard set of symbols is used to prepare drawings of processes . the instrument symbols used in these drawings are generally based on isa s5 . 1 . in the paper the isa standard is referred to as isa 5 . 1 . the article develops the concept of the standard and poses some of the questions that the standard can answer"}
{"title": "Quantitative speed control for SRM drive using fuzzy adapted inverse model", "abstract": "Quantitative and robust speed control for a switched reluctance motor (SRM) drive is considered to be rather difficult and challenging owing to its highly nonlinear dynamic behavior. A speed control scheme having two-degree-of-freedom (2DOF) structure is developed here to improve the speed dynamic response of an SRM drive. In the proposed control scheme, the feedback controller is quantitatively designed to meet the desired regulation control requirements first. Then a reference model and a command feedforward controller based on an inverse plant model are employed to yield the desired tracking response at nominal case. As the variations of system parameters and operating conditions occur, the prescribed control specifications may not be satisfied any more. To improve this, the inverse model is adaptively tuned by a fuzzy control scheme so that the model-following tracking error is significantly reduced. In addition, a simple disturbance cancellation robust controller is added to improve the tracking and regulation control performances further", "id": "462", "src": "quantitative speed control for srm drive using fuzzy adapted inverse model . quantitative and robust speed control for a switched reluctance motor ( srm ) drive is considered to be rather difficult and challenging owing to its highly nonlinear dynamic behavior . a speed control scheme having two degree of freedom ( 2dof ) structure is developed here to improve the speed dynamic response of an srm drive . in the proposed control scheme , the feedback controller is quantitatively designed to meet the desired regulation control requirements first . then a reference model and a command feedforward controller based on an inverse plant model are employed to yield the desired tracking response at nominal case . as the variations of system parameters and operating conditions occur , the prescribed control specifications may not be satisfied any more . to improve this , the inverse model is adaptively tuned by a fuzzy control scheme so that the model following tracking error is significantly reduced . in addition , a simple disturbance cancellation robust controller is added to improve the tracking and regulation control performances further"}
{"title": "Robust fuzzy controlled photovoltaic power inverter with Taguchi method", "abstract": "This paper presents design and implementation of a robust fuzzy controlled photovoltaic (PV) power inverter with Taguchi tuned scaling factors. To achieve fast transient response, small steady-state error and system robustness, a robust fuzzy controller is adopted, in which its input and output scaling factors are determined efficiently by using the Taguchi-tuning algorithm. The proposed system can operate in different modes, grid-connection mode and stand-alone mode, and can accommodate wide load variations. Simulation results and hardware measurements obtained from a prototype with a microcontroller (Intel 80196KC) are presented to verify the theoretical discussions, and its adaptivity, robustness and feasibility", "id": "463", "src": "robust fuzzy controlled photovoltaic power inverter with taguchi method . this paper presents design and implementation of a robust fuzzy controlled photovoltaic ( pv ) power inverter with taguchi tuned scaling factors . to achieve fast transient response , small steady state error and system robustness , a robust fuzzy controller is adopted , in which its input and output scaling factors are determined efficiently by using the taguchi tuning algorithm . the proposed system can operate in different modes , grid connection mode and stand alone mode , and can accommodate wide load variations . simulation results and hardware measurements obtained from a prototype with a microcontroller ( intel 80196kc ) are presented to verify the theoretical discussions , and its adaptivity , robustness and feasibility"}
{"title": "Robust wavelet neuro control for linear brushless motors", "abstract": "Design, simulation and experimental implementation of a wavelet basis function network learning controller for linear brushless dc motors (LBDCM) are considered. Stability robustness with position tracking is the primary concern. The proposed controller deals mainly with external disturbances, e.g. nonlinear friction force and payload variation in motion control of linear motors. It consists of two parts, one is a state feedback component, and the other one is a learning feedback component. The state feedback controller is designed on the basis of a simple linear model, and the learning feedback component is a wavelet neural controller. The attenuation effect of wavelet neural networks on friction force is first verified by the numerical method. The learning effect of wavelet neural networks on friction force is also shown in the numerical results. Then, a wavelet neural network is applied on a real LBDCM to on-line suppress the friction force, which may be variable due to the different lubrication. The effectiveness of the proposed control schemes is demonstrated by simulated and experimental results", "id": "464", "src": "robust wavelet neuro control for linear brushless motors . design , simulation and experimental implementation of a wavelet basis function network learning controller for linear brushless dc motors ( lbdcm ) are considered . stability robustness with position tracking is the primary concern . the proposed controller deals mainly with external disturbances , e . g . nonlinear friction force and payload variation in motion control of linear motors . it consists of two parts , one is a state feedback component , and the other one is a learning feedback component . the state feedback controller is designed on the basis of a simple linear model , and the learning feedback component is a wavelet neural controller . the attenuation effect of wavelet neural networks on friction force is first verified by the numerical method . the learning effect of wavelet neural networks on friction force is also shown in the numerical results . then , a wavelet neural network is applied on a real lbdcm to on line suppress the friction force , which may be variable due to the different lubrication . the effectiveness of the proposed control schemes is demonstrated by simulated and experimental results"}
{"title": "Outlier resistant adaptive matched filtering", "abstract": "Robust adaptive matched filtering (AMF) whereby outlier data vectors are censored from the covariance matrix estimate is considered in a maximum likelihood estimation (MLE) setting. It is known that outlier data vectors whose steering vector is highly correlated with the desired steering vector, can significantly degrade the performance of AMF algorithms such as sample matrix inversion (SMI) or fast maximum likelihood (FML). Four new algorithms that censor outliers are presented which are derived via approximation to the MLE solution. Two algorithms each are related to using the SMI or the FML to estimate the unknown underlying covariance matrix. Results are presented using computer simulations which demonstrate the relative effectiveness of the four algorithms versus each other and also versus the SMI and FML algorithms in the presence of outliers and no outliers. It is shown that one of the censoring algorithms, called the reiterative censored fast maximum likelihood (CFML) technique is significantly superior to the other three censoring methods in stressful outlier scenarios", "id": "465", "src": "outlier resistant adaptive matched filtering . robust adaptive matched filtering ( amf ) whereby outlier data vectors are censored from the covariance matrix estimate is considered in a maximum likelihood estimation ( mle ) setting . it is known that outlier data vectors whose steering vector is highly correlated with the desired steering vector , can significantly degrade the performance of amf algorithms such as sample matrix inversion ( smi ) or fast maximum likelihood ( fml ) . four new algorithms that censor outliers are presented which are derived via approximation to the mle solution . two algorithms each are related to using the smi or the fml to estimate the unknown underlying covariance matrix . results are presented using computer simulations which demonstrate the relative effectiveness of the four algorithms versus each other and also versus the smi and fml algorithms in the presence of outliers and no outliers . it is shown that one of the censoring algorithms , called the reiterative censored fast maximum likelihood ( cfml ) technique is significantly superior to the other three censoring methods in stressful outlier scenarios"}
{"title": "Brightness-independent start-up routine for star trackers", "abstract": "Initial attitude acquisition by a modern star tracker is investigated here. Criteria for efficient organization of the on-board database are discussed with reference to a brightness-independent initial acquisition algorithm. Star catalog generation preprocessing is described, with emphasis on the identification of minimum star brightness for detection by a sensor based on a charge coupled device (CCD) photodetector. This is a crucial step for proper evaluation of the attainable sky coverage when selecting the stars to be included in the on-board catalog. Test results are also reported, both for reliability and accuracy, even if the former is considered to be the primary target. Probability of erroneous solution is 0.2% in the case of single runs of the procedure, while attitude determination accuracy is in the order of 0.02 degrees in the average for the computation of the inertial pointing of the boresight axis", "id": "466", "src": "brightness independent start up routine for star trackers . initial attitude acquisition by a modern star tracker is investigated here . criteria for efficient organization of the on board database are discussed with reference to a brightness independent initial acquisition algorithm . star catalog generation preprocessing is described , with emphasis on the identification of minimum star brightness for detection by a sensor based on a charge coupled device ( ccd ) photodetector . this is a crucial step for proper evaluation of the attainable sky coverage when selecting the stars to be included in the on board catalog . test results are also reported , both for reliability and accuracy , even if the former is considered to be the primary target . probability of erroneous solution is 0 . 2 in the case of single runs of the procedure , while attitude determination accuracy is in the order of 0 . <digit> degrees in the average for the computation of the inertial pointing of the boresight axis"}
{"title": "Multiple model adaptive estimation with filter spawning", "abstract": "Multiple model adaptive estimation (MMAE) with filter spawning is used to detect and estimate partial actuator failures on the VISTA F-16. The truth model is a full six-degree-of-freedom simulation provided by Calspan and General Dynamics. The design models are chosen as 13-state linearized models, including first order actuator models. Actuator failures are incorporated into the truth model and design model assuming a \"failure to free stream.\" Filter spawning is used to include additional filters with partial actuator failure hypotheses into the MMAE bank. The spawned filters are based on varying degrees of partial failures (in terms of effectiveness) associated with the complete-actuaton-failure hypothesis with the highest conditional probability of correctness at the current time. Thus, a blended estimate of the failure effectiveness is found using the filters' estimates based upon a no-failure hypothesis, a complete actuator failure hypothesis, and the spawned filters' partial-failure hypotheses. This yields substantial precision in effectiveness estimation, compared with what is possible without spawning additional filters, making partial failure adaptation a viable methodology", "id": "467", "src": "multiple model adaptive estimation with filter spawning . multiple model adaptive estimation ( mmae ) with filter spawning is used to detect and estimate partial actuator failures on the vista f <digit> . the truth model is a full six degree of freedom simulation provided by calspan and general dynamics . the design models are chosen as <digit> state linearized models , including first order actuator models . actuator failures are incorporated into the truth model and design model assuming a failure to free stream . filter spawning is used to include additional filters with partial actuator failure hypotheses into the mmae bank . the spawned filters are based on varying degrees of partial failures ( in terms of effectiveness ) associated with the complete actuaton failure hypothesis with the highest conditional probability of correctness at the current time . thus , a blended estimate of the failure effectiveness is found using the filters ' estimates based upon a no failure hypothesis , a complete actuator failure hypothesis , and the spawned filters ' partial failure hypotheses . this yields substantial precision in effectiveness estimation , compared with what is possible without spawning additional filters , making partial failure adaptation a viable methodology"}
{"title": "Matched-filter template generation via spatial filtering: application to fetal", "abstract": "biomagnetic recordings We have developed a two-step procedure for signal processing of fetal biomagnetic recordings that removes cardiac interference and noise. First, a modified matched filter (MF) is applied to remove maternal cardiac interference; then, a simple signal space projection (SSP) is applied to remove noise. The key difference between our MF and a conventional one is that the interference template and the template scaling are derived from a signal that has been spatially filtered to isolate the interference, rather than from the raw signal. Unlike conventional MFs, ours is able to separate maternal and fetal cardiac complexes, even when they have similar morphology and overlap strongly. When followed by a SSP that preserves only the signal subspace, the noise is reduced to a low level", "id": "468", "src": "matched filter template generation via spatial filtering application to fetal . biomagnetic recordings we have developed a two step procedure for signal processing of fetal biomagnetic recordings that removes cardiac interference and noise . first , a modified matched filter ( mf ) is applied to remove maternal cardiac interference then , a simple signal space projection ( ssp ) is applied to remove noise . the key difference between our mf and a conventional one is that the interference template and the template scaling are derived from a signal that has been spatially filtered to isolate the interference , rather than from the raw signal . unlike conventional mfs , ours is able to separate maternal and fetal cardiac complexes , even when they have similar morphology and overlap strongly . when followed by a ssp that preserves only the signal subspace , the noise is reduced to a low level"}
{"title": "Design and implementation of a brain-computer interface with high transfer", "abstract": "rates This paper presents a brain-computer interface (BCI) that can help users to input phone numbers. The system is based on the steady-state visual evoked potential (SSVEP). Twelve buttons illuminated at different rates were displayed on a computer monitor. The buttons constituted a virtual telephone keypad, representing the ten digits 0-9, BACKSPACE, and ENTER. Users could input phone number by gazing at these buttons. The frequency-coded SSVEP was used to judge which button the user desired. Eight of the thirteen subjects succeeded in ringing the mobile phone using the system. The average transfer rate over all subjects was 27.15 bits/min. The attractive features of the system are noninvasive signal recording, little training required for use, and high information transfer rate. Approaches to improve the performance of the system are discussed", "id": "469", "src": "design and implementation of a brain computer interface with high transfer . rates this paper presents a brain computer interface ( bci ) that can help users to input phone numbers . the system is based on the steady state visual evoked potential ( ssvep ) . twelve buttons illuminated at different rates were displayed on a computer monitor . the buttons constituted a virtual telephone keypad , representing the ten digits 0 9 , backspace , and enter . users could input phone number by gazing at these buttons . the frequency coded ssvep was used to judge which button the user desired . eight of the thirteen subjects succeeded in ringing the mobile phone using the system . the average transfer rate over all subjects was <digit> . <digit> bits min . the attractive features of the system are noninvasive signal recording , little training required for use , and high information transfer rate . approaches to improve the performance of the system are discussed"}
{"title": "Noninvasive myocardial activation time imaging: a novel inverse algorithm", "abstract": "applied to clinical ECG mapping data Linear approaches like the minimum-norm least-square algorithm show insufficient performance when it comes to estimating the activation time map on the surface of the heart from electrocardiographic (ECG) mapping data. Additional regularization has to be considered leading to a nonlinear problem formulation. The Gauss-Newton approach is one of the standard mathematical tools capable of solving this kind of problem. To our experience, this algorithm has specific drawbacks which are caused by the applied regularization procedure. In particular, under clinical conditions the amount of regularization cannot be determined clearly. For this reason, we have developed an iterative algorithm solving this nonlinear problem by a sequence of regularized linear problems. At each step of iteration, an individual L-curve is computed. Subsequent iteration steps are performed with the individual optimal regularization parameter. This novel approach is compared with the standard Gauss-Newton approach. Both methods are applied to simulated ECG mapping data as well as to single beat sinus rhythm data from two patients recorded in the catheter laboratory. The proposed approach shows excellent numerical and computational performance, even under clinical conditions at which the Gauss-Newton approach begins to break down", "id": "470", "src": "noninvasive myocardial activation time imaging a novel inverse algorithm . applied to clinical ecg mapping data linear approaches like the minimum norm least square algorithm show insufficient performance when it comes to estimating the activation time map on the surface of the heart from electrocardiographic ( ecg ) mapping data . additional regularization has to be considered leading to a nonlinear problem formulation . the gauss newton approach is one of the standard mathematical tools capable of solving this kind of problem . to our experience , this algorithm has specific drawbacks which are caused by the applied regularization procedure . in particular , under clinical conditions the amount of regularization cannot be determined clearly . for this reason , we have developed an iterative algorithm solving this nonlinear problem by a sequence of regularized linear problems . at each step of iteration , an individual l curve is computed . subsequent iteration steps are performed with the individual optimal regularization parameter . this novel approach is compared with the standard gauss newton approach . both methods are applied to simulated ecg mapping data as well as to single beat sinus rhythm data from two patients recorded in the catheter laboratory . the proposed approach shows excellent numerical and computational performance , even under clinical conditions at which the gauss newton approach begins to break down"}
{"title": "Bayesian nonstationary autoregressive models for biomedical signal analysis", "abstract": "We describe a variational Bayesian algorithm for the estimation of a multivariate autoregressive model with time-varying coefficients that adapt according to a linear dynamical system. The algorithm allows for time and frequency domain characterization of nonstationary multivariate signals and is especially suited to the analysis of event-related data. Results are presented on synthetic data and real electroencephalogram data recorded in event-related desynchronization and photic synchronization scenarios", "id": "471", "src": "bayesian nonstationary autoregressive models for biomedical signal analysis . we describe a variational bayesian algorithm for the estimation of a multivariate autoregressive model with time varying coefficients that adapt according to a linear dynamical system . the algorithm allows for time and frequency domain characterization of nonstationary multivariate signals and is especially suited to the analysis of event related data . results are presented on synthetic data and real electroencephalogram data recorded in event related desynchronization and photic synchronization scenarios"}
{"title": "Supervisory control design based on hybrid systems and fuzzy events detection.", "abstract": "Application to an oxichlorination reactor This paper presents a supervisory control scheme based on hybrid systems theory and fuzzy events detection. The fuzzy event detector is a linguistic model, which synthesizes complex relations between process variables and process events incorporating experts' knowledge about the process operation. This kind of detection allows the anticipation of appropriate control actions, which depend upon the selected membership functions used to characterize the process under scrutiny. The proposed supervisory control scheme was successfully implemented for an oxichlorination reactor in a vinyl monomer plant", "id": "472", "src": "supervisory control design based on hybrid systems and fuzzy events detection . . application to an oxichlorination reactor this paper presents a supervisory control scheme based on hybrid systems theory and fuzzy events detection . the fuzzy event detector is a linguistic model , which synthesizes complex relations between process variables and process events incorporating experts ' knowledge about the process operation . this kind of detection allows the anticipation of appropriate control actions , which depend upon the selected membership functions used to characterize the process under scrutiny . the proposed supervisory control scheme was successfully implemented for an oxichlorination reactor in a vinyl monomer plant"}
{"title": "Automated breath detection on long-duration signals using feedforward", "abstract": "backpropagation artificial neural networks A new breath-detection algorithm is presented, intended to automate the analysis of respiratory data acquired during sleep. The algorithm is based on two independent artificial neural networks (ANN/sub insp/ and ANN/sub expi/) that recognize, in the original signal, windows of interest where the onset of inspiration and expiration occurs. Postprocessing consists in finding inside each of these windows of interest minimum and maximum corresponding to each inspiration and expiration. The ANN/sub insp/ and ANN/sub expi/ correctly determine respectively 98.0% and 98.7% of the desired windows, when compared with 29 820 inspirations and 29 819 expirations detected by a human expert, obtained from three entire-night recordings. Postprocessing allowed determination of inspiration and expiration onsets with a mean difference with respect to the same human expert of (mean +or- SD) 34 +or- 71 ms for inspiration and 5 +or- 46 ms for expiration. The method proved to be effective in detecting the onset of inspiration and expiration in full night continuous recordings. A comparison of five human experts performing the same classification task yielded that the automated algorithm was undifferentiable from these human experts, failing within the distribution of human expert results. Besides being applicable to adult respiratory volume data, the presented algorithm was also successfully applied to infant sleep data, consisting of uncalibrated rib cage and abdominal movement recordings. A comparison with two previously published algorithms for breath detection in respiratory volume signal shows that the presented algorithm has a higher specificity, while presenting similar or higher positive predictive values", "id": "473", "src": "automated breath detection on long duration signals using feedforward . backpropagation artificial neural networks a new breath detection algorithm is presented , intended to automate the analysis of respiratory data acquired during sleep . the algorithm is based on two independent artificial neural networks ( ann sub insp and ann sub expi ) that recognize , in the original signal , windows of interest where the onset of inspiration and expiration occurs . postprocessing consists in finding inside each of these windows of interest minimum and maximum corresponding to each inspiration and expiration . the ann sub insp and ann sub expi correctly determine respectively <digit> . 0 and <digit> . 7 of the desired windows , when compared with <digit> <digit> inspirations and <digit> <digit> expirations detected by a human expert , obtained from three entire night recordings . postprocessing allowed determination of inspiration and expiration onsets with a mean difference with respect to the same human expert of ( mean +or sd ) <digit> +or <digit> ms for inspiration and 5 +or <digit> ms for expiration . the method proved to be effective in detecting the onset of inspiration and expiration in full night continuous recordings . a comparison of five human experts performing the same classification task yielded that the automated algorithm was undifferentiable from these human experts , failing within the distribution of human expert results . besides being applicable to adult respiratory volume data , the presented algorithm was also successfully applied to infant sleep data , consisting of uncalibrated rib cage and abdominal movement recordings . a comparison with two previously published algorithms for breath detection in respiratory volume signal shows that the presented algorithm has a higher specificity , while presenting similar or higher positive predictive values"}
{"title": "Model selection in electromagnetic source analysis with an application to VEFs", "abstract": "In electromagnetic source analysis, it is necessary to determine how many sources are required to describe the electroencephalogram or magnetoencephalogram adequately. Model selection procedures (MSPs) or goodness of fit procedures give an estimate of the required number of sources. Existing and new MSPs are evaluated in different source and noise settings: two sources which are close or distant and noise which is uncorrelated or correlated. The commonly used MSP residual variance is seen to be ineffective, that is it often selects too many sources. Alternatives like the adjusted Hotelling's test, Bayes information criterion and the Wald test on source amplitudes are seen to be effective. The adjusted Hotelling's test is recommended if a conservative approach is taken and MSPs such as Bayes information criterion or the Wald test on source amplitudes are recommended if a more liberal approach is desirable. The MSPs are applied to empirical data (visual evoked fields)", "id": "474", "src": "model selection in electromagnetic source analysis with an application to vefs . in electromagnetic source analysis , it is necessary to determine how many sources are required to describe the electroencephalogram or magnetoencephalogram adequately . model selection procedures ( msps ) or goodness of fit procedures give an estimate of the required number of sources . existing and new msps are evaluated in different source and noise settings two sources which are close or distant and noise which is uncorrelated or correlated . the commonly used msp residual variance is seen to be ineffective , that is it often selects too many sources . alternatives like the adjusted hotelling ' s test , bayes information criterion and the wald test on source amplitudes are seen to be effective . the adjusted hotelling ' s test is recommended if a conservative approach is taken and msps such as bayes information criterion or the wald test on source amplitudes are recommended if a more liberal approach is desirable . the msps are applied to empirical data ( visual evoked fields )"}
{"title": "Time-varying properties of renal autoregulatory mechanisms", "abstract": "In order to assess the possible time-varying properties of renal autoregulation, time-frequency and time-scaling methods were applied to renal blood flow under broad-band forced arterial blood pressure fluctuations and single-nephron renal blood flow with spontaneous oscillations obtained from normotensive (Sprague-Dawley, Wistar, and Long-Evans) rats, and spontaneously hypertensive rats. Time-frequency analyses of normotensive and hypertensive blood flow data obtained from either the whole kidney or the single-nephron show that indeed both the myogenic and tubuloglomerular feedback (TGF) mechanisms have time-varying characteristics. Furthermore, we utilized the Renyi entropy to measure the complexity of blood-flow dynamics in the time-frequency plane in an effort to discern differences between normotensive and hypertensive recordings. We found a clear difference in Renyi entropy between normotensive and hypertensive blood flow recordings at the whole kidney level for both forced (p < 0.037) and spontaneous arterial pressure fluctuations (p < 0.033), and at the single-nephron level (p < 0.008). Especially at the single-nephron level, the mean Renyi entropy is significantly larger for hypertensive than normotensive rats, suggesting more complex dynamics in the hypertensive condition. To further evaluate whether or not the separation of dynamics between normotensive and hypertensive rats is found in the prescribed frequency ranges of the myogenic and TGF mechanisms, we employed multiresolution wavelet transform. Our analysis revealed that exclusively over scale ranges corresponding to the frequency intervals of the myogenic and TGF mechanisms, the widths of the blood flow wavelet coefficients fall into disjoint sets for normotensive and hypertensive rats. The separation of the scales at the myogenic and TGF frequency ranges is distinct and obtained with 100% accuracy. However, this observation remains valid only for the whole kidney blood pressure/flow data. The results suggest that understanding of the time-varying properties of the two mechanisms is required for a complete description of renal autoregulation", "id": "475", "src": "time varying properties of renal autoregulatory mechanisms . in order to assess the possible time varying properties of renal autoregulation , time frequency and time scaling methods were applied to renal blood flow under broad band forced arterial blood pressure fluctuations and single nephron renal blood flow with spontaneous oscillations obtained from normotensive ( sprague dawley , wistar , and long evans ) rats , and spontaneously hypertensive rats . time frequency analyses of normotensive and hypertensive blood flow data obtained from either the whole kidney or the single nephron show that indeed both the myogenic and tubuloglomerular feedback ( tgf ) mechanisms have time varying characteristics . furthermore , we utilized the renyi entropy to measure the complexity of blood flow dynamics in the time frequency plane in an effort to discern differences between normotensive and hypertensive recordings . we found a clear difference in renyi entropy between normotensive and hypertensive blood flow recordings at the whole kidney level for both forced ( p < 0 . <digit> ) and spontaneous arterial pressure fluctuations ( p < 0 . <digit> ) , and at the single nephron level ( p < 0 . <digit> ) . especially at the single nephron level , the mean renyi entropy is significantly larger for hypertensive than normotensive rats , suggesting more complex dynamics in the hypertensive condition . to further evaluate whether or not the separation of dynamics between normotensive and hypertensive rats is found in the prescribed frequency ranges of the myogenic and tgf mechanisms , we employed multiresolution wavelet transform . our analysis revealed that exclusively over scale ranges corresponding to the frequency intervals of the myogenic and tgf mechanisms , the widths of the blood flow wavelet coefficients fall into disjoint sets for normotensive and hypertensive rats . the separation of the scales at the myogenic and tgf frequency ranges is distinct and obtained with <digit> accuracy . however , this observation remains valid only for the whole kidney blood pressure flow data . the results suggest that understanding of the time varying properties of the two mechanisms is required for a complete description of renal autoregulation"}
{"title": "The use of the SPSA method in ECG analysis", "abstract": "The classification, monitoring, and compression of electrocardiogram (ECG) signals recorded of a single patient over a relatively long period of time is considered. The particular application we have in mind is high-resolution ECG analysis, such as late potential analysis, morphology changes in QRS during arrythmias, T-wave alternants, or the study of drug effects on ventricular activation. We propose to apply a modification of a classical method of cluster analysis or vector quantization. The novelty of our approach is that we use a new distortion measure to quantify the distance of two ECG cycles, and the class-distortion measure is defined using a min-max criterion. The new class-distortion-measure is much more sensitive to outliers than the usual distortion measures using average-distance. The price of this practical advantage is that computational complexity is significantly increased. The resulting nonsmooth optimization problem is solved by an adapted version of the simultaneous perturbation stochastic approximation (SPSA) method of J. Spall (IEEE Trans. Automat. Contr., vol. 37, p. 332-41, Mar. 1992). The main idea is to generate a smooth approximation by a randomization procedure. The viability of the method is demonstrated on both simulated and real data. An experimental comparison with the widely used correlation method is given on real data", "id": "476", "src": "the use of the spsa method in ecg analysis . the classification , monitoring , and compression of electrocardiogram ( ecg ) signals recorded of a single patient over a relatively long period of time is considered . the particular application we have in mind is high resolution ecg analysis , such as late potential analysis , morphology changes in qrs during arrythmias , t wave alternants , or the study of drug effects on ventricular activation . we propose to apply a modification of a classical method of cluster analysis or vector quantization . the novelty of our approach is that we use a new distortion measure to quantify the distance of two ecg cycles , and the class distortion measure is defined using a min max criterion . the new class distortion measure is much more sensitive to outliers than the usual distortion measures using average distance . the price of this practical advantage is that computational complexity is significantly increased . the resulting nonsmooth optimization problem is solved by an adapted version of the simultaneous perturbation stochastic approximation ( spsa ) method of j . spall ( ieee trans . automat . contr . , vol . <digit> , p . <digit> <digit> , mar . <digit> ) . the main idea is to generate a smooth approximation by a randomization procedure . the viability of the method is demonstrated on both simulated and real data . an experimental comparison with the widely used correlation method is given on real data"}
{"title": "Model intestinal microflora in computer simulation: a simulation and modeling", "abstract": "package for host-microflora interactions The ecology of the human intestinal microflora and its interaction with the host are poorly understood. Though more and more data are being acquired, in part using modern molecular methods, development of a quantitative theory has not kept pace with this increase in observing power. This is in part due to the complexity of the system and to the lack of simulation environments in which to test what the ecological effect of a hypothetical mechanism of interaction would be, before resorting to laboratory experiments. The MIMICS project attempts to address this through the development of a cellular automaton for simulation of the intestinal microflora. In this paper, the design and evaluation of this simulator is discussed", "id": "477", "src": "model intestinal microflora in computer simulation a simulation and modeling . package for host microflora interactions the ecology of the human intestinal microflora and its interaction with the host are poorly understood . though more and more data are being acquired , in part using modern molecular methods , development of a quantitative theory has not kept pace with this increase in observing power . this is in part due to the complexity of the system and to the lack of simulation environments in which to test what the ecological effect of a hypothetical mechanism of interaction would be , before resorting to laboratory experiments . the mimics project attempts to address this through the development of a cellular automaton for simulation of the intestinal microflora . in this paper , the design and evaluation of this simulator is discussed"}
{"title": "Conformal-mapping design tools for coaxial couplers with complex cross section", "abstract": "Numerical conformal mapping is exploited as a simple, accurate, and efficient tool for the analysis and design of coaxial waveguides and couplers of complex cross section. An implementation based on the Schwarz-Christoffel Toolbox, a public-domain MATLAB package, is applied to slotted coaxial cables and to symmetrical coaxial couplers, with circular or polygonal inner conductors and external shields. The effect of metallic diaphragms of arbitrary thickness, partially separating the inner conductors, is also easily taken into account. The proposed technique is validated against the results of the finite-element method, showing excellent agreement at a fraction of the computational cost, and is also extended to the case of nonsymmetrical couplers, providing the designer with important additional degrees of freedom", "id": "478", "src": "conformal mapping design tools for coaxial couplers with complex cross section . numerical conformal mapping is exploited as a simple , accurate , and efficient tool for the analysis and design of coaxial waveguides and couplers of complex cross section . an implementation based on the schwarz christoffel toolbox , a public domain matlab package , is applied to slotted coaxial cables and to symmetrical coaxial couplers , with circular or polygonal inner conductors and external shields . the effect of metallic diaphragms of arbitrary thickness , partially separating the inner conductors , is also easily taken into account . the proposed technique is validated against the results of the finite element method , showing excellent agreement at a fraction of the computational cost , and is also extended to the case of nonsymmetrical couplers , providing the designer with important additional degrees of freedom"}
{"title": "Convolution-based global simulation technique for millimeter-wave photodetector", "abstract": "and photomixer circuits A fast convolution-based time-domain approach to global photonic-circuit simulation is presented that incorporates a physical device model in the complete detector or mixer circuit. The device used in the demonstration of this technique is a GaAs metal-semiconductor-metal (MSM) photodetector that offers a high response speed for the detection and generation of millimeter waves. Global simulation greatly increases the accuracy in evaluating the complete circuit performance because it accounts for the effects of the millimeter-wave embedding circuit. Device and circuit performance are assessed by calculating optical responsivity and bandwidth. Device-only simulations using GaAs MSMs are compared with global simulations that illustrate the strong interdependence between device and external circuit", "id": "479", "src": "convolution based global simulation technique for millimeter wave photodetector . and photomixer circuits a fast convolution based time domain approach to global photonic circuit simulation is presented that incorporates a physical device model in the complete detector or mixer circuit . the device used in the demonstration of this technique is a gaas metal semiconductor metal ( msm ) photodetector that offers a high response speed for the detection and generation of millimeter waves . global simulation greatly increases the accuracy in evaluating the complete circuit performance because it accounts for the effects of the millimeter wave embedding circuit . device and circuit performance are assessed by calculating optical responsivity and bandwidth . device only simulations using gaas msms are compared with global simulations that illustrate the strong interdependence between device and external circuit"}
{"title": "Accurate modeling of lossy nonuniform transmission lines by using differential", "abstract": "quadrature methods This paper discusses an efficient numerical approximation technique, called the differential quadrature method (DQM), which has been adapted to model lossy uniform and nonuniform transmission lines. The DQM can quickly compute the derivative of a function at any point within its bounded domain by estimating a weighted linear sum of values of the function at a small set of points belonging to the domain. Using the DQM, the frequency-domain Telegrapher's partial differential equations for transmission lines can be discretized into a set of easily solvable algebraic equations. DQM reduces interconnects into multiport models whose port voltages and currents are related by rational formulas in the frequency domain. Although the rationalization process in DQM is comparable with the Pade approximation of asymptotic waveform evaluation (AWE) applied to transmission lines, the derivation mechanisms in these two disparate methods are significantly different. Unlike AWE, which employs a complex moment-matching process to obtain rational approximation, the DQM requires no approximation of transcendental functions, thereby avoiding the process of moment generation and moment matching. Due to global sampling of points in the DQM approximation, it requires far fewer grid points in order to build accurate discrete models than other numerical methods do. The DQM-based time-domain model can be readily integrated in a circuit simulator like SPICE", "id": "480", "src": "accurate modeling of lossy nonuniform transmission lines by using differential . quadrature methods this paper discusses an efficient numerical approximation technique , called the differential quadrature method ( dqm ) , which has been adapted to model lossy uniform and nonuniform transmission lines . the dqm can quickly compute the derivative of a function at any point within its bounded domain by estimating a weighted linear sum of values of the function at a small set of points belonging to the domain . using the dqm , the frequency domain telegrapher ' s partial differential equations for transmission lines can be discretized into a set of easily solvable algebraic equations . dqm reduces interconnects into multiport models whose port voltages and currents are related by rational formulas in the frequency domain . although the rationalization process in dqm is comparable with the pade approximation of asymptotic waveform evaluation ( awe ) applied to transmission lines , the derivation mechanisms in these two disparate methods are significantly different . unlike awe , which employs a complex moment matching process to obtain rational approximation , the dqm requires no approximation of transcendental functions , thereby avoiding the process of moment generation and moment matching . due to global sampling of points in the dqm approximation , it requires far fewer grid points in order to build accurate discrete models than other numerical methods do . the dqm based time domain model can be readily integrated in a circuit simulator like spice"}
{"title": "An unconditionally stable extended (USE) finite-element time-domain solution of", "abstract": "active nonlinear microwave circuits using perfectly matched layers This paper proposes an extension of the unconditionally stable finite-element time-domain (FETD) method for the global electromagnetic analysis of active microwave circuits. This formulation has two advantages. First, the time-step size is no longer governed by the spatial discretization of the mesh, but rather by the Nyquist sampling criterion. Second, the implementation of the truncation by the perfectly matched layers (PML) is straightforward. An anisotropic PML absorbing material is presented for the truncation of FETD lattices. Reflection less than -50 dB is obtained numerically over the entire propagation bandwidth in waveguide and microstrip line. A benchmark test on a microwave amplifier indicates that this extended FETD algorithm is not only superior to finite-difference time-domain-based algorithm in mesh flexibility and simulation accuracy, but also reduces computation time dramatically", "id": "481", "src": "an unconditionally stable extended ( use ) finite element time domain solution of . active nonlinear microwave circuits using perfectly matched layers this paper proposes an extension of the unconditionally stable finite element time domain ( fetd ) method for the global electromagnetic analysis of active microwave circuits . this formulation has two advantages . first , the time step size is no longer governed by the spatial discretization of the mesh , but rather by the nyquist sampling criterion . second , the implementation of the truncation by the perfectly matched layers ( pml ) is straightforward . an anisotropic pml absorbing material is presented for the truncation of fetd lattices . reflection less than <digit> db is obtained numerically over the entire propagation bandwidth in waveguide and microstrip line . a benchmark test on a microwave amplifier indicates that this extended fetd algorithm is not only superior to finite difference time domain based algorithm in mesh flexibility and simulation accuracy , but also reduces computation time dramatically"}
{"title": "Low-voltage DRAM sensing scheme with offset-cancellation sense amplifier", "abstract": "A novel bitline sensing scheme is proposed for low-voltage DRAM to achieve low power dissipation and compatibility with low-voltage CMOS. One of the major obstacles in low-voltage DRAM is the degradation of data-retention time due to low signal level at the memory cell, which requires power-consuming refresh operations more frequently. This paper proposes an offset-cancellation sense-amplifier scheme (OCSA) that improves data-retention time significantly even at low supply voltage. It also improves die efficiency, because the proposed scheme reduces the number of sense amplifiers by supporting more cells in each sense amplifier. Measurements show that the data-retention time of the proposed scheme at 1.5-V supply voltage is 2.4 times of the conventional scheme at 2.0 V", "id": "482", "src": "low voltage dram sensing scheme with offset cancellation sense amplifier . a novel bitline sensing scheme is proposed for low voltage dram to achieve low power dissipation and compatibility with low voltage cmos . one of the major obstacles in low voltage dram is the degradation of data retention time due to low signal level at the memory cell , which requires power consuming refresh operations more frequently . this paper proposes an offset cancellation sense amplifier scheme ( ocsa ) that improves data retention time significantly even at low supply voltage . it also improves die efficiency , because the proposed scheme reduces the number of sense amplifiers by supporting more cells in each sense amplifier . measurements show that the data retention time of the proposed scheme at 1 . 5 v supply voltage is 2 . 4 times of the conventional scheme at 2 . 0 v"}
{"title": "Industry insiders loading up on cheap company stock", "abstract": "A surge of telecom executives and directors purchasing their own companies, stock in the last two months points toward a renewed optimism in the beleaguered sector, say some observers, who view the rash of insider buying as a vote of confidence from management. Airgate PCS, Charter Communications, Cox Communications, Crown Castle International, Nextel Communications and Nortel Networks all have seen infusions of insider investment this summer, echoing trends in both the telecom industry and the national economy", "id": "483", "src": "industry insiders loading up on cheap company stock . a surge of telecom executives and directors purchasing their own companies , stock in the last two months points toward a renewed optimism in the beleaguered sector , say some observers , who view the rash of insider buying as a vote of confidence from management . airgate pcs , charter communications , cox communications , crown castle international , nextel communications and nortel networks all have seen infusions of insider investment this summer , echoing trends in both the telecom industry and the national economy"}
{"title": "New tuning method for PID controller", "abstract": "In this paper, a tuning method for proportional-integral-derivative (PID) controller and the performance assessment formulas for this method are proposed. This tuning method is based on a genetic algorithm based PID controller design method. For deriving the tuning formula, the genetic algorithm based design method is applied to design PID controllers for a variety of processes. The relationship between the controller parameters and the parameters that characterize the process dynamics are determined and the tuning formula is then derived. Using simulation studies, the rules for assessing the performance of a PID controller tuned by the proposed method are also given. This makes it possible to incorporate the capability to determine if the PID controller is well tuned or not into an autotuner. An autotuner based on this new tuning method and the corresponding performance assessment rules is also established. Simulations and real-time experimental results are given to demonstrate the effectiveness and usefulness of these formulas", "id": "484", "src": "new tuning method for pid controller . in this paper , a tuning method for proportional integral derivative ( pid ) controller and the performance assessment formulas for this method are proposed . this tuning method is based on a genetic algorithm based pid controller design method . for deriving the tuning formula , the genetic algorithm based design method is applied to design pid controllers for a variety of processes . the relationship between the controller parameters and the parameters that characterize the process dynamics are determined and the tuning formula is then derived . using simulation studies , the rules for assessing the performance of a pid controller tuned by the proposed method are also given . this makes it possible to incorporate the capability to determine if the pid controller is well tuned or not into an autotuner . an autotuner based on this new tuning method and the corresponding performance assessment rules is also established . simulations and real time experimental results are given to demonstrate the effectiveness and usefulness of these formulas"}
{"title": "A 120-mW 3-D rendering engine with 6-Mb embedded DRAM and 3.2-GB/s runtime", "abstract": "reconfigurable bus for PDA chip A low-power three-dimensional (3-D) rendering engine is implemented as part of a mobile personal digital assistant (PDA) chip. Six-megabit embedded DRAM macros attached to 8-pixel-parallel rendering logic are logically localized with a 3.2-GB/s runtime reconfigurable bus, reducing the area by 25% compared with conventional local frame-buffer architectures. The low power consumption is achieved by polygon-dependent access to the embedded DRAM macros with line-block mapping providing read-modify-write data transaction. The 3-D rendering engine with 2.22-Mpolygons/s drawing speed was fabricated using 0.18- mu m CMOS embedded memory logic technology. Its area is 24 mm/sup 2/ and its power consumption is 120 mW", "id": "485", "src": "a <digit> mw 3 d rendering engine with 6 mb embedded dram and 3 . 2 gb s runtime . reconfigurable bus for pda chip a low power three dimensional ( 3 d ) rendering engine is implemented as part of a mobile personal digital assistant ( pda ) chip . six megabit embedded dram macros attached to 8 pixel parallel rendering logic are logically localized with a 3 . 2 gb s runtime reconfigurable bus , reducing the area by <digit> compared with conventional local frame buffer architectures . the low power consumption is achieved by polygon dependent access to the embedded dram macros with line block mapping providing read modify write data transaction . the 3 d rendering engine with 2 . <digit> mpolygons s drawing speed was fabricated using 0 . <digit> mu m cmos embedded memory logic technology . its area is <digit> mm sup 2 and its power consumption is <digit> mw"}
{"title": "A digital-to-analog converter based on differential-quad switching", "abstract": "A high-conversion-rate high-resolution oversampling digital-to-analog converter (DAC) for direct digital modulation is addressed in this paper. A new type of switching scheme, called differential-quad switching, is presented. To verify the feasibility of this scheme, essential parts with some auxiliary circuitry for interfacing were fabricated in a 0.8- mu m CMOS technology. Measured results show that the switching scheme provides 11-b resolution at 100 MSamples/s and 6-b at 1 GSamples/s. The degradation in signal-to-noise ratio is not observed for the variation of the supply voltage down to 1.5 V, which means the proposed scheme is suitable for low-voltage applications", "id": "486", "src": "a digital to analog converter based on differential quad switching . a high conversion rate high resolution oversampling digital to analog converter ( dac ) for direct digital modulation is addressed in this paper . a new type of switching scheme , called differential quad switching , is presented . to verify the feasibility of this scheme , essential parts with some auxiliary circuitry for interfacing were fabricated in a 0 . 8 mu m cmos technology . measured results show that the switching scheme provides <digit> b resolution at <digit> msamples s and 6 b at 1 gsamples s . the degradation in signal to noise ratio is not observed for the variation of the supply voltage down to 1 . 5 v , which means the proposed scheme is suitable for low voltage applications"}
{"title": "Fast frequency acquisition phase-frequency detectors for Gsamples/s", "abstract": "phase-locked loops This paper describes two techniques for designing phase-frequency detectors (PFDs) with higher operating frequencies [periods of less than 8* the delay of a fan-out-4 inverter (FO-4)] and faster frequency acquisition. Prototypes designed in 0.25- mu m CMOS process exhibit operating frequencies of 1.25 GHz [=1/(8.FO-4)] and 1.5 GHz [=1/(6.7.FO-4)] for two techniques, respectively, whereas a conventional PFD operates at <1 GHz [=1/(10.FO-4)]. The two proposed PFDs achieve a capture range of 1.7* and 1.4* the conventional design, respectively", "id": "487", "src": "fast frequency acquisition phase frequency detectors for gsamples s . phase locked loops this paper describes two techniques for designing phase frequency detectors ( pfds ) with higher operating frequencies periods of less than 8* the delay of a fan out 4 inverter ( fo 4 ) and faster frequency acquisition . prototypes designed in 0 . <digit> mu m cmos process exhibit operating frequencies of 1 . <digit> ghz 1 ( 8 . fo 4 ) and 1 . 5 ghz 1 ( 6 . 7 . fo 4 ) for two techniques , respectively , whereas a conventional pfd operates at < 1 ghz 1 ( <digit> . fo 4 ) . the two proposed pfds achieve a capture range of 1 . 7* and 1 . 4* the conventional design , respectively"}
{"title": "High-voltage transistor scaling circuit techniques for high-density", "abstract": "negative-gate channel-erasing NOR flash memories In order to scale high-voltage transistors for high-density negative-gate channel-erasing NOR flash memories, two circuit techniques were developed. A proposed level shifter with low operating voltage is composed of three parts, a latch holding the negative erasing voltage, two coupling capacitors connected with the latched nodes in the latch, and high-voltage drivers inverting the latch, resulting in reduction of the maximum internal voltage by 0.5 V. A proposed high-voltage generator adds a path-gate logic to a conventional high-voltage generator to realize both low noise and low ripple voltage, resulting in a reduction of the maximum internal voltage by 0.5 V. As a result, these circuit techniques along with high coupling-ratio cell technology can scale down the high-voltage transistors by 15% and can realize higher density negative-gate channel-erase NOR flash memories in comparison with the source-erase NOR flash memories", "id": "488", "src": "high voltage transistor scaling circuit techniques for high density . negative gate channel erasing nor flash memories in order to scale high voltage transistors for high density negative gate channel erasing nor flash memories , two circuit techniques were developed . a proposed level shifter with low operating voltage is composed of three parts , a latch holding the negative erasing voltage , two coupling capacitors connected with the latched nodes in the latch , and high voltage drivers inverting the latch , resulting in reduction of the maximum internal voltage by 0 . 5 v . a proposed high voltage generator adds a path gate logic to a conventional high voltage generator to realize both low noise and low ripple voltage , resulting in a reduction of the maximum internal voltage by 0 . 5 v . as a result , these circuit techniques along with high coupling ratio cell technology can scale down the high voltage transistors by <digit> and can realize higher density negative gate channel erase nor flash memories in comparison with the source erase nor flash memories"}
{"title": "A 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using", "abstract": "two-stage wordline/bitline-oriented tag-compare (WLOTC/BLOTC) scheme This paper reports a 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using a novel two-stage wordline/bitline-oriented tag-compare (WLOTC/BLOTC) and sense wordline/bitline (SWL/SBL) tag-sense amplifiers with an eight-transistor (8-T) tag cell in Level 2 (L2) and a 10-T shrunk logic swing (SLS) memory cell. with the ground/floating (G/F) data sense amplifier in Level 1 (L1) for high-speed operation for low-voltage low-power VLSI system applications. Owing to the reduced loading at the SWL in the new 11-T tag cell using the WLOTC scheme, the 10-T SLS memory cell with G/F sense amplifier in L1, and the split comparison of the index signal in the 8-T tag cells with SWL/SBL tag sense amplifiers in L2, this 0.8-V cache memory implemented in a 1.8-V 0.18- mu m CMOS technology has a measured L1/L2 hit time of 11.6/20.5 ns at the average dissipation of 0.77 mW at 50 MHz", "id": "489", "src": "a 0 . 8 v <digit> kb four way set associative two level cmos cache memory using . two stage wordline bitline oriented tag compare ( wlotc blotc ) scheme this paper reports a 0 . 8 v <digit> kb four way set associative two level cmos cache memory using a novel two stage wordline bitline oriented tag compare ( wlotc blotc ) and sense wordline bitline ( swl sbl ) tag sense amplifiers with an eight transistor ( 8 t ) tag cell in level 2 ( l2 ) and a <digit> t shrunk logic swing ( sls ) memory cell . with the ground floating ( g f ) data sense amplifier in level 1 ( l1 ) for high speed operation for low voltage low power vlsi system applications . owing to the reduced loading at the swl in the new <digit> t tag cell using the wlotc scheme , the <digit> t sls memory cell with g f sense amplifier in l1 , and the split comparison of the index signal in the 8 t tag cells with swl sbl tag sense amplifiers in l2 , this 0 . 8 v cache memory implemented in a 1 . 8 v 0 . <digit> mu m cmos technology has a measured l1 l2 hit time of <digit> . 6 <digit> . 5 ns at the average dissipation of 0 . <digit> mw at <digit> mhz"}
{"title": "Learning spatial relations using an inductive logic programming system", "abstract": "The ability to learn spatial relations is a prerequisite for performing many relevant tasks such as those associated with motion, orientation, navigation, etc. This paper reports on using an Inductive Logic Programming (ILP) system for learning function-free Horn-clause descriptions of spatial knowledge. Its main contribution, however, is to show that an existing relation between two reference systems-the speaker-relative and the absolute-can be automatically learned by an ILP system, given the proper background knowledge and positive examples", "id": "490", "src": "learning spatial relations using an inductive logic programming system . the ability to learn spatial relations is a prerequisite for performing many relevant tasks such as those associated with motion , orientation , navigation , etc . this paper reports on using an inductive logic programming ( ilp ) system for learning function free horn clause descriptions of spatial knowledge . its main contribution , however , is to show that an existing relation between two reference systems the speaker relative and the absolute can be automatically learned by an ilp system , given the proper background knowledge and positive examples"}
{"title": "Windows XP fast user switching", "abstract": "The Windows NT family of operating systems has always supported the concept of multiple user accounts, but they've taken the concept a step further with Windows XP's Fast User Switching feature. Fast User Switching is a new feature of Windows XP that allows multiple users to log on to the same machine and quickly switch between the logged on accounts. Fast User Switching is implemented using some of the built-in capabilities of Terminal Services. Terminal Server has been around for a while but is much more feature rich and integrated in Windows XP. A machine with the terminal services (Remote Desktop) client can log on to and run applications on a remote machine running the terminal server", "id": "491", "src": "windows xp fast user switching . the windows nt family of operating systems has always supported the concept of multiple user accounts , but they ' ve taken the concept a step further with windows xp ' s fast user switching feature . fast user switching is a new feature of windows xp that allows multiple users to log on to the same machine and quickly switch between the logged on accounts . fast user switching is implemented using some of the built in capabilities of terminal services . terminal server has been around for a while but is much more feature rich and integrated in windows xp . a machine with the terminal services ( remote desktop ) client can log on to and run applications on a remote machine running the terminal server"}
{"title": "Generating code at run time with Reflection.Emit", "abstract": "The .NET framework SDK includes several tools that convert source code into executable code-the C# and VB.NET compilers get most of the attention, but there are others. The Regex class (in the System.Text.RegularExpressions namespace) has the ability to compile favorite regular expressions into a .NET assembly. In fact, the NET Common Language Runtime (CLR) contains a whole namespace full of classes to help us build assemblies, define types, and emit their implementations, all at run time. These classes, which comprise the System.Reflection.Emit namespace, are known collectively as Reflection. Emit", "id": "492", "src": "generating code at run time with reflection . emit . the . net framework sdk includes several tools that convert source code into executable code the c# and vb . net compilers get most of the attention , but there are others . the regex class ( in the system . text . regularexpressions namespace ) has the ability to compile favorite regular expressions into a . net assembly . in fact , the net common language runtime ( clr ) contains a whole namespace full of classes to help us build assemblies , define types , and emit their implementations , all at run time . these classes , which comprise the system . reflection . emit namespace , are known collectively as reflection . emit"}
{"title": ".NET obfuscation and intellectual property", "abstract": "The author considers obfuscation options for protecting .NET code. Many programs won't need obfuscation because the loss caused by reverse engineering will be nonexistent. Numerous obfuscators are already available for the .NET platform, ranging from a basic renaming obfuscator to a fully functional obfuscator that handles mixed IL/native code assemblies created in any managed language, including Microsoft's C++ with Managed Extensions. An obfuscator simply makes your application harder to reverse engineer. It does not prevent reverse engineering. However, the cost of obfuscation is insignificant when compared to the cost of a typical software development project. If you feel like an obfuscator provides you any benefit at all, it's probably worth the price", "id": "493", "src": ". net obfuscation and intellectual property . the author considers obfuscation options for protecting . net code . many programs won ' t need obfuscation because the loss caused by reverse engineering will be nonexistent . numerous obfuscators are already available for the . net platform , ranging from a basic renaming obfuscator to a fully functional obfuscator that handles mixed il native code assemblies created in any managed language , including microsoft ' s c++ with managed extensions . an obfuscator simply makes your application harder to reverse engineer . it does not prevent reverse engineering . however , the cost of obfuscation is insignificant when compared to the cost of a typical software development project . if you feel like an obfuscator provides you any benefit at all , it ' s probably worth the price"}
{"title": "Controller performance analysis with LQG benchmark obtained under closed loop", "abstract": "conditions This paper proposes a new method for obtaining a linear quadratic Gaussian (LQG) benchmark in terms of the variances of process input and output from closed-loop data, for assessing the controller performance. LQG benchmark has been proposed in the literature to assess controller performance since the LQG tradeoff curve represents the limit of performance in terms of input and output variances. However, an explicit parametric model is required to calculate the LQG benchmark. In this work, we propose a data driven subspace approach to calculate the LQG benchmark under closed-loop conditions with certain external excitations. The optimal LQG-benchmark variances are obtained directly from the subspace matrices corresponding to the deterministic inputs and the stochastic inputs, which are identified using closed-loop data with setpoint excitation. These variances are used for assessing the controller performance. The method proposed in this paper is applicable to both univariate and multivariate systems. Profit analysis for the implementation of feedforward control to the existing feedback-only control system is also analyzed under the optimal LQG performance framework", "id": "494", "src": "controller performance analysis with lqg benchmark obtained under closed loop . conditions this paper proposes a new method for obtaining a linear quadratic gaussian ( lqg ) benchmark in terms of the variances of process input and output from closed loop data , for assessing the controller performance . lqg benchmark has been proposed in the literature to assess controller performance since the lqg tradeoff curve represents the limit of performance in terms of input and output variances . however , an explicit parametric model is required to calculate the lqg benchmark . in this work , we propose a data driven subspace approach to calculate the lqg benchmark under closed loop conditions with certain external excitations . the optimal lqg benchmark variances are obtained directly from the subspace matrices corresponding to the deterministic inputs and the stochastic inputs , which are identified using closed loop data with setpoint excitation . these variances are used for assessing the controller performance . the method proposed in this paper is applicable to both univariate and multivariate systems . profit analysis for the implementation of feedforward control to the existing feedback only control system is also analyzed under the optimal lqg performance framework"}
{"title": "Lossy SPICE models produce realistic averaged simulations", "abstract": "In previous averaged models, the state-space averaging technique or switch waveforms analysis were usually applied over perfect elements, non-inclusive of the ohmic losses. However, if these elements play an active role in the DC transfer function, they affect the small-signal AC analysis by introducing various damping effects. A model is introduced in a boost voltage-mode application", "id": "495", "src": "lossy spice models produce realistic averaged simulations . in previous averaged models , the state space averaging technique or switch waveforms analysis were usually applied over perfect elements , non inclusive of the ohmic losses . however , if these elements play an active role in the dc transfer function , they affect the small signal ac analysis by introducing various damping effects . a model is introduced in a boost voltage mode application"}
{"title": "CAD/CAE software aids converter design [DC/DC power conversion]", "abstract": "Typically, power supply design involves electronic and magnetic components. In this paper, the authors describe, using a flyback converter example, how CAD/CAE tools can aid the power supply engineer in both areas, reducing prototyping costs and providing insights into system performance", "id": "496", "src": "cad cae software aids converter design dc dc power conversion . typically , power supply design involves electronic and magnetic components . in this paper , the authors describe , using a flyback converter example , how cad cae tools can aid the power supply engineer in both areas , reducing prototyping costs and providing insights into system performance"}
{"title": "Using virtual reality to teach disability awareness", "abstract": "A desktop virtual reality (VR) program was designed and evaluated to teach children about the accessibility and attitudinal barriers encountered by their peers with mobility impairments. Within this software, children sitting in a virtual wheelchair experience obstacles such as stairs, narrow doors, objects too high to reach, and attitudinal barriers such as inappropriate comments. Using a collaborative research methodology, 15 youth with mobility impairments assisted in developing and beta-testing the software. The effectiveness of the program was then evaluated with 60 children in Grades 4-6 using a controlled pretest/posttest design. The results indicated that the program was effective for increasing children's knowledge of accessibility barriers. Attitudes, grade level, familiarity with individuals with a disability, and gender were also investigated", "id": "497", "src": "using virtual reality to teach disability awareness . a desktop virtual reality ( vr ) program was designed and evaluated to teach children about the accessibility and attitudinal barriers encountered by their peers with mobility impairments . within this software , children sitting in a virtual wheelchair experience obstacles such as stairs , narrow doors , objects too high to reach , and attitudinal barriers such as inappropriate comments . using a collaborative research methodology , <digit> youth with mobility impairments assisted in developing and beta testing the software . the effectiveness of the program was then evaluated with <digit> children in grades 4 6 using a controlled pretest posttest design . the results indicated that the program was effective for increasing children ' s knowledge of accessibility barriers . attitudes , grade level , familiarity with individuals with a disability , and gender were also investigated"}
{"title": "Effects of white space in learning via the Web", "abstract": "This study measured the effect of specific white space features on learning from instructional Web materials. The study also measured learners' beliefs regarding Web-based instruction. Prior research indicated that small changes in the handling of presentation elements can affect learning. Achievement results from this study indicated that in on-line materials, when content and overall structure are sound, minor differences regarding table borders and vertical spacing in text do not hinder learning. Beliefs regarding Web-based instruction and instructors who use it did not differ significantly between treatment groups. Implications of the study and cautions regarding generalizing from the results are discussed", "id": "498", "src": "effects of white space in learning via the web . this study measured the effect of specific white space features on learning from instructional web materials . the study also measured learners ' beliefs regarding web based instruction . prior research indicated that small changes in the handling of presentation elements can affect learning . achievement results from this study indicated that in on line materials , when content and overall structure are sound , minor differences regarding table borders and vertical spacing in text do not hinder learning . beliefs regarding web based instruction and instructors who use it did not differ significantly between treatment groups . implications of the study and cautions regarding generalizing from the results are discussed"}
{"title": "The efficacy of electronic telecommunications in fostering interpersonal", "abstract": "relationships The effectiveness of electronic telecommunications as a supplementary aid to instruction and as a communication link between students, and between students and instructors in fostering interpersonal relationships was explored in this study. More specifically, the impacts of e-mail, one of the most accessible, convenient, and easy to use computer-mediated communications, on student attitudes toward the instructor, group-mates, and other classmates were investigated. A posttest-only experimental design was adopted. In total, 68 prospective teachers enrolling in a \"Computers in Education\" course participated in the study for a whole semester. Results from the study provided substantial evidence supporting e-mail's beneficial effects on student attitudes toward the instructor and other classmates", "id": "499", "src": "the efficacy of electronic telecommunications in fostering interpersonal . relationships the effectiveness of electronic telecommunications as a supplementary aid to instruction and as a communication link between students , and between students and instructors in fostering interpersonal relationships was explored in this study . more specifically , the impacts of e mail , one of the most accessible , convenient , and easy to use computer mediated communications , on student attitudes toward the instructor , group mates , and other classmates were investigated . a posttest only experimental design was adopted . in total , <digit> prospective teachers enrolling in a computers in education course participated in the study for a whole semester . results from the study provided substantial evidence supporting e mail ' s beneficial effects on student attitudes toward the instructor and other classmates"}
