	sentences	label	title	language	index
0	"The main result of this paper is an explicit disperser for two independent sources on n bits, each of entropy k = n o(1). Put differently, setting N = 2n and K = 2k , we construct explicit N  N Boolean matrices for which no K  K sub-matrix is monochromatic. Viewed as adjacency matrices of bipartite graphs, this gives an explicit construction of K-Ramsey bipartite graphs of size N . This greatly improves the previous bound of k = o(n) of Barak, Kindler, Shaltiel, Sudakov and Wigderson [4]. It also significantly improves the 25-year record of k = ~ O(n) on the special case of Ramsey graphs, due to Frankl and Wilson [9]. The construction uses (besides ""classical"" extractor ideas) almost all of the machinery developed in the last couple of years for extraction from independent sources, including: Bourgain's extractor for 2 independent sources of some entropy rate &lt; 1/2 [5] Raz's extractor for 2 independent sources, one of which has any entropy rate &gt; 1/2 [18] Rao's extractor for 2 independent block-sources of entropy n (1) [17] The ""Challenge-Response"" mechanism for detecting ""entropy concentration"" of [4]. The main novelty comes in a bootstrap procedure which allows the Challenge-Response mechanism of [4] to be used with sources of less and less entropy, using recursive calls to itself. Subtleties arise since the success of this mechanism depends on restricting the given sources, and so recursion constantly changes the original sources. These are resolved via a new construct, in between a disperser and an extractor, which behaves like an extractor on sufficiently large subsources of the given ones. This version is only an extended abstract, please see the full version, available on the authors' homepages, for more details."	sum-product theorem , distribution , explicit disperser , construction of disperser , Extractors , recursion , subsource somewhere extractor , structure , bipartite graph , extractors , independent sources , extractor , tools , Ramsey Graphs , disperser , polynomial time computable disperser , resiliency , Theorem , Ramsey graphs , block-sources , deficiency , termination , entropy , Ramsey graph , Independent Sources , algorithms , independent source , subsource , Dispersers , randomness extraction	2-Source Dispersers for Sub-Polynomial Entropy and Ramsey Graphs Beating the Frankl-Wilson Construction	en	0
1	This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ). We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events. By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative. The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.	inverse document frequency (idf) , independent and disjoint documents , computer science , information search , probability theories , Poisson based probability , Term frequency , probabilistic retrieval models , Probability of being informative , Independent documents , Disjoint documents , Normalisation , relevance-based ranking of retrieved objects , information theory , Noise probability , frequency-based term noise probability , Poisson-based probability of being informative , Assumptions , Collection space , Poisson distribution , Probabilistic information retrieval , Document space , document retrieval , entropy , Frequency-based probability , Document frequency , Inverse document frequency , Information theory , independence assumption , inverse document frequency , maximal informative signal	A Frequency-based and a Poisson-based Definition of the Probability of Being Informative	en	1
2	In the present paper, we will describe the design and implementation of a real-time distributed system of Web crawling running on a cluster of machines. The system crawls several thousands of pages every second, includes a high-performance fault manager, is platform independent and is able to adapt transparently to a wide range of configurations without incurring additional hardware expenditure. We will then provide details of the system architecture and describe the technical choices for very high performance crawling. Finally, we will discuss the experimental results obtained, comparing them with other documented systems.	Breadth first crawling , Hierarchical Cooperation , limiting disk access , fault tolerance , Dominos nodes , dominos process , Dominos distributed database , breadth-first crawling , repetitive crawling , URL caching , Dominos Generic server , Document fingerprint , Deep web crawling , Dominos RPC concurrent , Random walks and sampling , Web Crawler , maintaiability and configurability , deep web crawling , High Availability System , real-time distributed system , crawling system , high performance crawling system , high availability , Erlang development kit , targeted crawling	High Performance Crawling System	en	2
3	This paper presents a technical overview of the Hiperlan/2 3G interworking concept. It does not attempt to provide any business justification or plan for Public Access operation. After a brief resume of public access operation below, section 2 then introduces an overview of the technologies concerned. Section 3 describes the system approach and presents the current reference architecture used within the BRAN standardisation activity. Section 4 then goes on to cover in more detail the primary functions of the system such as authentication, mobility, quality of service (QoS) and subscription. It is worth noting that since the Japanese WLAN standard HiSWANa is very similar to Hiperlan/2, much of the technical information within this paper is directly applicable to this system, albeit with some minor changes to the authentication scheme. Additionally the high level 3G and external network interworking reference architecture is also applicable to IEEE 802.11. Finally, section 5 briefly introduces the standardisation relationships between ETSI BRAN, WIG, 3GPP, IETF, IEEE 802.11 and MMAC HSWA.	Hiperlan/2 , interworking , 3G , ETSI , BRAN , WIG , public access	Hiperlan/2 Public Access Interworking with 3G Cellular Systems	en	3
4	Many exploration and manipulation tasks benefit from a coherent integration of multiple views onto complex information spaces. This paper proposes the concept of Illustrative Shadows for a tight integration of interactive 3D graphics and schematic depictions using the shadow metaphor. The shadow metaphor provides an intuitive visual link between 3D and 2D visualizations integrating the different displays into one combined information display. Users interactively explore spatial relations in realistic shaded virtual models while functional correlations and additional textual information are presented on additional projection layers using a semantic network approach. Manipulations of one visualization immediately influence the others, resulting in an in-formationally and perceptibly coherent presentation.	Information visualization , Spreading activation	2D Information Displays	en	4
5	"The current boom of the Web is associated with the revenues originated from on-line advertising. While search-based advertising is dominant, the association of ads with a Web page (during user navigation) is becoming increasingly important . In this work, we study the problem of associating ads with a Web page, referred to as content-targeted advertising , from a computer science perspective. We assume that we have access to the text of the Web page, the keywords declared by an advertiser, and a text associated with the advertiser's business. Using no other information and operating in fully automatic fashion, we propose ten strategies for solving the problem and evaluate their effectiveness. Our methods indicate that a matching strategy that takes into account the semantics of the problem (referred to as AAK for ""ads and keywords"") can yield gains in average precision figures of 60% compared to a trivial vector-based strategy. Further, a more sophisticated impedance coupling strategy, which expands the text of the Web page to reduce vocabulary impedance with regard to an advertisement, can yield extra gains in average precision of 50%. These are first results . They suggest that great accuracy in content-targeted advertising can be attained with appropriate algorithms."	 , advertisements , triggering page , Bayesian networks , Advertising , matching , kNN , Web , content-targeted advertising , impedance coupling	Impedance Coupling in Content-targeted Advertising	en	5
6	The recently promulgated IT model curriculum contains IT fundamentals as one of its knowledge areas. It is intended to give students a broad understanding of (1) the IT profession and the skills that students must develop to become successful IT professionals and (2) the academic discipline of IT and its relationship to other disciplines. As currently defined, the IT fundamentals knowledge area requires 33 lecture hours to complete. The model curriculum recommends that the material relevant to the IT fundamentals knowledge area be offered early in the curriculum, for example in an introduction to IT course; however, many institutions will have to include additional material in an introductory IT course. For example, the Introduction of IT course at Georgia Southern University is used to introduce students to the available second disciplines (an important part of the Georgia Southern IT curriculum aimed at providing students with in-depth knowledge of an IT application domain), some productivity tools, and SQL. For many programs there may be too much material in an introductory IT course. This paper describes how Georgia Southern University resolved this dilemma.	IT Fundamentals Knowledge Area , IT Model Curriculum	Implementing the IT Fundamentals Knowledge Area	en	6
7	"Information retrieval systems (e.g., web search engines) are critical for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. For example, a tourist and a programmer may use the same word ""java"" to search for different information, but the current search systems would return the same results. In this paper, we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search . We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback, e.g., query expansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine."	user model , interactive retrieval , personalized search , information retrieval systems , user modelling , implicit feedback , retrieval accuracy , clickthrough information , UCAIR	Implicit User Modeling for Personalized Search	en	7
8	Nearest neighbour (NN) searches and k nearest neighbour (k-NN) searches are widely used in pattern recognition and image retrieval. An NN (k-NN) search finds the closest object (closest k objects) to a query object. Although the definition of the distance between objects depends on applications, its computation is generally complicated and time-consuming. It is therefore important to reduce the number of distance computations. TLAESA (Tree Linear Approximating and Eliminating Search Algorithm) is one of the fastest algorithms for NN searches. This method reduces distance computations by using a branch and bound algorithm. In this paper we improve both the data structure and the search algorithm of TLAESA. The proposed method greatly reduces the number of distance computations. Moreover, we extend the improved method to an approximation search algorithm which ensures the quality of solutions. Experimental results show that the proposed method is efficient and finds an approximate solution with a very low error rate.	Approximation Search , TLAESA , Distance Computaion , k Nearest Neighbour Search , Nearest Neighbour Search	Improvements of TLAESA Nearest Neighbour Search Algorithm and Extension to Approximation Search	en	8
9	Programs in embedded languages contain invariants that are not automatically detected or enforced by their host language. We show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language . We demonstrate the effectiveness of this technique in improving the results of a value flow analysis.	macros , interpreter , value flow analysis , flow analysis , set-based analysis , partial evaluation , embedded language , Partial evaluation , regular expression , embedded languages , Scheme	Improving the Static Analysis of Embedded Languages via Partial Evaluation	en	9
10	Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow, or when some new sequences are added into the database. Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates . However, it is nontrivial to mine sequential patterns incrementally, especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study, we develop an efficient algorithm, IncSpan, for incremental mining of sequential patterns, by exploring some interesting properties. Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin.	database updates , sequence database , shared projection , frequent itemsets , optimization , buffering pattern , sequential pattern , buffering patterns , reverse pattern matching , incremental mining	IncSpan: Incremental Mining of Sequential Patterns in Large Database	en	10
11	A technical infrastructure for storing, querying and managing RDF data is a key element in the current semantic web development. Systems like Jena, Sesame or the ICS-FORTH RDF Suite are widely used for building semantic web applications. Currently, none of these systems supports the integrated querying of distributed RDF repositories. We consider this a major shortcoming since the semantic web is distributed by nature. In this paper we present an architecture for querying distributed RDF repositories by extending the existing Sesame system. We discuss the implications of our architecture and propose an index structure as well as algorithms for query processing and optimization in such a distributed context.	index structure , external sources , query optimization , distributed architecture , repositories , RDF , infrastructure , RDF Querying , Optimization , Index Structures , semantic web , join ordering problem	Index Structures and Algorithms for Querying Distributed RDF Repositories	en	11
12	We bridge the gap between functional evaluators and abstract machines for the λ-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization. We illustrate this approach by deriving Krivine's abstract machine from an ordinary call-by-name evaluator and by deriving an ordinary call-by-value evaluator from Felleisen et al.'s CEK machine. The first derivation is strikingly simpler than what can be found in the literature. The second one is new. Together, they show that Krivine's abstract machine and the CEK machine correspond to the call-by-name and call-by-value facets of an ordinary evaluator for the λ-calculus. We then reveal the denotational content of Hannan and Miller's CLS machine and of Landin's SECD machine. We formally compare the corresponding evaluators and we illustrate some degrees of freedom in the design spaces of evaluators and of abstract machines for the λ-calculus with computational effects. Finally, we consider the Categorical Abstract Machine and the extent to which it is more of a virtual machine than an abstract machine	call-by-name , Interpreters , closure conversion , evaluator , defunctionalization , call-by-value , transformation into continuation-passing style (CPS) , abstract machines , abstract machine	A Functional Correspondence between Evaluators and Abstract Machines	en	12
13	Although most time-series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for a single index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of trajectory similarities. Trajectory datasets are very common in environmental applications, mobility experiments, video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the Longest Common Subsequence (LCSS) model, that offers enhanced robustness, particularly for noisy data, which are encountered very often in real world applications . However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance, and the increasingly popular Dynamic Time Warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW.	Dynamic Time Warping , indexing , trajectory , distance function , Dynamic Time Warping (DTW) , similarity , Longest Common Subsequence , Trajectories , Longest Common Subsequence (LCSS) , measure	Indexing Multi-Dimensional Time-Series with Support for Multiple Distance Measures	en	13
14	INTRODUCTION Typical Web search engines are designed to run short queries against a huge collection of hyperlinked documents quickly and cheaply, and are often tuned for the types of queries people submit most often [2].  Many other types of applications exist for which large, open collections like the Web would be a valuable resource.  However, these applications may require much more advanced support from information retrieval technology than is currently available.  In particular, an application may have to describe more complex information needs, with a varied set of properties and data models, including aspects of the user's context and goals. In this paper we present an overview of one such application, the REAP project, whose main purpose is to provide reader-specific practice for improved reading comprehension.  (REAP stands for REAder-specific Practice.)  A key component of REAP is an advanced search model that can find documents satisfying a set of diverse and possibly complex lexical constraints, including a passage's topic, reading level (e.g. 3rd grade), use of syntax (simple vs. complex sentence structures), and vocabulary that is known or unknown to the student. Searching is performed on a database of documents automatically gathered from the Web which have been analyzed and annotated with a rich set of linguistic metadata. The Web is a potentially valuable resource for providing reading material of interest to the student because of its extent, variety, and currency for popular topics.	computer-assisted learning , user model , searching , reading comprehension , Information retrieval , information retrieval	Information Retrieval for Language Tutoring: An Overview of the REAP Project	en	14
15	Web pages (and resources, in general) can be characterized according to their geographical locality. For example, a web page with general information about wildflowers could be considered a global page, likely to be of interest to a ge-ographically broad audience. In contrast, a web page with listings on houses for sale in a specific city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. Similarly, some search engine queries (implicitly) target global pages, while other queries are after local pages. For example, the best results for query [wildflowers] are probably global pages about wildflowers such as the one discussed above. However, local pages that are relevant to, say, San Francisco are likely to be good matches for a query [houses for sale] that was issued by a San Francisco resident or by somebody moving to that city. Unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub-optimal results. Thus query [wildflowers ] might return pages that discuss wildflowers in specific U.S. states (and not general information about wildflowers), while query [houses for sale] might return pages with real estate listings for locations other than that of interest to the person who issued the query. Deciding whether an unseen query should produce mostly local or global pages--without placing this burden on the search engine users--is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after. In this paper, we address this problem by first defining how to categorize queries according to their (often implicit) geographical locality. We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme, using a variety of state-of-the-art machine learning tools. We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine, and conclude by discussing how our query categorization approach can help improve query result quality.	geographical locality , categorization scheme , query modification , web search , query categorization / query classification , web queries , search engines , global page , local page , information retrieval , search engine , query classification	Categorizing Web Queries According to Geographical Locality	en	15
16	Participation in social networking sites has dramatically increased in recent years. Services such as Friendster, Tribe, or the Facebook allow millions of individuals to create online profiles and share personal information with vast networks of friends - and, often, unknown numbers of strangers. In this paper we study patterns of information revelation in online social networks and their privacy implications. We analyze the online behavior of more than 4,000 Carnegie Mellon University students who have joined a popular social networking site catered to colleges. We evaluate the amount of information they disclose and study their usage of the site's privacy settings. We highlight potential attacks on various aspects of their privacy, and we show that only a minimal percentage of users changes the highly permeable privacy preferences.	information relevation , privacy , social networking sites , information revelation , privacy risk , Online privacy , online social networking , online behavior , college , social network theory , facebook , stalking , re-identification , data visibility , privacy perference	Information Revelation and Privacy in Online Social Networks	en	16
17	"Topic distillation is the process of finding authoritative Web pages and comprehensive ""hubs"" which reciprocally endorse each other and are relevant to a given query. Hyperlink-based topic distillation has been traditionally applied to a macroscopic Web model where documents are nodes in a directed graph and hyperlinks are edges. Macroscopic models miss valuable clues such as banners, navigation panels , and template-based inclusions, which are embedded in HTML pages using markup tags. Consequently, results of macroscopic distillation algorithms have been deteriorating in quality as Web pages are becoming more complex. We propose a uniform fine-grained model for the Web in which pages are represented by their tag trees (also called their Document Object Models or DOMs) and these DOM trees are interconnected by ordinary hyperlinks. Surprisingly, macroscopic distillation algorithms do not work in the fine-grained scenario. We present a new algorithm suitable for the fine-grained model. It can dis-aggregate hubs into coherent regions by segmenting their DOMtrees. Mutual endorsement between hubs and authorities involve these regions , rather than single nodes representing complete hubs. Anecdotes and measurements using a 28-query, 366000-document benchmark suite, used in earlier topic distillation research, reveal two benefits from the new algorithm: distillation quality improves, and a by-product of distillation is the ability to extract relevant snippets from hubs which are only partially relevant to the query."	PageRank algorithm , segmentation , HITS , link localization , Topic distillation , DOM , Document Object Model , XML , microscopic distillation , text analysis , Minimum Description Length principle , Google , hub fragmentation , hyperlink , topic distillation	Integrating the Document Object Model with Hyperlinks for Enhanced Topic Distillation and Information Extraction	en	17
18	"In this paper we present the context of the work of the Curriculum Committee on IT2005, the IT curriculum volume described in the Overview Draft document of the Joint Task Force for Computing Curriculum 2004.  We also provide a brief introduction to the history and work of the Information Assurance Education community. These two perspectives provide the foundation for the main thrust of the paper, which is a description of the Information Assurance and Security (IAS) component of the IT2005 document.  Finally, we end the paper with an example of how IAS is being implemented at BYU as a ""pervasive theme"" that is woven throughout the curriculum and conclude with some observations about the first year's experience."	Information assurance , IT2005 volume , Pervasive Themes , BYU curriculum , NIETP Program , Training standards , In-service training development , Committee on National Security Systems , CITC-1 , Information Technology , IT , CC2005 , IA , SIGITE Curriculum committee , Education , IT2005 , Security Knowledge , Information Assurance , IAS	Integration of Information Assurance and Security into the IT2005 Model Curriculum	en	18
19	Perceptual user interfaces (PUIs) are an important part of ubiquitous computing.  Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers.  We propose an interactive machine-learning (IML) model that allows users to train, classify/view and correct the classifications.  The concept and implementation details of IML are discussed and contrasted with classical machine learning models.  Evaluations of two algorithms are also presented.  We also briefly describe Image Processing with Crayons (Crayons), which is a tool for creating new camera-based interfaces using a simple painting metaphor.  The Crayons tool embodies our notions of interactive machine learning.	classification , Perceptual interface , image processing , perceptive user interfaces , Perceptual user iinterfaces , Machine learning , image/pixel classifier , Predict correct behaviour , Classification design loop , Interactive machine learning , interaction , Crayons prototype , Image processing with crayons , Crayons design process , Classical machine learning	Interactive Machine Learning	en	19
20	The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network. The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network. Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets, and for finding all attribute sets with a given minimum interestingness. Practical usefulness of the algorithms and their efficiency have been verified experimentally.	Bayesian network , frequent itemsets , association rules , interestingness , emerging pattern , association rule , background knowledge , frequent itemset	Interestingness of Frequent Itemsets Using Bayesian Networks as Background Knowledge	en	20
21	The emergence of several radio technologies, such as Bluetooth and IEEE 802.11, operating in the 2.4 GHz unlicensed ISM frequency band, may lead to signal interference and result in significant performance degradation when devices are colocated in the same environment. The main goal of this paper is to evaluate the effect of mutual interference on the performance of Bluetooth and IEEE 802.11b systems. We develop a simulation framework for modeling interference based on detailed MAC and PHY models. First, we use a simple simulation scenario to highlight the effects of parameters, such as transmission power, offered load, and traffic type. We then turn to more complex scenarios involving multiple Bluetooth piconets and WLAN devices.	evaluation , packet loss , performance degradation , IEEE 802.11b , simulation framework , Bluetooth , interference , hop rate , tranmission power , topology , WPANs , WLAN , offered load	Interference Evaluation of Bluetooth and IEEE 802.11b Systems	en	21
22	What makes a peripheral or ambient display more effective at presenting awareness information than another ?  Presently, little is known in this regard and techniques for evaluating these types of displays are just beginning to be developed. In this article, we focus on one aspect of a peripheral display's effectiveness-its ability to communicate information at a glance.  We conducted an evaluation of the InfoCanvas, a peripheral display that conveys awareness information graphically as a form of information art, by assessing how well people recall information when it is presented for a brief period of time.  We compare performance of the InfoCanvas to two other electronic information displays , a Web portal style and a text-based display, when each display was viewed for a short period of time.  We found that participants noted and recalled significantly more information when presented by the InfoCanvas than by either of the other displays despite having to learn the additional graphical representations employed by the InfoCanvas.	evaluation , peripheral display , graphical representation , awareness information , ambient display , text-based display , information conveyance , InfoCanvas display , Peripheral display , information recall , empirical evaluation , information visualization , Web portal-like display	Is a Picture Worth a Thousand Words?	en	22
23	Recent computer technologies have enabled fast high-quality 3D graphics on personal computers, and also have made the development of 3D graphical applications easier. However , most of such technologies do not sufficiently support layout and behavior aspects of 3D graphics. Geometric constraints are, in general, a powerful tool for specifying layouts and behaviors of graphical objects, and have been applied to 2D graphical user interfaces and specialized 3D graphics packages. In this paper, we present Chorus3D, a geometric constraint library for 3D graphical applications. It enables programmers to use geometric constraints for various purposes such as geometric layout, constrained dragging, and inverse kinematics. Its novel feature is to handle scene graphs by processing coordinate transformations in geometric constraint satisfaction. We demonstrate the usefulness of Chorus3D by presenting sample constraint-based 3D graphical applications.	layout , scene graphs , 3D graphics , geometric layout , constraint satisfaction , 3D graphical applications , geometric constraints , graphical objects , behaviors , coordinate transformation	A Geometric Constraint Library for 3D Graphical Applications	en	23
24	We propose an In-Network Data-Centric Storage (INDCS) scheme for answering ad-hoc queries in sensor networks. Previously proposed In-Network Storage (INS) schemes suffered from Storage Hot-Spots that are formed if either the sensors' locations are not uniformly distributed over the coverage area, or the distribution of sensor readings is not uniform over the range of possible reading values. Our K-D tree based Data-Centric Storage (KDDCS) scheme maintains the invariant that the storage of events is distributed reasonably uniformly among the sensors. KDDCS is composed of a set of distributed algorithms whose running time is within a poly-log factor of the diameter of the network. The number of messages any sensor has to send, as well as the bits in those messages, is poly-logarithmic in the number of sensors. Load balancing in KDDCS is based on defining and distributively solving a theoretical problem that we call the Weighted Split Median problem . In addition to analytical bounds on KDDCS individual algorithms , we provide experimental evidence of our scheme's general efficiency, as well as its ability to avoid the formation of storage hot-spots of various sizes, unlike all previous INDCS schemes.	quality of data (QoD) , KDDCS , routing algorithm , Power-Aware , energy saving , Sensor Network , sensor network , Distributed Algorithms , weighted split median problem , DIM , data persistence , storage hot-spots , ad-hoc queries	KDDCS: A Load-Balanced In-Network Data-Centric Storage Scheme for Sensor Networks	en	24
25	Topic tracking is complicated when the stories in the stream occur in multiple languages. Typically, researchers have trained only English topic models because the training stories have been provided in English. In tracking, non-English test stories are then machine translated into English to compare them with the topic models. We propose a native language hypothesis stating that comparisons would be more effective in the original language of the story. We first test and support the hypothesis for story link detection. For topic tracking the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream. We compare different methods of incrementally building such native language topic models.	topic models ,  , classification , crosslingual , native topic models , similarity , story link , topic tracking , native language hypothesis , multilingual topic tracking , multilingual , Arabic , TDT , machine translation	Language-specific Models in Multilingual Topic Tracking	en	25
26	"Backup of websites is often not considered until after a catastrophic event has occurred to either the website or its webmaster. We introduce ""lazy preservation""  digital preservation performed as a result of the normal operation of web crawlers and caches. Lazy preservation is especially suitable for third parties; for example, a teacher reconstructing a missing website used in previous classes. We evaluate the effectiveness of lazy preservation by reconstructing 24 websites of varying sizes and composition using Warrick, a web-repository crawler. Because of varying levels of completeness in any one repository, our reconstructions sampled from four different web repositories: Google (44%), MSN (30%), Internet Archive (19%) and Yahoo (7%). We also measured the time required for web resources to be discovered and cached (10-103 days) as well as how long they remained in cache after deletion (7-61 days)."	Search engines (SEs) , cached resources , web repositories , recovery , reconstruction , crawling , caching , lazy preservation , search engine , digital preservation	Lazy Preservation: Reconstructing Websites by Crawling the Crawlers	en	26
27	This paper considers the problem of using Support Vector Machines (SVMs) to learn concepts from large scale imbalanced data sets. The objective of this paper is twofold. Firstly, we investigate the effects of large scale and imbalance on SVMs. We highlight the role of linear non-separability in this problem. Secondly, we develop a both practical and theoretical guaranteed meta-algorithm to handle the trouble of scale and imbalance. The approach is named Support Cluster Machines (SCMs). It incorporates the informative and the representative under-sampling mechanisms to speedup the training procedure. The SCMs differs from the previous similar ideas in two ways, (a) the theoretical foundation has been provided, and (b) the clustering is performed in the feature space rather than in the input space. The theoretical analysis not only provides justification , but also guides the technical choices of the proposed approach. Finally, experiments on both the synthetic and the TRECVID data are carried out. The results support the previous analysis and show that the SCMs are efficient and effective while dealing with large scale imbalanced data sets.	Support Vector Machines , concept modelling , Concept Modelling , Imbalance , Support Vector Machines (SVMs) , Large Scale , Clustering , imbalanced data , kernel k-means , support cluster machines (SCMs) , TRECVID , meta-algorithm , large scale data , shrinking techniques , clusters , Kernel k-means	Learning Concepts from Large Scale Imbalanced Data Sets Using Support Cluster Machines	en	27
28	This paper studies the problem of automatic acquisition of the query languages supported by a Web information resource . We describe a system that automatically probes the search interface of a resource with a set of test queries and analyses the returned pages to recognize supported query operators. The automatic acquisition assumes the availability of the number of matches the resource returns for a submitted query. The match numbers are used to train a learning system and to generate classification rules that recognize the query operators supported by a provider and their syntactic encodings. These classification rules are employed during the automatic probing of new providers to determine query operators they support. We report on results of experiments with a set of real Web resources.	query operators , automatic acquisition , learning , hidden web , search interface , web resources , machine learning , search engine , query languages , Hidden Web , web interfaces	Learning Query Languages of Web Interfaces	en	28
29	Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space. This makes the type of clusters identified highly dependent on the assumed similarity measure. Building on recent work in this area, we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data. The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure. Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature.	Clustering , feature weighting , spatially varying dissimilarity (SVaD) , Learning Dissimilarity Measures , clustering , dissimilarity measure	Learning Spatially Variant Dissimilarity (SVaD) Measures	en	29
30	Kernel machines have been shown as the state-of-the-art learning techniques for classification. In this paper, we propose a novel general framework of learning the Unified Kernel Machines (UKM) from both labeled and unlabeled data. Our proposed framework integrates supervised learning, semi-supervised kernel learning, and active learning in a unified solution. In the suggested framework, we particularly focus our attention on designing a new semi-supervised kernel learning method, i.e., Spectral Kernel Learning (SKL), which is built on the principles of kernel target alignment and unsupervised kernel design. Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved. Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches. Based on the framework, we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions (KLR), i.e., Unified Kernel Logistic Regression (UKLR). We evaluate our proposed UKLR classification scheme in comparison with traditional solutions. The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches.	Active Learning , data mining , classification , unified kernel machine(UKM) , Kernel Machines , spectral kernel learning (SKL) , Kernel Logistic Regressions , Supervised Learning , supervised learning , Semi-Supervised Learning , active learning , Classification , Unsuper-vised Kernel Design , framework , Spectral Kernel Learning , semi-supervised kernel learning	Learning the Unified Kernel Machines for Classification	en	30
31	A physically compact, low cost, high performance 3D graphics accelerator is presented. It supports shaded rendering of triangles and antialiased lines into a double-buffered 24-bit true color frame buffer with a 24-bit Z-buffer. Nearly the only chips used besides standard memory parts are 11 ASICs (of four types). Special geometry data reformatting hardware on one ASIC greatly speeds and simplifies the data input pipeline. Floating-point performance is enhanced by another ASIC: a custom graphics microprocessor, with specialized graphics instructions and features. Screen primitive rasterization is carried out in parallel by five drawing ASICs, employing a new partitioning of the back-end rendering task. For typical rendering cases, the only system performance bottleneck is that intrinsically imposed by VRAM.	input processing , 3D graphics hardware , parallel algorithms , video output , general graphics processing , parallel graphics algorithms , small physical size , geometry data , 3D shaded graphics , rendering , screen space rendering , antialiased lines , floating-point microprocessors , low cost , floating point processing , gouraud shading	Leo: A System for Cost Effective 3D Shaded Graphics	en	31
32	Data dissemination through wireless channels for broadcasting information to consumers is becoming quite common.  Many dissemination schemes have been proposed but most of them push data to wireless channels for general consumption. Push based broadcast [1] is essentially asymmetric, i.e., the volume of data being higher from the server to the users than from the users back to the server. Push based scheme requires some indexing which indicates when the data will be broadcast and its position in the broadcast. Access latency and tuning time are the two main parameters which may be used to evaluate an indexing scheme. Two of the important indexing schemes proposed earlier were tree based and the exponential indexing schemes. None of these schemes were able to address the requirements of location dependent data (LDD) which is highly desirable feature of data dissemination. In this paper, we discuss the broadcast of LDD in our project DAta in Your Space (DAYS), and propose a scheme for indexing LDD. We argue that this scheme, when applied to LDD, significantly improves performance in terms of tuning time over the above mentioned schemes. We prove our argument with the help of simulation results.	containment , indexing scheme , access efficiency , indexing , Wireless data broadcast , mapping function , location based services , wireless , energy conservation , location dependent data , broadcast , push based architecture , data dissemination , data staging	Location based Indexing Scheme for DAYS	en	32
33	Bagging frequently improves the predictive performance of a model. An online version has recently been introduced, which attempts to gain the benefits of an online algorithm while approximating regular bagging. However, regular online bagging is an approximation to its batch counterpart and so is not lossless with respect to the bagging operation. By operating under the Bayesian paradigm, we introduce an online Bayesian version of bagging which is exactly equivalent to the batch Bayesian version, and thus when combined with a lossless learning algorithm gives a completely lossless online bagging algorithm. We also note that the Bayesian formulation resolves a theoretical problem with bagging, produces less variability in its estimates, and can improve predictive performance for smaller data sets.	classification , Dirichlet Distribution , online bagging , bootstrap , Classification Tree , Bayesian Bootstrap , mean-squared prediction error , Bayesian bagging , bagging , lossless learning algorithm	Lossless Online Bayesian Bagging	en	33
34	Table is a commonly used presentation scheme, especially for describing relational information. However, table understanding remains an open problem. In this paper, we consider the problem of table detection in web documents. Its potential applications include web mining, knowledge management , and web content summarization and delivery to narrow-bandwidth devices. We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine . Various features re ecting the layout as well as content characteristics of tables are studied. In order to facilitate the training and evaluation of our table classi er, we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database. The database consists of 1,393 HTML les collected from hundreds of di erent web sites and contains 11,477 leaf &lt;TABLE&gt; elements, out of which 1,740 are genuine tables. Experiments were conducted using the cross validation method and an F-measure of 95 : 89% was achieved.	Table detection , table ground truthing protocol , Layout Analysis , classifers , word group , presentation , Information Retrieval , Algorithms , Support Vector Machine , classifcation schemes , Machine Learning , Table Detection , Layout , machine learning based approach , content type , Decision tree , HTML document	A Machine Learning Based Approach for Table Detection on The Web	en	34
35	For hardware accelerated rendering, photon mapping is especially useful for simulating caustic lighting effects on non-Lambertian surfaces. However, an efficient hardware algorithm for the computation of the k nearest neighbours to a sample point is required. Existing algorithms are often based on recursive spatial subdivision techniques, such as kd-trees. However, hardware implementation of a tree-based algorithm would have a high latency, or would require a large cache to avoid this latency on average. We present a neighbourhood-preserving hashing algorithm that is low-latency and has sub-linear access time. This algorithm is more amenable to fine-scale parallelism than tree-based recursive spatial subdivision, and maps well onto coherent block-oriented pipelined memory access. These properties make the algorithm suitable for implementation using future programmable fragment shaders with only one stage of dependent texturing.	photon mapping , block hashing (BH) , hashing techniques , AkNN , kNN , accelerator	Low Latency Photon Mapping Using Block Hashing	en	35
36	In this paper we study the problem of assigning unit-size tasks to related machines when only limited online information is provided to each task. This is a general framework whose special cases are the classical multiple-choice games for the assignment of unit-size tasks to identical machines. The latter case was the subject of intensive research for the last decade. The problem is intriguing in the sense that the natural extensions of the greedy oblivious schedulers, which are known to achieve near-optimal performance in the case of identical machines, are proved to perform quite poorly in the case of the related machines. In this work we present a rather surprising lower bound stating that any oblivious scheduler that assigns an arbitrary number of tasks to n related machines would need log n polls of machine loads per task, in order to achieve a constant competitive ratio versus the optimum offline assignment of the same input sequence to these machines . On the other hand, we prove that the missing information for an oblivious scheduler to perform almost optimally , is the amount of tasks to be inserted into the system. In particular, we provide an oblivious scheduler that only uses O(loglog n) polls, along with the additional information of the size of the input sequence, in order to achieve a constant competitive ratio vs. the optimum offline assignment . The philosophy of this scheduler is based on an interesting exploitation of the slowfit concept ([1, 5, 3]; for a survey see [6, 9, 16]) for the assignment of the tasks to the related machines despite the restrictions on the provided online information, in combination with a layered induction argument for bounding the tails of the number of tasks passing from slower to faster machines. We finally use this oblivious scheduler as the core of an adaptive scheduler that does not demand the knowledge of the input sequence and yet achieves almost the same performance.	oblivious scheduler , HOPS , related machines , Limited Information , lower bounds , online information , scheduling , Online Load Balancing , input sequence , unit-size task , Related Machines	Lower Bounds & Competitive Algorithms for Online Scheduling of Unit-Size Tasks to Related Machines	en	36
37	This paper describes ongoing research into the application of machine learning techniques for improving access to governmental information in complex digital libraries. Under the auspices of the GovStat Project, our goal is to identify a small number of semantically valid concepts that adequately spans the intellectual domain of a collection. The goal of this discovery is twofold. First we desire a practical aid for information architects. Second, automatically derived document-concept relationships are a necessary precondition for real-world deployment of many dynamic interfaces. The current study compares concept learning strategies based on three document representations: keywords, titles, and full-text. In statistical and user-based studies, human-created keywords provide significant improvements in concept learning over both title-only and full-text representations. Categories and Subject Descriptors	information architecture , Information Architecture , BLS , digital libraries , document classification , Machine Learning , topic discovery , document representation , Interface Design , clustering , relational browser	Machine Learning for Information Architecture in a Large Governmental Website	en	37
38	The development of microarray technology has supplied a large volume of data to many fields. In particular, it has been applied to prediction and diagnosis of cancer, so that it expectedly helps us to exactly predict and diagnose cancer. To precisely classify cancer we have to select genes related to cancer because extracted genes from microarray have many noises. In this paper, we attempt to explore many features and classifiers using three benchmark datasets to systematically evaluate the performances of the feature selection methods and machine learning classifiers. Three benchmark datasets are Leukemia cancer dataset, Colon cancer dataset and Lymphoma cancer data set. Pearson's and Spearman's correlation coefficients, Euclidean distance, cosine coefficient, information gain, mutual information and signal to noise ratio have been used for feature selection. Multi-layer perceptron, k-nearest neighbour, support vector machine and structure adaptive selforganizing map have been used for classification. Also, we have combined the classifiers to improve the performance of classification. Experimental results show that the ensemble with several basis classifiers produces the best recognition rate on the benchmark dataset.	classification , MLP , SASOM , gene expression profile , SVM , KNN , Biological data mining , ensemble classifier , feature selection	Machine Learning in DNA Microarray Analysis for Cancer Classification	en	38
39	Machine learning and data mining have found a multitude of successful applications in microarray analysis, with gene clustering and classification of tissue samples being widely cited examples. Low-level microarray analysis  often associated with the pre-processing stage within the microarray life-cycle  has increasingly become an area of active research, traditionally involving techniques from classical statistics. This paper explores opportunities for the application of machine learning and data mining methods to several important low-level microarray analysis problems: monitoring gene expression, transcript discovery, genotyping and resequencing . Relevant methods and ideas from the machine learning community include semi-supervised learning, learning from heterogeneous data, and incremental learning.	low-level analysis , data mining , transductive learning , learning from heterogeneous data , heterogeneous data , semi-supervised learning , incremental learning , transcript discovery , microarray , gene expression estimation , statistics , genotyping , Low-level microarray analysis , re-sequencing	Machine Learning in Low-level Microarray Analysis	en	39
40	Ada95 is an object-oriented programming language. Pack-ages are basic program units in Ada 95 to support OO programming, which allow the specification of groups of logically related entities. Thus, the cohesion of a package is mainly about how tightly the entities are encapsulated in the package. This paper discusses the relationships among these entities based on dependence analysis and presents the properties to obtain these dependencies. Based on these, the paper proposes an approach to measure the package cohesion, which satisfies the properties that a good measure should have.	Object-Oriented , Ada95 , cohesion , dependence , Measurement , OO programming , measure , Cohesion	Measuring Cohesion of Packages in Ada95	en	40
41	Public administrations of all over the world invest an enormous amount of resources in e-government. How the success of e-government can be measured is often not clear. E-government involves many aspects of public administration ranging from introducing new technology to business process (re-)engineering. The measurement of the effectiveness of e-government is a complicated endeavor. In this paper current practices of e-government measurement are evaluated. A number of limitations of current measurement instruments are identified. Measurement focuses predominantly on the front (primarily counting the number of services offered) and not on the back-office processes. Interpretation of measures is difficult as all existing measurement instruments lack a framework depicting the relationships between the indicators and the use of resources. The different measures may fit the aim of the owners of the e-governmental services, however, due to conflicting aims and priorities little agreement exists on a uniform set of measures, needed for comparison of e-government development. Traditional methods of measuring e-government impact and resource usage fall short of the richness of data required for the effective evaluation of e-government strategies.	E-government , law , benchmark , interoperability , evaluation , measurement , architectures , e-government , business process , public administration	Measurement of e-Government Impact: Existing practices and shortcomings	en	41
42	We give the first on-line poly-logarithmic competitve algorithm for minimizing average flow time with preemption on related machines, i.e., when machines can have different speeds. This also yields the first poly-logarithmic polynomial time approximation algorithm for this problem. More specifically, we give an O(log P  log S)-competitive algorithm, where P is the ratio of the biggest and the smallest processing time of a job, and S is the ratio of the highest and the smallest speed of a machine. Our algorithm also has the nice property that it is non-migratory. The scheduling algorithm is based on the concept of making jobs wait for a long enough time before scheduling them on slow machines.	non-migratory algorithm , flow-time , average flow time , approximation algorithms , processing time , competitive ratio , related machines , poly-logarithmic factor , preemption , multiprocessor environment , scheduling , Scheduling	Minimizing Average Flow Time on Related Machines	en	42
43	In this paper, we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile, called CommunityNet, is established for each individual based on a novel algorithm incorporating contact, content, and time information simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering, prediction, and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance, &quot;to whom a person is going to send a specific email&quot; can be predicted by one's personal social network and content analysis. Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58% better than the social network-based predictions, and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed.	user behavior modeling , information dissemination , personal information management	Modeling and Predicting Personal Information Dissemination Behavior	en	43
44	Object-oriented software development practices are being rapidly adopted within increasingly complex systems, including reactive, real-time and concurrent system applications. While data modeling is performed very well under current object-oriented development practices, behavioral modeling necessary to capture critical information in real-time, reactive, and concurrent systems is often lacking.  Addressing this deficiency, we offer an approach for modeling and analyzing concurrent object-oriented software designs through the use of behavioral design patterns, allowing us to map stereotyped UML objects to colored Petri net (CPN) representations in the form of reusable templates.  The resulting CPNs are then used to model and analyze behavioral properties of the software architecture, applying the results of the analysis to the original software design.	Software Architecture , Behavioral Design Patterns , Colored Petri Nets , COMET	Modeling behavioral design patterns of concurrent objects	en	44
45	This paper is concerned with `intranet search'. By intranet search, we mean searching for information on an intranet within an organization. We have found that search needs on an intranet can be categorized into types, through an analysis of survey results and an analysis of search log data. The types include searching for definitions, persons, experts, and homepages. Traditional information retrieval only focuses on search of relevant documents, but not on search of special types of information. We propose a new approach to intranet search in which we search for information in each of the special types, in addition to the traditional relevance search. Information extraction technologies can play key roles in such kind of `search by type' approach, because we must first extract from the documents the necessary information in each type. We have developed an intranet search system called `Information Desk'. In the system, we try to address the most important types of search first - finding term definitions, homepages of groups or topics, employees' personal information and experts on topics. For each type of search, we use information extraction technologies to extract, fuse, and summarize information in advance. The system is in operation on the intranet of Microsoft and receives accesses from about 500 employees per month. Feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information. This paper describes the architecture, features, component technologies, and evaluation results of the system.	Search Needs , metadata extraction , features , architecture , Experimentation , definition search , INFORMATION DESK , information extraction , expert finding , Algorithms , intranet search , Human Factors , information retrieval , component technologies , Intranet search , types of information	A New Approach to Intranet Search Based on Information Extraction	en	45
46	Motivated by recent surfacing viruses that can spread over the air interfaces, in this paper, we investigate the potential disastrous threat of node compromise spreading in wireless sensor networks. Originating from a single infected node, we assume such a compromise can propagate to other sensor nodes via communication and preestablished mutual trust. We focus on the possible epidemic breakout of such propagations where the whole network may fall victim to the attack. Based on epidemic theory, we model and analyze this spreading process and identify key factors determining potential outbreaks. In particular, we perform our study on random graphs precisely constructed according to the parameters of the network, such as distance, key sharing constrained communication and node recovery, thereby reflecting the true characteristics therein. The analytical results provide deep insights in designing potential defense strategies against this threat. Furthermore , through extensive simulations, we validate our model and perform investigations on the system dynamics. Index Terms-- Sensor Networks, Epidemiology, Random Key Predistribution, Random Graph.	Random Key Predistribution , Sensor Networks , Random Graph , Epidemiology	Modeling Node Compromise Spread in Wireless Sensor Networks Using Epidemic Theory	en	46
47	"The literature is very broad considering routing protocols in wireless sensor networks (WSNs). However, security of these routing protocols has fallen beyond the scope so far. Routing is a fundamental functionality in wireless networks, thus hostile interventions aiming to disrupt and degrade the routing service have a serious impact on the overall operation of the entire network. In order to analyze the security of routing protocols in a precise and rigorous way, we propose a formal framework encompassing the definition of an adversary model as well as the ""general"" definition of secure routing in sensor networks. Both definitions take into account the feasible goals and capabilities of an adversary in sensor environments and the variety of sensor routing protocols. In spirit, our formal model is based on the simulation paradigm that is a successfully used technique to prove the security of various cryptographic protocols. However, we also highlight some differences between our model and other models that have been proposed for wired or wireless networks. Finally, we illustrate the practical usage of our model by presenting the formal description of a simple attack against an authenticated routing protocol, which is based on the well-known TinyOS routing."	Simulatability , Adversary Model , Routing Protocols , Sensor Networks , Provable Security	Modelling Adversaries and Security Objectives for Routing Protocols in Wireless Sensor Networks	en	47
48	"We investigate whether it is possible to encrypt a database and then give it away in such a form that users can still access it, but only in a restricted way. In contrast to conventional privacy mechanisms that aim to prevent any access to individual records, we aim to restrict the set of queries that can be feasibly evaluated on the encrypted database. We start with a simple form of database obfuscation which makes database records indistinguishable from lookup functions . The only feasible operation on an obfuscated record is to look up some attribute Y by supplying the value of another attribute X that appears in the same record (i.e., someone who does not know X cannot feasibly retrieve Y ). We then (i) generalize our construction to conjunctions of equality tests on any attributes of the database, and (ii) achieve a new property we call group privacy. This property ensures that it is easy to retrieve individual records or small subsets of records from the encrypted database by identifying them precisely, but ""mass harvesting"" queries matching a large number of records are computationally infeasible. Our constructions are non-interactive. The database is transformed in such a way that all queries except those ex-plicitly allowed by the privacy policy become computationally infeasible, i.e., our solutions do not rely on any access-control software or hardware."	Database privacy , Obfuscation	Obfuscated Databases and Group Privacy	en	48
49	"Peer-to-Peer (P2P ) data integration systems have recently attracted significant attention for their ability to manage and share data dispersed over different peer sources. While integrating data for answering user queries, it often happens that inconsistencies arise, because some integrity constraints specified on peers' global schemas may be violated. In these cases, we may give semantics to the inconsistent system by suitably ""repairing"" the retrieved data, as typically done in the context of traditional data integration systems. However , some specific features of P2P systems, such as peer autonomy and peer preferences (e.g., different source trusting ), should be properly addressed to make the whole approach effective. In this paper, we face these issues that were only marginally considered in the literature. We first present a formal framework for reasoning about autonomous peers that exploit individual preference criteria in repairing the data. The idea is that queries should be answered over the best possible database repairs with respect to the preferences of all peers, i.e., the states on which they are able to find an agreement. Then, we investigate the computational complexity of dealing with peer agreements and of answering queries in P2P data integration systems. It turns out that considering peer preferences makes these problems only mildly harder than in traditional data integration systems."	Peer-to-Peer Systems , Data Integration Systems	On the Complexity of Computing Peer Agreements for Consistent Query Answering in Peer-to-Peer Data Integration Systems	en	49
50	In this paper we study market share rules, rules that have a certain market share statistic associated with them. Such rules are particularly relevant for decision making from a business perspective. Motivated by market share rules, in this paper we consider statistical quantitative rules (SQ rules) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule. Building on prior work, we present a statistical approach for learning all significant SQ rules, i.e., SQ rules for which a desired statistic lies outside a confidence interval computed for this rule. In particular we show how resampling techniques can be effectively used to learn significant rules. Since our method considers the significance of a large number of rules in parallel, it is susceptible to learning a certain number of &quot;false&quot; rules. To address this, we present a technique that can determine the number of significant SQ rules that can be expected by chance alone, and suggest that this number can be used to determine a &quot;false discovery rate&quot; for the learning procedure. We apply our methods to online consumer purchase data and report the results.	nonparametric methods , resampling , Rule discovery , market share rules , statistical quantitative rules	On the Discovery of Significant Statistical Quantitative Rules	en	50
51	Prolonging the network lifetime is one of the most important designing objectives in wireless sensor networks (WSN). We consider a heterogeneous cluster-based WSN, which consists of two types of nodes: powerful cluster-heads and basic sensor nodes. All the nodes are randomly deployed in a specific area. To better balance the energy dissipation, we use a simple mixed communication modes where the sensor nodes can communicate with cluster-heads in either single-hop or multi-hop mode. Given the initial energy of the basic sensor nodes, we derive the optimal communication range and identify the optimal mixed communication mode to maximize the WSN's lifetime through optimizations. Moreover, we also extend our model from 2-D space to 3-D space.	Wireless sensor networks , heterogeneous cluster-based sensor network , optimization , network lifetime , optimal transmission range , energy optimization , Voronoi cell , numerical model , clustering	Optimal transmission range for cluster-based wireless sensor networks with mixed communication modes	en	51
52	In this paper we study how we can design an effective parallel crawler. As the size of the Web grows, it becomes imperative to parallelize a crawling process, in order to finish downloading pages in a reasonable amount of time. We first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling. Based on this understanding, we then propose metrics to evaluate a parallel crawler, and compare the proposed architectures using 40 million pages collected from the Web. Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture.	guideline , architecture , Parallelization , Web Spider , parallel crawler , Web Crawler , model evaluation	Parallel Crawlers	en	52
53	Unlike non-time-critical applications like email and file transfer , network games demand timely data delivery to maintain the seemingly interactive presence of players in the virtual game world. Yet the inherently large transmission delay mean and variance of 3G cellular links make on-time game data delivery difficult. Further complicating the timely game data delivery problem is the frequent packet drops at these links due to inter-symbol interference, fading and shadowing at the physical layer. In this paper, we propose a proxy architecture that enhances the timeliness and reliability of data delivery of interactive games over 3G wireless networks. In particular, a performance enhancing proxy is designed to optimize a new time-critical data type -- variable-deadline data, where the utility of a datum is inversely proportional to the time required to deliver it. We show how a carefully designed and configured proxy can noticeably improve the delivery of network game data.	Wireless Networks , 3G wireless network , time critical data , Network Gaming , congestion control , loss-optimized , RLC configuration , proxy architecture	Performance Enhancing Proxy for Interactive 3G Network Gaming	en	53
54	In this paper, we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware.  The simulations we implement use an extension of cellular automata known as the coupled map lattice (CML).  CML represents the state of a dynamic system as continuous values on a discrete lattice.  In our implementation we store the lattice values in a texture, and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors.  We apply these computations successively to produce interactive visual simulations of convection, reaction-diffusion, and boiling.  We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware, and have integrated them into interactive 3D graphics applications.	Coupled Map Lattice , Visual Simulation , Reaction-Diffusion , dynamic phenomena , Multipass Rendering , simulation , CML , graphic hardware , Graphics Hardware	Physically-Based Visual Simulation on Graphics Hardware	en	54
55	A common measure of the quality or effectiveness of a virtual environment (VE) is the amount of presence it evokes in users. Presence is often defined as the sense of being there in a VE. There has been much debate about the best way to measure presence, and presence researchers need, and have sought, a measure that is reliable, valid, sensitive, and objective. We hypothesized that to the degree that a VE seems real, it would evoke physiological responses similar to those evoked by the corresponding real environment, and that greater presence would evoke a greater response.  To examine this, we conducted three experiments, the results of which support the use of physiological reaction as a reliable, valid, sensitive, and objective presence measure.  The experiments compared participants' physiological reactions to a non-threatening virtual room and their reactions to a stressful virtual height situation.  We found that change in heart rate satisfied our requirements for a measure of presence, change in skin conductance did to a lesser extent, and that change in skin temperature did not.   Moreover, the results showed that inclusion of a passive haptic element in the VE significantly increased presence and that for presence evoked: 30FPS &gt; 20FPS &gt; 15FPS.	presence , Haptics , measurement , Frame Rate , virtual environment , Presence , Physiology	Physiological Measures of Presence in Stressful Virtual Environments	en	55
56	A new statistical formula for identifying 2-character words in Chinese text, called the contextual information formula, was developed empirically by performing stepwise logistic regression using a sample of sentences that had been manually segmented. Contextual information in the form of the frequency of characters that are adjacent to the bigram being processed as well as the weighted document frequency of the overlapping bigrams were found to be significant factors for predicting the probablity that the bigram constitutes a word. Local information (the number of times the bigram occurs in the document being segmented) and the position of the bigram in the sentence were not found to be useful in determining words. The contextual information formula was found to be significantly and substantially better than the mutual information formula in identifying 2-character words. The method can also be used for identifying multi-word terms in English text.	logistic regression , statistical formula , word boundary identification , Chinese text segmentation , word boundary , natural language processing , mutual information , regression model , contextual information , multi-word terms	A New Statistical Formula for Chinese Text Segmentation Incorporating Contextual Information	en	56
57	Automated trust negotiation is an approach which establishes trust between strangers through the bilateral, iterative disclosure of digital credentials. Sensitive credentials are protected by access control policies which may also be communicated to the other party. Ideally, sensitive information should not be known by others unless its access control policy has been satisfied. However, due to bilateral information exchange, information may flow to others in a variety of forms, many of which cannot be protected by access control policies alone. In particular, sensitive information may be inferred by observing negotiation participants' behavior even when access control policies are strictly enforced. In this paper, we propose a general framework for the safety of trust negotiation systems. Compared to the existing safety model, our framework focuses on the actual information gain during trust negotiation instead of the exchanged messages. Thus, it directly reflects the essence of safety in sensitive information protection. Based on the proposed framework, we develop policy databases as a mechanism to help prevent unauthorized information inferences during trust negotiation. We show that policy databases achieve the same protection of sensitive information as existing solutions without imposing additional complications to the interaction between negotiation participants or restricting users' autonomy in defining their own policies.	Privacy , Trust Negotiation , Attribute-based Access Control	Preventing Attribute Information Leakage in Automated Trust Negotiation	en	57
58	We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm . We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.	Gibbs sampling , text modeling , unsupervised learning	Probabilistic Author-Topic Models for Information Discovery	en	58
59	Speed,  accuracy,  and  subjective  satisfaction  are  the  most common  measures  for  evaluating  the  usability  of  search user  interfaces.  However,  these  measures  do  not  facilitate comparisons  optimally  and  they  leave  some  important aspects  of  search  user  interfaces  uncovered.  We  propose new, proportional measures to supplement the current ones. Search  speed  is  a  normalized  measure  for  the  speed  of  a search  user  interface  expressed  in  answers  per  minute. Qualified search speed reveals the trade-off between speed and  accuracy  while  immediate  search  accuracy  addresses the need to measure success in typical web search behavior where  only  the  first  few  results  are  interesting.  The proposed measures are evaluated by applying them to raw data  from  two  studies  and  comparing  them  to  earlier measures. The evaluations indicate that they have desirable features.	usability evaluation , Search user interface , speed , usability measure , accuracy	Proportional Search Interface Usability Measures	en	59
60	Valuable 3D graphical models, such as high-resolution digital scans of cultural heritage objects, may require protection to prevent piracy or misuse, while still allowing for interactive display and manipulation by a widespread audience. We have investigated techniques for protecting 3D graphics content, and we have developed a remote rendering system suitable for sharing archives of 3D models while protecting the 3D geometry from unauthorized extraction . The system consists of a 3D viewer client that includes low-resolution versions of the 3D models, and a rendering server that renders and returns images of high-resolution models according to client requests. The server implements a number of defenses to guard against 3D reconstruction attacks, such as monitoring and limiting request streams, and slightly perturbing and distorting the rendered images. We consider several possible types of reconstruction attacks on such a rendering server, and we examine how these attacks can be defended against without excessively compromising the interactive experience for non-malicious users.	digital rights management , remote rendering , security , 3D models	Protected Interactive 3D Graphics Via Remote Rendering	en	60
61	In order to enable the widespread use of robots in home and office environments, systems with natural interaction capabilities have to be developed. A prerequisite for natural interaction is the robot's ability to automatically recognize when and how long a person's attention is directed towards it for communication. As in open environments several persons can be present simultaneously, the detection of the communication partner is of particular importance. In this paper we present an attention system for a mobile robot which enables the robot to shift its attention to the person of interest and to maintain attention during interaction. Our approach is based on a method for multi-modal person tracking which uses a pan-tilt camera for face recognition, two microphones for sound source localization, and a laser range finder for leg detection. Shifting of attention is realized by turning the camera into the direction of the person which is currently speaking. From the orientation of the head it is decided whether the speaker addresses the robot. The performance of the proposed approach is demonstrated with an evaluation. In addition, qualitative results from the performance of the robot at the exhibition part of the ICVS'03 are provided.	Multi-modal person tracking , Attention , Human-robot-interaction	Providing the Basis for Human-Robot-Interaction: A Multi-Modal Attention System for a Mobile Robot	en	61
62	In this paper, we describe a framework for a personalization system to systematically induce desired emotion and attention related states and promote information processing in viewers of online advertising and e-commerce product information. Psychological Customization entails personalization of the way of presenting information (user interface, visual layouts, modalities, structures) per user to create desired transient psychological effects and states, such as emotion, attention, involvement, presence, persuasion and learning. Conceptual foundations and empiric evidence for the approach are presented.	personalization emotion , persuasion , advertising , E-commerce	Psychologically Targeted Persuasive Advertising and Product Information in E-Commerce	en	62
63	Today, watermarking techniques have been extended from the multimedia context to relational databases so as to protect the ownership of data even after the data are published or distributed. However , all existing watermarking schemes for relational databases are secret key based , thus require a secret key to be presented in proof of ownership. This means that the ownership can only be proven once to the public (e.g., to the court). After that, the secret key is known to the public and the embedded watermark can be easily destroyed by malicious users. Moreover, most of the existing techniques introduce distortions to the underlying data in the watermarking process, either by modifying least significant bits or exchanging categorical values. The distortions inevitably reduce the value of the data. In this paper, we propose a watermarking scheme by which the ownership of data can be publicly proven by anyone, as many times as necessary. The proposed scheme is distortion-free , thus suitable for watermarking any type of data without fear of error constraints. The proposed scheme is robust against typical database attacks including tuple/attribute insertion/deletion, ran-dom/selective value modification, data frame-up, and additive attacks	public verifiability , certificate , Relational database , watermark , ownership protection	Publicly Verifiable Ownership Protection for Relational Databases	en	63
64	A person working with diverse information sources--with possibly different formats and information models--may recognize and wish to express conceptual structures that are not explicitly present in those sources. Rather than replicate the portions of interest and recast them into a single, combined data source, we leave base information where it is and superimpose a conceptual model that is appropriate to the task at hand. This superimposed model can be distinct from the model(s) employed by the sources in the base layer. An application that superimposes a new conceptual model over diverse sources, with varying capabilities, needs to accommodate the various types of information and differing access protocols for the base information sources. The  Superimposed Pluggable Architecture for Contexts and Excerpts (SPARCE) defines a collection of architectural abstractions, placed between superimposed and base applications, to demarcate and revisit information elements inside base sources and provide access to content and context for elements inside these sources. SPARCE accommodates new base information types without altering existing superimposed applications. In this paper, we briefly introduce several superimposed applications that we have built, and describe the conceptual model each superimposes. We then focus on the use of context in superimposed applications.  We describe how SPARCE supports context and excerpts.  We demonstrate how SPARCE facilitates building superimposed applications by describing its use in building our two, quite diverse applications.	superimposed information , SPARCE . , excerpts , Conceptual modelling , software architecture , context	Putting Integrated Information in Context: Superimposing Conceptual Models with SPARCE	en	64
65	Along with the development of multimedia and wireless networking technologies, mobile multimedia applications are playing more important roles in information access. Quality of Service (QoS) is a critical issue in providing guaranteed service in a low bandwidth wireless environment. To provide Bluetooth-IP services with differentiated quality requirements, a QoS-centric cascading mechanism is proposed in this paper. This innovative mechanism, composed of intra-piconet resource allocation, inter-piconet handoff and Bluetooth-IP access modules, is based on the Bluetooth Network Encapsulation Protocol (BNEP) operation scenario. From our simulations the handoff connection time for a Bluetooth device is up to 11.84 s and the maximum average transmission delay is up to 4e-05 s when seven devices join a piconet simultaneously. Increasing the queue length for the Bluetooth-IP access system will decrease the traffic loss rate by 0.02 per 1000 IP packets at the expense of a small delay performance.	handoff , quality of service , Bluetooth-IP access system , BNEP protocol , resource allocation	Quality-of-Service in IP Services over Bluetooth Ad-Hoc Networks	en	65
66	This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online.   Most question answering systems use a wide variety of linguistic resources.  We focus instead on the redundancy available in large corpora as an important resource. We use this redundancy to simplify the query rewrites that we need to use, and to support answer mining from returned snippets. Our system performs quite well given the simplicity of the techniques being utilized.  Experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages.  Simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines.	rewrite query , n-gram extraction techniques , automatic QA , Experimentation , information extraction , Algorithms , question answering system , redundancy in large corpora , facilitates answer mining , natural language processing , information retrieval , machine learning , TREC QA , simple passage ranking	Web Question Answering: Is More Always Better?	en	66
67	Programming languages are a part of the core of computer science. Courses on programming languages are typically offered to junior or senior students, and textbooks are based on this assumption. However, our computer science curriculum offers the programming languages course in the first year. This unusual situation led us to design it from an untypical approach. In this paper, we first analyze and classify proposals for the programming languages course into different pure and hybrid approaches. Then, we describe a course for freshmen based on four pure approaches, and justify the main choices made. Finally, we identify the software used for laboratories and outline our experience after teaching it for seven years.	programming language course , language description , formal grammars , laboratory component , functional programming , computer science , programming methodology , Programming languages , recursion , curriculum , freshmen , topics , programming paradigms	A Programming Languages Course for Freshmen	en	67
68	"To deal with the problem of too many results returned from an E-commerce Web database in response to a user query, this paper proposes a novel approach to rank the query results. Based on the user query, we speculate how much the user cares about each attribute and assign a corresponding weight to it. Then, for each tuple in the query result, each attribute value is assigned a score according to its ""desirableness"" to the user. These attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple. Tuples with the top ranking scores are presented to the user first. Our ranking method is domain independent and requires no user feedback. Experimental results demonstrate that this ranking method can effectively capture a user's preferences."	many query result problem , rank the query results , query result ranking , QRRE , algorithms , experimentation , attribute value , Attribute weight assignment , Query result ranking , attribute preference , design , PIR , e-commerce web databases , human factors , E-commerce	Query Result Ranking over E-commerce Web Databases	en	68
69	The heterogeneous Web exacerbates IR problems and short user queries make them worse. The contents of web documents are not enough to find good answer documents. Link information and URL information compensates for the insufficiencies of content information. However, static combination of multiple evidences may lower the retrieval performance . We need different strategies to find target documents according to a query type. We can classify user queries as three categories, the topic relevance task, the homepage finding task, and the service finding task. In this paper, a user query classification scheme is proposed. This scheme uses the difference of distribution, mutual information , the usage rate as anchor texts, and the POS information for the classification. After we classified a user query, we apply different algorithms and information for the better results. For the topic relevance task, we emphasize the content information, on the other hand, for the homepage finding task, we emphasize the Link information and the URL information. We could get the best performance when our proposed classification method with the OKAPI scoring algorithm was used.	URL Information , web document , URL , improvement , frequency , task , information , model , rate , IR , Combination of Multiple Evidences , Link Information , query , Query Classification	Query Type Classification for Web Document Retrieval	en	69
70	In our research on superimposed information management, we have developed applications where information elements in the superimposed layer serve to annotate, comment, restructure, and combine selections from one or more existing documents in the base layer. Base documents tend to be unstructured or semi-structured (HTML pages, Excel spreadsheets, and so on) with marks delimiting selections. Selections in the base layer can be programmatically accessed via marks to retrieve content and context. The applications we have built to date allow creation of new marks and new superimposed elements (that use marks), but they have been browse-oriented and tend to expose the line between superimposed and base layers. Here, we present a new access capability, called bi-level queries, that allows an application or user to query over both layers as a whole. Bi-level queries provide an alternative style of data integration where only relevant portions of a base document are mediated (not the whole document) and the superimposed layer can add information not present in the base layer. We discuss our framework for superimposed information management, an initial implementation of a bi-level query system with an XML Query interface, and suggest mechanisms to improve scalability and performance.	Bi-level queries , implementation , system , Superimposed information management , SPARCE , superimposed , document , management , RIDPAD , query , information , Information integration , METAXPath , hyperlink	Querying Bi-level Information	en	70
71	Most of the theoretical work on sampling has addressed the inversion of general traffic properties such as flow size distribution , average flow size, or total number of flows. In this paper, we make a step towards understanding the impact of packet sampling on individual flow properties. We study how to detect and rank the largest flows on a link. To this end, we develop an analytical model that we validate on real traces from two networks. First we study a blind ranking method where only the number of sampled packets from each flow is known. Then, we propose a new method, protocol-aware ranking, where we make use of the packet sequence number (when available in transport header) to infer the number of non-sampled packets from a flow, and hence to improve the ranking. Surprisingly, our analytical and experimental results indicate that a high sampling rate (10% and even more depending on the number of top flows to be ranked) is required for a correct blind ranking of the largest flows. The sampling rate can be reduced by an order of magnitude if one just aims at detecting these flows or by using the protocol-aware method.	largest flow detection and ranking , validation with real traces , Packet sampling , performance evaluation	Ranking Flows from Sampled Traffic	en	71
72	Web navigation plays an important role in exploring public interconnected data sources such as life science data. A navigational query in the life science graph produces a result graph which is a layered directed acyclic graph (DAG). Traversing the result paths in this graph reaches a target object set (TOS). The challenge for ranking the target objects is to provide recommendations that reflect the relative importance of the retrieved object, as well as its relevance to the specific query posed by the scientist. We present a metric layered graph PageRank (lgPR) to rank target objects based on the link structure of the result graph. LgPR is a modification of PageRank; it avoids random jumps to respect the path structure of the result graph. We also outline a metric layered graph ObjectRank (lgOR) which extends the metric ObjectRank to layered graphs. We then present an initial evaluation of lgPR. We perform experiments on a real-world graph of life sciences objects from NCBI and report on the ranking distribution produced by lgPR. We compare lgPR with PageRank. In order to understand the characteristics of lgPR, an expert compared the Top K target objects (publications in the PubMed source) produced by lgPR and a word-based ranking method that uses text features extracted from an external source (such as Entrez Gene) to rank publications.	Navigational Query , Link Analysis , PageRank , Ranking	Ranking Target Objects of Navigational Queries	en	72
73	Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users. In this paper, we study the Web object-ranking problem, one of the key issues in building a vertical search engine. More specifically, we focus on this problem in cases when objects lack relationships between different Web communities , and take high-quality photo search as the test bed for this investigation. We proposed two score fusion methods that can automatically integrate as many Web communities (Web forums) with rating information as possible. The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm, and aims at minimizing score differences of duplicate photos in different forums . Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described. Though the experiments were conducted on high-quality photo ranking , the proposed algorithms are also applicable to other ranking problems, such as movie ranking and music ranking	image search , ranking , Web objects	Ranking Web Objects from Multiple Communities	en	73
74	While users disseminate various information in the open and widely distributed environment of the Semantic Web, determination of who shares access to particular information is at the center of looming privacy concerns. We propose a real-world -oriented information sharing system that uses social networks. The system automatically obtains users' social relationships by mining various external sources. It also enables users to analyze their social networks to provide awareness of the information dissemination process. Users can determine who has access to particular information based on the social relationships and network analysis.	Social network , Information sharing	Real-world Oriented Information Sharing Using Social Networks	en	74
75	Enterprises in the public and private sectors have been making their large spatial data archives available over the Internet . However, interactive work with such large volumes of online spatial data is a challenging task. We propose two efficient approaches to remote access to large spatial data. First, we introduce a client-server architecture where the work is distributed between the server and the individual clients for spatial query evaluation, data visualization, and data management. We enable the minimization of the requirements for system resources on the client side while maximizing system responsiveness as well as the number of connections one server can handle concurrently. Second, for prolonged periods of access to large online data, we introduce APPOINT (an Approach for Peer-to-Peer Offloading the INTernet). This is a centralized peer-to-peer approach that helps Internet users transfer large volumes of online data efficiently. In APPOINT, active clients of the client-server architecture act on the server's behalf and communicate with each other to decrease network latency, improve service bandwidth, and resolve server congestions.	GIS , Client/server , Peer-to-peer , Internet	Remote Access to Large Spatial Databases	en	75
76	An increasing amount of heterogeneous information about scientific research is becoming available on-line. This potentially allows users to explore the information from multiple perspectives and derive insights and not just raw data about a topic of interest. However, most current scientific information search systems lag behind this trend; being text-based, they are fundamentally incapable of dealing with multimedia data. An even more important limitation is that their information environments are information-centric and therefore are not suitable if insights are desired. Towards this goal, in this paper, we describe the design of a system, called ResearchExplorer, which facilitates exploring multimedia scientific data to gain insights. This is accomplished by providing an interaction environment for insights where users can explore multimedia scientific information sources. The multimedia information is united around the notion of research event and can be accessed in a unified way. Experiments are conducted to show how ResearchExplorer works and how it cardinally differs from other search systems.	Event , Research Event , Multimedia Data , Spatio-Temporal Data , Exploration , Interaction Environment , Insight	ResearchExplorer: Gaining Insights through Exploration in Multimedia Scientific Data	en	76
77	Cognitive information complexity measure is based on cognitive informatics, which helps in comprehending the software characteristics. For any complexity measure to be robust, Weyuker properties must be satisfied to qualify as good and comprehensive one. In this paper, an attempt has also been made to evaluate cognitive information complexity measure in terms of nine Weyuker properties, through examples. It has been found that all the nine properties have been satisfied by cognitive information complexity measure and hence establishes cognitive information complexity measure based on information contained in the software as a robust and well-structured one.	cognitive weight , cognitive information complexity measure , basic control structures , cognitive information complexity unit , Weighted information count	Robustness Analysis of Cognitive Information Complexity Measure using Weyuker Properties	en	77
78	The emergence of Bluetooth as a default radio interface allows handheld devices to be rapidly interconnected into ad hoc networks. Bluetooth allows large numbers of piconets to form a scatternet using designated nodes that participate in multiple piconets. A unit that participates in multiple piconets can serve as a bridge and forwards traffic between neighbouring piconets. Since a Bluetooth unit can transmit or receive in only one piconet at a time, a bridging unit has to share its time among the different piconets. To schedule communication with bridging nodes one must take into account their availability in the different piconets, which represents a difficult , scatternet wide coordination problem and can be an important performance bottleneck in building scatternets. In this paper we propose the Pseudo-Random Coordinated Scatternet Scheduling (PCSS) algorithm to perform the scheduling of both intra and inter-piconet communication. In this algorithm Bluetooth nodes assign meeting points with their peers such that the sequence of meeting points follows a pseudo random process that is different for each pair of nodes. The uniqueness of the pseudo random sequence guarantees that the meeting points with different peers of the node will collide only occasionally. This removes the need for explicit information exchange between peer devices, which is a major advantage of the algorithm. The lack of explicit signaling between Bluetooth nodes makes it easy to deploy the PCSS algorithm in Bluetooth devices, while conformance to the current Bluetooth specification is also maintained. To assess the performance of the algorithm we define two reference case schedulers and perform simulations in a number of scenarios where we compare the performance of PCSS to the performance of the reference schedulers.	checkpoint , total utilization , piconets , threshold , scatternet , PCSS algorithm , Bluetooth , slaves , inter-piconet communication , scheduling , intensity , Network Access Point , bridging unit	A Pseudo Random Coordinated Scheduling Algorithm for Bluetooth Scatternets	en	78
79	From experience with wireless sensor networks it has become apparent that dynamic reprogramming of the sensor nodes is a useful feature. The resource constraints in terms of energy, memory, and processing power make sensor network reprogramming a challenging task. Many different mechanisms for reprogramming sensor nodes have been developed ranging from full image replacement to virtual machines. We have implemented an in-situ run-time dynamic linker and loader that use the standard ELF object file format. We show that run-time dynamic linking is an effective method for reprogramming even resource constrained wireless sensor nodes. To evaluate our dynamic linking mechanism we have implemented an application-specific virtual machine and a Java virtual machine and compare the energy cost of the different linking and execution models. We measure the energy consumption and execution time overhead on real hardware to quantify the energy costs for dynamic linking. Our results suggest that while in general the overhead of a virtual machine is high, a combination of native code and virtual machine code provide good energy efficiency. Dynamic run-time linking can be used to update the native code, even in heterogeneous networks.	Wireless sensor networks , Embedded systems , Dynamic linking , Operating systems , Virtual machines	Run-Time Dynamic Linking for Reprogramming Wireless Sensor Networks	en	79
80	Sensor network computing can be characterized as resource-constrained distributed computing using unreliable, low bandwidth communication. This combination of characteristics poses significant software development and maintenance challenges. Effective and efficient debugging tools for sensor network are thus critical. Existent development tools, such as TOSSIM, EmStar, ATEMU and Avrora, provide useful debugging support, but not with the fidelity, scale and functionality that we believe are sufficient to meet the needs of the next generation of applications. In this paper, we propose a debugger, called S2DB, based on a distributed full system sensor network simulator with high fidelity and scalable performance, DiSenS. By exploiting the potential of DiSenS as a scalable full system simulator, S2DB extends conventional debugging methods by adding novel device level, program source level, group level, and network level debugging abstractions. The performance evaluation shows that all these debugging features introduce overhead that is generally less than 10% into the simulator and thus making S2DB an efficient and effective debugging tool for sensor networks.	Sensor Network , Simulation , Debugging	S2DB : A Novel Simulation-Based Debugger for Sensor Network Applications	en	80
81	Computing and maintaining network structures for efficient data aggregation incurs high overhead for dynamic events where the set of nodes sensing an event changes with time. Moreover, structured approaches are sensitive to the waiting-time which is used by nodes to wait for packets from their children before forwarding the packet to the sink. Although structure-less approaches can address these issues, the performance does not scale well with the network size. We propose a semi-structured approach that uses a structure-less technique locally followed by Dynamic Forwarding on an implicitly constructed packet forwarding structure to support network scalability. The structure, ToD, is composed of multiple shortest path trees. After performing local aggregation , nodes dynamically decide the forwarding tree based on the location of the sources. The key principle behind ToD is that adjacent nodes in a graph will have low stretch in one of these trees in ToD, thus resulting in early aggregation of packets. Based on simulations on a 2000 nodes network and real experiments on a 105 nodes Mica2-based network, we conclude that efficient aggregation in large scale networks can be achieved by our semi-structured approach.	ToD , Structure-free , Anycasting , Data Aggregation	Scalable Data Aggregation for Dynamic Events in Sensor Networks	en	81
82	Mining frequent structural patterns from graph databases is an interesting problem with broad applications. Most of the previous studies focus on pruning unfruitful search subspaces effectively, but few of them address the mining on large, disk-based databases. As many graph databases in applications cannot be held into main memory, scalable mining of large, disk-based graph databases remains a challenging problem. In this paper, we develop an effective index structure, ADI (for adjacency index), to support mining various graph patterns over large databases that cannot be held into main memory. The index is simple and efficient to build. Moreover, the new index structure can be easily adopted in various existing graph pattern mining algorithms. As an example , we adapt the well-known gSpan algorithm by using the ADI structure. The experimental results show that the new index structure enables the scalable graph pattern mining over large databases. In one set of the experiments, the new disk-based method can mine graph databases with one million graphs, while the original gSpan algorithm can only handle databases of up to 300 thousand graphs. Moreover, our new method is faster than gSpan when both can run in main memory.	index , Edge table , Graph mining , Subgraph mine , Frequent graph pattern mining , Adjacency list representation , graph database , DFS code , ADI Index structure , frequent graph pattern , Gspan algorithm , Disk bases databases , GRaph databases , Memory based databases	Scalable Mining of Large Disk-based Graph Databases	en	82
83	The IP Multimedia Subsystem (IMS) defined by Third Generation Partnership Projects (3GPP and 3GPP2) is a technology designed to provide robust multimedia services across roaming boundaries and over diverse access technologies with promising features like quality-of-service (QoS), reliability and security. The IMS defines an overlay service architecture that merges the paradigms and technologies of the Internet with the cellular and fixed telecommunication worlds. Its architecture enables the efficient provision of an open set of potentially highly integrated multimedia services, combining web browsing, email, instant messaging, presence, VoIP, video conferencing, application sharing, telephony, unified messaging, multimedia content delivery, etc. on top of possibly different network technologies. As such IMS enables various business models for providing seamless business and consumer multimedia applications. In this communication converged world, the challenging issues are security, quality of service (QoS) and management & administration. In this paper our focus is to manage secure access to multimedia services and applications based on SIP and HTTP on top of IP Multimedia Subsystem (IMS). These services include presence, video conferencing, messaging, video broadcasting, and push to talk etc. We will utilize Generic Bootstrapping Architecture (GBA) model to authenticate multimedia applications before accessing these multimedia services offered by IMS operators. We will make enhancement in GBA model to access these services securely by introducing Authentication Proxy (AP) which is responsible to implement Transport Layer Security (TLS) for HTTP and SIP communication. This research work is part of Secure Service Provisioning (SSP) Framework for IP Multimedia System at Fokus Fraunhofer IMS 3Gb Testbed.	TLS Tunnel end points , Generic Authentication Architecture , GLMS/XDMS , General bootstrapping architecture , Transport Layer Security , Network authentication function , Signalling protocols , Generic Bootstrapping Architecture , Authentication Proxy , GBA , Diameter proxy , Transport layer security , IP Multimedia System , Authentication proxy , TLS , IP multimedia subsystem , IMS platform , Fokus IMS Testbed , NAF , AP , Security and Privacy	Secure Access to IP Multimedia Services Using Generic Bootstrapping Architecture (GBA) for 3G & Beyond Mobile Networks	en	83
84	In-network aggregation is an essential primitive for performing queries on sensor network data. However, most aggregation algorithms assume that all intermediate nodes are trusted. In contrast, the standard threat model in sensor network security assumes that an attacker may control a fraction of the nodes, which may misbehave in an arbitrary (Byzantine) manner. We present the first algorithm for provably secure hierarchical in-network data aggregation. Our algorithm is guaranteed to detect any manipulation of the aggregate by the adversary beyond what is achievable through direct injection of data values at compromised nodes. In other words, the adversary can never gain any advantage from misrepresenting intermediate aggregation computations. Our algorithm incurs only O(log2n) node congestion, supports arbitrary tree-based aggregator topologies and retains its resistance against aggregation manipulation in the presence of arbitrary numbers of malicious nodes. The main algorithm is based on performing the SUM aggregation securely by first forcing the adversary to commit to its choice of intermediate aggregation results, and then having the sensor nodes independently verify that their contributions to the aggregate are correctly incorporated. We show how to reduce secure MEDIAN , COUNT , and AVERAGE to this primitive.	algorithm , Secure aggregation , commitment forest , in-network data aggregation , commitment tree , Sensor Networks , secure hierarchical data aggregation protocol , sensor network , aggregation commit , result checking , query dissemination , congestion complexity , Data aggregation	Secure Hierarchical In-Network Aggregation in Sensor Networks	en	84
85	The use of middleware eases the development of distributed applications by abstracting the intricacies (communication and coordination among software components) of the distributed network environment. In wireless sensor networks, this is even trickier because of their specific issues such as addressing, mobility, number of sensors and energy-limited nodes. This paper describes SensorBus, a message-oriented middleware (MOM) model for wireless sensor networks based on the publish-subscribe paradigm and that allows the free exchange of the communication mechanism among sensor nodes allowing as result the capability of using more than one communication mechanism to address the requirements of larger number of applications. We intend to provide a platform which addresses the main characteristics of wireless sensor networks and also allows the development of energy-efficient applications. SensorBus incorporates constraint and query languages which will aid the development of interactive applications. It intends with the utilization of filters reduces data movement minimizing the energy consumption of nodes.	message service , publish-subscribe paradigm , message-oriented middleware model , environmental monitoring applications , application filters , context service , Middleware , constraint and query languages , design pattern , wireless sensor networks , application service , wireless sensor network	SensorBus: A Middleware Model for Wireless Sensor Networks	en	85
86	We investigate the design space of sensor network broadcast authentication . We show that prior approaches can be organized based on a taxonomy of seven fundamental proprieties, such that each approach can satisfy at most six of the seven proprieties. An empirical study of the design space reveals possibilities of new approaches, which we present in the following two new authentication protocols : RPT and LEA. Based on this taxonomy, we offer guidance in selecting the most appropriate protocol based on an application's desired proprieties. Finally, we pose the open challenge for the research community to devise a protocol simultaneously providing all seven properties.	Sensor Network , Broadcast Authentication , Taxonomy	Seven Cardinal Properties of Sensor Network Broadcast Authentication	en	86
87	Many methods for classification and gene selection with microarray data have been developed. These methods usually give a ranking of genes. Evaluating the statistical significance of the gene ranking is important for understanding the results and for further biological investigations, but this question has not been well addressed for machine learning methods in existing works. Here, we address this problem by formulating it in the framework of hypothesis testing and propose a solution based on resampling. The proposed r-test methods convert gene ranking results into position p-values to evaluate the significance of genes. The methods are tested on three real microarray data sets and three simulation data sets with support vector machines as the method of classification and gene selection. The obtained position p-values help to determine the number of genes to be selected and enable scientists to analyze selection results by sophisticated multivariate methods under the same statistical inference paradigm as for simple hypothesis testing methods.	Significance of gene ranking , gene selection , microarray data analysis , classification	Significance of gene ranking for classification of microarray samples	en	87
88	The contour tree, an abstraction of a scalar field that encodes the nesting relationships of isosurfaces, can be used to accelerate isosurface extraction, to identify important isovalues for volume-rendering transfer functions, and to guide exploratory visualization through a flexible isosurface interface. Many real-world data sets produce unmanageably large contour trees which require meaningful simplification. We define local geometric measures for individual contours, such as surface area and contained volume, and provide an algorithm to compute these measures in a contour tree. We then use these geometric measures to simplify the contour trees, suppressing minor topological features of the data. We combine this with a flexible isosurface interface to allow users to explore individual contours of a dataset interactively.	Isosurfaces , topological simplification , contour trees	Simplifying Flexible Isosurfaces Using Local Geometric Measures	en	88
89	This paper focuses on defending against compromised nodes' dropping of legitimate reports and investigates the misbehavior of a maliciously packet-dropping node in sensor networks . We present a resilient packet-forwarding scheme using Neighbor Watch System (NWS), specifically designed for hop-by-hop reliable delivery in face of malicious nodes that drop relaying packets, as well as faulty nodes that fail to relay packets. Unlike previous work with multipath data forwarding, our scheme basically employs single-path data forwarding, which consumes less power than multipath schemes. As the packet is forwarded along the single-path toward the base station, our scheme, however, converts into multipath data forwarding at the location where NWS detects relaying nodes' misbehavior. Simulation experiments show that, with the help of NWS, our forwarding scheme achieves a high success ratio in face of a large number of packet-dropping nodes, and effectively adjusts its forwarding style, depending on the number of packet-dropping nodes en-route to the base station.	Neighbor Watch System , legitimate node , Reliable Delivery , Packet-dropping Attacks , aggregation protocols , malicious node , robustness , critical area , single-path forwarding , Sensor Network Security , cluster key , secure ad-hoc network routing protocol , Secure Routing , degree of multipath	A Resilient Packt-Forwarding Scheme against Maliciously Packet-Dropping Nodes in Sensor Networks	en	89
90	In this paper we introduce the intermediate rank or higher rank lattice rule for the general case when the number of quadrature points is n t m, where m is a composite integer, t is the rank of the rule, n is an integer such that (n, m) = 1. Our emphasis is the applications of higher rank lattice rules to a class of option pricing problems. The higher rank lattice rules are good candidates for applications to finance based on the following reasons: the higher rank lattice rule has better asymptotic convergence rate than the conventional good lattice rule does and searching higher rank lattice points is much faster than that of good lattice points for the same number of quadrature points; furthermore, numerical tests for application to option pricing problems showed that the higher rank lattice rules are not worse than the conventional good lattice rule on average.	Monte Carlo and Quasi-Monte Carlo methods , Simulation of multivariate integrations , Lattice rules , Option Pricing	SIMULATING OPTION PRICES AND SENSITIVITIES BY HIGHER RANK LATTICE RULES	en	90
91	The way current search engines work leaves a large amount of information available in the World Wide Web outside their catalogues. This is due to the fact that crawlers work by following hyperlinks and a few other references and ignore HTML forms. In this paper, we propose a search engine prototype that can retrieve information behind HTML forms by automatically generating queries for them. We describe the architecture, some implementation details and an experiment that proves that the information is not in fact indexed by current search engines.	implementation , architecture , Label Extraction , experimentation , html form , SmartCrawl , web crawler , hidden web content , information retrieval , search engine , Search Engine , extraction algorithm , Hidden Web	SmartCrawl: A New Strategy for the Exploration of the Hidden Web	en	91
92	Braille and audio feedback based systems have vastly improved the lives of the visually impaired across a wide majority of the globe. However, more than 13 million visually impaired people in the Indian sub-continent could not benefit much from such systems. This was primarily due to the difference in the technology required for Indian languages compared to those corresponding to other popular languages of the world. In this paper, we describe the Sparsha toolset. The contribution made by this research has enabled the visually impaired to read and write in Indian vernaculars with the help of a computer.	audio feedback , Indian languages , Braille , Visual impairment	Sparsha: A Comprehensive Indian Language Toolset for the Blind	en	92
93	This paper describes StyleCam, an approach for authoring 3D viewing experiences that incorporate stylistic elements that are not available in typical 3D viewers. A key aspect of StyleCam is that it allows the author to significantly tailor what the user sees and when they see it. The resulting viewing experience can approach the visual richness and pacing of highly authored visual content such as television commercials or feature films. At the same time, StyleCam allows for a satisfying level of interactivity while avoiding the problems inherent in using unconstrained camera models. The main components of StyleCam are camera surfaces which spatially constrain the viewing camera; animation clips that allow for visually appealing transitions between different camera surfaces; and a simple, unified, interaction technique that permits the user to seamlessly and continuously move between spatial-control of the camera and temporal-control of the animated transitions. Further, the user's focus of attention is always kept on the content, and not on extraneous interface widgets. In addition to describing the conceptual model of StyleCam, its current implementation, and an example authored experience, we also present the results of an evaluation involving real users.	3D viewers , camera controls , 3D navigation , 3D visualization , interaction techniques	StyleCam: Interactive Stylized 3D Navigation using Integrated Spatial & Temporal Controls	en	93
94	Tactile displays are now becoming available in a form that can be easily used in a user interface. This paper describes a new form of tactile output. Tactons, or tactile icons, are structured, abstract messages that can be used to communicate messages non-visually. A range of different parameters can be used for Tacton construction including : frequency, amplitude and duration of a tactile pulse, plus other parameters such as rhythm and location. Tactons have the potential to improve interaction in a range of different areas, particularly where the visual display is overloaded, limited in size or not available, such as interfaces for blind people or in mobile and wearable devices . . This paper describes Tactons, the parameters used to construct them and some possible ways to design them. Examples of where Tactons might prove useful in user interfaces are given.	tactile displays , multimodal interaction , Tactons , non-visual cues	Tactons: Structured Tactile Messages for Non-Visual Information Display	en	94
95	Wireless link losses result in poor TCP throughput since losses are perceived as congestion by TCP, resulting in source throttling. In order to mitigate this effect, 3G wireless link designers have augmented their system with extensive local retransmission mechanisms. In addition, in order to increase throughput, intelligent channel state based scheduling have also been introduced. While these mechanisms have reduced the impact of losses on TCP throughput and improved the channel utilization, these gains have come at the expense of increased delay and rate variability. In this paper, we comprehensively evaluate the impact of variable rate and variable delay on long-lived TCP performance. We propose a model to explain and predict TCP's throughput over a link with variable rate and/or delay. We also propose a network-based solution called Ack Regulator that mitigates the effect of variable rate and/or delay without significantly increasing the round trip time, while improving TCP performance by up to 40%.	algorithm , architecture , TCP , wireless communication , performance evaluation , 3G wireless links , prediction model , design , Link and Rate Variation , 3G Wireless , simulation result , congestion solution , Network	TCP/IP Performance over 3G Wireless Links with Rate and Delay Variation	en	95
96	Students in a sophomore-level database fundamentals course were taught SQL and database concepts using both Oracle and SQL Server. Previous offerings of the class had used one or the other database. Classroom experiences suggest that students were able to handle learning SQL in the dual environment, and, in fact, benefited from this approach by better understanding ANSI-standard versus database-specific SQL and implementation differences in the two database systems.	SQL , SQL Server , training vs. education , database systems , Oracle , database , educational fundamentals , student feedbacks , ANSI-Standard SQL , teaching in IT , dual environment , SQL Language , course design , practical results	The Forest and the Trees: Using Oracle and SQL Server Together to Teach ANSI-Standard SQL	en	96
97	We present a model, based on the maximum entropy method, for analyzing various measures of retrieval performance such as average precision, R-precision, and precision-at-cutoffs. Our methodology treats the value of such a measure as a constraint on the distribution of relevant documents in an unknown list, and the maximum entropy distribution can be determined subject to these constraints. For good measures of overall performance (such as average precision), the resulting maximum entropy distributions are highly correlated with actual distributions of relevant documents in lists as demonstrated through TREC data; for poor measures of overall performance, the correlation is weaker. As such, the maximum entropy method can be used to quantify the overall quality of a retrieval measure. Furthermore, for good measures of overall performance (such as average precision), we show that the corresponding maximum entropy distributions can be used to accurately infer precision-recall curves and the values of other measures of performance, and we demonstrate that the quality of these inferences far exceeds that predicted by simple retrieval measure correlation, as demonstrated through TREC data.	Average Precision , Evaluation , Maximum Entropy	The Maximum Entropy Method for Analyzing Retrieval Measures	en	97
98	Recognition of motion streams such as data streams generated by different sign languages or various captured human body motions requires a high performance similarity measure . The motion streams have multiple attributes, and motion patterns in the streams can have different lengths from those of isolated motion patterns and different attributes can have different temporal shifts and variations. To address these issues, this paper proposes a similarity measure based on singular value decomposition (SVD) of motion matrices . Eigenvector differences weighed by the corresponding eigenvalues are considered for the proposed similarity measure . Experiments with general hand gestures and human motion streams show that the proposed similarity measure gives good performance for recognizing motion patterns in the motion streams in real time.	motion stream , segmentation , data streams , eigenvector , singular value decomposition , gesture , recognition , eigenvalue , similarity measure , Pattern recognition	A Similarity Measure for Motion Stream Segmentation and Recognition	en	98
99	This paper presents a formalization for Topic Maps (TM). We first simplify TMRM, the current ISO standard proposal for a TM reference model and then characterize topic map instances. After defining a minimal merging operator for maps we propose a formal foundation for a TM query language. This path expression language allows us to navigate through given topic maps and to extract information. We also show how such a language can be the basis for a more industrial version of a query language and how it may serve as foundation for a constraint language to define TM-based ontologies.	Semantic Web , Topic Maps , Knowledge Engineering	The  Model, Formalizing Topic Maps	en	99
100	The slowing pace of commodity microprocessor performance improvements combined with ever-increasing chip power demands has become of utmost concern to computational scientists . As a result, the high performance computing community is examining alternative architectures that address the limitations of modern cache-based designs. In this work, we examine the potential of using the forthcoming STI Cell processor as a building block for future high-end computing systems. Our work contains several novel contributions. First, we introduce a performance model for Cell and apply it to several key scientific computing kernels: dense matrix multiply, sparse matrix vector multiply, stencil computations , and 1D/2D FFTs. The difficulty of programming Cell, which requires assembly level intrinsics for the best performance, makes this model useful as an initial step in algorithm design and evaluation. Next, we validate the accuracy of our model by comparing results against published hardware results, as well as our own implementations on the Cell full system simulator. Additionally, we compare Cell performance to benchmarks run on leading superscalar (AMD Opteron), VLIW (Intel Itanium2), and vector (Cray X1E) architectures. Our work also explores several different mappings of the kernels and demonstrates a simple and effective programming model for Cell's unique architecture . Finally, we propose modest microarchitectural modifications that could significantly increase the efficiency of double-precision calculations. Overall results demonstrate the tremendous potential of the Cell architecture for scientific computations in terms of both raw performance and power efficiency.	GEMM , FFT , Cell processor , three level memory , SpMV , Stencil , sparse matrix	The Potential of the Cell Processor for Scientific Computing	en	100
101	Component Based Development aims at constructing software through the inter-relationship between pre-existing components. However, these components should be bound to a specific application domain in order to be effectively reused. Reusable domain components and their related documentation are usually stored in a great variety of data sources. Thus, a possible solution for accessing this information is to use a software layer that integrates different component information sources. We present a component information integration data layer, based on mediators. Through mediators, domain ontology acts as a technique/formalism for specifying ontological commitments or agreements between component users and providers, enabling more accurate software component information search.	Domain Engineering , Software Classification and Identification , Component Repositories , Component Based Engineering	The Use of Mediation and Ontology Technologies for Software Component Information Retrieval	en	101
102	We present enhancements for UDDI / DAML-S registries allowing cooperative discovery and selection of Web services with a focus on personalization. To find the most useful service in each instance of a request, not only explicit parameters of the request have to be matched against the service offers. Also user preferences or implicit assumptions of a user with respect to common knowledge in a certain domain have to be considered to improve the quality of service provisioning. In the area of Web services the notion of service ontologies together with cooperative answering techniques can take a lot of this responsibility. However, without quality assessments for the relaxation of service requests and queries a personalized service discovery and selection is virtually impossible. This paper focuses on assessing the semantic meaning of query relaxation plans over multiple conceptual views of the service ontology, each one representing a soft query constraint of the user request. Our focus is on the question what constitutes a minimum amount of necessary relaxation to answer each individual request in a cooperative manner. Incorporating such assessments as early as possible we propose to integrate ontology-based discovery directly into UDDI directories or query facilities in service provisioning portals. Using the quality assessments presented here, this integration promises to propel today's Web services towards an intuitive user-centered service provisioning. Categories and Subject Descriptors	selection of the most useful , Web Service Definition Language , Web services , Tree-shaped clipping of ontologies , subsequent execution , Semantic Web , user profiling , The generalization throughout ontologies , ontology resembles common knowledge , Universal Description Discovery and Integration , discovery of possible services , generalization hierarchy of concepts , cooperative service discovery , personalization , preference-based service provisioning , Domain-specific understanding of concepts , Relaxing multiple ontologies	Through Different Eyes  Assessing Multiple Conceptual Views for Querying Web Services	en	102
103	Word prediction can be used for enhancing the communication ability of persons with speech and language impair-ments . In this work, we explore two methods of adapting a language model to the topic of conversation, and apply these methods to the prediction of fringe words.	core vocabulary , identify current topic of conversation , AAC , language modeling , accuracy of prediction method , fringe vocabulary , prediction of fringe words , conversations in the Switchboard corpus , Word prediction , immediate prediction , decrease probability of unrelated words , increase probability of related words , prediction window size , communication rate , construct a new language model , topic modeling	Topic Modeling in Fringe Word Prediction for AAC	en	103
104	In this paper we introduce a probabilistic framework to exploit hierarchy, structure sharing and duration information for topic transition detection in videos. Our probabilistic detection framework is a combination of a shot classification step and a detection phase using hierarchical probabilistic models. We consider two models in this paper: the extended Hierarchical Hidden Markov Model (HHMM) and the Coxian Switching Hidden semi-Markov Model (S-HSMM) because they allow the natural decomposition of semantics in videos, including shared structures, to be modeled directly, and thus enabling efficient inference and reducing the sample complexity in learning. Additionally, the S-HSMM allows the duration information to be incorporated, consequently the modeling of long-term dependencies in videos is enriched through both hierarchical and duration modeling . Furthermore, the use of the Coxian distribution in the S-HSMM makes it tractable to deal with long sequences in video. Our experimentation of the proposed framework on twelve educational and training videos shows that both models outperform the baseline cases (flat HMM and HSMM) and performances reported in earlier work in topic detection . The superior performance of the S-HSMM over the HHMM verifies our belief that duration information is an important factor in video content modeling.	domain knowledge , Topic Transition Detection , A variety in directional styles , semantic relationship of neighborhood scenes , coxian switching hidden semi-markov model , natural hierarchical organization of videos , model educational video content , extended Hierarchical Hidden Markov Model , unified and coherent probabilistic framework , Educational Videos , shot-based semantic classification , their semantically shared substructures , topic transition detection , probabilistic framework , Coxian Switching Hidden semi-Markov Model , Coxian , Hierarchical Markov (Semi-Markov) Models , typical durations of important semantics , modeling temporal correlation , hierarchical hidden markov model	Topic Transition Detection Using Hierarchical Hidden Markov and Semi-Markov Models	en	104
105	Most existing web video search engines index videos by file names, URLs, and surrounding texts. These types of video metadata roughly describe the whole video in an abstract level without taking the rich content, such as semantic content descriptions and speech within the video, into consideration. Therefore the relevance ranking of the video search results is not satisfactory as the details of video contents are ignored.  In this paper we propose a novel relevance ranking approach for Web-based video search using both video metadata and the rich content contained in the videos. To leverage real content into ranking, the videos are segmented into shots, which are smaller and more semantic-meaningful retrievable units, and then more detailed information of video content such as semantic descriptions and speech of each shots are used to improve the retrieval and ranking performance. With video metadata and content information of shots, we developed an integrated ranking approach, which achieves improved ranking performance. We also introduce machine learning into the ranking system, and compare them with IRmodel (information retrieval model) based method. The evaluation results demonstrate the effectiveness of the proposed ranking methods.	video index, relevance ranking , content-based relevance ranking , video retrieval , metadata , learning based ranking , neutral network based ranking , Relevance ranking , content information , content-based approach , ranking method , integrated ranking , video metadata , IR-model , segmented , Content-based ranking , machine learning model , video segmentation , IR model based ranking , Video search , video search	Towards Content-Based Relevance Ranking for Video Search	en	105
106	The growing importance of access control has led to the definition of numerous languages for specifying policies. Since these languages are based on different foundations, language users and designers would benefit from formal means to compare them. We present a set of properties that examine the behavior of policies under enlarged requests, policy growth, and policy decomposition. They therefore suggest whether policies written in these languages are easier or harder to reason about under various circumstances. We then evaluate multiple policy languages, including XACML and Lithium, using these properties.	common features , Access control , lithium , modularity , reasonability property , policy decomposition , properties , access control , policy combinator , XACML , comtemporary policy , access-control policy , policy language property , first order logic , xacml , multiple policy language , policy language , policy , security , formalize , policy languague	Towards Reasonability Properties for Access-Control Policy Languages	en	106
107	In a wide range of business areas dealing with text data streams, including CRM, knowledge management, and Web monitoring services, it is an important issue to discover topic trends and analyze their dynamics in real-time.Specifically we consider the following three tasks in topic trend analysis: 1)Topic Structure Identification; identifying what kinds of main topics exist and how important they are, 2)Topic Emergence Detection; detecting the emergence of a new topic and recognizing how it grows, 3)Topic Characterization ; identifying the characteristics for each of main topics. For real topic analysis systems, we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way, and be dealt with in a single framework. This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically.In this framework we propose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure identification .This enables tracking the topic structure adaptively by forgetting out-of-date statistics.Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection.We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion.	finite mixture model , CRM , time-stamp based discounting learning algorithm , topic structure identification , topic characterization , topic detection and tracking , time-stamp based learning algorithm , Topic Structure Identification , topic emergence detection , text mining , Topic Emergence Detection , tracking dynamics , dynamic model selection , Data Mining , information gain , topic trends , Topic Characterization , text data streams , model selection , topic trend , topic analysis	Tracking Dynamics of Topic Trends Using a Finite Mixture Model	en	107
108	This paper shows the importance that management plays in the protection of information and in the planning to handle a security breach when a theft of information happens.  Recent thefts of information that have hit major companies have caused concern. These thefts were caused by companies' inability to determine risks associated with the protection of their data and these companies lack of planning to properly manage a security breach when it occurs.  It is becoming necessary, if not mandatory, for organizations to perform ongoing risk analysis to protect their systems.  Organizations need to realize that the theft of information is a management issue as well as a technology one, and that these recent security breaches were mainly caused by business decisions by management and not a lack of technology.	security breach , risk analysis , Information Security , business practises and policy , information system , cases of information theft , privacy , management issue , Information Security Management , theft of information , human factor , data protection procedure , Security Management , information security , cyber crime , confidential information , incident response plan , encryption , data protection , personal information	A Case Study on How to Manage the Theft of Information	en	108
109	Information seeking and management practices are an integral aspect of people's daily work. However, we still have little understanding of collaboration in the information seeking process. Through a survey of collaborative information seeking practices of academic researchers, we found that researchers reported that (1) the lack of expertise is the primary reason that they collaborate when seeking information; (2) traditional methods, including face-to-face, phone, and email are the preferred communication mediums for collaboration; and (3) collaborative information seeking activities are usually successful and more useful than individually seeking information. These results begin to highlight the important role that collaborative information seeking plays in daily work.	Academic Researchers , communication media , information seeking , Group Work , Survey , collaboration , Collaborative Information Seeking	A Survey of Collaborative Information Seeking Practices of Academic Researchers	en	109
110	A transactional agent is a mobile agent which manipulates objects in multiple computers by autonomously finding a way to visit the computers. The transactional agent commits only if its commitment condition like atomicity is satisfied in presence of faults of computers. On leaving a computer , an agent creates a surrogate agent which holds objects manipulated. A surrogate can recreate a new incarnation of the agent if the agent itself is faulty. If a destination computer is faulty, the transactional agent finds another operational computer to visit. After visiting computers, a transactional agent makes a destination on commitment according to its commitment condition. We discuss design and implementation of the transactional agent which is tolerant of computer faults.	fault-tolerant agent , transactional agent , Transaction , ACID transaction , surrogate agent , Mobile agent , Fault-Tolerant , fault-tolerant , computer fault , mobile agent , transaction processing	Transactional Agent Model for Fault-Tolerant Object Systems	en	110
111	Users' cross-lingual queries to a digital library system might be short and not included in a common translation dictionary (unknown terms). In this paper, we investigate the feasibility of exploiting the Web as the corpus source to translate unknown query terms for cross-language information retrieval (CLIR) in digital libraries. We propose a Web-based term translation approach to determine effective translations for unknown query terms by mining bilingual search-result pages obtained from a real Web search engine. This approach can enhance the construction of a domain-specific bilingual lexicon and benefit CLIR services in a digital library that only has monolingual document collections. Very promising results have been obtained in generating effective translation equivalents for many unknown terms, including proper nouns, technical terms and Web query terms.	Information Search and Retrieval , Web Mining , Term Translation , translation dictionary , Context Vector Analysis , Unknown Cross-Lingual Queries , Web-based term translation approach , Cross-Language Information Retrieval , BILINGUAL LEXICON CONSTRUCTION , Digital Library , PAT-Tree Based Local Maxima Algorithm , CLIR services , Term Extraction , Digital Libraries	Translating Unknown Cross-Lingual Queries in Digital Libraries Using a Web-based Approach	en	111
112	A type-indexed function is a function that is defined for each member of some family of types. Haskell's type class mechanism provides collections of open type-indexed functions, in which the indexing family can be extended by defining a new type class instance but the collection of functions is fixed. The purpose of this paper is to present TypeCase: a design pattern that allows the definition of closed type-indexed functions, in which the index family is fixed but the collection of functions is extensible. It is inspired by Cheney and Hinze's work on lightweight approaches to generic programming. We generalise their techniques as a design pattern . Furthermore, we show that type-indexed functions with type-indexed types, and consequently generic functions with generic types, can also be encoded in a lightweight manner, thereby overcoming one of the main limitations of the lightweight approaches.	Generic programming , type-indexed functions , type classes	TypeCase: A Design Pattern for Type-Indexed Functions	en	112
113	The research field of Intelligent Service Robots, which has become more and more popular over the last years, covers a wide range of applications from climbing machines for cleaning large storefronts to robotic assistance for disabled or elderly people. When developing service robot software, it is a challenging problem to design the robot architecture by carefully considering user needs and requirements, implement robot application components based on the architecture, and integrate these components in a systematic and comprehensive way for maintainability and reusability. Furthermore, it becomes more difficult to communicate among development teams and with others when many engineers from different teams participate in developing the service robot.  To solve these problems, we applied the COMET design method, which uses the industry-standard UML notation, to developing the software of an intelligent service robot for the elderly, called T-Rot, under development at Center for Intelligent Robotics (CIR). In this paper, we discuss our experiences with the project in which we successfully addressed these problems and developed the autonomous navigation system of the robot with the COMET/UML method.	Software engineering , object-oriented analysis and design methods , service robot development , UML	UML-Based Service Robot Software Development: A Case Study	en	113
114	This  paper  presents  a  unified  utility  framework  for  resource selection  of  distributed  text  information  retrieval.  This  new framework  shows  an  efficient  and  effective  way  to  infer  the probabilities  of  relevance  of  all  the  documents  across  the  text databases.  With  the  estimated  relevance  information,  resource selection  can  be  made  by  explicitly  optimizing  the  goals  of different  applications.  Specifically,  when  used  for  database recommendation, the selection is optimized for the goal of high-recall (include  as  many  relevant  documents  as  possible  in  the selected  databases);  when  used  for  distributed  document retrieval,  the  selection  targets  the  high-precision  goal  (high precision in the final merged list of documents). This new model provides  a  more  solid  framework  for  distributed  information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.	resource selection , distributed information retrieval	Unified Utility Maximization Framework for Resource Selection	en	114
115	"The presence of ""unwanted"" (or background) traffic in the Internet is a well-known fact. In principle any network that has been engineered without taking its presence into account might experience troubles during periods of massive exposure to unwanted traffic, e.g. during large-scale infections. A concrete example was provided by the spreading of Code-Red-II in 2001, which caused several routers crashes worldwide. Similar events might take place in 3G networks as well, with further potential complications arising from their high functional complexity and the scarcity of radio resources. For example, under certain hypothetical network configuration settings unwanted traffic, and specifically scanning traffic from infected Mobile Stations, can cause large-scale wastage of logical resources, and in extreme cases even starvation. Unwanted traffic is present nowdays also in GPRS/UMTS, mainly due to the widespread use of 3G connect cards for laptops. We urge the research community and network operators to consider the issue of 3G robustness to unwanted traffic as a prominent research area."	Unwanted traffic , Cellular networks , 3G	Unwanted Traffic in 3G Networks	en	115
116	The tools used to search and find Learning Objects in different systems do not provide a meaningful and scalable way to rank or recommend learning material.  This work propose and detail the use of  Contextual Attention Metadata, gathered from the different tools used in the lifecycle of the Learning Object, to create ranking and recommending metrics to improve the user experience.  Four types of metrics are detailed:  Link Analysis Ranking, Similarity Recommendation, Personalized Ranking and Contextual Recommendation. While designed for Learning Objects, it is shown that these metrics could also be applied to rank and recommend other types of reusable components like software libraries.	Learning Objects , Ranking , Attention Metadata , Recommending	Use of Contextualized Attention Metadata for Ranking and Recommending Learning Objects	en	116
117	Software  systems  evolve  over  time  due  to  changes  in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change.  We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code churn are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.	principal component analysis , Relative code churn , defect density , fault-proneness , multiple regression	Use of Relative Code Churn Measures to Predict System Defect Density	en	117
118	With the underlying W-CDMA technique in 3G networks, resource management is a very significant issue as it can directly influence the system capacity and also lead to system QoS. However, the resource can be dynamically managed in order to maintain the QoS according to the SLA. In this paper, CBR is used as part of an intelligent-based agent management system. It uses information from previously managed situations to maintain the QoS in order to meet the SLA. The results illustrate the performance of an agent in traffic pattern recognition in order to identify the specific type of problem and finally propose the right solution.	Service Level Agreement , Intelligent agent and Case-based reasoning , 3G Resource Management	Using Case-Based Reasoning in Traffic Pattern Recognition for Best Resource Management in 3G Networks	en	118
119	Business process modeling focus on describing how activities interact with other business objects while sustaining the organization's strategy. Business objects are object-oriented representations of organizational concepts, such as resources and actors, which collaborate with one another in order to achieve business goals. These objects exhibit different behavior according to each specific collaboration context. This means the perception of a business object depends on its collaborations with other objects. Business process modeling techniques do not clearly separate the multiple collaborative aspects of a business object from its internal aspects, making it difficult to understand objects which are used in different contexts, thus hindering reuse. To cope with such issues, this paper proposes using role modeling as a separation of concerns mechanism to increase the understandability and reusability of business process models. The approach divides a business process model into a business object model and a role model. The business object models deals with specifying the structure and intrinsic behavior of business objects while the role model specifies its collaborative aspects.	Business Object , Business Process Modeling , Role Modeling , Organizational Engineering	Using Roles and Business Objects to Model and Understand Business Processes	en	119
120	Researchers  have  explored  the  design  of  ambient  information systems across  a wide range of physical and screen-based media. This work has yielded rich examples of design approaches to the problem of presenting information about a user's world in a way that is not distracting, but is aesthetically pleasing, and tangible to varying degrees. Despite these successes, accumulating theoretical and  craft  knowledge  has  been  stymied  by  the  lack  of  a  unified vocabulary  to  describe  these  systems  and  a  consequent  lack  of  a framework for understanding their design attributes. We argue that this  area  would  significantly  benefit  from  consensus  about  the design  space  of  ambient  information  systems  and  the  design attributes  that  define  and  distinguish  existing  approaches.  We present  a  definition  of  ambient  information  systems  and  a taxonomy  across  four  design  dimensions:  Information  Capacity, Notification  Level,  Representational  Fidelity,  and  Aesthetic Emphasis.  Our  analysis  has  uncovered  four  patterns  of  system design  and  points  to  unexplored  regions  of  the  design  space, which may motivate future work in the field.	Peripheral Display , four main design patterns , calm computing , symbolic Sculptural display , high throughput textual display , Notification System , information monitor display , ambient information system , Taxonomy , framework to understand design attributes , user interface , notification systems and peripheral displays , Design Guidelines , multiple-information consolidators , Ambient Display , definition and characteristics of ambient systems , Ubiquitous Computing	A Taxonomy of Ambient Information Systems: Four Patterns of Design	en	120
121	ABSTRACT Personalized information agents can help overcome some of the limitations of communal Web information sources such as portals and search engines. Two important components of these agents are: user profiles and information filtering or gathering services. Ideally, these components can be sep-arated so that a single user profile can be leveraged for a variety of information services. Toward that end, we are building an information agent called SurfAgent;in previous studies, we have developed and tested methods for automatically learning a user profile [20]. In this paper, we evaluate alternative methods for recommending new documents to a user by generating queries from the user profile and submitting them to a popular search engine. Our study focuses on three questions: How do different algorithms for query generation perform relative to each other? Is positive relevance feedback adequate to support the task? Can a user profile be learned independent of the service? We found that three algorithms appear to excel and that using only positive feedback does degrade the results somewhat. We conclude with the results of a pilot user study for assessing interaction of the profile and the query generation mechanisms.	query generation , information agents , user modeling	Using Web Helper Agent Profiles in Query Generation	en	121
122	This paper presents a novel macroblock mode decision algorithm for inter-frame prediction based on machine learning techniques to be used as part of a very low complexity MPEG-2 to H.264 video transcoder. Since coding mode decisions take up the most resources in video transcoding, a fast macro block (MB) mode estimation would lead to reduced complexity. The proposed approach is based on the hypothesis that MB coding mode decisions in H.264 video have a correlation with the distribution of the motion compensated residual in MPEG-2 video. We use machine learning tools to exploit the correlation and derive decision trees to classify the incoming MPEG-2 MBs into one of the 11 coding modes in H.264. The proposed approach reduces the H.264 MB mode computation process into a decision tree lookup with very low complexity. The proposed transcoder is compared with a reference transcoder comprised of a MPEG-2 decoder and an H.264 encoder. Our results show that the proposed transcoder reduces the H.264 encoding time by over 95% with negligible loss in quality and bitrate.	H.264 , Inter-frame , Machine Learning , Transcoding , MPEG-2	Very Low Complexity MPEG-2 to H.264 Transcoding Using Machine Learning	en	122
123	The emergence of third-generation (3G) mobile networks offers new opportunities for the effective delivery of data with rich content including multimedia messaging and video-streaming. Provided that streaming services have proved highly successful over stationary networks in the past, we anticipate that the same trend will soon take place in 3G networks. Although mobile operators currently make available pertinent services, the available resources of the underlying networks for the delivery of rich data remain in-herently constrained. At this stage and in light of large numbers of users moving fast across cells, 3G networks may not be able to warrant the needed quality-of-service requirements. The support for streaming services necessitates the presence of content or media servers properly placed over the 3G network; such servers essen-tially become the source for streaming applications. Evidently, a centralized approach in organizing streaming content might lead to highly congested media-nodes which in presence of moving users will certainly yield increased response times and jitter to user requests . In this paper, we propose a workaround that enables 3G networks to offer uninterrupted video-streaming services in the presence of a large number of users moving in high-speed. At the same time, we offer a distributed organization for the network's media-servers to better handle over-utilization.	mobile multimedia services , rate adaptation , real-time streaming , Streaming for moving users	Video-Streaming for Fast Moving Users in 3G Mobile Networks	en	123
124	We address the problem of integrating objects from a source taxonomy into a master taxonomy. This problem is not only currently pervasive on the web, but also important to the emerging semantic web. A straightforward approach to automating this process would be to learn a classifier that can classify objects from the source taxonomy into categories of the master taxonomy. The key insight is that the availability of the source taxonomy data could be helpful to build better classifiers for the master taxonomy if their categorizations have some semantic overlap. In this paper, we propose a new approach, co-bootstrapping , to enhance the classification by exploiting such implicit knowledge. Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration.	Taxonomy Integration , Bootstrapping , Semantic Web , Classification , Ontology Mapping , Machine Learning , Boosting	Web Taxonomy Integration through Co-Bootstrapping	en	124
125	Today web search engines provide the easiest way to reach information on the web. In this scenario, more than 95% of Indian language content on the web is not searchable due to multiple encodings of web pages. Most of these encodings are proprietary and hence need some kind of standardization for making the content accessible via a search engine. In this paper we present a search engine called WebKhoj which is capable of searching multi-script and multi-encoded Indian language content on the web. We describe a language focused crawler and the transcoding processes involved to achieve accessibility of Indian langauge content. In the end we report some of the experiments that were conducted along with results on Indian language web content.	web search , Indian languages , non-standard encodings	WebKhoj: Indian language IR from Multiple Character Encodings	en	125
126	Some large scale topical digital libraries, such as CiteSeer, harvest online academic documents by crawling open-access archives, university and author homepages, and authors' self-submissions. While these approaches have so far built reasonable size libraries, they can suffer from having only a portion of the documents from specific publishing venues. We propose to use alternative online resources and techniques that maximally exploit other resources to build the complete document collection of any given publication venue. We investigate the feasibility of using publication metadata to guide the crawler towards authors' homepages to harvest what is missing from a digital library collection. We collect a real-world dataset from two Computer Science publishing venues, involving a total of 593 unique authors over a time frame of 1998 to 2004. We then identify the missing papers that are not indexed by CiteSeer. Using a fully automatic heuristic-based system that has the capability of locating authors' homepages and then using focused crawling to download the desired papers, we demonstrate that it is practical to harvest using a focused crawler academic papers that are missing from our digital library. Our harvester achieves a performance with an average recall level of 0.82 overall and 0.75 for those missing documents. Evaluation of the crawler's performance based on the harvest rate shows definite advantages over other crawling approaches and consistently outperforms a defined baseline crawler on a number of measures.	Digital libraries , CiteSeer , focused crawler , DBLP , harvesting , ACM	What's There and What's Not? Focused Crawling for Missing Documents in Digital Libraries	en	126
127	Hidden Web databases maintain a collection of specialised documents, which are dynamically generated in response to users' queries. However, the documents are generated by Web page templates, which contain information that is irrelevant to queries. This paper presents a Two-Phase Sampling (2PS) technique that detects templates and extracts query-related information from the sampled documents of a database. In the first phase, 2PS queries databases with terms contained in their search interface pages and the subsequently sampled documents. This process retrieves a required number of documents. In the second phase, 2PS detects Web page templates in the sampled documents in order to extract information relevant to queries. We test 2PS on a number of real-world Hidden Web databases. Experimental results demonstrate that 2PS effectively eliminates irrelevant information contained in Web page templates and generates terms and frequencies with improved accuracy.	Hidden Web Databases , search interface pages , Information Extraction , hypertext markup langauges , hidden web databases , 2-phase sampling technique , neighbouring adjacent tag segments , string matching techniques , information extraction , web page templates , Document Sampling , query-based sampling , irrelavant information extraction	A Two-Phase Sampling Technique for Information Extraction from Hidden Web Databases	en	127
128	In this paper, we introduce a unified approach for the adaptive control of 3G mobile networks in order to improve both quality of service (QoS) for mobile subscribers and to increase revenue for service providers. The introduced approach constantly monitors QoS measures as packet loss probability and the current number of active mobile users during operation of the network. Based on the values of the QoS measures just observed, the system parameters of the admission controller and packet scheduler are controlled by the adaptive performance management entity. Considering UMTS, we present performance curves showing that handover failure probability is improved by more than one order of magnitude. Moreover, the packet loss probability can be effectively regulated to a predefined level and provider revenue is significantly increased for all pricing policies.	QoS , packet loss probability , Quality of Service in mobile systems , provider revenue , performance evaluation of next generation mobile systems , packet scheduler , adaptive performance management , admission control in mobile system , pricing policy , admission control , 3G mobile networks , pricing and revenue optimization	A Unified Approach for Improving QoS and Provider Revenue in 3G Mobile Networks	en	128
129	Facet-based component retrieval techniques have been proved to be an effective way for retrieving. These Techniques are widely adopted by component library systems, but they usually simply list out all the retrieval results without any kind of ranking. In our work, we focus on the problem that how to determine the ranks of the components retrieved by user. Factors which can influence the ranking are extracted and identified through the analysis of ER-Diagram of facet-based component library system. In this paper, a mathematical model of weighted ranking algorithm is proposed and the timing of ranks calculation is discussed. Experiment results show that this algorithm greatly improves the efficiency of component retrieval system.	retrieval system , facet , component rank , component retrieval , and component library , ranking algorithm , Weighted ranking algorithm , matching degree , facet-based component retrieval , component library	A WEIGHTED RANKING ALGORITHM FOR FACET-BASED COMPONENT RETRIEVAL SYSTEM	en	129
130	The organization of HTML into a tag tree structure, which is rendered by browsers as roughly rectangular regions with embedded text and HREF links, greatly helps surfers locate and click on links that best satisfy their information need. Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need, based on information limited to the HREF source page? Such a capability would be of great interest in focused crawling and resource discovery, because it can fine-tune the priority of unvisited URLs in the crawl frontier, and reduce the number of irrelevant pages which are fetched and discarded. We show that there is indeed a great deal of usable information on a HREF source page about the relevance of the target page. This information, encoded suitably, can be exploited by a supervised apprentice which takes online lessons from a traditional focused crawler by observing a carefully designed set of features and events associated with the crawler. Once the apprentice gets a sufficient number of examples, the crawler starts consulting it to better prioritize URLs in the crawl frontier. Experiments on a dozen topics using a 482-topic taxonomy from the Open Directory (Dmoz) show that online relevance feedback can reduce false positives by 30% to 90%.	focused crawlers , Reinforcement learning , URLs , Focused crawling , taxonomy , DOM , HREF link , classifiers , Document object model	Accelerated Focused Crawling through Online Relevance Feedback	en	130
131	Many volume filtering operations used for image enhancement, data processing or feature detection can be written in terms of three-dimensional convolutions. It is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines. As modern graphics workstations have the ability to render two-dimensional convoluted images to the frame buffer, this feature can be used to accelerate the process significantly. This way generic 3D convolution can be added as a powerful tool in interactive volume visualization toolkits.	3D convolution , Convolution , visualization , filtering , Volume Visualization , Hardware Acceleration , volume rendering	Accelerating 3D Convolution using Graphics Hardware	en	131
132	"As today the amount of accessible information is overwhelming, the intelligent and personalized filtering of available information is a main challenge. Additionally, there is a growing need for the seamless mobile and multi-modal system usage throughout the whole day to meet the requirements of the modern society (""anytime, anywhere, anyhow""). A personal information agent that is delivering the right information at the right time by accessing, filtering and presenting information in a situation-aware matter is needed. Applying Agent-technology is promising, because the inherent capabilities of agents like autonomy, pro- and reactiveness offer an adequate approach. We developed an agent-based personal information system called PIA for collecting, filtering, and integrating information at a common point, offering access to the information by WWW, e-mail, SMS, MMS, and J2ME clients. Push and pull techniques are combined allowing the user to search explicitly for specific information on the one hand and to be informed automatically about relevant information divided in pre-, work and recreation slots on the other hand. In the core of the PIA system advanced filtering techniques are deployed through multiple filtering agent communities for content-based and collaborative filtering. Information-extracting agents are constantly gathering new relevant information from a variety of selected sources (internet, files, databases, web-services etc.). A personal agent for each user is managing the individual information provisioning, tailored to the needs of this specific user, knowing the profile, the current situation and learning from feedback."	Adaptation and Learning , filtering , Recommendation systems , Agent-based deployed applications , Evolution , Intelligent and personalized filtering , Agents and complex systems , personal information agent , agent technology , Ubiquitous access	Agent Technology for Personalized Information Filtering: The PIA-System	en	132
133	In this paper we present a multilingual information retrieval system that provides access to Tourism information by exploiting the intuitiveness of natural language. In particular, we describe the knowledge representation model underlying the information retrieval system. This knowledge representation approach is based on associative networks and allows the definition of semantic relationships between domain-intrinsic information items. The network structure is used to define weighted associations between information items and augments the system with a fuzzy search strategy. This particular search strategy is performed by a constrained spreading activation algorithm that implements information retrieval on associative networks. Strictly speaking, we take the relatedness of terms into account and show, how this fuzzy search strategy yields beneficial results and, moreover, determines highly associated matches to users' queries. Thus, the combination of the associative network and the constrained spreading activation approach constitutes a search algorithm that evaluates the relatedness of terms and, therefore, provides a means for implicit query expansion.	natural language information retrieval , constrained spreading activation , query expansion , spreading activation , multilingual information retrieval system , knowledge representation model , associative networks , knowledge representation , natural language query	An Adaptive Information Retrieval System based on Associative Networks	en	133
134	The dramatic increase in demand for wireless Internet access has lead to the introduction of new wireless architectures and systems including 3G, Wi-Fi and WiMAX.  3G systems such as UMTS and CDMA2000 are leaning towards an all-IP architecture for transporting IP multimedia services, mainly due to its scalability and promising capability of inter-working heterogeneous wireless access networks. During the last ten years, substantial work has been done to understand the nature of wired IP traffic and it has been proven that IP traffic exhibits self-similar properties and burstiness over a large range of time scales. Recently, because of the large deployment of new wireless architectures, researchers have focused their attention towards understanding the nature of traffic carried by different wireless architecture and early studies have shown that wireless data traffic also exhibits strong long-range dependency. Thus, the classical tele-traffic theory based on a simple Markovian process cannot be used to evaluate the performance of wireless networks. Unfortunately, the area of understanding and modeling of different kinds of wireless traffic is still immature which constitutes a problem since it is crucial to guarantee tight bound QoS parameters to heterogeneous end users of the mobile Internet. In this paper, we make several contributions to the accurate modeling of wireless IP traffic by presenting a novel analytical model that takes into account four different classes of self-similar traffic. The model consists of four queues and is based on a G/M/1 queueing system. We analyze it on the basis of priority with no preemption and find exact packet delays. To date, no closed form expressions have been presented for G/M/1 with priority.	QoS , 3G networks , traffic modelling , 3G , Self-Similar , GGSN , self-similar traffic , wireless IP traffic , UMTS , queuing model	An Analytical Model Based on G/M/1 with Self-Similar Input to Provide End-to-End QoS in 3G Networks	en	134
135	We present a foundation for a computational meta-theory of languages with bindings implemented in a computer-aided formal reasoning environment. Our theory provides the ability to reason abstractly about operators, languages, open-ended languages, classes of languages, etc. The theory is based on the ideas of higher-order abstract syntax, with an appropriate induction principle parameterized over the language (i.e. a set of operators) being used. In our approach , both the bound and free variables are treated uniformly and this uniform treatment extends naturally to variable-length bindings . The implementation is reflective, namely there is a natural mapping between the meta-language of the theorem-prover and the object language of our theory. The object language substitution operation is mapped to the meta-language substitution and does not need to be defined recursively. Our approach does not require designing a custom type theory; in this paper we describe the implementation of this foundational theory within a general-purpose type theory. This work is fully implemented in the MetaPRL theorem prover, using the pre-existing NuPRL-like Martin-Lof-style computational type theory. Based on this implementation, we lay out an outline for a framework for programming language experimentation and exploration as well as a general reflective reasoning framework. This paper also includes a short survey of the existing approaches to syntactic reflection.	system reflection , programming language , High order abstract syntax , formal languages , Theorem prover , NuPRL , Meta-syntax , MetaPRL theorem prover , Languages with bindings , Uniform reflection framework , Higher-Order Abstract Syntax , Bruijn-style operations , HOAS-style operations , NuPRL-like Martin-Lof-style computational type theory , higher-order abstract syntax , Type Theory , formal definition and theory , computer aided reasoning , Meta-reasoning , Recursive definition , Reflective reasoning , Reflection , Languages with Bindings , Substitution , MetaPRL , Runtime code generation , Meta-language , uniform reflection framework , Theory of syntax , Programming Language Experimentation	A Computational Approach to Reflective Meta-Reasoning about Languages with Bindings	en	135
136	Researchers with deep knowledge of scientific domains are now developing highly-adaptive and irregular (asymmetrical ) parallel computations, leading to challenges in both delivery of data for computation and mapping of processes to physical resources. Using software engineering principles, we have developed a new communications protocol and architectural style for asymmetrical parallel computations called ADaPT. Utilizing the support of architecturally-aware middleware, we show that ADaPT provides a more efficient solution in terms of message passing and load balancing than asymmetrical parallel computations using collective calls in the Message-Passing Interface (MPI) or more advanced frameworks implementing explicit load-balancing policies. Additionally , developers using ADaPT gain significant windfall from good practices in software engineering, including implementation-level support of architectural artifacts and separation of computational loci from communication protocols	collective calls , ADaPT , MPI , software engineering , asymamtrical parallel computations , load balancing , communication protocols , high-performance computing , High-Performance Computing , Asymmetrical Parallel Computations	An Architectural Style for High-Performance Asymmetrical Parallel Computations	en	136
137	"Research in bioinformatics is driven by the experimental data. Current biological databases are populated by vast amounts of experimental data. Machine learning has been widely applied to bioinformatics and has gained a lot of success in this research area.  At present, with various learning algorithms available in the literature, researchers are facing difficulties in choosing the best method that can apply to their data. We performed an empirical study on 7 individual learning systems and 9 different combined methods on 4 different biological data sets, and provide some suggested issues to be considered when answering the following questions: (i) How does one choose which algorithm is best suitable for their data set? (ii) Are combined methods better than a single approach? (iii) How does one compare the effectiveness of a particular algorithm to <A href=""31.html#1"">the others?"	classification , Supervised machine learning , cross validation , performance evaluation , training data , biological data , supervised machine learning , machine learning , ensemble methods , bioinformatics	An empirical comparison of supervised machine learning techniques in bioinformatics	en	137
138	C applications, in particular those using operating system level services, frequently comprise multiple crosscutting concerns : network protocols and security are typical examples of such concerns. While these concerns can partially be addressed during design and implementation of an application, they frequently become an issue at runtime, e.g., to avoid server downtime. A deployed network protocol might not be efficient enough and may thus need to be replaced. Buffer overflows might be discovered that imply critical breaches in the security model of an application. A prefetching strategy may be required to enhance performance. While aspect-oriented programming seems attractive in this context, none of the current aspect systems is expressive and efficient enough to address such concerns. This paper presents a new aspect system to provide a solution to this problem. While efficiency considerations have played an important part in the design of the aspect language, the language allows aspects to be expressed more concisely than previous approaches. In particular, it allows aspect programmers to quantify over sequences of execution points as well as over accesses through variable aliases. We show how the former can be used to modularize the replacement of network protocols and the latter to prevent buffer overflows. We also present an implementation of the language as an extension of Arachne, a dynamic weaver for C applications. Finally, we present performance evaluations supporting that Arachne is fast enough to extend high performance applications , such as the Squid web cache.	aspect language , buffer overflows , prefetching , sequence pointcut , system applications , binary code , dynamic weaving , Arachne , web cache , operating system , network protocol , C applications	An expressive aspect language for system applications with Arachne	en	138
139	It is important for successful electronic business to have a hi-quality business website. So we need an accurate and effective index system to evaluate and analyses the quality of the business website. In this paper, the evaluation index system following the `grey box' principle is proposed which considers both efficiency of business website and performance of electronic business system. Using R-Hierarchical clustering method to extract the typical indexes from sub-indexes is theoretically proved to have a rationality and effectiveness. Finally, the evaluation method is briefly discussed.	Business website , B2C websites , System performance , representitive indexes , fuzzy analysis , R-Hierarchical clustering , Evaluation system , quality synthesis evaluation , quality evaluation index , R-Hierarchical clustering method , index optimization , e-commerce , clustering , correlation index	An Index System for Quality Synthesis Evaluation of BtoC Business Website	en	139
140	In this paper, we present an expressive 3D animation environment that enables users to rapidly and visually prototype animated worlds with a fully 3D user-interface. A 3D device allows the specification of complex 3D motion, while virtual tools are visible mediators that live in the same 3D space as application objects and supply the interaction metaphors to control them.  In our environment, there is no intrinsic difference between user interface and application objects. Multi-way constraints provide the necessary tight coupling among components that makes it possible to seamlessly compose animated and interactive behaviors. By recording the effects of manipulations, all the expressive power of the 3D user interface is exploited to define animations. Effective editing of recorded manipulations is made possible by compacting all continuous parameter evolutions with an incremental data-reduction algorithm, designed to preserve both geometry and timing. The automatic generation of editable representations of interactive performances overcomes one of the major limitations of current performance animation systems. Novel interactive solutions to animation problems are made possible by the tight integration of all system components. In particular, animations can be synchronized by using constrained manipulation during playback. The accompanying video-tape illustrates our approach with interactive sequences showing the visual construction of 3D animated worlds. All the demonstrations in the video were recorded live and were not edited.	animation synchronization , computer graphics , Object-Oriented Graphics , 3d animation environment , data reduction , visualization , multi-way constrained architecture , human interaction , Data Reduction , 3D Animation , Local Propagation Constraints , recording 3d manipulation , Virtual Tools , 3D Widgets , 3d user interface , 3D Interaction , dynamic model	An Integrated Environment to Visually Construct 3D Animations	en	140
141	When testing database applications, in addition to creating in-memory fixtures it is also necessary to create an initial database state that is appropriate for each test case. Current approaches either require exact database states to be specified in advance, or else generate a single initial state (under guidance from the user) that is intended to be suitable for execution of all test cases. The first method allows large test suites to be executed in batch, but requires considerable programmer effort to create the test cases (and to maintain them). The second method requires less programmer effort, but increases the likelihood that test cases will fail in non-fault situations, due to unexpected changes to the content of the database. In this paper, we propose a new approach in which the database states required for testing are specified intensionally, as constrained queries, that can be used to prepare the database for testing automatically . This technique overcomes the limitations of the other approaches, and does not appear to impose significant performance overheads.	database testing , Efficient Testing , software testing , Seamless Integration , Query Based Language , Improvement for the Intensional Test Cases , DOT-UNIT , databases , Lesser Programmer Effort for Test Cases , Intensional Test Cases , Testing Framework , Testing for Database Systems , Performance Testing	An Intensional Approach to the Specification of Test Cases for Database Applications	en	141
142	A neural network based clustering method for the analysis of soft handovers in 3G network is introduced. The method is highly visual and it could be utilized in explorative analysis of mobile networks. In this paper, the method is used to find groups of similar mobile cell pairs in the sense of handover measurements. The groups or clusters found by the method are characterized by the rate of successful handovers as well as the causes of failing handover attempts. The most interesting clusters are those which represent certain type of problems in handover attempts. By comparing variable histograms of a selected cluster to histograms of the whole data set an application domain expert may find some explanations on problems. Two clusters are investigated further and causes of failing handover attempts are discussed.	Two-Phase Clustering Algorithm , data mining , mobility management , Key Performance Indicator of Handover , soft handover , Data Mining , Soft Handover , Visualization Capability , Neural Network Algorithm , neural networks , hierarchical clustering , Self-Organizing Map , Cluster Analysis , Histograms , 3G network , Decrease in Computational Complexity	Analysis of Soft Handover Measurements in 3G Network	en	142
143	Aspect Oriented Programming, a relatively new programming paradigm, earned the scientific community's attention . The paradigm is already evaluated for traditional OOP and component-based software development with remarkable results. However, most of the published work, while of excellent quality, is mostly theoretical or involves evaluation of AOP for research oriented and experimental software . Unlike the previous work, this study considers the AOP paradigm for solving real-life problems, which can be faced in any commercial software. We evaluate AOP in the development of a high-performance component-based web-crawling system, and compare the process with the development of the same system without AOP. The results of the case study mostly favor the aspect oriented paradigm.	case study , evaluation , Web Crawler Implementation , AOP , component-based application , Aspect Oriented Programming , development process experiment metrics , Software development process experiment , programming paradigm comparison , OOP , Object Oriented Programming , programming paradigms , Aspect Oriented Programming application	Aspect Oriented Programming for a component-based real life application: A case study	en	143
144	In a large sensor network, in-network data aggregation, i.e., combining partial results at intermediate nodes during message routing, significantly reduces the amount of communication and hence the energy consumed. Recently several researchers have proposed robust aggregation frameworks, which combine multi-path routing schemes with duplicate-insensitive algorithms, to accurately compute aggregates (e.g., Sum, Count, Average) in spite of message losses resulting from node and transmission failures. However, these aggregation frameworks have been designed without security in mind. Given the lack of hardware support for tamper-resistance and the unattended nature of sensor nodes, sensor networks are highly vulnerable to node compromises. We show that even if a few compromised nodes contribute false sub-aggregate values, this results in large errors in the aggregate computed at the root of the hierarchy. We present modifications to the aggregation algorithms that guard against such attacks, i.e., we present algorithms for resilient hierarchical data aggregation despite the presence of compromised nodes in the aggregation hierarchy. We evaluate the performance and costs of our approach via both analysis and simulation . Our results show that our approach is scalable and efficient.	sensor networks , node compromise prevention , falsified local value attack , in-network data aggregation , Attack resilient heirarchical data aggregation , Sum aggregate , falsified sub-aggregate attack , Attack-Resilient , Count aggregate , Sensor Network Security , robust aggregation , Synopsis Diffusion , Data Aggregation , synopsis diffusion aggregation framework , network aggregation algorithms , Hierarchical Aggregation	Attack-Resilient Hierarchical Data Aggregation in Sensor Networks	en	144
145	To have a rich presentation of a topic, it is not only expected that many relevant multimodal information, including images, text, audio and video, could be extracted; it is also important to organize and summarize the related information, and provide users a concise and informative storyboard about the target topic. It facilitates users to quickly grasp and better understand the content of a topic.  In this paper, we present a novel approach to automatically generating a rich presentation of a given semantic topic. In our proposed approach, the related multimodal information of a given topic is first extracted from available multimedia databases or websites.  Since each topic usually contains multiple events, a text-based event clustering algorithm is then performed with a generative model.  Other media information, such as the representative images, possibly available video clips and flashes (interactive animates), are associated with each related event. A storyboard of the target topic is thus generated by integrating each event and its corresponding multimodal information.  Finally, to make the storyboard more expressive and attractive, an incidental music is chosen as background and is aligned with the storyboard. A user study indicates that the presented system works quite well on our testing examples.	documentary and movie , Rich presentation , events clustering , Communication and Multimedia , Representative Media , Images, videos and Audio Technologies , Rich video clips and flashes , Multi-modal information , Generate storyboard , storyboard , Subjective multiple events , multimedia fusion , High-level semantics , Event clustering , multimodality , multimedia authoring	Automated Rich Presentation of a Semantic Topic	en	145
146	Database security has paramount importance in industrial, civilian and government domains. Despite its importance, our search reveals that only a small number of database security courses are being offered. In this paper, we share our experience in developing and offering an undergraduate elective course on database security with limited resources. We believe that database security should be considered in its entirety rather than being component specific. Therefore , we emphasize that students develop and implement a database security plan for a typical real world application . In addition to the key theoretical concepts, students obtain hands-on experience with two popular database systems . We encourage students to learn independently making use of the documentation and technical resources freely available on the Internet. This way, our hope is that they will be able to adapt to emerging systems and application scenarios.	Database security course , Statistical Database Security , Statistical Security , Database security , High Risk of Sensitive Information , Database security plan , Security breaches , Labs , Database system , Undergraduate Database Security Course , Real Life Database Security , Database Security Education , XML Security , Undergraduate students , Cryptography , Secure information , Hands-on experience , Database Security Plan , Data access , Security Plan , Database Privacy , Administrators , Hands-on , Database Security Course , Undergraduate course , Access Privilege System , Database Security , Privacy Issues , Laboratory/Active Learning , Right Blend of Theory and Practice , Assignments , Real Life Databases Hands-on , MySQL Security , Topics , Importance of Database Security , Few Database Security Courses , Oracle Security	A Database Security Course on a Shoestring	en	146
147	In this paper, we propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.	Digital Copies , metadata extraction , Metadata processing , Search ranking results , File Formats extraction , Information search and Retrieval , PowerPoint documents , information extraction , Precision extraction , File extraction , search , generic languages , machine learning , Microsoft Office Automation	Automatic Extraction of Titles from General Documents using Machine Learning	en	147
148	Intrusion or misbehaviour detection systems are an important and widely accepted security tool in computer and wireless sensor networks. Their aim is to detect misbehaving or faulty nodes in order to take appropriate countermeasures, thus limiting the damage caused by adversaries as well as by hard or software faults. So far, however, once detected, misbehaving nodes have just been isolated from the rest of the sensor network and hence are no longer usable by running applications. In the presence of an adversary or software faults, this proceeding will inevitably lead to an early and complete loss of the whole network. For this reason, we propose to no longer expel misbehaving nodes, but to recover them into normal operation. In this paper, we address this problem and present a formal specification of what is considered a secure and correct node recovery algorithm together with a distributed algorithm that meets these properties. We discuss its requirements on the soft- and hardware of a node and show how they can be fulfilled with current and upcoming technologies. The algorithm is evaluated analytically as well as by means of extensive simulations, and the findings are compared to the outcome of a real implementation for the BTnode sensor platform. The results show that recovering sensor nodes is an expensive, though feasible and worthwhile task. Moreover , the proposed program code update algorithm is not only secure but also fair and robust.	sensor networks , countermeasures , intrusion detection , Wireless Sensor Networks , Node Recovery , Intrusion Detection , node recovery , IDS , sensor nodes , security , distributed algorithm	Autonomous and Distributed Node Recovery in Wireless Sensor Networks	en	148
149	This paper explores the use of Bayesian online classifiers to classify text documents. Empirical results indicate that these classifiers are comparable with the best text classification systems. Furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.	Text Classification , perceptron , text classification , filtering , information gain , Bayesian online classifiers , Online , Machine Learning , continous learning , machine learning , Text Filtering , Gaussian process , Bayesian	Bayesian Online Classifiers for Text Classification and Filtering	en	149
150	Since  the  publication  of  Brin  and  Page's  paper  on  PageRank, many in the Web community have depended on PageRank for the static  (query-independent)  ordering  of  Web  pages.  We  show  that we can significantly outperform PageRank using features that are independent  of  the  link  structure  of  the  Web.  We  gain  a  further boost  in  accuracy  by  using  data  on  the  frequency  at  which  users visit  Web  pages.  We  use  RankNet,  a  ranking  machine  learning algorithm,  to  combine  these  and  other  static  features  based  on anchor  text  and  domain  characteristics.  The  resulting  model achieves  a  static  ranking  pairwise  accuracy  of  67.3%  (vs.  56.7% for PageRank or 50% for random).	anchor text , relevance , Web pages , pairwise accuracy , fRank , popularity data , dynamic ranking , search engines , PageRank , static ranking , Static ranking , static features , RankNet	Beyond PageRank: Machine Learning for Static Ranking	en	150
151	It is well known that the secure computation of non-trivial functionalities in the setting of no honest majority requires computational assumptions. We study the way such computational assumptions are used. Specifically, we ask whether the secure protocol can use the underlying primitive (e.g., one-way trapdoor permutation) in a black-box way, or must it be nonblack-box (by referring to the code that computes this primitive)? Despite the fact that many general constructions of cryptographic schemes (e.g., CPA-secure encryption ) refer to the underlying primitive in a black-box way only, there are some constructions that are inherently nonblack-box. Indeed, all known constructions of protocols for general secure computation that are secure in the presence of a malicious adversary and without an honest majority use the underlying primitive in a nonblack-box way (requiring to prove in zero-knowledge statements that relate to the primitive). In this paper, we study whether such nonblack-box use is essential. We present protocols that use only black-box access to a family of (enhanced) trapdoor permutations or to a homomorphic public-key encryption scheme. The result is a protocol whose communication complexity is independent of the computational complexity of the underlying primitive (e.g., a trapdoor permutation) and whose computational complexity grows only linearly with that of the underlying primitive. This is the first protocol to exhibit these properties.	oblivious transfer , encryption scheme , oblivious transfer protocol , secure computation , nonblack-box , malicious adversary , black-box , Theory of cryptography , cryptographic , black-box reductions , trapdoor permutation	Black-Box Constructions for Secure Computation	en	151
152	Bluetooth is a cable replacement technology for Wireless Personal Area Networks. It is designed to support a wide variety of applications such as voice, streamed audio and video, web browsing, printing, and file sharing, each imposing a number of quality of service constraints including packet loss, latency, delay variation, and throughput. In addition to QOS support, another challenge for Bluetooth stems from having to share the 2.4 GHz ISM band with other wireless devices such as IEEE 802.11. The main goal of this paper is to investigate the use of a dynamic scheduling algorithm that guarantees QoS while reducing the impact of interference. We propose a mapping between some common QoS parameters such as latency and bit rate and the parameters used in the algorithm. We study the algorithm's performance and obtain simulation results for selected scenarios and configurations of interest.	WLAN , BIAS , QoS , inteference , dynamic scheduling , Bluetooth , scheduling priorities , interference , coexistence , MAC scheduling , WPANs , WPAN	Bluetooth Dynamic Scheduling and Interference Mitigation	en	152
153	This paper examines the average page quality over time of pages downloaded during a web crawl of 328 million unique pages. We use the connectivity-based metric PageRank to measure the quality of a page. We show that traversing the web graph in breadth-first search order is a good crawling strategy, as it tends to discover high-quality pages early on in the crawl.	 , high quality pages , breadth first search , crawl order , ordering metrics , Crawling , crawling , PageRank , page quality metric , breadth-first search , connectivity-based metrics	Breadth-First Search Crawling Yields High-Quality Pages	en	153
154	"Many instant messenger (IM) clients let a person specify  the identifying  name that appears in another person's contact list. We have noticed that many people add extra information to this name as a way to broadcast information to their contacts. Twelve IM contact lists  comprising 444 individuals  were monitored over three weeks  to  observe  how  these individuals used and altered  their  display names.  Almost half of them changed their display names at varying frequencies, where the new information fell into seventeen different categories  of communication supplied to others. Three themes encompass these categories:  Identification (""who am I""?),  Information About Self (""this is what is going on with me"") and Broadcast Message (""I am directing information to the community"").  The design implication is that systems supporting person to person casual interaction, such as IM, should explicitly include facilities that allow people to broadcast these types of information to their community of contacts."	communication , Communication Catogories , Name Variation Handles , Identification Is Fundamental , Related IM Research , Distribution Frequency Of Various Catogories , Display Names , Instant messenger , awareness , MSN messager , Broadcast Information , Catorgorisation Of Display Names , Instant Messaging , display name	Broadcasting Information via Display Names in Instant Messaging	en	154
155	This paper describes the building of a research library for studying the Web, especially research on how the structure and content of the Web change over time. The library is particularly aimed at supporting social scientists for whom the Web is both a fascinating social phenomenon and a mirror on society. The library is built on the collections of the Internet Archive, which has been preserving a crawl of the Web every two months since 1996. The technical challenges in organizing this data for research fall into two categories: high-performance computing to transfer and manage the very large amounts of data, and human-computer interfaces that empower research by non-computer specialists.	User Interface , Dataflow , Internet Archive. 1. BACKGROUND 1.1 Research in the History of the Web The Web is one of the most interesting artifacts of our time. For social scientists , history of the Web , basic parameters are hard to come by and it is almost impossible to generate random samples for statistical purposes. But the biggest problem that social scientists face in carrying out Web research is historical , or via tools such as the Google Web API , Database Management , it is a subject of study both for itself and for the manner in which it illuminates contemporary social phenomena. Yet a researcher who wishes to study the Web is faced with major difficulties. An obvious problem is that the Web is huge. Any study of the Web as a whole must be prepared to analyze billions of pages and hundreds of terabytes of data. Furthermore , Storage , Flexible Preload System , Internet Archive , digital libraries , Scalability , the Web changes continually. It is never possible to repeat a study on the actual Web with quite the same data. Any snapshot of the whole Web requires a crawl that will take several weeks to gather data. Because the size and boundaries of the Web are ill defined , Database Access , User Support , computational social science , Full Text Indexes , the desire to track activities across time. The Web of today can be studied by direct Web crawling	Building a Research Library for the History of the Web	en	155
156	This  working  group  laid  the  groundwork  for  the  collection  and analysis  of  oral  histories  of  women  computing  educators.  This endeavor  will  eventually  create  a  body  of  narratives  to  serve  as role  models  to  attract  students,  in  particular  women,  to computing; it will also serve to preserve the history of the female pioneers  in  computing  education.  Pre-conference  work  included administration of a survey to assess topical interest. The working group  produced  aids  for  conducting  interviews,  including  an opening  script,  an  outline  of  topics  to  be  covered,  guidelines  for conducting  interviews,  and  a  set  of  probing  questions  to  ensure consistency  in  the  interviews.  The  group  explored  issues  such  as copyright and archival that confront the large-scale implementation  of  the  project  and  suggested  extensions  to  this research.  This  report  includes  an  annotated  bibliography  of resources.  The  next  steps will include training colleagues in how to conduct interviews and establishing guidelines for archival and use of the interviews.	Oral History , Computing Education History	Building a Sense of History: Narratives and Pathways of Women Computing Educators	en	156
157	"Emerging technologies are set to provide further provisions for computing in times when the limits of current technology of microelectronics become an ever closer presence. A technology roadmap document lists biologically-inspired computing and quantum computing as two emerging technology vectors for novel computing architectures <A href=""5.html#12"">[43]. But the potential benefits that will come from entering the nanoelectronics era and from exploring novel nanotechnologies are foreseen to come at the cost of increased sensitivity to influences from the surrounding environment. This paper elaborates on a dependability perspective over these two emerging technology vectors from a designer's standpoint. Maintaining or increasing the dependability of unconventional computational processes is discussed in two different contexts: one of a bio-inspired computing architecture (the Embryonics project) and another of a quantum computational architecture (the QUERIST project)."	emerging technologies , Self replication , Embryonics , Computing technology , Error detection , Fault tolerance , Digital devices , Computing architecture , environment , Soft errors , Dependable system , Computing system , System design , Correction techniques , bio-inspired digital design , Bio-inspired computing , Reliability , Dependability , Nano computing , Failure rate , Emerging technologies , Nanoelectronics , bio-inspired computing , Self repair , evolvable hardware , Computer system , quantum computing , fault-tolerance assessment , QUERIST , Bio-computing , reliability , Quantum computing	A Dependability Perspective on Emerging Technologies	en	157
158	This paper introduces a rationale for and approach to the study of sustainability in computerized community information systems.  It begins by presenting a theoretical framework for posing questions about sustainability predicated upon assumptions from social construction of technology and adaptive structuration theories. Based in part on the literature and in part on our own experiences in developing a community information system, we introduce and consider three issues related to sustainability:  stakeholder involvement, commitment from key players, and the development of critical mass.	critical mass , construction of technology , key players , community network , computerized community information system , participatory design , sustainability , skateholder involvement , Community networks	Building Sustainable Community Information Systems: Lessons from a Digital Government Project	en	158
159	"Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However , machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, ""Can machine learning be secure?"" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems."	Indiscriminate attack , Targeted attack , Statistical Learning , Exploratory attack , Machine learning , Security Metrics , Integrity , Availability , Spam Filters , Causative attack , Intrusion Detection , Machine Learning , Intrusion detection system , Computer Security , Game Theory , Adversarial Learning , Learning algorithms , Computer Networks , Security	Can Machine Learning Be Secure?	en	159
160	The Catenaccio system integrates information retrieval with sketch manipulations. The system is designed especially for pen-based computing and allows users to retrieve information by simple pen manipulations such as drawing a picture. When a user draws a circle and writes a keyword, information nodes related to the keyword are collected automatically inside the circle. In addition, the user can create a Venn diagram by repeatedly drawing circles and keywords to form more complex queries. Thus, the user can retrieve information both interactively and visually without complex manipulations. Moreover, the sketch interaction is so simple that it is possible to combine it with other types of data such as images and real-world information for information retrieval. In this paper, we describe our Catenaccio system and how it can be effectively applied.	Sketch manipulation , Information node , Venn diagram , Visual information retrieval , Pen-based computing , Image data , Information retrieval , keyword searching , 2D system , sketch manipulations , interactive system , Catnaccio system , Interactive information retrieval system	Catenaccio: Interactive Information Retrieval System through Drawing	en	160
161	Compression reduces both the size of indexes and the time needed to evaluate queries. In this paper, we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms, considering two approaches to improving retrieval efficiency:better implementation and better choice of integer compression schemes. First, we propose several simple optimisations to well-known integer compression schemes, and show experimentally that these lead to significant reductions in time. Second, we explore the impact of choice of compression scheme on retrieval efficiency. In experiments on large collections of data, we show two surprising results:use of simple byte-aligned codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes; and, even when an index fits entirely in memory, byte-aligned codes result in faster query evaluation than does an uncompressed index, emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index. Moreover, byte-aligned schemes have only a modest space overhead:the most compact schemes result in indexes that are around 10% of the size of the collection, while a byte-aligned scheme is around 13%. We conclude that fast byte-aligned codes should be used to store integers in inverted lists.	Variable byte , Decoding , Efficiency , integer coding , Bytewise compression , Search engine , retrieval efficiency , Integer Compression , Inverted indexes , index compression , Optimisation , Compression , Inverted index , Document retrieval	Compression of Inverted Indexes For Fast Query Evaluation	en	161
162	A consistent query answer in a possibly inconsistent database is an answer which is true in every (minimal) repair of the database. We present here a practical framework for computing consistent query answers for large, possibly inconsistent relational databases. We consider relational algebra queries without projection , and denial constraints. Because our framework handles union queries, we can effectively (and efficiently) extract indefinite disjunctive information from an inconsistent database. We describe a number of novel optimization techniques applicable in this context and summarize experimental results that validate our approach.	Knowledge gathering , Conflict hypergraph , inconsistency , Denial constraints , Inconsistent database , Optimization , Relational algebra , Polynomial time , Consistent Query answer , Disjunctive query , integrity constraints , query processing , Repair	Computing Consistent Query Answers using Conflict Hypergraphs	en	162
163	"Research in consistent query answering studies the definition and computation of ""meaningful"" answers to queries posed to inconsistent databases, i.e., databases whose data do not satisfy the integrity constraints (ICs) declared on their schema. Computing consistent answers to conjunctive queries is generally coNP-hard in data complexity, even in the presence of very restricted forms of ICs (single, unary keys). Recent studies on consistent query answering for database schemas containing only key dependencies have an-alyzed the possibility of identifying classes of queries whose consistent answers can be obtained by a first-order rewriting of the query, which in turn can be easily formulated in SQL and directly evaluated through any relational DBMS. In this paper we study consistent query answering in the presence of key dependencies and exclusion dependencies. We first prove that even in the presence of only exclusion dependencies the problem is coNP-hard in data complexity , and define a general method for consistent answering of conjunctive queries under key and exclusion dependencies, based on the rewriting of the query in Datalog with negation . Then, we identify a subclass of conjunctive queries that can be first-order rewritten in the presence of key and exclusion dependencies, and define an algorithm for computing the first-order rewriting of a query belonging to such a class of queries. Finally, we compare the relative efficiency of the two methods for processing queries in the subclass above mentioned. Experimental results, conducted on a real and large database of the computer science engineering degrees of the University of Rome ""La Sapienza"", clearly show the computational advantage of the first-order based technique."	relational database , Query Rewriting , integrity constraints , query rewriting , consistent query answering , Computational Complexity , conjunctive queries , inconsistent database , Inconsistency , database schemas	Consistent Query Answering under Key and Exclusion Dependencies: Algorithms and Experiments	en	163
164	Apart from completeness usability, performance and maintainability are the key quality aspects for Web information systems. Considering usability as key implies taking usage processes into account right from the beginning of systems development. Context-awareness appears as a promising idea for increasing usability of Web Information Systems. In the present paper we propose an approach to context-awareness of Web Information Systems that systematically distinguishes among the various important kinds of context. We show how parts of this context can be operationalized for increasing customers' usage comfort. Our approach permits designing Web information systems such that they meet high quality expectations concerning usability, performance and maintainability. We demonstrate the validity of our approach by discussing the part of a banking Web Information System dedicated to online home-loan application.	scenarios , Web Information Systems , web site , Web services , usability , story boarding , context-awareness , context-aware information systems , web information system , media objects , scenes , media type , SiteLang	Context-Aware Web Information Systems	en	164
165	This paper discusses the problem of partial object recognition in image databases. We propose the method to reconstruct and estimate partially occluded shapes and regions of objects in images from overlapping and cutting.  We present the robust method for recognizing partially occluded objects based on symmetry properties, which is based on the contours of objects. Our method provides simple techniques to reconstruct occluded regions via a region copy using the symmetry axis within an object.  Based on the estimated parameters for partially occluded objects, we perform object recognition on the classification tree. Since our method relies on reconstruction of the object based on the symmetry rather than statistical estimates, it has proven to be remarkably robust in recognizing partially occluded objects in the presence of scale changes, rotation, and viewpoint changes.	object recognition , reconstruction , Object , contour , Recognition , Symmetry , Image , Contour , occlusion , estimation , symmetry	Contour-based Partial Object Recognition using Symmetry in Image Databases	en	165
166	In this paper we explore the connection between clustering categorical data and entropy: clusters of similar poi lower entropy than those of dissimilar ones. We use this connection to design an incremental heuristic algorithm, COOLCAT , which is capable of efficiently clustering large data sets of records with categorical attributes, and data streams. In contrast with other categorical clustering algorithms published in the past, COOLCAT's clustering results are very stable for different sample sizes and parameter settings. Also, the criteria for clustering is a very intuitive one, since it is deeply rooted on the well-known notion of entropy. Most importantly, COOLCAT is well equipped to deal with clustering of data streams (continuously arriving streams of data point) since it is an incremental algorithm capable of clustering new points without having to look at every point that has been clustered so far. We demonstrate the efficiency and scalability of COOLCAT by a series of experiments on real and synthetic data sets.	data streams , incremental algorithm , COOLCAT , categorical clustering , data stream , entropy , clustering	COOLCAT: An entropy-based algorithm for categorical clustering	en	166
167	This paper provides an account of new measures of coupling and cohesion developed to assess the reusability of Java components retrieved from the internet by a search engine. These measures differ from the majority of established metrics in two respects: they reflect the degree to which entities are coupled or resemble each other, and they take account of indirect couplings or similarities. An empirical comparison of the new measures with eight established metrics shows the new measures are consistently superior at ranking components according to their reusability.	Binary Quantity , Experimentary Comparsion , Component search engine , Search Engine Technology , Spearman Rank Correlation , Intransitive Relation , Reusability , Coupling , Cohesion Metric , Linear Regression , Cohesion , Java components	Coupling and Cohesion Measures for Evaluation of Component Reusability	en	167
168	"We present Repo-3D, a general-purpose, object-oriented library for developing distributed, interactive 3D graphics applications across a range of heterogeneous workstations. Repo-3D is designed to make it easy for programmers to rapidly build prototypes using a familiar multi-threaded, object-oriented programming paradigm. All data sharing of both graphical and non-graphical data is done via general-purpose remote and replicated objects, presenting the illusion of a single distributed shared memory. Graphical objects are directly distributed, circumventing the ""duplicate database"" problem and allowing programmers to focus on the application details. Repo-3D is embedded in Repo, an interpreted, lexically-scoped, distributed programming language, allowing entire applications to be rapidly prototyped. We discuss Repo-3D's design, and introduce the notion of local variations to the graphical objects, which allow local changes to be applied to shared graphical structures. Local variations are needed to support transient local changes, such as highlighting, and responsive local editing operations. Finally, we discuss how our approach could be applied using other programming languages, such as Java."	Data sharing , programming language , Distributed graphics , Data structures , Interactive graphical application , 3D graphics library , Change notification , Library , Replicated object , Object representation , distributed virtual environments , Shared memory , Syncronisation , data distribution , object-oriented graphics , Java , Programming , duplicate database , local variations , multi-threaded programming , Heterogeneous workstation , Multi-user interaction , 3D graphics application , Repo-3D , 3D graphics , Callbacks , Prototyping , Distributed systems , Client-server approach , object-oriented library , Programming language , shared-data object model , prototype , Graphical objects , Client-Server , Object-oriented , Local variation , 3D Graphics , Object oriented , Distributed applications , distributed shared memory , Distributed processes , Properties	A Distributed 3D Graphics Library	en	168
169	It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries, which are typically very short. In this paper we study several machine learning methods, including naive Bayes model, maximum entropy model, support vector machine (SVM), and stochastic gradient boosting tree (SGBT), for navigational query identification in Web search. To boost the performance of these machine techniques, we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features, in this paper, we study the problem of identifying navigational queries with thousands of available features, extracted from major commercial search engine results, Web search user click data, query log, and the whole Web's relational content. A multi-level feature extraction system is constructed. Our results on real search data show that 1) Among all the features we tested, user click distribution features are the most important set of features for identifying navigational queries. 2) In order to achieve good performance, machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree, coupled with linear SVM feature selection is most effective. 3) With carefully coupled feature selection and classification approaches, navigational queries can be accurately identified with 88.1% F1 score, which is 33% error rate reduction compared to the best uncoupled system, and 40% error rate reduction compared to a well tuned system without feature selection.	Stochastic Gradient Boosting Tree , Linear SVM Feature Ranking , Gradient Boosting Tree , Information Gain , Naive Bayes Classifier , Support Vector Machine , Experiments Results , Machine Learning , Maximum Entropy Classifier , Navigational Query Classification , Navigational and Informational query , Multiple Level feature system	Coupling Feature Selection and Machine Learning Methods for Navigational Query Identification	en	169
170	Functional verification is widely acknowledged as the bottleneck in the hardware design cycle. This paper addresses one of the main challenges of simulation based verification (or dynamic verification ), by providing a new approach for Coverage Directed Test Generation (CDG). This approach is based on Bayesian networks and computer learning techniques. It provides an efficient way for closing a feedback loop from the coverage domain back to a generator that produces new stimuli to the tested design. In this paper, we show how to apply Bayesian networks to the CDG problem. Applying Bayesian networks to the CDG framework has been tested in several experiments, exhibiting encouraging results and indicating that the suggested approach can be used to achieve CDG goals.	Coverage directed test generation , conditional probability , Functional Verification , Bayesian Networks , bidirectional inferences , Maximal A Posteriori , Dynamic Bayesian Network , design under test , coverage model , Coverage Analysis , Most Probable Explanation , Markov Chain	Coverage Directed Test Generation for Functional Verification using Bayesian Networks	en	170
171	An index connects readers with information. Creating an index for a single book is a time-honored craft. Creating an index for a massive library of HTML topics is a modern craft that has largely been discarded in favor of robust search engines. The authors show how they optimized a single-sourced index for collections of HTML topics, printed books, and PDF books. With examples from a recent index of 24,000 entries for 7,000 distinct HTML topics also published as 40 different PDF books, the authors discuss the connections between modern technology and traditional information retrieval methods that made the index possible, usable, and efficient to create and maintain.	book indexes , Information retrieval methods , Massive Master , drop-down selection of terms , Indexing , SQL data type , Search , primary index entry , Internally developed format , automation , indexing problems , Human factors , HTML master index , Online information , Navigation	Creating a Massive Master Index for HTML and Print	en	171
172	Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings, and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods (SVMs, neural nets, k-nearest neighbor, bagged and boosted trees, and boosted stumps) to compare nine boolean classification performance metrics: Accuracy, Lift, F-Score, Area under the ROC Curve, Average Precision, Precision/Recall Break-Even Point, Squared Error, Cross Entropy, and Probability Calibration. Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities: squared error, cross entropy, and calibration, lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area, average precision, break-even point, and lift. In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score. As expected, maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy , but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric, SAR, that combines squared error, accuracy, and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known.	Lift , Precision , performance metric , ROC , Supervised Learning , supervised learning , squared error , SVMs , pairwise , Recall , algorithmns , Cross Entropy , ordering metric , Euclidean distance , Performance Evaluation , standard deviation , backpropagation , Metrics	Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria	en	172
173	Database Security course is an important part of the InfoSec curriculum.  In many institutions this is not taught as an independent course.  Parts of the contents presented in this paper are usually incorporated in other courses such as Network Security.  The importance of database security concepts stems from the fact that a compromise of data at rest could expose an organization to a greater security threat than otherwise.  Database vulnerabilities exposed recently in several high profile incidents would be a good reason to dedicate a full course to this important topic.  In this paper we present key topics such as technologies for database protection, access control, multilevel security, database vulnerabilities and defenses, privacy and legal issues, impact of policies and some well known secure database models.	inference channel , access control , buffer overflows , CIA , privacy , polyinstantiation , database , inference , Database , encryption , multilevel security , authentication , policy , security	Database Security Curriculum in InfoSec Program	en	173
174	This paper addresses the problem of scatternet formation for single-hop Bluetooth based ad hoc networks, with minimal communication overhead. We adopt the well-known structure de Bruijn graph to form the backbone of Bluetooth scatternet, hereafter called dBBlue, such that every master node has at most seven slaves, every slave node is in at most two piconets, and no node assumes both master and slave roles. Our structure dBBlue also enjoys a nice routing property: the diameter of the graph is O(log n) and we can find a path with at most O(log n) hops for every pair of nodes without any routing table . Moreover, the congestion of every node is at most O(log n/n), assuming that a unit of total traffic demand is equally distributed among all pair of nodes. We discuss in detail a vigorous method to locally update the structure dBBlue using at most O(log n) communications when a node joins or leaves the network. In most cases, the cost of updating the scatternet is actually O(1) since a node can join or leave without affecting the remaining scatternet. The number of nodes affected when a node joins or leaves the network is always bounded from above by a constant. To facilitate self-routing and easy updating, we design a scalable MAC assigning mechanism for piconet, which guarantees the packet delivery during scatternet updating. The dBBlue scatternet can be constructed incrementally when the nodes join the network one by one. Previously no method can guarantee all these properties although some methods can achieve some of the properties.	scalable MAC assignment , scatternet formation , Low Diameter , ad hoc networks , self-routing , const updating cost , de Bruijn graph , Bluetooth , Network topology , Self-routing Scatternet , Bluetooth networks , Bruijn graph , equal traffic , easy updating , low diameter , single-hop	dBBlue: Low Diameter and Self-routing Bluetooth Scatternet	en	174
175	This text has analyzed the development of E-commerce in some developed countries such as Canada, U.S.A., Japan, etc and put forward several suggestions on how to set up the system of E-commerce in our country taking the national conditions of our country into account.	Development stage , Definition of E-commerce , Survey , Authority , Statistics , Statistical methods , Measurement , Implications , E-commerce Statistics , Statistical Survey , China , E-commerce	Development of E-commerce Statistics and the Implications	en	175
176	Many authors have recognised the importance of structure in shaping information system (IS) design and use. Structuration theory has been used in IS research and design to assist with the identification and understanding of the structures in which the IS is situated. From a critical theoretical perspective, focusing on the Habermas' theory of communicative action, a community based child health information system was designed and implemented in a municipality in rural South Africa. The structures which shaped and influenced the design of this IS (the restructured health services and social tradition) are explored and discussed. From this case study the implications of using IS design as a developmental tool are raised: namely the development of a shared understanding, the participation of key players and the agreement on joint action.	communicative action , critical social theory , moral codes or norms , community information systems , information system design , Structuration theory , interpretative schemes , critical social theory in IS design , conducive environment , community monitoring system , marginalisation , health information systems , duality of structure , structuration theory , the ideal speech situation	Development through communicative action and information system design: a case study from South Africa	en	176
177	When failures occur in Internet overlay connections today, it is difficult for users to determine the root cause of failure. An overlay connection may require TCP connections between a series of overlay nodes to succeed, but accurately determining which of these connections has failed is difficult for users without access to the internal workings of the overlay. Diagnosis using active probing is costly and may be inaccurate if probe packets are filtered or blocked. To address this problem, we develop a passive diagnosis approach that infers the most likely cause of failure using a Bayesian network modeling the conditional probability of TCP failures given the IP addresses of the hosts along the overlay path. We collect TCP failure data for 28.3 million TCP connections using data from the new Planetseer overlay monitoring system and train a Bayesian network for the diagnosis of overlay connection failures . We evaluate the accuracy of diagnosis using this Bayesian network on a set of overlay connections generated from observations of CoDeeN traffic patterns and find that our approach can accurately diagnose failures.	fault diagnosis , passive diagnosis , NAT , Bayesian networks , Planetseer overlay monitoring system , active probing for diagnosis , inter-AS TCP failure probabilities , TCP overlay connections , Bayesian networks modelling , CoDeeN traffic patterns , TCP overlay path diagnosis , Planetseer , clustering , network address translation	Diagnosis of TCP Overlay Connection Failures using Bayesian Networks	en	177
178	"Digital Asset Management (DAM), the management of digital content so that it can be cataloged, searched and re-purposed, is extremely challenging for organizations that rely on image handling and expect to gain business value from these assets. Metadata plays a crucial role in their management, and XML, with its inherent support for structural representation, is an ideal technology for this. This paper analyzes the capabilities of a native XML database solution via the development of a ""proof of concept"" and describes implementation requirements, strategy, and advantages and disadvantages of this solution."	keyword search , metadata , multimedia , digital asset management , semi structured data , database , digital images , Digital Asset Management , XML database , storage and retrieval , native XML , DAM , heterogenous data , proof of concept	Digital Asset Management Using A Native XML Database Implementation	en	178
179	mechanisms and algorithms necessary to set up and maintain them. The operation of a scatternet requires some Bluetooth units to be inter-piconet units (gateways), which need to time-division multiplex their presence among their piconets. This requires a scatternet-scheduling algorithm that can schedule the presence of these units in an efficient manner. In this paper, we propose a distributed scatternet-scheduling scheme that is implemented using the HOLD mode of Bluetooth and adapts to non-uniform and changing traffic. Another attribute of the scheme is that it results in fair allocation of bandwidth to each Bluetooth unit. This scheme provides an integrated solution for both intra-and inter-piconet scheduling, i.e., for polling of slaves and scheduling of gateways.	scheduling scheme , Round Robin , Distributed algorithm , scheduling , traffic adaptive , Scatternet presence fraction , Fairness , traffic rate , scatternets , Scatternet , Bluetooth , Rendezvous Points , Scheduling algorithm , Information exchange , heuristic , Gateway , Scheduling of gateways , Slaves , Non-uniform traffic , changing traffic , Efficiency , fair share , virtual slave , Rendezvous Point , Blueooth , Piconet presence fraction , HOLD mode , Slave unit , polling algorithm , Gateway slave traffic , Scatternets , Master unit , Allocation of bandwidth , Rendezvous point , fairness , Piconet , piconet , slave , Traffic Dependent Scheduling , Time-division multiplex , scatternet , Fair share , Bluetooth technology , Non-gateway slave traffic , bandwidth utilization , allocation of bandwidth , gateway , master , Round Robin polling	A Fair and Traffic Dependent Scheduling Algorithm for Bluetooth Scatternets	en	179
180	ABSTRACT Web Directories are repositories of Web pages organized in a hierarchy of topics and sub-topics. In this paper, we present DirectoryRank , a ranking framework that orders the pages within a given topic according to how informative they are about the topic. Our method works in three steps: first, it processes Web pages within a topic in order to extract structures that are called lexical chains, which are then used for measuring how informative a page is for a particular topic. Then, it measures the relative semantic similarity of the pages within a topic. Finally, the two metrics are combined for ranking all the pages within a topic before presenting them to the users.	topic hierachy , semantic similarity , ranking metric , scoring , web directory , ranking , lexical chains , DirectoryRank , topic importance , PageRank , information retrieval , Web Directory , semantic similarities	DirectoryRank: Ordering Pages in Web Directories	en	180
181	In this paper we present a personalized web service discovery and ranking technique for discovering and ranking relevant data-intensive web services. Our first prototype  called BASIL supports a personalized view of data-intensive web services through source-biased focus. BASIL provides service discovery and ranking through source-biased probing and source-biased relevance metrics. Concretely, the BASIL approach has three unique features: (1) It is able to determine in very few interactions whether a target service is relevant to the given source service by probing the target with very precise probes; (2) It can evaluate and rank the relevant services discovered based on a set of source-biased relevance metrics; and (3) It can identify interesting types of relationships for each source service with respect to other discovered services, which can be used as value-added metadata for each service. We also introduce a performance optimization technique called source-biased probing with focal terms to further improve the effectiveness of the basic source-biased service discovery algorithm. The paper concludes with a set of initial experiments showing the effectiveness of the BASIL system.	focal terms , biased discovery , ranking , data-intensive web services , data-intensive services , query-biased probing , web service discovery , source-biased probing	Discovering and Ranking Web Services with BASIL: A Personalized Approach with Biased Focus	en	181
182	In visual information retrieval the careful choice of suitable proximity measures is a crucial success factor. The evaluation presented in this paper aims at showing that the distance measures suggested by the MPEG-7 group for the visual descriptors can be beaten by general-purpose measures. Eight visual MPEG-7 descriptors were selected and 38 distance measures implemented. Three media collections were created and assessed, performance indicators developed and more than 22500 tests performed. Additionally, a quantisation model was developed to be able to use predicate-based distance measures on continuous data as well. The evaluation shows that the distance measures recommended in the MPEG-7-standard are among the best but that other measures perform even better.	Similarity Perception , MPEG-7 descriptors , Distance Measurement , Content-based Image Retrieval , MPEG-7 , distance measure , quantisation , Content-based Video Retrieval , Similarity Measurement , visual information retrieval , Visual Information Retrieval , human similarity perception	Distance Measures for MPEG-7-based Retrieval	en	182
183	In the not too distant future Intelligent Creatures (robots, smart devices, smart vehicles, smart buildings , etc) will share the everyday living environment of human beings. It is important then to analyze the attitudes humans are to adopt for interaction with morphologically different devices, based on their appearance and behavior. In particular, these devices will become multi-modal interfaces, with computers or networks of computers, for a large and complex universe of applications. Our results show that children are quickly attached to the word `dog' reflecting a conceptualization that robots that look like dogs (in particular SONY Aibo) are closer to living dogs than they are to other devices. By contrast, adults perceive Aibo as having stronger similarities to machines than to dogs (reflected by definitions of robot). Illustration of the characteristics structured in the definition of robot are insufficient to convince children Aibo is closer to a machine than to a dog.	intelligent creatures , human attitudes , language , essence , agency , robots , perceived attitude , behavioral science , zoo-morphological autonomous mobile robots , robot attributes , multi-modal interfaces , interaction , feelings , hci	Dogs or Robots: Why do Children see them as Robotic Pets rather than Canine Machines?	en	183
184	"An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites. These pages are often referred to as the Hidden Web or the Deep Web. Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results. However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users. In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web. Since the only ""entry point"" to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site. Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically. Our policies proceed iteratively, issuing a different query in every iteration . We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising. For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents ) after issuing fewer than 100 queries."	crawler , deep web , hidden web , Hidden Web crawling , query selection , efficiency , Deep Web crawler , coverage , keyword selection , adaptive algorithm , potential bias , adaptive algorithmn , accurate language model , keyword query , keyword queries	Downloading Textual Hidden Web Content Through Keyword Queries	en	184
185	Domain-specific languages hold the potential of automating the software development process. Nevertheless, the adop-tion of a domain-specific language is hindered by the difficulty of transitioning to different language syntax and employing a separate translator in the software build process. We present a methodology that simplifies the development and deployment of small language extensions, in the context of Java. The main language design principle is that of language extension through unobtrusive annotations. The main language implementation idea is to express the language as a generator of customized AspectJ aspects, using our Meta-AspectJ tool. The advantages of the approach are twofold. First, the tool integrates into an existing software application much as a regular API or library, instead of as a language extension. This means that the programmer can remove the language extension at any point and choose to implement the required functionality by hand without needing to rewrite the client code. Second, a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language	language extensions , annotation , domain-specific language , language extension , Meta-AspectJ , Java , simplicity , domain-specific languages	Easy Language Extension with Meta-AspectJ	en	185
186	We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general-purpose commercial web search service.  Previously, query logs have been studied from a single, cumulative view.  In contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day.  We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13% of the query traffic.  We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories.  This analysis provides valuable insight for improving retrieval effectiveness and efficiency.  It is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms.	query traffic , query stream , frequency distribution , topical categories , log analysis , query log , Query Log Analysis , Web Search	Hourly Analysis of a Very Large Topically Categorized Web Query Log	en	186
187	Text categorization is an important research area and has been receiving much attention due to the growth of the on-line information and of Internet. Automated text categorization is generally cast as a multi-class classification problem. Much of previous work focused on binary document classification problems. Support vector machines (SVMs) excel in binary classification, but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification. In addition, the training time and scaling are also important concerns. On the other hand, other techniques naturally extensible to handle multi-class classification are generally not as accurate as SVM. This paper presents a simple and efficient solution to multi-class text categorization. Classification problems are first formulated as optimization via discriminant analysis . Text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data. While most of the previous approaches decompose a multiclass classification problem into multiple independent binary classification tasks, the proposed approach enables direct multi-class classification. By using Generalized Singular Value Decomposition (GSVD), a coordinate transformation that reflects the inherent class structure indicated by the generalized singular values is identified. Extensive experiments demonstrate the efficiency and effectiveness of the proposed approach.	multi-class classification , text categorization , GSVD , Discriminant Analysis , Multi-class Text Categorization , SVMs , GDA , discriminant analysis	Efficient Multi-way Text Categorization via Generalized Discriminant Analysis	en	187
188	Search engines need to evaluate queries extremely fast, a challenging task given the vast quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this paper we consider how phrase queries can be efficiently supported with low disk overheads. Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. We propose a combination of nextword indexes with inverted files as a solution to this problem. Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone, and the space overhead is only 10% of the size of the inverted file. Further time savings are available with only slight increases in disk requirements. Categories and Subject Descriptors	common words , evaluation efficiency , stopping , Indexing , nextword index , index representation , phrase query evaluation , query evaluation , phrase query , inverted index	Efficient Phrase Querying with an Auxiliary Index	en	188
189	We propose an indexing technique for the fast retrieval of objects in 2D images basedon similarity between their boundary shapes. Our technique is robust in the presence of noise andsupports several important notions of similarity including optimal matches irrespective of variations in orientation and/or position. Our method can also handle size-invariant matches using a normalization technique, although optimality is not guaranteedhere. We implementedour method and performed experiments on real (hand-written digits) data. Our experimental results showedthe superiority of our method comparedto search basedon sequential scanning, which is the only obvious competitor. The performance gain of our method increases with any increase in the number or the size of shapes.	fingerprint , Shape retrieval Similarity retrieval Fourier descriptors , non textual objects , efficiency , database , handwriting recognition , Fourier descriptors , Image databases , search , queries , shape classification , indexing techniques , Similarity queries	Efficient retrieval of similar shapes	en	189
190	In this paper we present a simple but general 3D slicer for voxelizing polygonal model. Instead of voxelizing a model by projecting and rasterizing triangles with clipping planes, the distance field is used for more accurate and stable voxelization. Distance transform is used with triangles on voxels of each slice. A voxel is marked with opacity only when the shortest distance between it and triangles is judged as intersection. With advanced programmable graphics hardware assistance, surface and solid voxelization are feasible and more efficient than on a CPU.	Graphics hardware , Hausdorff distance , Voxelization , Distance field , voxelization , local distance field , Object representation , rasterization , Polygonal object , GPU-based 3D slicer approach , GPU , 3D slicer , slice-based approach , Rendering , rendering , adaptive dense voxelization , Volumetric representation , pixel shader , opacity , Surface voxelization , polygonal model , Surface representation , Rendering cost , GPU computation , Hausforff distance , object representation , polygonal objects , volumetric representation , triangles , Rendering pipeline , distance transform , volume construction , Modeling , Computational Geometry , geometric representation , hausdorff distance , distance field , Computer Graphics , Polygonal model , 3D modelling , infinitude	A Flexible 3D Slicer for Voxelization Using Graphics Hardware	en	190
191	This paper presents the notion of Semantic Associations as complex relationships between resource entities. These relationships capture both a connectivity of entities as well as similarity of entities based on a specific notion of similarity called -isomorphism. It formalizes these notions for the RDF data model, by introducing a notion of a Property Sequence as a type. In the context of a graph model such as that for RDF, Semantic Associations amount to specific certain graph signatures. Specifically, they refer to sequences (i.e. directed paths) here called Property Sequences, between entities, networks of Property Sequences (i.e. undirected paths), or subgraphs of ρ-isomorphic Property Sequences. The ability to query about the existence of such relationships is fundamental to tasks in analytical domains such as national security and business intelligence, where tasks often focus on finding complex yet meaningful and obscured relationships between entities.  However, support for such queries is lacking in contemporary query systems, including those for RDF. This paper discusses how querying for Semantic Associations might be enabled on the Semantic Web, through the use of an operator ρ. It also discusses two approaches for processing ρ-queries on available persistent RDF stores and memory resident RDF data graphs, thereby building on current RDF query languages.	AI , analysis , isomorphism , Complex Data Relationships , RDF , Rooted Directed Path , Semantic Associations , automation , graph traversals , semantic association , Semantic Web Querying , relationship , semantic web , query processing , Property Sequence	ρ-Queries: Enabling Querying for Semantic Associations on the Semantic Web	en	191
192	With the tremendous growth of system memories, memory-resident databases are increasingly becoming important in various domains. Newer memories provide a structured way of storing data in multiple chips, with each chip having a bank of memory modules. Current memory-resident databases are yet to take full advantage of the banked storage system, which offers a lot of room for performance and energy optimizations. In this paper, we identify the implications of a banked memory environment in supporting memory-resident databases, and propose hardware (memory-directed) and software (query-directed) schemes to reduce the energy consumption of queries executed on these databases. Our results show that high-level query-directed schemes (hosted in the query optimizer) better utilize the low-power modes in reducing the energy consumption than the respective hardware schemes (hosted in the memory controller), due to their complete knowledge of query access patterns. We extend this further and propose a query restructuring scheme and a multi-query optimization . Queries are restructured and regrouped based on their table access patterns to maximize the likelihood that data accesses are clustered. This helps increase the inter-access idle times of memory modules, which in turn enables a more effective control of their energy behavior. This heuristic is eventually integrated with our hardware optimizations to achieve maximum savings. Our experimental results show that the memory energy reduces by 90% if query restructuring method is applied along with basic energy optimizations over the unoptimized version. The system-wide performance impact of each scheme is also studied simultaneously.	hardware energy scheme , query-directed energy management , power consumption , memory-resident databases , database , energy , low-power modes , query-directed scheme , banked memory , multi-query optimization , DRAM , energy optimization , query restructuring	Energy Management Schemes for Memory-Resident Database Systems	en	192
193	"Existing security models require that information of a given security level be prevented from ""leaking"" into lower-security information. High-security applications must be demonstrably free of such leaks, but such demonstration may require substantial manual analysis. Other authors have argued that the natural way to enforce these models automatically is with information-flow analysis, but have not shown this to be practicable for general purpose programming languages in current use. Modern safety-critical systems can contain software components with differing safety integrity levels, potentially operating in the same address space. This case poses problems similar to systems with differing security levels; failure to show separation of data may require the entire system to be validated at the higher integrity level. In this paper we show how the information flow model enforced by the SPARK Examiner provides support for enforcing these security and safety models. We describe an extension to the SPARK variable annotations which allows the specification of a security or safety level for each state variable, and an extension to the SPARK analysis which automatically enforces a given information flow policy on a SPARK program."	security level , SPARK Ada , integrity , Information flow , Dolev-Yao , subprogram , SPARK , information flow , safety , security , Bell-LaPadula	Enforcing Security and Safety Models with an Information Flow Analysis Tool	en	193
194	"Emergent self-organization in multi-agent systems appears to contradict the second law of thermodynamics. This paradox has been explained in terms of a coupling between the macro level that hosts self-organization (and an apparent reduction in entropy), and the micro level (where random processes greatly increase entropy). Metaphorically, the micro level serves as an entropy ""sink,"" permitting overall system entropy to increase while sequestering this increase from the interactions where self-organization is desired. We make this metaphor precise by constructing a simple example of pheromone-based coordination, defining a way to measure the Shannon entropy at the macro (agent) and micro (pheromone) levels, and exhibiting an entropy-based view of the coordination."	thermodynamic , Pheromones , entropy , Entropy , coordination , autonomy , pheromones , multi-agent system , self-organization , Self-Organization	Entropy and Self-Organization in Multi-Agent Systems	en	194
195	We propose an entropy-based sensor selection heuristic for localization. Given 1) a prior probability distribution of the target location, and 2) the locations and the sensing models of a set of candidate sensors for selection, the heuristic selects an informative sensor such that the fusion of the selected sensor observation with the prior target location distribution would yield on average the greatest or nearly the greatest reduction in the entropy of the target location distribution. The heuristic greedily selects one sensor in each step without retrieving any actual sensor observations. The heuristic is also computationally much simpler than the mutual-information-based approaches. The effectiveness of the heuristic is evaluated using localization simulations in which Gaussian sensing models are assumed for simplicity. The heuristic is more effective when the optimal candidate sensor is more informative.	Shannon entropy , entropy , target localization , localization , target tracking , wireless sensor networks , mutual information , information-directed resource management , sensor selection , heuristic , information fusion	Entropy-based Sensor Selection Heuristic for Target Localization	en	195
196	Localized search engines are small-scale systems that index a particular community on the web. They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build, and can provide more precise and complete search capability over their relevant domains. One disadvantage such systems have over large-scale search engines is the lack of global PageRank values. Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole. In this paper, we present well-motivated algorithms to estimate the global PageRank values of a local domain. The algorithms are all highly scalable in that, given a local domain of size n, they use O(n) resources that include computation time, bandwidth, and storage. We test our methods across a variety of localized domains, including site-specific domains and topic-specific domains. We demonstrate that by crawling as few as n or 2n additional pages, our methods can give excellent global PageRank estimates.	node selection , Experimentation , global PageRank , Algorithms , crawling , site specific domain , localized search engines	Estimating the Global PageRank of Web Communities	en	196
197	Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering, which makes recommendations to users based on their collective past behavior. While many similarity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network. We determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities. We also examine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network.	collaborative filtering , online communities , community , recommender system , social network , social networks , similarity measure , Data mining	Evaluating Similarity Measures: A Large-Scale Study in the Orkut Social Network	en	197
198	We present in this paper the design and an evaluation of a novel interface called the Relation Browser++ (RB++) for searching and browsing large information collections. RB++ provides visualized category overviews of an information space and allows dynamic filtering and exploration of the result set by tightly coupling the browsing and searching functions. A user study was conducted to compare the effectiveness, efficiency and user satisfaction of completing various types of searching and browsing using the RB++ interface and a traditional form-fillin interface for a video library. An exploration set of tasks was also included to examine the effectiveness of and user satisfaction with the RB++ when applied to a large federal statistics website. The comparison study strongly supported that RB++ was more effective, efficient, and satisfying for completing data exploration tasks.  Based on the results, efforts to automatically populate the underlying database using machine learning techniques are underway.  Preliminary implementations for two large-scale federal statistical websites have been installed on government servers for internal evaluation.	searching , efficiency , user satisfaction , user study , information search , Information Storage and Retrieval , interaction patterns with interface , Interface design , visualization , Relation Browser++ , browsing , Browse and Search Interface , RB++ , interactive system , Faceted category structure , information browsing , effectiveness , search , dynamic query , facets , browse , Human Factor , visual display , Modeling User Interaction , satisfaction , User interface , category overview , user interface	Evaluation and Evolution of a Browse and Search Interface: Relation Browser++	en	198
199	With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies event threading. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories. We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effec-tively identify the events and capture dependencies among them.	Complete-Link model , Event , Intelligent Information Retrieval , Event Threading , threading , meaningful and efficient analysis and presentation of news , Information browsing and organization , Nearest Parent Model , information searching , Dependency modeling , Agglomerative clustering with time decay , dependency , News topic modeling , Topic detection and tracking , clustering , temporal localization of news stories	Event Threading within News Topics	en	199
200	In this paper we embed evolutionary computation into statistical learning theory. First, we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems. We then embed evolutionary computation into the most prominent representative of this class of learning methods, namely into Support Vector Machines (SVM). In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters . We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem. Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions. Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved. We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets. The evolutionary SVM variants frequently outperform their quadratic programming competitors. Additionally, the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several, possibly competing, performance criteria.	Support vector machines , statistical learning theory , kernel methods , SVM , evolution strategies , large margin , particle swarms , machine learning , hybrid mutation , evolutionary computation , kernels	Evolutionary Learning with Kernels: A Generic Solution for Large Margin Problems	en	200
201	This paper presents a CORBA-compliant middleware architecture that is more flexible and extensible compared to standard CORBA. The portable design of this architecture is easily integrated in any standard CORBA middleware; for this purpose, mainly the handling of object references (IORs) has to be changed. To encapsulate those changes, we introduce the concept of a generic reference manager with portable profile managers. Profile managers are pluggable and in extreme can be downloaded on demand. To illustrate the use of this approach, we present a profile manager implementation for fragmented objects and another one for bridging CORBA to the Jini world. The first profile manager supports truly distributed objects, which allow seamless integration of partitioning , scalability, fault tolerance, end-to-end quality of service, and many more implementation aspects into a distributed object without losing distribution and location transparency. The second profile manager illustrates how our architecture enables fully transparent access from CORBA applications to services on non-CORBA platforms	integration , Flexible and extensible object middleware , IIOP , Software architecture for middleware , CORBA , object oriented , Extensions , Extensibility , extensibility , Middleware architecture , Distiributed applications , Ubiquitous computing , Object references , Middleware , extensible and reconfigurable middleware , Interoperability , distributed objects , implementation , Fault tolerant CORBA , Reference manager , profile manager , middleware interoperability , middleware architecture , Profile manager , Remote object , encapsulation , Flexibility , IOR , Middleware platform , Middleware systems	A Flexible and Extensible Object Middleware: CORBA and Beyond	en	201
202	A goal of human-robot interaction is to allow one user to operate multiple robots simultaneously. In such a scenario the robots provide leverage to the user's attention. The number of such robots that can be operated is called the fan-out of a human-robot team. Robots that have high neglect tolerance and lower interaction time will achieve higher fan-out. We define an equation that relates fan-out to a robot's activity time and its interaction time. We describe how to measure activity time and fan-out. We then use the fan-out equation to compute interaction effort. We can use this interaction effort as a measure of the effectiveness of a human-robot interaction design. We describe experiments that validate the fan-out equation and its use as a metric for improving human-robot interaction.	multiple robots , Human-robot interaction , interaction time , fan-out , interaction effort , human-robot interaction , neglect time , user interface , fan-out equation , activity time	Fan-out Measuring Human Control of Multiple Robots	en	202
203	We give experimental evidence for the benefits of order-preserving compression in sorting algorithms . While, in general, any algorithm might benefit from compressed data because of reduced paging requirements, we identified two natural candidates that would further benefit from order-preserving compression, namely string-oriented sorting algorithms and word-RAM algorithms for keys of bounded length. The word-RAM model has some of the fastest known sorting algorithms in practice. These algorithms are designed for keys of bounded length, usually 32 or 64 bits, which limits their direct applicability for strings. One possibility is to use an order-preserving compression scheme, so that a bounded-key-length algorithm can be applied. For the case of standard algorithms, we took what is considered to be the among the fastest nonword RAM string sorting algorithms, Fast MKQSort, and measured its performance on compressed data. The Fast MKQSort algorithm of Bentley and Sedgewick is optimized to handle text strings. Our experiments show that order-compression techniques results in savings of approximately 15% over the same algorithm on noncompressed data. For the word-RAM, we modified Andersson's sorting algorithm to handle variable-length keys. The resulting algorithm is faster than the standard Unix sort by a factor of 1.5X . Last, we used an order-preserving scheme that is within a constant additive term of the optimal HuTucker, but requires linear time rather than O(m log m), where m = | | is the size of the alphabet.	Order-preserving compression , String sorting , random access , Order-preserving compression scheme , linear time algorithm , Sorting algorithms , word-RAM sorting algorithm , RAM , sorting , unit-cost , compression scheme , compression ratio , word-RAM , data collection , Keys of bounded length	Fast String Sorting Using Order-Preserving Compression	en	203
204	FBRAM, a new form of dynamic random access memory that greatly accelerates the rendering of Z-buffered primitives, is presented . Two key concepts make this acceleration possible. The first is to convert the read-modify-write Z-buffer compare and RGBα blend into a single write only operation. The second is to support two levels of rectangularly shaped pixel caches internal to the memory chip. The result is a 10 megabit part that, for 3D graphics, performs read-modify-write cycles ten times faster than conventional 60 ns VRAMs. A four-way interleaved 100 MHz FBRAM frame buffer can Z-buffer up to 400 million pixels per second. Working FBRAM prototypes have been fabricated.	FBRAM , Video output bandwidth , Dynamic random access memory , RGBa blend , dynamic memory , DRAM , Rendering rate , Z buffer , rendering , graphics , caching , memory , Dynamic memory chips , Pixel processing , 3D graphics hardware , Acceleration , 3D graphics , Z-buffer , Optimisation , Z-compare , pixel caching , VRAM , FBRam , Z-buffering , parallel graphics algorithms , Video buffers , pixel processing , Pixel Cache , Frame buffer , SRAM , Caches	FBRAM: A new Form of Memory Optimized for 3D Graphics	en	204
205	In this paper we study the problem of finding most topical named entities among all entities in a document, which we refer to as focused named entity recognition. We show that these focused named entities are useful for many natural language processing applications, such as document summarization , search result ranking, and entity detection and tracking. We propose a statistical model for focused named entity recognition by converting it into a classification problem . We then study the impact of various linguistic features and compare a number of classification algorithms. From experiments on an annotated Chinese news corpus, we demonstrate that the proposed method can achieve near human-level accuracy.	named entities , Information retrieval , naive Bayes , topic identification , classification model , sentence extraction , Summarization , entity recognition , Natural language processing applications , ranking , information retrieval , Linguistic features , automatic summarization , Classification methods , robust risk minimization , decision tree , electronic documents , machine learning , Focused named entity recognition , Statistical model , Features , Machine learning approach , text summarization , Main topics , natural language processing	Focused Named Entity Recognition Using Machine Learning	en	205
206	Starting from P. Sestoft semantics for lazy evaluation, we define a new semantics in which normal forms consist of variables pointing to lambdas or constructions. This is in accordance with the more recent changes in the Spineless Tagless G-machine (STG) machine, where constructions only appear in closures (lambdas only appeared in closures already in previous versions). We prove the equivalence between the new semantics and Sestoft's. Then, a sequence of STG machines are derived, formally proving the correctness of each derivation. The last machine consists of a few imperative instructions and its distance to a conventional language is minimal. The paper also discusses the differences between the final machine and the actual STG machine implemented in the Glasgow Haskell Compiler.	Lazy evaluation , operational semantics , Functional programming , STG machine , compiler verification , Closures , Translation scheme , Abstract machine , Stepwise derivation , Operational semantics , abstract machines , Haskell compiler	Formally Deriving an STG Machine	en	206
207	Web query classification (QC) aims to classify Web users' queries, which are often short and ambiguous, into a set of target categories. QC has many applications including page ranking in Web search, targeted advertisement in response to queries, and personalization. In this paper, we present a novel approach for QC that outperforms the winning solution of the ACM KDDCUP 2005 competition, whose objective is to classify 800,000 real user queries. In our approach, we first build a bridging classifier on an intermediate taxonomy in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the above intermediate taxonomy. A major innovation is that by leveraging the similarity distribution over the intermediate taxonomy, we do not need to retrain a new classifier for each new set of target categories, and therefore the bridging classifier needs to be trained only once. In addition, we introduce category selection as a new method for narrowing down the scope of the intermediate taxonomy based on which we classify the queries. Category selection can improve both efficiency and effectiveness of the online classification. By combining our algorithm with the winning solution of KDDCUP 2005, we made an improvement by 9.7% and 3.8% in terms of precision and F1 respectively compared with the best results of KDDCUP 2005.	Bridging classifier , Category Selection , Ensemble classifier , Bridging Classifier , Target categories , Similarity distribution , Mapping functions , Search engine , Matching approaches , Intermediate categories , Taxonomy , Query enrichment , KDDCUP 2005 , Query classification , Category selection , Web Query Classification	Building Bridges for Web Query Classification	en	207
208	A collaborative crawler is a group of crawling nodes, in which each crawling node is responsible for a specific portion of the web. We study the problem of collecting geographically -aware pages using collaborative crawling strategies. We first propose several collaborative crawling strategies for the geographically focused crawling, whose goal is to collect web pages about specified geographic locations, by considering features like URL address of page, content of page, extended anchor text of link, and others. Later, we propose various evaluation criteria to qualify the performance of such crawling strategies. Finally, we experimentally study our crawling strategies by crawling the real web data showing that some of our crawling strategies greatly outperform the simple URL-hash based partition collaborative crawling, in which the crawling assignments are determined according to the hash-value computation over URLs. More precisely, features like URL address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling.	geographical nodes , crawling strategies , Collaborative crawler , Evaluation criteria , URL based , Geographic Locality , Normalization and Disambiguation of City Names , Search engine , Geographically focused crawling , Scalability , Geographic locality , Collaborative crawling , Anchor text , collaborative crawling , Problems of aliasing and ambiguity , Search localization , IP address based , Focused crawler , Geo-focus , geographic entities , Full Content based , Extracted URL , pattern matching , Crawling strategies , Geo-coverage , Hash based collaboration , geographically focused crawling , Quality issue	Geographically Focused Collaborative Crawling	en	208
209	In this paper we consider implementations of embedded 3D graphics and provide evidence indicating that 3D benchmarks employed for desktop computers are not suitable for mobile environments. Consequently, we present GraalBench, a set of 3D graphics workloads representative for contemporary and emerging mobile devices . In addition, we present detailed simulation results for a typical rasterization pipeline. The results show that the proposed benchmarks use only a part of the resources offered by current 3D graphics libraries. For instance, while each benchmark uses the texturing unit for more than 70% of the generated fragments, the alpha unit is employed for less than 13% of the fragments. The Fog unit was used for 84% of the fragments by one benchmark, but the other benchmarks did not use it at all. Our experiments on the proposed suite suggest that the texturing, depth and blending units should be implemented in hardware, while, for instance, the dithering unit may be omitted from a hardware implementation. Finally, we discuss the architectural implications of the obtained results for hardware implementations.	mechanism , workload characterization , API , Mobile devices , embedded 3D graphics , accelerators , 3D graphics benchmarking , real-time , bottlenecks , rasterization , Graalbench , architecture , embedded systems , workload , benchmark , 3D graphics , pipeline , Mobile environments , 3D graphics applications , mobile phones , triangles , openGL , unit , GraalBench , performance , measurement , statistics , OpenGL , transform and lighting , embedded 3D graphics architectures , 3D graphics benchmarks	GraalBench: A 3D Graphics Benchmark Suite for Mobile Phones	en	209
210	Vertical handoff is a switching process between heterogeneous wireless networks in a hybrid 3G/WLAN network. Vertical handoffs fromWLAN to 3G network often fail due to the abrupt degrade of the WLAN signal strength in the transition areas. In this paper, a Handoff Trigger Table is introduced to improve the performance of vertical handoff. Based on this table, a proactive handoff scheme is proposed. Simulation results show that with the proposed scheme, the vertical handoff decisions will be more efficient so that dropping probability can be decreased dramatically.	WLAN , integrated networks , vertical handoff , cellular network , 3G , Wireless communications , Handoff trigger table , wireles networks	Handoff Trigger Table for Integrated 3G/WLAN Networks	en	210
