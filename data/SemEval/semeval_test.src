{"title": "Scalable Grid Service Discovery Based on UDDI", "abstract": "Efficient discovery of grid services is essential for the success of grid computing. The standardization of grids based on web services has resulted in the need for scalable web service discovery mechanisms to be deployed in grids Even though UDDI has been the de facto industry standard for web-services discovery, imposed requirements of tight-replication among registries and lack of autonomous control has severely hindered its widespread deployment and usage. With the advent of grid computing the scalability issue of UDDI will become a roadblock that will prevent its deployment in grids. In this paper we present our distributed web-service discovery architecture, called DUDE (Distributed UDDI Deployment Engine). DUDE leverages DHT (Distributed Hash Tables) as a rendezvous mechanism between multiple UDDI registries. DUDE enables consumers to query multiple registries, still at the same time allowing organizations to have autonomous control over their registries.. Based on preliminary prototype on PlanetLab, we believe that DUDE architecture can support effective distribution of UDDI registries thereby making UDDI more robust and also addressing its scaling issues. Furthermore, The DUDE architecture for scalable distribution can be applied beyond UDDI to any Grid Service Discovery mechanism.", "id": "0", "src": "scalable grid service discovery based on uddi . efficient discovery of grid services is essential for the success of grid computing . the standardization of grids based on web services has resulted in the need for scalable web service discovery mechanisms to be deployed in grids even though uddi has been the de facto industry standard for web services discovery , imposed requirements of tight replication among registries and lack of autonomous control has severely hindered its widespread deployment and usage . with the advent of grid computing the scalability issue of uddi will become a roadblock that will prevent its deployment in grids . in this paper we present our distributed web service discovery architecture , called dude ( distributed uddi deployment engine ) . dude leverages dht ( distributed hash tables ) as a rendezvous mechanism between multiple uddi registries . dude enables consumers to query multiple registries , still at the same time allowing organizations to have autonomous control over their registries . . based on preliminary prototype on planetlab , we believe that dude architecture can support effective distribution of uddi registries thereby making uddi more robust and also addressing its scaling issues . furthermore , the dude architecture for scalable distribution can be applied beyond uddi to any grid service discovery mechanism ."}
{"title": "Sensor Deployment Strategy for Target Detection", "abstract": "In order to monitor a region for traffic traversal, sensors can be deployed to perform collaborative target detection. Such a sensor network achieves a certain level of detection performance with an associated cost of deployment. This paper addresses this problem by proposing path exposure as a measure of the goodness of a deployment and presents an approach for sequential deployment in steps. It illustrates that the cost of deployment can be minimized to achieve the desired detection performance by appropriately choosing the number of sensors deployed in each step.", "id": "1", "src": "sensor deployment strategy for target detection . in order to monitor a region for traffic traversal , sensors can be deployed to perform collaborative target detection . such a sensor network achieves a certain level of detection performance with an associated cost of deployment . this paper addresses this problem by proposing path exposure as a measure of the goodness of a deployment and presents an approach for sequential deployment in steps . it illustrates that the cost of deployment can be minimized to achieve the desired detection performance by appropriately choosing the number of sensors deployed in each step ."}
{"title": "Deployment Issues of a VoIP Conferencing System in a Virtual Conferencing Environment", "abstract": "Real-time services have been supported by and large on circuitswitched networks. Recent trends favour services ported on packet-switched networks. For audio conferencing, we need to consider many issues - scalability, quality of the conference application, floor control and load on the clients/servers - to name a few. In this paper, we describe an audio service framework designed to provide a Virtual Conferencing Environment (VCE). The system is designed to accommodate a large number of end users speaking at the same time and spread across the Internet. The framework is based on Conference Servers [14], which facilitate the audio handling, while we exploit the SIP capabilities for signaling purposes. Client selection is based on a recent quantifier called \"Loudness Number\" that helps mimic a physical face-to-face conference. We deal with deployment issues of the proposed solution both in terms of scalability and interactivity, while explaining the techniques we use to reduce the traffic. We have implemented a Conference Server (CS) application on a campus-wide network at our Institute.", "id": "2", "src": "deployment issues of a voip conferencing system in a virtual conferencing environment . real time services have been supported by and large on circuitswitched networks . recent trends favour services ported on packet switched networks . for audio conferencing , we need to consider many issues scalability , quality of the conference application , floor control and load on the clients servers to name a few . in this paper , we describe an audio service framework designed to provide a virtual conferencing environment ( vce ) . the system is designed to accommodate a large number of end users speaking at the same time and spread across the internet . the framework is based on conference servers <digit> , which facilitate the audio handling , while we exploit the sip capabilities for signaling purposes . client selection is based on a recent quantifier called loudness number that helps mimic a physical face to face conference . we deal with deployment issues of the proposed solution both in terms of scalability and interactivity , while explaining the techniques we use to reduce the traffic . we have implemented a conference server ( cs ) application on a campus wide network at our institute ."}
{"title": "An Initial Analysis and Presentation of Malware Exhibiting Swarm-Like Behavior", "abstract": "The Slammer, which is currently the fastest computer worm in recorded history, was observed to infect 90 percent of all vulnerable Internets hosts within 10 minutes. Although the main action that the Slammer worm takes is a relatively unsophisticated replication of itself, it still spreads so quickly that human response was ineffective. Most proposed countermeasures strategies are based primarily on rate detection and limiting algorithms. However, such strategies are being designed and developed to effectively contain worms whose behaviors are similar to that of Slammer. In our work, we put forth the hypothesis that next generation worms will be radically different, and potentially such techniques will prove ineffective. Specifically, we propose to study a new generation of worms called Swarm Worms, whose behavior is predicated on the concept of emergent intelligence. Emergent Intelligence is the behavior of systems, very much like biological systems such as ants or bees, where simple local interactions of autonomous members, with simple primitive actions, gives rise to complex and intelligent global behavior. In this manuscript we will introduce the basic principles behind the idea of Swarm Worms, as well as the basic structure required in order to be considered a swarm worm. In addition, we will present preliminary results on the propagation speeds of one such swarm worm, called the ZachiK worm. We will show that ZachiK is capable of propagating at a rate 2 orders of magnitude faster than similar worms without swarm capabilities. Categories and Subject Descriptors", "id": "3", "src": "an initial analysis and presentation of malware exhibiting swarm like behavior . the slammer , which is currently the fastest computer worm in recorded history , was observed to infect <digit> percent of all vulnerable internets hosts within <digit> minutes . although the main action that the slammer worm takes is a relatively unsophisticated replication of itself , it still spreads so quickly that human response was ineffective . most proposed countermeasures strategies are based primarily on rate detection and limiting algorithms . however , such strategies are being designed and developed to effectively contain worms whose behaviors are similar to that of slammer . in our work , we put forth the hypothesis that next generation worms will be radically different , and potentially such techniques will prove ineffective . specifically , we propose to study a new generation of worms called swarm worms , whose behavior is predicated on the concept of emergent intelligence . emergent intelligence is the behavior of systems , very much like biological systems such as ants or bees , where simple local interactions of autonomous members , with simple primitive actions , gives rise to complex and intelligent global behavior . in this manuscript we will introduce the basic principles behind the idea of swarm worms , as well as the basic structure required in order to be considered a swarm worm . in addition , we will present preliminary results on the propagation speeds of one such swarm worm , called the zachik worm . we will show that zachik is capable of propagating at a rate 2 orders of magnitude faster than similar worms without swarm capabilities . categories and subject descriptors"}
{"title": "Service Interface: A New Abstraction for Implementing and Composing Protocols", "abstract": "In this paper we compare two approaches to the design of protocol frameworks - tools for implementing modular network protocols. The most common approach uses events as the main abstraction for a local interaction between protocol modules. We argue that an alternative approach, that is based on service abstraction, is more suitable for expressing modular protocols. It also facilitates advanced features in the design of protocols, such as dynamic update of distributed protocols. We then describe an experimental implementation of a service-based protocol framework in Java.", "id": "4", "src": "service interface a new abstraction for implementing and composing protocols . in this paper we compare two approaches to the design of protocol frameworks tools for implementing modular network protocols . the most common approach uses events as the main abstraction for a local interaction between protocol modules . we argue that an alternative approach , that is based on service abstraction , is more suitable for expressing modular protocols . it also facilitates advanced features in the design of protocols , such as dynamic update of distributed protocols . we then describe an experimental implementation of a service based protocol framework in java ."}
{"title": "Live Data Center Migration across WANs: A Robust Cooperative Context Aware Approach", "abstract": "A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocate a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities. We make use of server virtualization technologies to enable the replication and migration of server functions. We propose new network functions to enable server migration and replication across wide area networks (e.g., the Internet), and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.", "id": "5", "src": "live data center migration across wans a robust cooperative context aware approach . a significant concern for internet based service providers is the continued operation and availability of services in the face of outages , whether planned or unplanned . in this paper we advocate a cooperative , context aware approach to data center migration across wans to deal with outages in a non disruptive manner . we specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities . we make use of server virtualization technologies to enable the replication and migration of server functions . we propose new network functions to enable server migration and replication across wide area networks ( e . g . , the internet ) , and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives ."}
{"title": "Runtime Metrics Collection for Middleware Supported Adaptation of Mobile Applications", "abstract": "This paper proposes, implements, and evaluates in terms of worst case performance, an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware. The solution is based upon an abstract representation of the mobile object system, which holds containers aggregating metrics for each specific component including host managers, runtimes and mobile objects. A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system. The MobJeX platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability, efficiency and the application of simple measurement and propagation criteria to reduce collection overhead.", "id": "6", "src": "runtime metrics collection for middleware supported adaptation of mobile applications . this paper proposes , implements , and evaluates in terms of worst case performance , an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware . the solution is based upon an abstract representation of the mobile object system , which holds containers aggregating metrics for each specific component including host managers , runtimes and mobile objects . a key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system . the mobjex platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability , efficiency and the application of simple measurement and propagation criteria to reduce collection overhead ."}
{"title": "Implementation of a Dynamic Adjustment Mechanism with Efficient Replica Selection in Data Grid Environments", "abstract": "The co-allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers. Several co-allocation strategies have been coupled and used to exploit rate differences among various client-server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes. However, a major obstacle, the idle time of faster servers having to wait for the slowest server to deliver the final block, makes it important to reduce differences in finishing time among replica servers. In this paper, we propose a dynamic coallocation scheme, namely Recursive-Adjustment Co-Allocation scheme, to improve the performance of data transfer in Data Grids. Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time. We also provide an effective scheme for reducing the cost of reassembling data blocks.", "id": "7", "src": "implementation of a dynamic adjustment mechanism with efficient replica selection in data grid environments . the co allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers . several co allocation strategies have been coupled and used to exploit rate differences among various client server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes . however , a major obstacle , the idle time of faster servers having to wait for the slowest server to deliver the final block , makes it important to reduce differences in finishing time among replica servers . in this paper , we propose a dynamic coallocation scheme , namely recursive adjustment co allocation scheme , to improve the performance of data transfer in data grids . our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time . we also provide an effective scheme for reducing the cost of reassembling data blocks ."}
{"title": "A High-Accuracy, Low-Cost Localization System for Wireless Sensor Networks", "abstract": "The problem of localization of wireless sensor nodes has long been regarded as very difficult to solve, when considering the realities of real world environments. In this paper, we formally describe, design, implement and evaluate a novel localization system, called Spotlight. Our system uses the spatio-temporal properties of well controlled events in the network (e.g., light), to obtain the locations of sensor nodes. We demonstrate that a high accuracy in localization can be achieved without the aid of expensive hardware on the sensor nodes, as required by other localization systems. We evaluate the performance of our system in deployments of Mica2 and XSM motes. Through performance evaluations of a real system deployed outdoors, we obtain a 20cm localization error. A sensor network, with any number of nodes, deployed in a 2500m2 area, can be localized in under 10 minutes, using a device that costs less than $1000. To the best of our knowledge, this is the first report of a sub-meter localization error, obtained in an outdoor environment, without equipping the wireless sensor nodes with specialized ranging hardware.", "id": "8", "src": "a high accuracy , low cost localization system for wireless sensor networks . the problem of localization of wireless sensor nodes has long been regarded as very difficult to solve , when considering the realities of real world environments . in this paper , we formally describe , design , implement and evaluate a novel localization system , called spotlight . our system uses the spatio temporal properties of well controlled events in the network ( e . g . , light ) , to obtain the locations of sensor nodes . we demonstrate that a high accuracy in localization can be achieved without the aid of expensive hardware on the sensor nodes , as required by other localization systems . we evaluate the performance of our system in deployments of mica2 and xsm motes . through performance evaluations of a real system deployed outdoors , we obtain a 20cm localization error . a sensor network , with any number of nodes , deployed in a 2500m2 area , can be localized in under <digit> minutes , using a device that costs less than <digit> . to the best of our knowledge , this is the first report of a sub meter localization error , obtained in an outdoor environment , without equipping the wireless sensor nodes with specialized ranging hardware ."}
{"title": "PackageBLAST: An Adaptive Multi-Policy Grid Service for Biological Sequence Comparison", "abstract": "In this paper, we propose an adaptive task allocation framework to perform BLAST searches in a grid environment against sequence database segments. The framework, called PackageBLAST, provides an infrastructure to choose or incorporate task allocation strategies. Furthermore, we propose a mechanism to compute grid nodes execution weight, adapting the chosen allocation policy to the current computational power of the nodes. Our results present very good speedups and also show that no single allocation strategy is able to achieve the lowest execution times for all scenarios.", "id": "9", "src": "packageblast an adaptive multi policy grid service for biological sequence comparison . in this paper , we propose an adaptive task allocation framework to perform blast searches in a grid environment against sequence database segments . the framework , called packageblast , provides an infrastructure to choose or incorporate task allocation strategies . furthermore , we propose a mechanism to compute grid nodes execution weight , adapting the chosen allocation policy to the current computational power of the nodes . our results present very good speedups and also show that no single allocation strategy is able to achieve the lowest execution times for all scenarios ."}
{"title": "Implementation and Performance Evaluation of CONFLEX-G: Grid-enabled Molecular Conformational Space Search Program with OmniRPC", "abstract": "CONFLEX-G is the grid-enabled version of a molecular conformational space search program called CONFLEX. We have implemented CONFLEX-G using a grid RPC system called OmniRPC. In this paper, we report the performance of CONFLEX-G in a grid testbed of several geographically distributed PC clusters. In order to explore many conformation of large bio-molecules, CONFLEX-G generates trial structures of the molecules and allocates jobs to optimize a trial structure with a reliable molecular mechanics method in the grid. OmniRPC provides a restricted persistence model to support the parametric search applications. In this model, when the initialization procedure is defined in the RPC module, the module is automatically initialized at the time of invocation by calling the initialization procedure. This can eliminate unnecessary communication and initialization at each call in CONFLEX-G. CONFLEXG can achieve performance comparable to CONFLEX MPI and can exploit more computing resources by allowing the use of a cluster of multiple clusters in the grid. The experimental result shows that CONFLEX-G achieved a speedup of 56.5 times in the case of the 1BL1 molecule, where the molecule consists of a large number of atoms, and each trial structure optimization requires significant time. The load imbalance of the optimization time of the trial structure may also cause performance degradation.", "id": "10", "src": "implementation and performance evaluation of conflex g grid enabled molecular conformational space search program with omnirpc . conflex g is the grid enabled version of a molecular conformational space search program called conflex . we have implemented conflex g using a grid rpc system called omnirpc . in this paper , we report the performance of conflex g in a grid testbed of several geographically distributed pc clusters . in order to explore many conformation of large bio molecules , conflex g generates trial structures of the molecules and allocates jobs to optimize a trial structure with a reliable molecular mechanics method in the grid . omnirpc provides a restricted persistence model to support the parametric search applications . in this model , when the initialization procedure is defined in the rpc module , the module is automatically initialized at the time of invocation by calling the initialization procedure . this can eliminate unnecessary communication and initialization at each call in conflex g . conflexg can achieve performance comparable to conflex mpi and can exploit more computing resources by allowing the use of a cluster of multiple clusters in the grid . the experimental result shows that conflex g achieved a speedup of <digit> . 5 times in the case of the 1bl1 molecule , where the molecule consists of a large number of atoms , and each trial structure optimization requires significant time . the load imbalance of the optimization time of the trial structure may also cause performance degradation ."}
{"title": "Self-Adaptive Applications on the Grid", "abstract": "Grids are inherently heterogeneous and dynamic. One important problem in grid computing is resource selection, that is, finding an appropriate resource set for the application. Another problem is adaptation to the changing characteristics of the grid environment. Existing solutions to these two problems require that a performance model for an application is known. However, constructing such models is a complex task. In this paper, we investigate an approach that does not require performance models. We start an application on any set of resources. During the application run, we periodically collect the statistics about the application run and deduce application requirements from these statistics. Then, we adjust the resource set to better fit the application needs. This approach allows us to avoid performance bottlenecks, such as overloaded WAN links or very slow processors, and therefore can yield significant performance improvements. We evaluate our approach in a number of scenarios typical for the Grid.", "id": "11", "src": "self adaptive applications on the grid . grids are inherently heterogeneous and dynamic . one important problem in grid computing is resource selection , that is , finding an appropriate resource set for the application . another problem is adaptation to the changing characteristics of the grid environment . existing solutions to these two problems require that a performance model for an application is known . however , constructing such models is a complex task . in this paper , we investigate an approach that does not require performance models . we start an application on any set of resources . during the application run , we periodically collect the statistics about the application run and deduce application requirements from these statistics . then , we adjust the resource set to better fit the application needs . this approach allows us to avoid performance bottlenecks , such as overloaded wan links or very slow processors , and therefore can yield significant performance improvements . we evaluate our approach in a number of scenarios typical for the grid ."}
{"title": "Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh", "abstract": "In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel. Key contributions of this work include: i) an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node, ii) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii) a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing. In a tree, it is critical that a node\"s parent delivers a high rate of application data to each child. In Bullet however, nodes simultaneously receive data from multiple sources in parallel, making it less important to locate any single source capable of sustaining a high transmission rate.", "id": "12", "src": "bullet high bandwidth data dissemination using an overlay mesh . in recent years , overlay networks have become an effective alternative to ip multicast for efficient point to multipoint communication across the internet . typically , nodes self organize with the goal of forming an efficient overlay tree , one that meets performance targets without placing undue burden on the underlying network . in this paper , we target high bandwidth data distribution from a single source to a large number of receivers . applications include large file transfers and real time multimedia streaming . for these applications , we argue that an overlay mesh , rather than a tree , can deliver fundamentally higher bandwidth and reliability relative to typical tree structures . this paper presents bullet , a scalable and distributed algorithm that enables nodes spread across the internet to self organize into a high bandwidth overlay mesh . we construct bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network . individual bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel . key contributions of this work include i ) an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node , ii ) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items , and iii ) a complete implementation and evaluation of bullet running across the internet and in a large scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances . in addition , we find that , relative to tree based solutions , bullet reduces the need to perform expensive bandwidth probing . in a tree , it is critical that a node s parent delivers a high rate of application data to each child . in bullet however , nodes simultaneously receive data from multiple sources in parallel , making it less important to locate any single source capable of sustaining a high transmission rate ."}
{"title": "Apocrita: A Distributed Peer-to-Peer File Sharing System for Intranets", "abstract": "Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all member of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are shared between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user\"s machine. Another problem arises when a document is made available on a user\"s machine and that user is offline, in which case the document is no longer accessible. In this paper we present Apocrita, a revolutionary distributed P2P file sharing system for Intranets.", "id": "13", "src": "apocrita a distributed peer to peer file sharing system for intranets . many organizations are required to author documents for various purposes , and such documents may need to be accessible by all member of the organization . this access may be needed for editing or simply viewing a document . in some cases these documents are shared between authors , via email , to be edited . this can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document . there may even be multiple different documents in the process of being edited . the user may be required to search for a particular document , which some search tools such as google desktop may be a solution for local documents but will not find a document on another user s machine . another problem arises when a document is made available on a user s machine and that user is offline , in which case the document is no longer accessible . in this paper we present apocrita , a revolutionary distributed p2p file sharing system for intranets ."}
{"title": "BuddyCache: High-Performance Object Storage for Collaborative Strong-Consistency Applications in a WAN", "abstract": "Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area networks because of high network latency. BuddyCache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers. We have implemented a BuddyCache prototype and evaluated its performance. Analytical results, confirmed by measurements of the BuddyCache prototype using the multiuser 007 benchmark indicate that for typical Internet latencies, e.g. ranging from 40 to 80 milliseconds round trip time to the storage server, peers using BuddyCache can reduce by up to 50% the latency of access to shared objects compared to accessing the remote servers directly.", "id": "14", "src": "buddycache high performance object storage for collaborative strong consistency applications in a wan . collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task . they require strong consistency for shared persistent data and efficient access to fine grained objects . these properties are difficult to provide in wide area networks because of high network latency . buddycache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong consistency applications in high latency network environments . the challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers . we have implemented a buddycache prototype and evaluated its performance . analytical results , confirmed by measurements of the buddycache prototype using the multiuser <digit> benchmark indicate that for typical internet latencies , e . g . ranging from <digit> to <digit> milliseconds round trip time to the storage server , peers using buddycache can reduce by up to <digit> the latency of access to shared objects compared to accessing the remote servers directly ."}
{"title": "Rewards-Based Negotiation for Providing Context Information", "abstract": "How to provide appropriate context information is a challenging problem in context-aware computing. Most existing approaches use a centralized selection mechanism to decide which context information is appropriate. In this paper, we propose a novel approach based on negotiation with rewards to solving such problem. Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds. In order to support our approach, we have designed a concrete negotiation model with rewards. We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly.", "id": "15", "src": "rewards based negotiation for providing context information . how to provide appropriate context information is a challenging problem in context aware computing . most existing approaches use a centralized selection mechanism to decide which context information is appropriate . in this paper , we propose a novel approach based on negotiation with rewards to solving such problem . distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds . in order to support our approach , we have designed a concrete negotiation model with rewards . we also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly ."}
{"title": "Researches on Scheme of Pairwise Key Establishment for DistributedSensor Networks", "abstract": "Security schemes of pairwise key establishment, which enable sensors to communicate with each other securely, play a fundamental role in research on security issue in wireless sensor networks. A new kind of cluster deployed sensor networks distribution model is presented, and based on which, an innovative Hierarchical Hypercube model - H(k,u,m,v,n) and the mapping relationship between cluster deployed sensor networks and the H(k,u,m,v,n) are proposed. By utilizing nice properties of H(k,u,m,v,n) model, a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed, which combines the idea of KDC(Key Distribution Center) and polynomial pool schemes. Furthermore, the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected. Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key, compared with previous related works.", "id": "16", "src": "researches on scheme of pairwise key establishment for distributedsensor networks . security schemes of pairwise key establishment , which enable sensors to communicate with each other securely , play a fundamental role in research on security issue in wireless sensor networks . a new kind of cluster deployed sensor networks distribution model is presented , and based on which , an innovative hierarchical hypercube model h ( k , u , m , v , n ) and the mapping relationship between cluster deployed sensor networks and the h ( k , u , m , v , n ) are proposed . by utilizing nice properties of h ( k , u , m , v , n ) model , a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed , which combines the idea of kdc ( key distribution center ) and polynomial pool schemes . furthermore , the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected . theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key , compared with previous related works ."}
{"title": "Encryption-Enforced Access Control in Dynamic Multi-Domain Publish/Subscribe Networks", "abstract": "Publish/subscribe systems provide an efficient, event-based, wide-area distributed communications infrastructure. Large scale publish/subscribe systems are likely to employ components of the event transport network owned by cooperating, but independent organisations. As the number of participants in the network increases, security becomes an increasing concern. This paper extends previous work to present and evaluate a secure multi-domain publish/subscribe infrastructure that supports and enforces fine-grained access control over the individual attributes of event types. Key refresh allows us to ensure forward and backward security when event brokers join and leave the network. We demonstrate that the time and space overheads can be minimised by careful consideration of encryption techniques, and by the use of caching to decrease unnecessary decryptions. We show that our approach has a smaller overall communication overhead than existing approaches for achieving the same degree of control over security in publish/subscribe networks.", "id": "17", "src": "encryption enforced access control in dynamic multi domain publish subscribe networks . publish subscribe systems provide an efficient , event based , wide area distributed communications infrastructure . large scale publish subscribe systems are likely to employ components of the event transport network owned by cooperating , but independent organisations . as the number of participants in the network increases , security becomes an increasing concern . this paper extends previous work to present and evaluate a secure multi domain publish subscribe infrastructure that supports and enforces fine grained access control over the individual attributes of event types . key refresh allows us to ensure forward and backward security when event brokers join and leave the network . we demonstrate that the time and space overheads can be minimised by careful consideration of encryption techniques , and by the use of caching to decrease unnecessary decryptions . we show that our approach has a smaller overall communication overhead than existing approaches for achieving the same degree of control over security in publish subscribe networks ."}
{"title": "A Framework for Architecting Peer-to-Peer Receiver-driven Overlays", "abstract": "This paper presents a simple and scalable framework for architecting peer-to-peer overlays called Peer-to-peer Receiverdriven Overlay (or PRO). PRO is designed for non-interactive streaming applications and its primary design goal is to maximize delivered bandwidth (and thus delivered quality) to peers with heterogeneous and asymmetric bandwidth. To achieve this goal, PRO adopts a receiver-driven approach where each receiver (or participating peer) (i) independently discovers other peers in the overlay through gossiping, and (ii) selfishly determines the best subset of parent peers through which to connect to the overlay to maximize its own delivered bandwidth. Participating peers form an unstructured overlay which is inherently robust to high churn rate. Furthermore, each receiver leverages congestion controlled bandwidth from its parents as implicit signal to detect and react to long-term changes in network or overlay condition without any explicit coordination with other participating peers. Independent parent selection by individual peers dynamically converge to an efficient overlay structure.", "id": "18", "src": "a framework for architecting peer to peer receiver driven overlays . this paper presents a simple and scalable framework for architecting peer to peer overlays called peer to peer receiverdriven overlay ( or pro ) . pro is designed for non interactive streaming applications and its primary design goal is to maximize delivered bandwidth ( and thus delivered quality ) to peers with heterogeneous and asymmetric bandwidth . to achieve this goal , pro adopts a receiver driven approach where each receiver ( or participating peer ) ( i ) independently discovers other peers in the overlay through gossiping , and ( ii ) selfishly determines the best subset of parent peers through which to connect to the overlay to maximize its own delivered bandwidth . participating peers form an unstructured overlay which is inherently robust to high churn rate . furthermore , each receiver leverages congestion controlled bandwidth from its parents as implicit signal to detect and react to long term changes in network or overlay condition without any explicit coordination with other participating peers . independent parent selection by individual peers dynamically converge to an efficient overlay structure ."}
{"title": "Intra-flow Loss Recovery and Control for VolP", "abstract": "Best effort packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such as voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold: on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal playout is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets. We first develop an end-to-end model based on loss runlengths with which we can describe the loss distribution within a flow. These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs (PCM) with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs (G.729) on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may break at transitions within the speech signal however. We then propose mechanisms which differentiate between packets within a voice data flow to minimize the impact of packet loss. We designate these methods as intra-flow loss recovery and control. At the end-to-end level, identification of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow to (statistically) trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both packets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term.", "id": "19", "src": "intra flow loss recovery and control for volp . best effort packet switched networks , like the internet , do not offer a reliable transmission of packets to applications with real time constraints such as voice . thus , the loss of packets impairs the application level utility . for voice this utility impairment is twofold on one hand , even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal playout is interrupted . on the other hand , some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets . we first develop an end to end model based on loss runlengths with which we can describe the loss distribution within a flow . these packet level metrics are then linked to user level objective speech quality metrics . using this framework , we find that for low compressing sample based codecs ( pcm ) with loss concealment isolated packet losses can be concealed well , whereas burst losses have a higher perceptual impact . for high compressing frame based codecs ( g . <digit> ) on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories , though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state . contrary to sample based codecs we show that the concealment performance may break at transitions within the speech signal however . we then propose mechanisms which differentiate between packets within a voice data flow to minimize the impact of packet loss . we designate these methods as intra flow loss recovery and control . at the end to end level , identification of packets sensitive to loss ( sender ) as well as loss concealment ( receiver ) takes place . hop by hop support schemes then allow to ( statistically ) trade the loss of one packet , which is considered more important , against another one of the same flow which is of lower importance . as both packets require the same cost in terms of network transmission , a gain in user perception is obtainable . we show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term ."}
{"title": "Edge Indexing in a Grid for Highly Dynamic Virtual Environments", "abstract": "Newly emerging game-based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real-time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client-server based grid subspace division where the virtual worlds are partitioned into manageable sub-worlds. In each sub-world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium. In such systems, the determination of the set of objects that are visible from a user\"s viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real-time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments. The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real-time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next-generation virtual frameworks that may merge into existing web-based services in the near future.", "id": "20", "src": "edge indexing in a grid for highly dynamic virtual environments . newly emerging game based application systems such as second life1 provide 3d virtual environments where multiple users interact with each other in real time . they are filled with autonomous , mutable virtual content which is continuously augmented by the users . to make the systems highly scalable and dynamically extensible , they are usually built on a client server based grid subspace division where the virtual worlds are partitioned into manageable sub worlds . in each sub world , the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint , rather than retrieving them from a local storage medium . in such systems , the determination of the set of objects that are visible from a user s viewpoint is one of the primary factors that affect server throughput and scalability . specifically , performing real time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub millions of active users are moving and interacting . we recognize that the described challenges are closely related to a spatial database problem , and hence we map the moving geometry objects in the virtual space to a set of multi dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query . unfortunately , existing spatial indexing methods are unsuitable for this kind of new environments . the main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real time visibility determination . we then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality , system throughput , and resource utilization . we expect our approach to lay the groundwork for next generation virtual frameworks that may merge into existing web based services in the near future ."}
{"title": "Design and Implementation of a Distributed Content Management System", "abstract": "The convergence of advances in storage, encoding, and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices. Keeping track of (or managing) such content remains challenging due to the sheer volume of data. Storing live continuous media (such as TV or radio content) adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with. Networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network, making the task of content management all but impossible to deal with without a content management system. In this paper we present the design and implementation of the Spectrum content management system, which deals with rich media content effectively in this environment. Spectrum has a modular architecture that allows its application to both stand-alone and various networked scenarios. A unique aspect of Spectrum is that it requires one (or more) retention policies to apply to every piece of content that is stored in the system. This means that there are no eviction policies. Content that no longer has a retention policy applied to it is simply removed from the system. Different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication. This approach also allows Spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media, to content. We not only describe the details of the Spectrum architecture but also give typical use cases.", "id": "21", "src": "design and implementation of a distributed content management system . the convergence of advances in storage , encoding , and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices . keeping track of ( or managing ) such content remains challenging due to the sheer volume of data . storing live continuous media ( such as tv or radio content ) adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with . networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network , making the task of content management all but impossible to deal with without a content management system . in this paper we present the design and implementation of the spectrum content management system , which deals with rich media content effectively in this environment . spectrum has a modular architecture that allows its application to both stand alone and various networked scenarios . a unique aspect of spectrum is that it requires one ( or more ) retention policies to apply to every piece of content that is stored in the system . this means that there are no eviction policies . content that no longer has a retention policy applied to it is simply removed from the system . different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication . this approach also allows spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media , to content . we not only describe the details of the spectrum architecture but also give typical use cases ."}
{"title": "Operation Context and Context-based Operational", "abstract": "Operational Transformation (OT) is a technique for consistency maintenance and group undo, and is being applied to an increasing number of collaborative applications. The theoretical foundation for OT is crucial in determining its capability to solve existing and new problems, as well as the quality of those solutions. The theory of causality has been the foundation of all prior OT systems, but it is inadequate to capture essential correctness requirements. Past research had invented various patches to work around this problem, resulting in increasingly intricate and complicated OT algorithms. After having designed, implemented, and experimented with a series of OT algorithms, we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving OT problems, reducing its complexity, and supporting its continual evolution. In this paper, we report the main results of this effort: the theory of operation context and the COT (Context-based OT) algorithm. The COT algorithm is capable of supporting both do and undo of any operations at anytime, without requiring transformation functions to preserve Reversibility Property, Convergence Property 2, Inverse Properties 2 and 3. The COT algorithm is not only simpler and more efficient than prior OT control algorithms, but also simplifies the design of transformation functions. We have implemented the COT algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications.", "id": "22", "src": "operation context and context based operational . operational transformation ( ot ) is a technique for consistency maintenance and group undo , and is being applied to an increasing number of collaborative applications . the theoretical foundation for ot is crucial in determining its capability to solve existing and new problems , as well as the quality of those solutions . the theory of causality has been the foundation of all prior ot systems , but it is inadequate to capture essential correctness requirements . past research had invented various patches to work around this problem , resulting in increasingly intricate and complicated ot algorithms . after having designed , implemented , and experimented with a series of ot algorithms , we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving ot problems , reducing its complexity , and supporting its continual evolution . in this paper , we report the main results of this effort the theory of operation context and the cot ( context based ot ) algorithm . the cot algorithm is capable of supporting both do and undo of any operations at anytime , without requiring transformation functions to preserve reversibility property , convergence property 2 , inverse properties 2 and 3 . the cot algorithm is not only simpler and more efficient than prior ot control algorithms , but also simplifies the design of transformation functions . we have implemented the cot algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications ."}
{"title": "Addressing Strategic Behavior in a Deployed Microeconomic Resource Allocator", "abstract": "While market-based systems have long been proposed as solutions for distributed resource allocation, few have been deployed for production use in real computer systems. Towards this end, we present our initial experience using Mirage, a microeconomic resource allocation system based on a repeated combinatorial auction. Mirage allocates time on a heavily-used 148-node wireless sensor network testbed. In particular, we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects. Based on these results, we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. Finally, we propose refinements to the system\"s current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction.", "id": "23", "src": "addressing strategic behavior in a deployed microeconomic resource allocator . while market based systems have long been proposed as solutions for distributed resource allocation , few have been deployed for production use in real computer systems . towards this end , we present our initial experience using mirage , a microeconomic resource allocation system based on a repeated combinatorial auction . mirage allocates time on a heavily used <digit> node wireless sensor network testbed . in particular , we focus on observed strategic user behavior over a four month period in which <digit> , <digit> node hours were allocated across <digit> research projects . based on these results , we present a set of key challenges for market based resource allocation systems based on repeated combinatorial auctions . finally , we propose refinements to the system s current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction ."}
{"title": "EDAS: Providing an Environment for Decentralized", "abstract": "As the idea of virtualisation of compute power, storage and bandwidth becomes more and more important, grid computing evolves and is applied to a rising number of applications. The environment for decentralized adaptive services (EDAS) provides a grid-like infrastructure for user-accessed, longterm services (e.g. webserver, source-code repository etc.). It aims at supporting the autonomous execution and evolution of services in terms of scalability and resource-aware distribution. EDAS offers flexible service models based on distributed mobile objects ranging from a traditional clientserver scenario to a fully peer-to-peer based approach. Automatic, dynamic resource management allows optimized use of available resources while minimizing the administrative complexity.", "id": "24", "src": "edas providing an environment for decentralized . as the idea of virtualisation of compute power , storage and bandwidth becomes more and more important , grid computing evolves and is applied to a rising number of applications . the environment for decentralized adaptive services ( edas ) provides a grid like infrastructure for user accessed , longterm services ( e . g . webserver , source code repository etc . ) . it aims at supporting the autonomous execution and evolution of services in terms of scalability and resource aware distribution . edas offers flexible service models based on distributed mobile objects ranging from a traditional clientserver scenario to a fully peer to peer based approach . automatic , dynamic resource management allows optimized use of available resources while minimizing the administrative complexity ."}
{"title": "Regularized Clustering for Documents", "abstract": "In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization (CLGR). We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.", "id": "25", "src": "regularized clustering for documents . in recent years , document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization , automatic topic extraction , and fast information retrieval or filtering . in this paper , we propose a novel method for clustering documents using regularization . unlike traditional globally regularized clustering methods , our method first construct a local regularized linear label predictor for each document vector , and then combine all those local regularizers with a global smoothness regularizer . so we call our algorithm clustering with local and global regularization ( clgr ) . we will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix , which can be efficiently solved by iterative methods . finally our experimental evaluations on several datasets are presented to show the superiorities of clgr over traditional document clustering methods ."}
{"title": "Laplacian Optimal Design for Image Retrieval", "abstract": "Relevance feedback is a powerful technique to enhance ContentBased Image Retrieval (CBIR) performance. It solicits the user\"s relevance judgments on the retrieved images returned by the CBIR systems. The user\"s labeling is then used to learn a classifier to distinguish between relevant and irrelevant images. However, the top returned images may not be the most informative ones. The challenge is thus to determine which unlabeled images would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. In this paper, we propose a novel active learning algorithm, called Laplacian Optimal Design (LOD), for relevance feedback image retrieval. Our algorithm is based on a regression model which minimizes the least square error on the measured (or, labeled) images and simultaneously preserves the local geometrical structure of the image space. Specifically, we assume that if two images are sufficiently close to each other, then their measurements (or, labels) are close as well. By constructing a nearest neighbor graph, the geometrical structure of the image space can be described by the graph Laplacian. We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images, which gives us the most amount of information. Experimental results on Corel database suggest that the proposed approach achieves higher precision in relevance feedback image retrieval.", "id": "26", "src": "laplacian optimal design for image retrieval . relevance feedback is a powerful technique to enhance contentbased image retrieval ( cbir ) performance . it solicits the user s relevance judgments on the retrieved images returned by the cbir systems . the user s labeling is then used to learn a classifier to distinguish between relevant and irrelevant images . however , the top returned images may not be the most informative ones . the challenge is thus to determine which unlabeled images would be the most informative ( i . e . , improve the classifier the most ) if they were labeled and used as training samples . in this paper , we propose a novel active learning algorithm , called laplacian optimal design ( lod ) , for relevance feedback image retrieval . our algorithm is based on a regression model which minimizes the least square error on the measured ( or , labeled ) images and simultaneously preserves the local geometrical structure of the image space . specifically , we assume that if two images are sufficiently close to each other , then their measurements ( or , labels ) are close as well . by constructing a nearest neighbor graph , the geometrical structure of the image space can be described by the graph laplacian . we discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images , which gives us the most amount of information . experimental results on corel database suggest that the proposed approach achieves higher precision in relevance feedback image retrieval ."}
{"title": "Fast Generation of Result Snippets in Web Search", "abstract": "The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache.", "id": "27", "src": "fast generation of result snippets in web search . the presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users . in this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets . we begin by proposing and analysing a document compression method that reduces snippet generation time by <digit> over a baseline using the zlib compression library . these experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets , and so caching documents in ram is essential for a fast snippet generation process . using simulation , we examine snippet generation performance for different size ram caches . finally we propose and analyse document reordering and compaction , revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality . this scheme effectively doubles the number of documents that can fit in a fixed size cache ."}
{"title": "The Influence of Caption Features on Clickthrough Patterns in Web Search", "abstract": "Web search engines present lists of captions, comprising title, snippet, and URL, to help users decide which search results to visit. Understanding the influence of features of these captions on Web search behavior may help validate algorithms and guidelines for their improved generation. In this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions. The findings of our study suggest that relatively simple caption features such as the presence of all terms query terms, the readability of the snippet, and the length of the URL shown in the caption, can significantly influence users\" Web search behavior.", "id": "28", "src": "the influence of caption features on clickthrough patterns in web search . web search engines present lists of captions , comprising title , snippet , and url , to help users decide which search results to visit . understanding the influence of features of these captions on web search behavior may help validate algorithms and guidelines for their improved generation . in this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions . the findings of our study suggest that relatively simple caption features such as the presence of all terms query terms , the readability of the snippet , and the length of the url shown in the caption , can significantly influence users web search behavior ."}
{"title": "Studying the Use of Popular Destinations to Enhance Web Search Interaction", "abstract": "We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity.", "id": "29", "src": "studying the use of popular destinations to enhance web search interaction . we present a novel web search interaction feature which , for a given query , provides links to websites frequently visited by other users with similar information needs . these popular destinations complement traditional search results , allowing direct navigation to authoritative resources for the query topic . destinations are identified using the history of search and browsing behavior of many users over an extended time period , whose collective behavior provides a basis for computing source authority . we describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries , as well as with traditional , unaided web search . results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks , with best performance obtained from mining past user behavior at query level granularity ."}
{"title": "The Impact of Caching on Search Engines", "abstract": "In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.", "id": "30", "src": "the impact of caching on search engines . in this paper we study the trade offs in designing efficient caching systems for web search engines . we explore the impact of different approaches , such as static vs . dynamic caching , and caching query results vs . caching posting lists . using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers . we propose a new algorithm for static caching of posting lists , which outperforms previous methods . we also study the problem of finding the optimal way to split the static cache between answers and posting lists . finally , we measure how the changes in the query log affect the effectiveness of static caching , given our observation that the distribution of the queries changes slowly over time . our results and observations are applicable to different levels of the data access hierarchy , for instance , for a memory disk layer or a broker remote server layer ."}
{"title": "Pruning Policies for Two-Tiered Inverted Index with Correctness Guarantee", "abstract": "The Web search engines maintain large-scale inverted indexes which are queried thousands of times per second by users eager for information. In order to cope with the vast amounts of query loads, search engines prune their index to keep documents that are likely to be returned as top results, and use this pruned index to compute the first batches of results. While this approach can improve performance by reducing the size of the index, if we compute the top results only from the pruned index we may notice a significant degradation in the result quality: if a document should be in the top results but was not included in the pruned index, it will be placed behind the results computed from the pruned index. Given the fierce competition in the online search market, this phenomenon is clearly undesirable. In this paper, we study how we can avoid any degradation of result quality due to the pruning-based performance optimization, while still realizing most of its benefit. Our contribution is a number of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guarantees that the top-matching pages are always placed at the top search results, even though we are computing the first batch from the pruned index most of the time. We also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of 130 million Web pages.", "id": "31", "src": "pruning policies for two tiered inverted index with correctness guarantee . the web search engines maintain large scale inverted indexes which are queried thousands of times per second by users eager for information . in order to cope with the vast amounts of query loads , search engines prune their index to keep documents that are likely to be returned as top results , and use this pruned index to compute the first batches of results . while this approach can improve performance by reducing the size of the index , if we compute the top results only from the pruned index we may notice a significant degradation in the result quality if a document should be in the top results but was not included in the pruned index , it will be placed behind the results computed from the pruned index . given the fierce competition in the online search market , this phenomenon is clearly undesirable . in this paper , we study how we can avoid any degradation of result quality due to the pruning based performance optimization , while still realizing most of its benefit . our contribution is a number of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guarantees that the top matching pages are always placed at the top search results , even though we are computing the first batch from the pruned index most of the time . we also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of <digit> million web pages ."}
{"title": "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents", "abstract": "Topic detection and tracking [26] and topic segmentation [15] play an important role in capturing the local and sequential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights. The basic idea is that the optimal segmentation maximizes MI(or WMI). Our approach can detect shared topics among documents. It can find the optimal boundaries in a document, and align segments among documents at the same time. It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.", "id": "32", "src": "topic segmentation with shared topic detection and alignment of multiple documents . topic detection and tracking <digit> and topic segmentation <digit> play an important role in capturing the local and sequential information of documents . previous work in this area usually focuses on single documents , although similar multiple documents are available in many domains . in this paper , we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information ( mi ) and weighted mutual information ( wmi ) that is a combination of mi and term weights . the basic idea is that the optimal segmentation maximizes mi ( or wmi ) . our approach can detect shared topics among documents . it can find the optimal boundaries in a document , and align segments among documents at the same time . it also can handle single document segmentation as a special case of the multi document segmentation and alignment . our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents . our experimental results show that our algorithm works well for the tasks of single document segmentation , shared topic detection , and multi document segmentation . utilizing information from multiple documents can tremendously improve the performance of topic segmentation , and using wmi is even better than using mi for the multi document segmentation ."}
{"title": "Analyzing Feature Trajectories for Event Detection", "abstract": "We consider the problem of analyzing word trajectories in both time and frequency domains, with the specific goal of identifying important and less-reported, periodic and aperiodic words. A set of words with identical trends can be grouped together to reconstruct an event in a completely unsupervised manner. The document frequency of each word across time is treated like a time series, where each element is the document frequency - inverse document frequency (DFIDF) score at one time point. In this paper, we 1) first applied spectral analysis to categorize features for different event characteristics: important and less-reported, periodic and aperiodic; 2) modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densities, and subsequently detected each feature\"s burst by the truncated Gaussian approach; 3) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events. All of the above methods can be applied to time series data in general. We extensively evaluated our methods on the 1-year Reuters News Corpus [3] and showed that they were able to uncover meaningful aperiodic and periodic events.", "id": "33", "src": "analyzing feature trajectories for event detection . we consider the problem of analyzing word trajectories in both time and frequency domains , with the specific goal of identifying important and less reported , periodic and aperiodic words . a set of words with identical trends can be grouped together to reconstruct an event in a completely unsupervised manner . the document frequency of each word across time is treated like a time series , where each element is the document frequency inverse document frequency ( dfidf ) score at one time point . in this paper , we 1 ) first applied spectral analysis to categorize features for different event characteristics important and less reported , periodic and aperiodic 2 ) modeled aperiodic features with gaussian density and periodic features with gaussian mixture densities , and subsequently detected each feature s burst by the truncated gaussian approach 3 ) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events . all of the above methods can be applied to time series data in general . we extensively evaluated our methods on the 1 year reuters news corpus 3 and showed that they were able to uncover meaningful aperiodic and periodic events ."}
{"title": "Personalized Query Expansion for the Web", "abstract": "The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each user\"s Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.", "id": "34", "src": "personalized query expansion for the web . the inherent ambiguity of short keyword queries demands for enhanced methods for web retrieval . in this paper we propose to improve such web queries by expanding them with terms collected from each user s personal information repository , thus implicitly personalizing the search output . we introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels , ranging from term and compound level analysis up to global co occurrence statistics , as well as to using external thesauri . our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well , especially on ambiguous queries , producing a very strong increase in the quality of the output rankings . subsequently , we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query . a separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach ."}
{"title": "New Event Detection Based on Indexing-tree and Named Entity", "abstract": "New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e. not reported previously). With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.", "id": "35", "src": "new event detection based on indexing tree and named entity . new event detection ( ned ) aims at detecting from one or multiple streams of news stories that which one is reported on a new event ( i . e . not reported previously ) . with the overwhelming volume of news available today , there is an increasing need for a ned system which is able to detect new events more efficiently and accurately . in this paper we propose a new ned model to speed up the ned task by using news indexing tree dynamically . moreover , based on the observation that terms of different types have different effects for ned task , two term reweighting approaches are proposed to improve ned accuracy . in the first approach , we propose to adjust term weights dynamically based on previous story clusters and in the second approach , we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories . experimental results on two linguistic data consortium ( ldc ) datasets tdt2 and tdt3 show that the proposed model can improve both efficiency and accuracy of ned task significantly , compared to the baseline system and other existing systems ."}
{"title": "Robust Classification of Rare Queries Using Web Knowledge", "abstract": "We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.", "id": "36", "src": "robust classification of rare queries using web knowledge . we propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy , while dealing in realtime with the query volume of a commercial web search engine . we use a blind feedback technique given a query , we determine its topic by classifying the web search results retrieved by the query . motivated by the needs of search advertising , we primarily focus on rare queries , which are the hardest from the point of view of machine learning , yet in aggregation account for a considerable fraction of search engine traffic . empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported . we believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience ."}
{"title": "Investigating the Querying and Browsing Behavior of Advanced Search Engine Users", "abstract": "One way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing, and use this knowledge to benefit everyone. In this paper we study the interaction logs of advanced search engine users (and those not so advanced) to better understand how these user groups search. The results show that there are marked differences in the queries, result clicks, post-query browsing, and search success of users we classify as advanced (based on their use of query operators), relative to those classified as non-advanced. Our findings have implications for how advanced users should be supported during their searches, and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies.", "id": "37", "src": "investigating the querying and browsing behavior of advanced search engine users . one way to help all users of commercial web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing , and use this knowledge to benefit everyone . in this paper we study the interaction logs of advanced search engine users ( and those not so advanced ) to better understand how these user groups search . the results show that there are marked differences in the queries , result clicks , post query browsing , and search success of users we classify as advanced ( based on their use of query operators ) , relative to those classified as non advanced . our findings have implications for how advanced users should be supported during their searches , and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies ."}
{"title": "Term Feedback for Information Retrieval with Language Models", "abstract": "In this paper we study term-based feedback for information retrieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top.", "id": "38", "src": "term feedback for information retrieval with language models . in this paper we study term based feedback for information retrieval in the language modeling approach . with term feedback a user directly judges the relevance of individual terms without interaction with feedback documents , taking full control of the query expansion process . we propose a cluster based method for selecting terms to present to the user for judgment , as well as effective algorithms for constructing refined query language models from user term feedback . our algorithms are shown to bring significant improvement in retrieval accuracy over a non feedback baseline , and achieve comparable performance to relevance feedback . they are helpful even when there are no relevant documents in the top ."}
{"title": "A Support Vector Method for Optimizing Average Precision", "abstract": "Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores.", "id": "39", "src": "a support vector method for optimizing average precision . machine learning is commonly used to improve ranked retrieval systems . due to computational difficulties , few learning techniques have been developed to directly optimize for mean average precision ( map ) , despite its widespread use in evaluating such systems . existing approaches optimizing map either do not find a globally optimal solution , or are computationally expensive . in contrast , we present a general svm learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of map . we evaluate our approach using the trec 9 and trec <digit> web track corpora ( wt10g ) , comparing against svms optimized for accuracy and rocarea . in most cases we show our method to produce statistically significant improvements in map scores ."}
{"title": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback", "abstract": "Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given query\"s top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.", "id": "40", "src": "estimation and use of uncertainty in pseudo relevance feedback . existing pseudo relevance feedback methods typically perform averaging over the top retrieved documents , but ignore an important statistical dimension the risk or variance associated with either the individual document models , or their combination . treating the baseline feedback method as a black box , and the output feedback model as a random variable , we estimate a posterior distribution for the feedback model by resampling a given query s top retrieved documents , using the posterior mean or mode as the enhanced feedback model . we then perform model combination over several enhanced models , each based on a slightly modified query sampled from the original query . we find that resampling documents helps increase individual feedback model precision by removing noise terms , while sampling from the query improves robustness ( worst case performance ) by emphasizing terms related to multiple query aspects . the result is a meta feedback algorithm that is both more robust and more precise than the original strong baseline method ."}
{"title": "Using Query Contexts in Information Retrieval", "abstract": "User query is an element that specifies an information need, but it is not the only one. Studies in literature have found many contextual factors that strongly influence the interpretation of a query. Recent studies have tried to consider the user\"s interests by creating a user profile. However, a single profile for a user may not be sufficient for a variety of queries of the user. In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query. The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations. In this paper, both types of context are integrated in an IR model based on language modeling. Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.", "id": "41", "src": "using query contexts in information retrieval . user query is an element that specifies an information need , but it is not the only one . studies in literature have found many contextual factors that strongly influence the interpretation of a query . recent studies have tried to consider the user s interests by creating a user profile . however , a single profile for a user may not be sufficient for a variety of queries of the user . in this study , we propose to use query specific contexts instead of user centric ones , including context around query and context within query . the former specifies the environment of a query such as the domain of interest , while the latter refers to context words within the query , which is particularly useful for the selection of relevant term relations . in this paper , both types of context are integrated in an ir model based on language modeling . our experiments on several trec collections show that each of the context factors brings significant improvements in retrieval effectiveness ."}
{"title": "Latent Concept Expansion Using Markov Random Fields", "abstract": "Query expansion, in the form of pseudo-relevance feedback or relevance feedback, is a common technique used to improve retrieval effectiveness. Most previous approaches have ignored important issues, such as the role of features and the importance of modeling term dependencies. In this paper, we propose a robust query expansion technique based on the Markov random field model for information retrieval. The technique, called latent concept expansion, provides a mechanism for modeling term dependencies during expansion. Furthermore, the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques. We evaluate our technique against relevance models, a state-of-the-art language modeling query expansion technique. Our model demonstrates consistent and significant improvements in retrieval effectiveness across several TREC data sets. We also describe how our technique can be used to generate meaningful multi-term concepts for tasks such as query suggestion/reformulation.", "id": "42", "src": "latent concept expansion using markov random fields . query expansion , in the form of pseudo relevance feedback or relevance feedback , is a common technique used to improve retrieval effectiveness . most previous approaches have ignored important issues , such as the role of features and the importance of modeling term dependencies . in this paper , we propose a robust query expansion technique based on the markov random field model for information retrieval . the technique , called latent concept expansion , provides a mechanism for modeling term dependencies during expansion . furthermore , the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques . we evaluate our technique against relevance models , a state of the art language modeling query expansion technique . our model demonstrates consistent and significant improvements in retrieval effectiveness across several trec data sets . we also describe how our technique can be used to generate meaningful multi term concepts for tasks such as query suggestion reformulation ."}
{"title": "A Study of Poisson Query Generation Model for Information Retrieval", "abstract": "Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson distribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods. We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling. We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections. The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing. The performance can be further improved with two-stage smoothing.", "id": "43", "src": "a study of poisson query generation model for information retrieval . many variants of language models have been proposed for information retrieval . most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model . in this paper , we propose and study a new family of query generation models based on poisson distribution . we show that while in their simplest forms , the new family of models and the existing multinomial models are equivalent , they behave differently for many smoothing methods . we show that the poisson model has several advantages over the multinomial model , including naturally accommodating per term smoothing and allowing for more accurate background modeling . we present several variants of the new model corresponding to different smoothing methods , and evaluate them on four representative trec test collections . the results show that while their basic models perform comparably , the poisson model can outperform multinomial model with per term smoothing . the performance can be further improved with two stage smoothing ."}
{"title": "Interesting Nuggets and Their Impact on Definitional Question Answering", "abstract": "Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a Human Interest Model from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.", "id": "44", "src": "interesting nuggets and their impact on definitional question answering . current approaches to identifying definitional sentences in the context of question answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets . this is insufficient as they do not address the novelty factor that a definitional nugget must also possess . this paper proposes to address the deficiency by building a human interest model from external knowledge . it is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic . we compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering ."}
{"title": "Towards Task-based Personal Information Management Evaluations", "abstract": "Personal Information Management (PIM) is a rapidly growing area of research concerned with how people store, manage and re-find information. A feature of PIM research is that many systems have been designed to assist users manage and re-find information, but very few have been evaluated. This has been noted by several scholars and explained by the difficulties involved in performing PIM evaluations. The difficulties include that people re-find information from within unique personal collections; researchers know little about the tasks that cause people to re-find information; and numerous privacy issues concerning personal information. In this paper we aim to facilitate PIM evaluations by addressing each of these difficulties. In the first part, we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation.", "id": "45", "src": "towards task based personal information management evaluations . personal information management ( pim ) is a rapidly growing area of research concerned with how people store , manage and re find information . a feature of pim research is that many systems have been designed to assist users manage and re find information , but very few have been evaluated . this has been noted by several scholars and explained by the difficulties involved in performing pim evaluations . the difficulties include that people re find information from within unique personal collections researchers know little about the tasks that cause people to re find information and numerous privacy issues concerning personal information . in this paper we aim to facilitate pim evaluations by addressing each of these difficulties . in the first part , we present a diary study of information re finding tasks . the study examines the kind of tasks that require users to re find information and produces a taxonomy of re finding tasks for email messages and web pages . in the second part , we propose a task based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation ."}
{"title": "Utility-based Information Distillation Over Temporally Sequenced Documents", "abstract": "This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs (\u2018tasks\" with multiple queries). Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.", "id": "46", "src": "utility based information distillation over temporally sequenced documents . this paper examines a new approach to information distillation over temporally ordered documents , and proposes a novel evaluation scheme for such a framework . it combines the strengths of and extends beyond conventional adaptive filtering , novelty detection and non redundant passage ranking with respect to long lasting information needs ( tasks with multiple queries ) . our approach supports fine grained user feedback via highlighting of arbitrary spans of text , and leverages such information for utility optimization in adaptive settings . for our experiments , we defined hypothetical tasks based on news events in the tdt4 corpus , with multiple queries per task . answer keys ( nuggets ) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses . we also propose an extension of the ndcg metric for assessing the utility of ranked passages as a combination of relevance and novelty . our results show encouraging utility enhancements using the new approach , compared to the baseline systems without incremental learning or the novelty detection components ."}
{"title": "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems", "abstract": "A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user\"s interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.", "id": "47", "src": "efficient bayesian hierarchical user modeling for recommendation systems . a content based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user s interest . a system serving millions of users can learn a better user profile for a new user , or a user with little feedback , by borrowing information from other users through the use of a bayesian hierarchical model . learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive . the commonly used em algorithm converges very slowly due to the sparseness of the data in ir applications . this paper proposes a new fast learning technique to learn a large number of individual user profiles . the efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from netflix and movielens ."}
{"title": "Robust Test Collections for Retrieval Evaluation", "abstract": "Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems.", "id": "48", "src": "robust test collections for retrieval evaluation . low cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments . while these judgments are very useful for a one time evaluation , it is not clear that they can be trusted when re used to evaluate new systems . in this work , we formally define what it means for judgments to be reusable the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments . we then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort . using this method practically guarantees reusability with as few as five judgments per topic taken from only two systems , we can reliably evaluate a larger set of ten systems . even the smallest sets of judgments can be useful for evaluation of new systems ."}
{"title": "Learn from Web Search Logs to Organize Search Results", "abstract": "Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user\"s perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.", "id": "49", "src": "learn from web search logs to organize search results . effective organization of search results is critical for improving the utility of any search engine . clustering search results is an effective way to organize search results , which allows a user to navigate into relevant documents quickly . however , two deficiencies of this approach make it not always work well ( 1 ) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user s perspective and ( 2 ) the cluster labels generated are not informative enough to allow a user to identify the right cluster . in this paper , we propose to address these two deficiencies by ( 1 ) learning interesting aspects of a topic from web search logs and organizing search results accordingly and ( 2 ) generating more meaningful cluster labels using past query words entered by users . we evaluate our proposed method on a commercial search engine log data . compared with the traditional methods of clustering search results , our method can give better result organization and more meaningful labels ."}
{"title": "Aborting Tasks in BDI Agents", "abstract": "Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans. In addition, such agents should be able to make rational decisions about an appropriate course of action, which may include aborting a task or plan, either as a result of the agent\"s own deliberations, or potentially at the request of another agent. In this paper we investigate the incorporation of aborts into a BDI-style architecture. We discuss some conditions under which aborting a task or plan is appropriate, and how to determine the consequences of such a decision. We augment each plan with an optional abort-method, analogous to the failure method found in some agent programming languages. We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN, which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system (such as JACK, Jadex, Jason, or SPARK). A key technical challenge we address is the presence of parallel execution threads and of sub-tasks, which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence.", "id": "50", "src": "aborting tasks in bdi agents . intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans . in addition , such agents should be able to make rational decisions about an appropriate course of action , which may include aborting a task or plan , either as a result of the agent s own deliberations , or potentially at the request of another agent . in this paper we investigate the incorporation of aborts into a bdi style architecture . we discuss some conditions under which aborting a task or plan is appropriate , and how to determine the consequences of such a decision . we augment each plan with an optional abort method , analogous to the failure method found in some agent programming languages . we provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language can , which enables us to specify a bdi based execution model without limiting our attention to a particular agent system ( such as jack , jadex , jason , or spark ) . a key technical challenge we address is the presence of parallel execution threads and of sub tasks , which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence ."}
{"title": "SMILE: Sound Multi-agent Incremental LEarning ;-)\u2217", "abstract": "This article deals with the problem of collaborative learning in a multi-agent system. Here each agent can update incrementally its beliefs B (the concept representation) so that it is in a way kept consistent with the whole set of information K (the examples) that he has received from the environment or other agents. We extend this notion of consistency (or soundness) to the whole MAS and discuss how to obtain that, at any moment, a same consistent concept representation is present in each agent. The corresponding protocol is applied to supervised concept learning. The resulting method SMILE (standing for Sound Multiagent Incremental LEarning) is described and experimented here. Surprisingly some difficult boolean formulas are better learned, given the same learning set, by a Multi agent system than by a single agent.", "id": "51", "src": "smile sound multi agent incremental learning ) . this article deals with the problem of collaborative learning in a multi agent system . here each agent can update incrementally its beliefs b ( the concept representation ) so that it is in a way kept consistent with the whole set of information k ( the examples ) that he has received from the environment or other agents . we extend this notion of consistency ( or soundness ) to the whole mas and discuss how to obtain that , at any moment , a same consistent concept representation is present in each agent . the corresponding protocol is applied to supervised concept learning . the resulting method smile ( standing for sound multiagent incremental learning ) is described and experimented here . surprisingly some difficult boolean formulas are better learned , given the same learning set , by a multi agent system than by a single agent ."}
{"title": "Real-Time Agent Characterization and Prediction", "abstract": "Reasoning about agents that we observe in the world is challenging. Our available information is often limited to observations of the agent\"s external behavior in the past and present. To understand these actions, we need to deduce the agent\"s internal state, which includes not only rational elements (such as intentions and plans), but also emotive ones (such as fear). In addition, we often want to predict the agent\"s future actions, which are constrained not only by these inward characteristics, but also by the dynamics of the agent\"s interaction with its environment. BEE (Behavior Evolution and Extrapolation) uses a faster-than-real-time agentbased model of the environment to characterize agents\" internal state by evolution against observed behavior, and then predict their future behavior, taking into account the dynamics of their interaction with the environment.", "id": "52", "src": "real time agent characterization and prediction . reasoning about agents that we observe in the world is challenging . our available information is often limited to observations of the agent s external behavior in the past and present . to understand these actions , we need to deduce the agent s internal state , which includes not only rational elements ( such as intentions and plans ) , but also emotive ones ( such as fear ) . in addition , we often want to predict the agent s future actions , which are constrained not only by these inward characteristics , but also by the dynamics of the agent s interaction with its environment . bee ( behavior evolution and extrapolation ) uses a faster than real time agentbased model of the environment to characterize agents internal state by evolution against observed behavior , and then predict their future behavior , taking into account the dynamics of their interaction with the environment ."}
{"title": "Sharing Experiences to Learn User Characteristics in Dynamic Environments with Sparse Data", "abstract": "This paper investigates the problem of estimating the value of probabilistic parameters needed for decision making in environments in which an agent, operating within a multi-agent system, has no a priori information about the structure of the distribution of parameter values. The agent must be able to produce estimations even when it may have made only a small number of direct observations, and thus it must be able to operate with sparse data. The paper describes a mechanism that enables the agent to significantly improve its estimation by augmenting its direct observations with those obtained by other agents with which it is coordinating. To avoid undesirable bias in relatively heterogeneous environments while effectively using relevant data to improve its estimations, the mechanism weighs the contributions of other agents\" observations based on a real-time estimation of the level of similarity between each of these agents and itself. The coordination autonomy module of a coordination-manager system provided an empirical setting for evaluation. Simulation-based evaluations demonstrated that the proposed mechanism outperforms estimations based exclusively on an agent\"s own observations as well as estimations based on an unweighted aggregate of all other agents\" observations.", "id": "53", "src": "sharing experiences to learn user characteristics in dynamic environments with sparse data . this paper investigates the problem of estimating the value of probabilistic parameters needed for decision making in environments in which an agent , operating within a multi agent system , has no a priori information about the structure of the distribution of parameter values . the agent must be able to produce estimations even when it may have made only a small number of direct observations , and thus it must be able to operate with sparse data . the paper describes a mechanism that enables the agent to significantly improve its estimation by augmenting its direct observations with those obtained by other agents with which it is coordinating . to avoid undesirable bias in relatively heterogeneous environments while effectively using relevant data to improve its estimations , the mechanism weighs the contributions of other agents observations based on a real time estimation of the level of similarity between each of these agents and itself . the coordination autonomy module of a coordination manager system provided an empirical setting for evaluation . simulation based evaluations demonstrated that the proposed mechanism outperforms estimations based exclusively on an agent s own observations as well as estimations based on an unweighted aggregate of all other agents observations ."}
{"title": "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems", "abstract": "The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches. In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions. However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents. In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions. Specifically, agents maintain estimates on the downstream agents\" abilities to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on this information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies. Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.", "id": "54", "src": "a reinforcement learning based distributed search algorithm for hierarchical peer to peer information retrieval systems . the dominant existing routing strategies employed in peerto peer ( p2p ) based information retrieval ( ir ) systems are similarity based approaches . in these approaches , agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions . however , such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents . in this paper , an online reinforcement learning based approach is developed to take advantage of the dynamic run time characteristics of p2p ir systems as represented by information about past search sessions . specifically , agents maintain estimates on the downstream agents abilities to provide relevant documents for incoming queries . these estimates are updated gradually by learning from the feedback information returned from previous search sessions . based on this information , the agents derive corresponding routing policies . thereafter , these agents route the queries based on the learned policies and update the estimates based on the new routing policies . experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed ir studies ."}
{"title": "Information Searching and Sharing in Large-Scale Dynamic Networks", "abstract": "Finding the right agents in a large and dynamic network to provide the needed resources in a timely fashion, is a long standing problem. This paper presents a method for information searching and sharing that combines routing indices with tokenbased methods. The proposed method enables agents to search effectively by acquiring their neighbors\" interests, advertising their information provision abilities and maintaining indices for routing queries, in an integrated way. Specifically, the paper demonstrates through performance experiments how static and dynamic networks of agents can be \u2018tuned\" to answer queries effectively as they gather evidence for the interests and information provision abilities of others, without altering the topology or imposing an overlay structure to the network of acquaintances.", "id": "55", "src": "information searching and sharing in large scale dynamic networks . finding the right agents in a large and dynamic network to provide the needed resources in a timely fashion , is a long standing problem . this paper presents a method for information searching and sharing that combines routing indices with tokenbased methods . the proposed method enables agents to search effectively by acquiring their neighbors interests , advertising their information provision abilities and maintaining indices for routing queries , in an integrated way . specifically , the paper demonstrates through performance experiments how static and dynamic networks of agents can be tuned to answer queries effectively as they gather evidence for the interests and information provision abilities of others , without altering the topology or imposing an overlay structure to the network of acquaintances ."}
{"title": "An Advanced Bidding Agent for Advertisement Selection on Public Displays", "abstract": "In this paper we present an advanced bidding agent that participates in first-price sealed bid auctions to allocate advertising space on BluScreen - an experimental public advertisement system that detects users through the presence of their Bluetooth enabled devices. Our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts, and the auctions that it participates within. It then uses these models to maximise the exposure that its adverts receive. We evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy, random allocation, and a centralised optimal allocation with perfect foresight. Our bidding agent significantly outperforms both the simple bidding strategy and the random allocation, and in a mixed population of agents it is able to expose its adverts to 25% more users than the simple bidding strategy. Moreover, its performance is within 7.5% of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate.", "id": "56", "src": "an advanced bidding agent for advertisement selection on public displays . in this paper we present an advanced bidding agent that participates in first price sealed bid auctions to allocate advertising space on bluscreen an experimental public advertisement system that detects users through the presence of their bluetooth enabled devices . our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts , and the auctions that it participates within . it then uses these models to maximise the exposure that its adverts receive . we evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy , random allocation , and a centralised optimal allocation with perfect foresight . our bidding agent significantly outperforms both the simple bidding strategy and the random allocation , and in a mixed population of agents it is able to expose its adverts to <digit> more users than the simple bidding strategy . moreover , its performance is within 7 . 5 of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate ."}
{"title": "Collaboration Among a Satellite Swarm", "abstract": "The paper deals with on-board planning for a satellite swarm via communication and negotiation. We aim at defining individual behaviours that result in a global behaviour that meets the mission requirements. We will present the formalization of the problem, a communication protocol, a solving method based on reactive decision rules, and first results. Categories and Subject Descriptors", "id": "57", "src": "collaboration among a satellite swarm . the paper deals with on board planning for a satellite swarm via communication and negotiation . we aim at defining individual behaviours that result in a global behaviour that meets the mission requirements . we will present the formalization of the problem , a communication protocol , a solving method based on reactive decision rules , and first results . categories and subject descriptors"}
{"title": "Bidding Optimally in Concurrent Second-Price Auctions of Perfectly Substitutable Goods", "abstract": "We derive optimal bidding strategies for a global bidding agent that participates in multiple, simultaneous second-price auctions with perfect substitutes. We first consider a model where all other bidders are local and participate in a single auction. For this case, we prove that, assuming free disposal, the global bidder should always place non-zero bids in all available auctions, irrespective of the local bidders\" valuation distribution. Furthermore, for non-decreasing valuation distributions, we prove that the problem of finding the optimal bids reduces to two dimensions. These results hold both in the case where the number of local bidders is known and when this number is determined by a Poisson distribution. This analysis extends to online markets where, typically, auctions occur both concurrently and sequentially. In addition, by combining analytical and simulation results, we demonstrate that similar results hold in the case of several global bidders, provided that the market consists of both global and local bidders. Finally, we address the efficiency of the overall market, and show that information about the number of local bidders is an important determinant for the way in which a global bidder affects efficiency.", "id": "58", "src": "bidding optimally in concurrent second price auctions of perfectly substitutable goods . we derive optimal bidding strategies for a global bidding agent that participates in multiple , simultaneous second price auctions with perfect substitutes . we first consider a model where all other bidders are local and participate in a single auction . for this case , we prove that , assuming free disposal , the global bidder should always place non zero bids in all available auctions , irrespective of the local bidders valuation distribution . furthermore , for non decreasing valuation distributions , we prove that the problem of finding the optimal bids reduces to two dimensions . these results hold both in the case where the number of local bidders is known and when this number is determined by a poisson distribution . this analysis extends to online markets where , typically , auctions occur both concurrently and sequentially . in addition , by combining analytical and simulation results , we demonstrate that similar results hold in the case of several global bidders , provided that the market consists of both global and local bidders . finally , we address the efficiency of the overall market , and show that information about the number of local bidders is an important determinant for the way in which a global bidder affects efficiency ."}
{"title": "Computing the Banzhaf Power Index in Network Flow Games", "abstract": "Preference aggregation is used in a variety of multiagent applications, and as a result, voting theory has become an important topic in multiagent system research. However, power indices (which reflect how much real power a voter has in a weighted voting system) have received relatively little attention, although they have long been studied in political science and economics. The Banzhaf power index is one of the most popular; it is also well-defined for any simple coalitional game. In this paper, we examine the computational complexity of calculating the Banzhaf power index within a particular multiagent domain, a network flow game. Agents control the edges of a graph; a coalition wins if it can send a flow of a given size from a source vertex to a target vertex. The relative power of each edge/agent reflects its significance in enabling such a flow, and in real-world networks could be used, for example, to allocate resources for maintaining parts of the network. We show that calculating the Banzhaf power index of each agent in this network flow domain is #P-complete. We also show that for some restricted network flow domains there exists a polynomial algorithm to calculate agents\" Banzhaf power indices.", "id": "59", "src": "computing the banzhaf power index in network flow games . preference aggregation is used in a variety of multiagent applications , and as a result , voting theory has become an important topic in multiagent system research . however , power indices ( which reflect how much real power a voter has in a weighted voting system ) have received relatively little attention , although they have long been studied in political science and economics . the banzhaf power index is one of the most popular it is also well defined for any simple coalitional game . in this paper , we examine the computational complexity of calculating the banzhaf power index within a particular multiagent domain , a network flow game . agents control the edges of a graph a coalition wins if it can send a flow of a given size from a source vertex to a target vertex . the relative power of each edge agent reflects its significance in enabling such a flow , and in real world networks could be used , for example , to allocate resources for maintaining parts of the network . we show that calculating the banzhaf power index of each agent in this network flow domain is #p complete . we also show that for some restricted network flow domains there exists a polynomial algorithm to calculate agents banzhaf power indices ."}
{"title": "Interactions between Market Barriers and Communication Networks in Marketing Systems", "abstract": "We investigate a framework where agents search for satisfying products by using referrals from other agents. Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents. Local behaviour incorporates a satisficing model of choice, a set of rules governing the interactions between agents, including learning about the trustworthiness of other agents over time, and external constraints on behaviour that may be imposed by market barriers or switching costs. Local behaviour takes place on a network substrate across which agents exchange positive and negative information about products. We use various degree distributions dictating the extent of connectivity, and incorporate both small-world effects and the notion of preferential attachment in our network models. We compare the effectiveness of referral systems over various network structures for easy and hard choice tasks, and evaluate how this effectiveness changes with the imposition of market barriers.", "id": "60", "src": "interactions between market barriers and communication networks in marketing systems . we investigate a framework where agents search for satisfying products by using referrals from other agents . our model of a mechanism for transmitting word of mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents . local behaviour incorporates a satisficing model of choice , a set of rules governing the interactions between agents , including learning about the trustworthiness of other agents over time , and external constraints on behaviour that may be imposed by market barriers or switching costs . local behaviour takes place on a network substrate across which agents exchange positive and negative information about products . we use various degree distributions dictating the extent of connectivity , and incorporate both small world effects and the notion of preferential attachment in our network models . we compare the effectiveness of referral systems over various network structures for easy and hard choice tasks , and evaluate how this effectiveness changes with the imposition of market barriers ."}
{"title": "Realistic Cognitive Load Modeling for Enhancing Shared Mental Models in Human-Agent Collaboration", "abstract": "Human team members often develop shared expectations to predict each other\"s needs and coordinate their behaviors. In this paper the concept Shared Belief Map is proposed as a basis for developing realistic shared expectations among a team of Human-Agent-Pairs (HAPs). The establishment of shared belief maps relies on inter-agent information sharing, the effectiveness of which highly depends on agents\" processing loads and the instantaneous cognitive loads of their human partners. We investigate HMM-based cognitive load models to facilitate team members to share the right information with the right party at the right time. The shared belief map concept and the cognitive/processing load models have been implemented in a cognitive agent architectureSMMall. A series of experiments were conducted to evaluate the concept, the models, and their impacts on the evolving of shared mental models of HAP teams.", "id": "61", "src": "realistic cognitive load modeling for enhancing shared mental models in human agent collaboration . human team members often develop shared expectations to predict each other s needs and coordinate their behaviors . in this paper the concept shared belief map is proposed as a basis for developing realistic shared expectations among a team of human agent pairs ( haps ) . the establishment of shared belief maps relies on inter agent information sharing , the effectiveness of which highly depends on agents processing loads and the instantaneous cognitive loads of their human partners . we investigate hmm based cognitive load models to facilitate team members to share the right information with the right party at the right time . the shared belief map concept and the cognitive processing load models have been implemented in a cognitive agent architecturesmmall . a series of experiments were conducted to evaluate the concept , the models , and their impacts on the evolving of shared mental models of hap teams ."}
{"title": "Sequential Decision Making in Parallel Two-Sided Economic Search", "abstract": "This paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships. In each search stage, each of the agents is randomly matched with several other agents in parallel, and makes a decision whether to accept a potential partnership with one of them. The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized (instantaneous) decision protocol and can sequentially accept and reject partnerships within the same search stage. We analyze the dynamics which drive the agents\" strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model. By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments.", "id": "62", "src": "sequential decision making in parallel two sided economic search . this paper presents a two sided economic search model in which agents are searching for beneficial pairwise partnerships . in each search stage , each of the agents is randomly matched with several other agents in parallel , and makes a decision whether to accept a potential partnership with one of them . the distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized ( instantaneous ) decision protocol and can sequentially accept and reject partnerships within the same search stage . we analyze the dynamics which drive the agents strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two sided parallel economic search model . by identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments ."}
{"title": "Distributed Management of Flexible Times Schedules", "abstract": "We consider the problem of managing schedules in an uncertain, distributed environment. We assume a team of collaborative agents, each responsible for executing a portion of a globally pre-established schedule, but none possessing a global view of either the problem or solution. The goal is to maximize the joint quality obtained from the activities executed by all agents, given that, during execution, unexpected events will force changes to some prescribed activities and reduce the utility of executing others. We describe an agent architecture for solving this problem that couples two basic mechanisms: (1) a flexible times representation of the agent\"s schedule (using a Simple Temporal Network) and (2) an incremental rescheduling procedure. The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions, and the latter acts to revise the agent\"s schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set. Basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter-dependent activities. Then, as time permits, the core local problem solving infra-structure is used to drive an inter-agent option generation and query process, aimed at identifying opportunities for solution improvement through joint change. Using a simulator to model the environment, we compare the performance of our multi-agent system with that of an expected optimal (but non-scalable) centralized MDP solver.", "id": "63", "src": "distributed management of flexible times schedules . we consider the problem of managing schedules in an uncertain , distributed environment . we assume a team of collaborative agents , each responsible for executing a portion of a globally pre established schedule , but none possessing a global view of either the problem or solution . the goal is to maximize the joint quality obtained from the activities executed by all agents , given that , during execution , unexpected events will force changes to some prescribed activities and reduce the utility of executing others . we describe an agent architecture for solving this problem that couples two basic mechanisms ( 1 ) a flexible times representation of the agent s schedule ( using a simple temporal network ) and ( 2 ) an incremental rescheduling procedure . the former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions , and the latter acts to revise the agent s schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set . basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter dependent activities . then , as time permits , the core local problem solving infra structure is used to drive an inter agent option generation and query process , aimed at identifying opportunities for solution improvement through joint change . using a simulator to model the environment , we compare the performance of our multi agent system with that of an expected optimal ( but non scalable ) centralized mdp solver ."}
{"title": "Distributed Task Allocation in Social Networks", "abstract": "This paper proposes a new variant of the task allocation problem, where the agents are connected in a social network and tasks arrive at the agents distributed over the network. We show that the complexity of this problem remains NPhard. Moreover, it is not approximable within some factor. We develop an algorithm based on the contract-net protocol. Our algorithm is completely distributed, and it assumes that agents have only local knowledge about tasks and resources. We conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time. Three different types of networks, namely small-world, random and scale-free networks, are used to represent various social relationships among agents in realistic applications. The results demonstrate that our algorithm works well and that it scales well to large-scale applications.", "id": "64", "src": "distributed task allocation in social networks . this paper proposes a new variant of the task allocation problem , where the agents are connected in a social network and tasks arrive at the agents distributed over the network . we show that the complexity of this problem remains nphard . moreover , it is not approximable within some factor . we develop an algorithm based on the contract net protocol . our algorithm is completely distributed , and it assumes that agents have only local knowledge about tasks and resources . we conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time . three different types of networks , namely small world , random and scale free networks , are used to represent various social relationships among agents in realistic applications . the results demonstrate that our algorithm works well and that it scales well to large scale applications ."}
{"title": "Reasoning about Judgment and Preference Aggregation", "abstract": "Agents that must reach agreements with other agents need to reason about how their preferences, judgments, and beliefs might be aggregated with those of others by the social choice mechanisms that govern their interactions. The recently emerging field of judgment aggregation studies aggregation from a logical perspective, and considers how multiple sets of logical formulae can be aggregated to a single consistent set. As a special case, judgment aggregation can be seen to subsume classical preference aggregation. We present a modal logic that is intended to support reasoning about judgment aggregation scenarios (and hence, as a special case, about preference aggregation): the logical language is interpreted directly in judgment aggregation rules. We present a sound and complete axiomatisation of such rules. We show that the logic can express aggregation rules such as majority voting; rule properties such as independence; and results such as the discursive paradox, Arrow\"s theorem and Condorcet\"s paradox - which are derivable as formal theorems of the logic. The logic is parameterised in such a way that it can be used as a general framework for comparing the logical properties of different types of aggregation - including classical preference aggregation.", "id": "65", "src": "reasoning about judgment and preference aggregation . agents that must reach agreements with other agents need to reason about how their preferences , judgments , and beliefs might be aggregated with those of others by the social choice mechanisms that govern their interactions . the recently emerging field of judgment aggregation studies aggregation from a logical perspective , and considers how multiple sets of logical formulae can be aggregated to a single consistent set . as a special case , judgment aggregation can be seen to subsume classical preference aggregation . we present a modal logic that is intended to support reasoning about judgment aggregation scenarios ( and hence , as a special case , about preference aggregation ) the logical language is interpreted directly in judgment aggregation rules . we present a sound and complete axiomatisation of such rules . we show that the logic can express aggregation rules such as majority voting rule properties such as independence and results such as the discursive paradox , arrow s theorem and condorcet s paradox which are derivable as formal theorems of the logic . the logic is parameterised in such a way that it can be used as a general framework for comparing the logical properties of different types of aggregation including classical preference aggregation ."}
{"title": "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions", "abstract": "Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model). We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents. We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms\" appropriateness.", "id": "66", "src": "an adversarial environment model for bounded rational agents in zero sum interactions . multiagent environments are often not cooperative nor collaborative in many cases , agents have conflicting interests , leading to adversarial interactions . this paper presents a formal adversarial environment model for bounded rational agents operating in a zero sum environment . in such environments , attempts to use classical utility based search methods can raise a variety of difficulties ( e . g . , implicitly modeling the opponent as an omniscient utility maximizer , rather than leveraging a more nuanced , explicit opponent model ) . we define an adversarial environment by describing the mental states of an agent in such an environment . we then present behavioral axioms that are intended to serve as design principles for building such adversarial agents . we explore the application of our approach by analyzing log files of completed connect four games , and present an empirical analysis of the axioms appropriateness ."}
{"title": "A Formal Road from Institutional Norms to Organizational Structures", "abstract": "Up to now, the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics. In order to develop systems actually implementing institutions and organizations, formal methods should take the place of heuristic ones. The paper presents a formal semantics for the notion of institution and its components (abstract and concrete norms, empowerment of agents, roles) and defines a formal relation between institutions and organizational structures. As a result, it is shown how institutional norms can be refined to constructsorganizational structures-which are closer to an implemented system. It is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification.", "id": "67", "src": "a formal road from institutional norms to organizational structures . up to now , the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics . in order to develop systems actually implementing institutions and organizations , formal methods should take the place of heuristic ones . the paper presents a formal semantics for the notion of institution and its components ( abstract and concrete norms , empowerment of agents , roles ) and defines a formal relation between institutions and organizational structures . as a result , it is shown how institutional norms can be refined to constructsorganizational structures which are closer to an implemented system . it is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification ."}
{"title": "Resolving Conflict and Inconsistency in Norm-Regulated Virtual Organizations", "abstract": "Norm-governed virtual organizations define, govern and facilitate coordinated resource sharing and problem solving in societies of agents. With an explicit account of norms, openness in virtual organizations can be achieved: new components, designed by various parties, can be seamlessly accommodated. We focus on virtual organizations realised as multi-agent systems, in which human and software agents interact to achieve individual and global goals. However, any realistic account of norms should address their dynamic nature: norms will change as agents interact with each other and their environment. Due to the changing nature of norms or due to norms stemming from different virtual organizations, there will be situations when an action is simultaneously permitted and prohibited, that is, a conflict arises. Likewise, there will be situations when an action is both obliged and prohibited, that is, an inconsistency arises. We introduce an approach, based on first-order unification, to detect and resolve such conflicts and inconsistencies. In our proposed solution, we annotate a norm with the set of values their variables should not have in order to avoid a conflict or an inconsistency with another norm. Our approach neatly accommodates the domain-dependent interrelations among actions and the indirect conflicts/inconsistencies these may cause. More generally, we can capture a useful notion of inter-agent (and inter-role) delegation of actions and norms associated to them, and use it to address conflicts/inconsistencies caused by action delegation. We illustrate our approach with an e-Science example in which agents support Grid services.", "id": "68", "src": "resolving conflict and inconsistency in norm regulated virtual organizations . norm governed virtual organizations define , govern and facilitate coordinated resource sharing and problem solving in societies of agents . with an explicit account of norms , openness in virtual organizations can be achieved new components , designed by various parties , can be seamlessly accommodated . we focus on virtual organizations realised as multi agent systems , in which human and software agents interact to achieve individual and global goals . however , any realistic account of norms should address their dynamic nature norms will change as agents interact with each other and their environment . due to the changing nature of norms or due to norms stemming from different virtual organizations , there will be situations when an action is simultaneously permitted and prohibited , that is , a conflict arises . likewise , there will be situations when an action is both obliged and prohibited , that is , an inconsistency arises . we introduce an approach , based on first order unification , to detect and resolve such conflicts and inconsistencies . in our proposed solution , we annotate a norm with the set of values their variables should not have in order to avoid a conflict or an inconsistency with another norm . our approach neatly accommodates the domain dependent interrelations among actions and the indirect conflicts inconsistencies these may cause . more generally , we can capture a useful notion of inter agent ( and inter role ) delegation of actions and norms associated to them , and use it to address conflicts inconsistencies caused by action delegation . we illustrate our approach with an e science example in which agents support grid services ."}
{"title": "Distributed Norm Management in Regulated Multi-Agent Systems", "abstract": "Norms are widely recognised as a means of coordinating multi-agent systems. The distributed management of norms is a challenging issue and we observe a lack of truly distributed computational realisations of normative models. In order to regulate the behaviour of autonomous agents that take part in multiple, related activities, we propose a normative model, the Normative Structure (NS), an artifact that is based on the propagation of normative positions (obligations, prohibitions, permissions), as consequences of agents\" actions. Within a NS, conflicts may arise due to the dynamic nature of the MAS and the concurrency of agents\" actions. However, ensuring conflict-freedom of a NS at design time is computationally intractable. We show this by formalising the notion of conflict, providing a mapping of NSs into Coloured Petri Nets and borrowing well-known theoretical results from that field. Since online conflict resolution is required, we present a tractable algorithm to be employed distributedly. We then demonstrate that this algorithm is paramount for the distributed enactment of a NS.", "id": "69", "src": "distributed norm management in regulated multi agent systems . norms are widely recognised as a means of coordinating multi agent systems . the distributed management of norms is a challenging issue and we observe a lack of truly distributed computational realisations of normative models . in order to regulate the behaviour of autonomous agents that take part in multiple , related activities , we propose a normative model , the normative structure ( ns ) , an artifact that is based on the propagation of normative positions ( obligations , prohibitions , permissions ) , as consequences of agents actions . within a ns , conflicts may arise due to the dynamic nature of the mas and the concurrency of agents actions . however , ensuring conflict freedom of a ns at design time is computationally intractable . we show this by formalising the notion of conflict , providing a mapping of nss into coloured petri nets and borrowing well known theoretical results from that field . since online conflict resolution is required , we present a tractable algorithm to be employed distributedly . we then demonstrate that this algorithm is paramount for the distributed enactment of a ns ."}
{"title": "Meta-Level Coordination for Solving Negotiation Chains in Semi-Cooperative Multi-Agent Systems", "abstract": "A negotiation chain is formed when multiple related negotiations are spread over multiple agents. In order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility, we present an extension to a singleagent concurrent negotiation framework. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility; however, the performance of each individual agent is tightly related to other agent\"s cooperation and the system\"s overall performance. We introduce a pre-negotiation phase that allows agents to transfer meta-level information. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context. The agent can also use this information to allocate appropriate time for each negotiation, hence to find a good ordering of all related negotiations. The experimental data shows that these mechanisms improve the agents\" and the system\"s overall performance significantly.", "id": "70", "src": "meta level coordination for solving negotiation chains in semi cooperative multi agent systems . a negotiation chain is formed when multiple related negotiations are spread over multiple agents . in order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility , we present an extension to a singleagent concurrent negotiation framework . this work is aimed at semi cooperative multi agent systems , where each agent has its own goals and works to maximize its local utility however , the performance of each individual agent is tightly related to other agent s cooperation and the system s overall performance . we introduce a pre negotiation phase that allows agents to transfer meta level information . using this information , the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability . this more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context . the agent can also use this information to allocate appropriate time for each negotiation , hence to find a good ordering of all related negotiations . the experimental data shows that these mechanisms improve the agents and the system s overall performance significantly ."}
{"title": "Towards Self-organising Agent-based Resource Allocation in a Multi-Server Environment", "abstract": "Distributed applications require distributed techniques for efficient resource allocation. These techniques need to take into account the heterogeneity and potential unreliability of resources and resource consumers in a distributed environments. In this paper we propose a distributed algorithm that solves the resource allocation problem in distributed multiagent systems. Our solution is based on the self-organisation of agents, which does not require any facilitator or management layer. The resource allocation in the system is a purely emergent effect. We present results of the proposed resource allocation mechanism in the simulated static and dynamic multi-server environment.", "id": "71", "src": "towards self organising agent based resource allocation in a multi server environment . distributed applications require distributed techniques for efficient resource allocation . these techniques need to take into account the heterogeneity and potential unreliability of resources and resource consumers in a distributed environments . in this paper we propose a distributed algorithm that solves the resource allocation problem in distributed multiagent systems . our solution is based on the self organisation of agents , which does not require any facilitator or management layer . the resource allocation in the system is a purely emergent effect . we present results of the proposed resource allocation mechanism in the simulated static and dynamic multi server environment ."}
{"title": "Dynamic Semantics for Agent Communication Languages", "abstract": "This paper proposes dynamic semantics for agent communication languages (ACLs) as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems. Based on the idea of providing alternative semantic variants for speech acts and transition rules between them that are contingent on previous agent behaviour, our framework provides an improved notion of grounding semantics in ongoing interaction, a simple mechanism for distinguishing between compliant and expected behaviour, and a way to specify sanction and reward mechanisms as part of the ACL itself. We extend a common framework for commitment-based ACL semantics to obtain these properties, discuss desiderata for the design of concrete dynamic semantics together with examples, and analyse their properties.", "id": "72", "src": "dynamic semantics for agent communication languages . this paper proposes dynamic semantics for agent communication languages ( acls ) as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems . based on the idea of providing alternative semantic variants for speech acts and transition rules between them that are contingent on previous agent behaviour , our framework provides an improved notion of grounding semantics in ongoing interaction , a simple mechanism for distinguishing between compliant and expected behaviour , and a way to specify sanction and reward mechanisms as part of the acl itself . we extend a common framework for commitment based acl semantics to obtain these properties , discuss desiderata for the design of concrete dynamic semantics together with examples , and analyse their properties ."}
{"title": "Commitment and Extortion", "abstract": "Making commitments, e.g., through promises and threats, enables a player to exploit the strengths of his own strategic position as well as the weaknesses of that of his opponents. Which commitments a player can make with credibility depends on the circumstances. In some, a player can only commit to the performance of an action, in others, he can commit himself conditionally on the actions of the other players. Some situations even allow for commitments on commitments or for commitments to randomized actions. We explore the formal properties of these types of (conditional) commitment and their interrelationships. So as to preclude inconsistencies among conditional commitments, we assume an order in which the players make their commitments. Central to our analyses is the notion of an extortion, which we define, for a given order of the players, as a profile that contains, for each player, an optimal commitment given the commitments of the players that committed earlier. On this basis, we investigate for different commitment types whether it is advantageous to commit earlier rather than later, and how the outcomes obtained through extortions relate to backward induction and Pareto efficiency.", "id": "73", "src": "commitment and extortion . making commitments , e . g . , through promises and threats , enables a player to exploit the strengths of his own strategic position as well as the weaknesses of that of his opponents . which commitments a player can make with credibility depends on the circumstances . in some , a player can only commit to the performance of an action , in others , he can commit himself conditionally on the actions of the other players . some situations even allow for commitments on commitments or for commitments to randomized actions . we explore the formal properties of these types of ( conditional ) commitment and their interrelationships . so as to preclude inconsistencies among conditional commitments , we assume an order in which the players make their commitments . central to our analyses is the notion of an extortion , which we define , for a given order of the players , as a profile that contains , for each player , an optimal commitment given the commitments of the players that committed earlier . on this basis , we investigate for different commitment types whether it is advantageous to commit earlier rather than later , and how the outcomes obtained through extortions relate to backward induction and pareto efficiency ."}
{"title": "Temporal Linear Logic as a Basis for Flexible Agent Interactions", "abstract": "Interactions between agents in an open system such as the Internet require a significant degree of flexibility. A crucial aspect of the development of such methods is the notion of commitments, which provides a mechanism for coordinating interactive behaviors among agents. In this paper, we investigate an approach to model commitments with tight integration with protocol actions. This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments. We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic, which incorporates both temporal and resource-sensitive reasoning. We also discuss the application of this framework to scenarios such as online commerce.", "id": "74", "src": "temporal linear logic as a basis for flexible agent interactions . interactions between agents in an open system such as the internet require a significant degree of flexibility . a crucial aspect of the development of such methods is the notion of commitments , which provides a mechanism for coordinating interactive behaviors among agents . in this paper , we investigate an approach to model commitments with tight integration with protocol actions . this means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments . we show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic , which incorporates both temporal and resource sensitive reasoning . we also discuss the application of this framework to scenarios such as online commerce ."}
{"title": "Generalized Trade Reduction Mechanisms", "abstract": "When designing a mechanism there are several desirable properties to maintain such as incentive compatibility (IC), individual rationality (IR), and budget balance (BB). It is well known [15] that it is impossible for a mechanism to maximize social welfare whilst also being IR, IC, and BB. There have been several attempts to circumvent [15] by trading welfare for BB, e.g., in domains such as double-sided auctions[13], distributed markets[3] and supply chain problems[2, 4]. In this paper we provide a procedure called a Generalized Trade Reduction (GTR) for single-value players, which given an IR and IC mechanism, outputs a mechanism which is IR, IC and BB with a loss of welfare. We bound the welfare achieved by our procedure for a wide range of domains. In particular, our results improve on existing solutions for problems such as double sided markets with homogenous goods, distributed markets and several kinds of supply chains. Furthermore, our solution provides budget balanced mechanisms for several open problems such as combinatorial double-sided auctions and distributed markets with strategic transportation edges.", "id": "75", "src": "generalized trade reduction mechanisms . when designing a mechanism there are several desirable properties to maintain such as incentive compatibility ( ic ) , individual rationality ( ir ) , and budget balance ( bb ) . it is well known <digit> that it is impossible for a mechanism to maximize social welfare whilst also being ir , ic , and bb . there have been several attempts to circumvent <digit> by trading welfare for bb , e . g . , in domains such as double sided auctions <digit> , distributed markets 3 and supply chain problems 2 , 4 . in this paper we provide a procedure called a generalized trade reduction ( gtr ) for single value players , which given an ir and ic mechanism , outputs a mechanism which is ir , ic and bb with a loss of welfare . we bound the welfare achieved by our procedure for a wide range of domains . in particular , our results improve on existing solutions for problems such as double sided markets with homogenous goods , distributed markets and several kinds of supply chains . furthermore , our solution provides budget balanced mechanisms for several open problems such as combinatorial double sided auctions and distributed markets with strategic transportation edges ."}
{"title": "Understanding User Behavior in Online Feedback Reporting", "abstract": "Online reviews have become increasingly popular as a way to judge the quality of various products and services. Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult. In this paper, we investigate underlying factors that influence user behavior when reporting feedback. We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports. We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature. Second, we show that a user\"s rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews. Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias. Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.", "id": "76", "src": "understanding user behavior in online feedback reporting . online reviews have become increasingly popular as a way to judge the quality of various products and services . previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult . in this paper , we investigate underlying factors that influence user behavior when reporting feedback . we look at two sources of information besides numerical ratings linguistic evidence from the textual comment accompanying a review , and patterns in the time sequence of reports . we first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature . second , we show that a user s rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews . both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias . our hypotheses were validated by statistical evidence from hotel reviews on the tripadvisor website ."}
{"title": "Trading Networks with Price-Setting Agents", "abstract": "In a wide range of markets, individual buyers and sellers often trade through intermediaries, who determine prices via strategic considerations. Typically, not all buyers and sellers have access to the same intermediaries, and they trade at correspondingly different prices that reflect their relative amounts of power in the market. We model this phenomenon using a game in which buyers, sellers, and traders engage in trade on a graph that represents the access each buyer and seller has to the traders. In this model, traders set prices strategically, and then buyers and sellers react to the prices they are offered. We show that the resulting game always has a subgame perfect Nash equilibrium, and that all equilibria lead to an efficient (i.e. socially optimal) allocation of goods. We extend these results to a more general type of matching market, such as one finds in the matching of job applicants and employers. Finally, we consider how the profits obtained by the traders depend on the underlying graph - roughly, a trader can command a positive profit if and only if it has an essential connection in the network structure, thus providing a graph-theoretic basis for quantifying the amount of competition among traders. Our work differs from recent studies of how price is affected by network structure through our modeling of price-setting as a strategic activity carried out by a subset of agents in the system, rather than studying prices set via competitive equilibrium or by a truthful mechanism.", "id": "77", "src": "trading networks with price setting agents . in a wide range of markets , individual buyers and sellers often trade through intermediaries , who determine prices via strategic considerations . typically , not all buyers and sellers have access to the same intermediaries , and they trade at correspondingly different prices that reflect their relative amounts of power in the market . we model this phenomenon using a game in which buyers , sellers , and traders engage in trade on a graph that represents the access each buyer and seller has to the traders . in this model , traders set prices strategically , and then buyers and sellers react to the prices they are offered . we show that the resulting game always has a subgame perfect nash equilibrium , and that all equilibria lead to an efficient ( i . e . socially optimal ) allocation of goods . we extend these results to a more general type of matching market , such as one finds in the matching of job applicants and employers . finally , we consider how the profits obtained by the traders depend on the underlying graph roughly , a trader can command a positive profit if and only if it has an essential connection in the network structure , thus providing a graph theoretic basis for quantifying the amount of competition among traders . our work differs from recent studies of how price is affected by network structure through our modeling of price setting as a strategic activity carried out by a subset of agents in the system , rather than studying prices set via competitive equilibrium or by a truthful mechanism ."}
{"title": "On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions", "abstract": "The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs). Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem. In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.", "id": "78", "src": "on the complexity of combinatorial auctions structured item graphs and hypertree decompositions . the winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices . while this problem is in general nphard , it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth ( called structured item graphs ) . formally , an item graph is a graph whose nodes are in one to one correspondence with items , and edges are such that for any bid , the items occurring in it induce a connected subgraph . note that many item graphs might be associated with a given combinatorial auction , depending on the edges selected for guaranteeing the connectedness . in fact , the tractability of determining whether a structured item graph of a fixed treewidth exists ( and if so , computing one ) was left as a crucial open problem . in this paper , we solve this problem by proving that the existence of a structured item graph is computationally intractable , even for treewidth 3 . motivated by this bad news , we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions . we show that the notion of hypertree decomposition , a recently introduced measure of hypergraph cyclicity , turns out to be most useful here . indeed , we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with ( dual ) hypergraphs having bounded hypertree width . even more surprisingly , we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph ."}
{"title": "Computing Good Nash Equilibria in Graphical Games \u2217", "abstract": "This paper addresses the problem of fair equilibrium selection in graphical games. Our approach is based on the data structure called the best response policy, which was proposed by Kearns et al. [13] as a way to represent all Nash equilibria of a graphical game. In [9], it was shown that the best response policy has polynomial size as long as the underlying graph is a path. In this paper, we show that if the underlying graph is a bounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants. Another attractive solution concept is a Nash equilibrium that maximizes the social welfare. We show that, while exactly computing the latter is infeasible (we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree), there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size. These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria.", "id": "79", "src": "computing good nash equilibria in graphical games . this paper addresses the problem of fair equilibrium selection in graphical games . our approach is based on the data structure called the best response policy , which was proposed by kearns et al . <digit> as a way to represent all nash equilibria of a graphical game . in 9 , it was shown that the best response policy has polynomial size as long as the underlying graph is a path . in this paper , we show that if the underlying graph is a bounded degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a nash equilibrium that guarantees certain payoffs to all participants . another attractive solution concept is a nash equilibrium that maximizes the social welfare . we show that , while exactly computing the latter is infeasible ( we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree ) , there exists an fptas for finding such an equilibrium as long as the best response policy has polynomial size . these two algorithms can be combined to produce nash equilibria that satisfy various fairness criteria ."}
{"title": "Generalized Value Decomposition and Structured Multiattribute Auctions", "abstract": "Multiattribute auction mechanisms generally either remain agnostic about traders\" preferences, or presume highly restrictive forms, such as full additivity. Real preferences often exhibit dependencies among attributes, yet may possess some structure that can be usefully exploited to streamline communication and simplify operation of a multiattribute auction. We develop such a structure using the theory of measurable value functions, a cardinal utility representation based on an underlying order over preference differences. A set of local conditional independence relations over such differences supports a generalized additive preference representation, which decomposes utility across overlapping clusters of related attributes. We introduce an iterative auction mechanism that maintains prices on local clusters of attributes rather than the full space of joint configurations. When traders\" preferences are consistent with the auction\"s generalized additive structure, the mechanism produces approximately optimal allocations, at approximate VCG prices.", "id": "80", "src": "generalized value decomposition and structured multiattribute auctions . multiattribute auction mechanisms generally either remain agnostic about traders preferences , or presume highly restrictive forms , such as full additivity . real preferences often exhibit dependencies among attributes , yet may possess some structure that can be usefully exploited to streamline communication and simplify operation of a multiattribute auction . we develop such a structure using the theory of measurable value functions , a cardinal utility representation based on an underlying order over preference differences . a set of local conditional independence relations over such differences supports a generalized additive preference representation , which decomposes utility across overlapping clusters of related attributes . we introduce an iterative auction mechanism that maintains prices on local clusters of attributes rather than the full space of joint configurations . when traders preferences are consistent with the auction s generalized additive structure , the mechanism produces approximately optimal allocations , at approximate vcg prices ."}
{"title": "Truthful Mechanism Design for Multi-Dimensional Scheduling via Cycle Monotonicity", "abstract": "We consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design, where the machines are the strategic players. This is a multidimensional scheduling domain, and the only known positive results for makespan minimization in such a domain are O(m)-approximation truthful mechanisms [22, 20]. We study a well-motivated special case of this problem, where the processing time of a job on each machine may either be low or high, and the low and high values are public and job-dependent. This preserves the multidimensionality of the domain, and generalizes the restricted-machines (i.e., {pj, \u221e}) setting in scheduling. We give a general technique to convert any c-approximation algorithm to a 3capproximation truthful-in-expectation mechanism. This is one of the few known results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms in a black-box fashion. When the low and high values are the same for all jobs, we devise a deterministic 2-approximation truthful mechanism. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Our constructions are novel in two respects. First, we do not utilize or rely on explicit price definitions to prove truthfulness; instead we design algorithms that satisfy cycle monotonicity. Cycle monotonicity [23] is a necessary and sufficient condition for truthfulness, is a generalization of value monotonicity for multidimensional domains. However, whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in singledimensional domains, ours is the first work that leverages cycle monotonicity in the multidimensional setting. Second, our randomized mechanisms are obtained by first constructing a fractional truthful mechanism for a fractional relaxation of the problem, and then converting it into a truthfulin-expectation mechanism. This builds upon a technique of [16], and shows the usefulness of fractional mechanisms in truthful mechanism design.", "id": "81", "src": "truthful mechanism design for multi dimensional scheduling via cycle monotonicity . we consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design , where the machines are the strategic players . this is a multidimensional scheduling domain , and the only known positive results for makespan minimization in such a domain are o ( m ) approximation truthful mechanisms <digit> , <digit> . we study a well motivated special case of this problem , where the processing time of a job on each machine may either be low or high , and the low and high values are public and job dependent . this preserves the multidimensionality of the domain , and generalizes the restricted machines ( i . e . , pj , ) setting in scheduling . we give a general technique to convert any c approximation algorithm to a 3capproximation truthful in expectation mechanism . this is one of the few known results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms in a black box fashion . when the low and high values are the same for all jobs , we devise a deterministic 2 approximation truthful mechanism . these are the first truthful mechanisms with non trivial performance guarantees for a multidimensional scheduling domain . our constructions are novel in two respects . first , we do not utilize or rely on explicit price definitions to prove truthfulness instead we design algorithms that satisfy cycle monotonicity . cycle monotonicity <digit> is a necessary and sufficient condition for truthfulness , is a generalization of value monotonicity for multidimensional domains . however , whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in singledimensional domains , ours is the first work that leverages cycle monotonicity in the multidimensional setting . second , our randomized mechanisms are obtained by first constructing a fractional truthful mechanism for a fractional relaxation of the problem , and then converting it into a truthfulin expectation mechanism . this builds upon a technique of <digit> , and shows the usefulness of fractional mechanisms in truthful mechanism design ."}
{"title": "Mediators in Position Auctions", "abstract": "A mediator is a reliable entity, which can play on behalf of agents in a given game. A mediator however can not enforce the use of its services, and each agent is free to participate in the game directly. In this paper we introduce a study of mediators for games with incomplete information, and apply it to the context of position auctions, a central topic in electronic commerce. VCG position auctions, which are currently not used in practice, possess some nice theoretical properties, such as the optimization of social surplus and having dominant strategies. These properties may not be satisfied by current position auctions and their variants. We therefore concentrate on the search for mediators that will allow to transform current position auctions into VCG position auctions. We require that accepting the mediator services, and reporting honestly to the mediator, will form an ex post equilibrium, which satisfies the following rationality condition: an agent\"s payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator\"s services, or by the agents who report false types to the mediator. We prove the existence of such desired mediators for the next-price (Google-like) position auctions, as well as for a richer class of position auctions, including all k-price position auctions, k > 1. For k=1, the self-price position auction, we show that the existence of such mediator depends on the tie breaking rule used in the auction.", "id": "82", "src": "mediators in position auctions . a mediator is a reliable entity , which can play on behalf of agents in a given game . a mediator however can not enforce the use of its services , and each agent is free to participate in the game directly . in this paper we introduce a study of mediators for games with incomplete information , and apply it to the context of position auctions , a central topic in electronic commerce . vcg position auctions , which are currently not used in practice , possess some nice theoretical properties , such as the optimization of social surplus and having dominant strategies . these properties may not be satisfied by current position auctions and their variants . we therefore concentrate on the search for mediators that will allow to transform current position auctions into vcg position auctions . we require that accepting the mediator services , and reporting honestly to the mediator , will form an ex post equilibrium , which satisfies the following rationality condition an agent s payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator s services , or by the agents who report false types to the mediator . we prove the existence of such desired mediators for the next price ( google like ) position auctions , as well as for a richer class of position auctions , including all k price position auctions , k > 1 . for k 1 , the self price position auction , we show that the existence of such mediator depends on the tie breaking rule used in the auction ."}
{"title": "Worst-Case Optimal Redistribution of VCG Payments", "abstract": "For allocation problems with one or more items, the wellknown Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents\" payments will sum to more than 0. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents. In 2006, Cavallo [3] proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property. In this paper, we extend this result in a restricted setting. We study allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand. (For this specific setting, Cavallo\"s mechanism coincides with a mechanism proposed by Bailey in 1997 [2].) Here we propose a family of mechanisms that redistribute some of the VCG payment back to the agents. All mechanisms in the family are efficient, strategyproof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case. We then provide an optimization model for finding the optimal mechanism-that is, the mechanism that maximizes redistribution in the worst case-inside the family, and show how to cast this model as a linear program. We give both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case). Finally, we prove that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties.", "id": "83", "src": "worst case optimal redistribution of vcg payments . for allocation problems with one or more items , the wellknown vickrey clarke groves ( vcg ) mechanism is efficient , strategy proof , individually rational , and does not incur a deficit . however , the vcg mechanism is not ( strongly ) budget balanced generally , the agents payments will sum to more than 0 . if there is an auctioneer who is selling the items , this may be desirable , because the surplus payment corresponds to revenue for the auctioneer . however , if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves , any surplus payment is undesirable , because it will have to flow out of the system of agents . in <digit> , cavallo 3 proposed a mechanism that redistributes some of the vcg payment back to the agents , while maintaining efficiency , strategy proofness , individual rationality , and the non deficit property . in this paper , we extend this result in a restricted setting . we study allocation settings where there are multiple indistinguishable units of a single good , and agents have unit demand . ( for this specific setting , cavallo s mechanism coincides with a mechanism proposed by bailey in <digit> 2 . ) here we propose a family of mechanisms that redistribute some of the vcg payment back to the agents . all mechanisms in the family are efficient , strategyproof , individually rational , and never incur a deficit . the family includes the bailey cavallo mechanism as a special case . we then provide an optimization model for finding the optimal mechanism that is , the mechanism that maximizes redistribution in the worst case inside the family , and show how to cast this model as a linear program . we give both numerical and analytical solutions of this linear program , and the ( unique ) resulting mechanism shows significant improvement over the bailey cavallo mechanism ( in the worst case ) . finally , we prove that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties ."}
{"title": "Clearing Algorithms for Barter Exchange Markets: Enabling Nationwide Kidney Exchanges", "abstract": "In barter-exchange markets, agents seek to swap their items with one another, in order to improve their own utilities. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. We focus mainly on the upcoming national kidney-exchange market, where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. The clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed. Long cycles are forbidden, since, for incentive reasons, all transplants in a cycle must be performed simultaneously. Also, in barter-exchanges generally, more agents are affected if one drops out of a longer cycle. We prove that the clearing problem with this cycle-length constraint is NP-hard. Solving it exactly is one of the main challenges in establishing a national kidney exchange. We present the first algorithm capable of clearing these markets on a nationwide scale. The key is incremental problem formulation. We adapt two paradigms for the task: constraint generation and column generation. For each, we develop techniques that dramatically improve both runtime and memory usage. We conclude that column generation scales drastically better than constraint generation. Our algorithm also supports several generalizations, as demanded by real-world kidney exchanges. Our algorithm replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges. The match runs are conducted every two weeks and transplants based on our optimizations have already been conducted.", "id": "84", "src": "clearing algorithms for barter exchange markets enabling nationwide kidney exchanges . in barter exchange markets , agents seek to swap their items with one another , in order to improve their own utilities . these swaps consist of cycles of agents , with each agent receiving the item of the next agent in the cycle . we focus mainly on the upcoming national kidney exchange market , where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors . with over <digit> , <digit> patients already waiting for a cadaver kidney in the us , this market is seen as the only ethical way to significantly reduce the 4 , <digit> deaths per year attributed to kidney disease . the clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed . long cycles are forbidden , since , for incentive reasons , all transplants in a cycle must be performed simultaneously . also , in barter exchanges generally , more agents are affected if one drops out of a longer cycle . we prove that the clearing problem with this cycle length constraint is np hard . solving it exactly is one of the main challenges in establishing a national kidney exchange . we present the first algorithm capable of clearing these markets on a nationwide scale . the key is incremental problem formulation . we adapt two paradigms for the task constraint generation and column generation . for each , we develop techniques that dramatically improve both runtime and memory usage . we conclude that column generation scales drastically better than constraint generation . our algorithm also supports several generalizations , as demanded by real world kidney exchanges . our algorithm replaced cplex as the clearing algorithm of the alliance for paired donation , one of the leading kidney exchanges . the match runs are conducted every two weeks and transplants based on our optimizations have already been conducted ."}
{"title": "A Strategic Model for Information Markets", "abstract": "Information markets, which are designed specifically to aggregate traders\" information, are becoming increasingly popular as a means for predicting future events. Recent research in information markets has resulted in two new designs, market scoring rules and dynamic parimutuel markets. We develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection game, that serves as a useful model for information markets. We demonstrate that this game can serve as a strategic model of dynamic parimutuel markets, and also captures the essence of the strategies in market scoring rules. The projection game is tractable to analyze, and has an attractive geometric visualization that makes the strategic moves and interactions more transparent. We use it to prove several strategic properties about the dynamic parimutuel market. We also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule, and it is strategically similar to other scoring rules. Finally, we illustrate two applications of the model to analysis of complex strategic scenarios: we analyze the precision of a market in which traders have inertia, and a market in which a trader can profit by manipulating another trader\"s beliefs.", "id": "85", "src": "a strategic model for information markets . information markets , which are designed specifically to aggregate traders information , are becoming increasingly popular as a means for predicting future events . recent research in information markets has resulted in two new designs , market scoring rules and dynamic parimutuel markets . we develop an analytic method to guide the design and strategic analysis of information markets . our central contribution is a new abstract betting game , the projection game , that serves as a useful model for information markets . we demonstrate that this game can serve as a strategic model of dynamic parimutuel markets , and also captures the essence of the strategies in market scoring rules . the projection game is tractable to analyze , and has an attractive geometric visualization that makes the strategic moves and interactions more transparent . we use it to prove several strategic properties about the dynamic parimutuel market . we also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule , and it is strategically similar to other scoring rules . finally , we illustrate two applications of the model to analysis of complex strategic scenarios we analyze the precision of a market in which traders have inertia , and a market in which a trader can profit by manipulating another trader s beliefs ."}
{"title": "Betting on Permutations", "abstract": "We consider a permutation betting scenario, where people wager on the final ordering of n candidates: for example, the outcome of a horse race. We examine the auctioneer problem of risklessly matching up wagers or, equivalently, finding arbitrage opportunities among the proposed wagers. Requiring bidders to explicitly list the orderings that they\"d like to bet on is both unnatural and intractable, because the number of orderings is n! and the number of subsets of orderings is 2n! . We propose two expressive betting languages that seem natural for bidders, and examine the computational complexity of the auctioneer problem in each case. Subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering, for example, horse A will finish in positions 4, 9, or 13-21, or that a position will be taken by some subset of candidates, for example horse A, B, or D will finish in position 2. For subset betting, we show that the auctioneer problem can be solved in polynomial time if orders are divisible. Pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate, for example horse A will beat horse B. We prove that the auctioneer problem becomes NP-hard for pair betting. We identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time. We also show that a natural greedy algorithm gives a poor approximation for indivisible orders.", "id": "86", "src": "betting on permutations . we consider a permutation betting scenario , where people wager on the final ordering of n candidates for example , the outcome of a horse race . we examine the auctioneer problem of risklessly matching up wagers or , equivalently , finding arbitrage opportunities among the proposed wagers . requiring bidders to explicitly list the orderings that they d like to bet on is both unnatural and intractable , because the number of orderings is n and the number of subsets of orderings is 2n . we propose two expressive betting languages that seem natural for bidders , and examine the computational complexity of the auctioneer problem in each case . subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering , for example , horse a will finish in positions 4 , 9 , or <digit> <digit> , or that a position will be taken by some subset of candidates , for example horse a , b , or d will finish in position 2 . for subset betting , we show that the auctioneer problem can be solved in polynomial time if orders are divisible . pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate , for example horse a will beat horse b . we prove that the auctioneer problem becomes np hard for pair betting . we identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time . we also show that a natural greedy algorithm gives a poor approximation for indivisible orders ."}
{"title": "Frugality Ratios And Improved Truthful Mechanisms for Vertex Cover", "abstract": "In set-system auctions, there are several overlapping teams of agents, and a task that can be completed by any of these teams. The auctioneer\"s goal is to hire a team and pay as little as possible. Examples of this setting include shortest-path auctions and vertex-cover auctions. Recently, Karlin, Kempe and Tamir introduced a new definition of frugality ratio for this problem. Informally, the frugality ratio is the ratio of the total payment of a mechanism to a desired payment bound. The ratio captures the extent to which the mechanism overpays, relative to perceived fair cost in a truthful auction. In this paper, we propose a new truthful polynomial-time auction for the vertex cover problem and bound its frugality ratio. We show that the solution quality is with a constant factor of optimal and the frugality ratio is within a constant factor of the best possible worst-case bound; this is the first auction for this problem to have these properties. Moreover, we show how to transform any truthful auction into a frugal one while preserving the approximation ratio. Also, we consider two natural modifications of the definition of Karlin et al., and we analyse the properties of the resulting payment bounds, such as monotonicity, computational hardness, and robustness with respect to the draw-resolution rule. We study the relationships between the different payment bounds, both for general set systems and for specific set-system auctions, such as path auctions and vertex-cover auctions. We use these new definitions in the proof of our main result for vertex-cover auctions via a bootstrapping technique, which may be of independent interest.", "id": "87", "src": "frugality ratios and improved truthful mechanisms for vertex cover . in set system auctions , there are several overlapping teams of agents , and a task that can be completed by any of these teams . the auctioneer s goal is to hire a team and pay as little as possible . examples of this setting include shortest path auctions and vertex cover auctions . recently , karlin , kempe and tamir introduced a new definition of frugality ratio for this problem . informally , the frugality ratio is the ratio of the total payment of a mechanism to a desired payment bound . the ratio captures the extent to which the mechanism overpays , relative to perceived fair cost in a truthful auction . in this paper , we propose a new truthful polynomial time auction for the vertex cover problem and bound its frugality ratio . we show that the solution quality is with a constant factor of optimal and the frugality ratio is within a constant factor of the best possible worst case bound this is the first auction for this problem to have these properties . moreover , we show how to transform any truthful auction into a frugal one while preserving the approximation ratio . also , we consider two natural modifications of the definition of karlin et al . , and we analyse the properties of the resulting payment bounds , such as monotonicity , computational hardness , and robustness with respect to the draw resolution rule . we study the relationships between the different payment bounds , both for general set systems and for specific set system auctions , such as path auctions and vertex cover auctions . we use these new definitions in the proof of our main result for vertex cover auctions via a bootstrapping technique , which may be of independent interest ."}
{"title": "Betting Boolean-Style: A Framework for Trading in Securities Based on Logical Formulas", "abstract": "We develop a framework for trading in compound securities: financial instruments that pay off contingent on the outcomes of arbitrary statements in propositional logic. Buying or selling securities-which can be thought of as betting on or against a particular future outcome-allows agents both to hedge risk and to profit (in expectation) on subjective predictions. A compound securities market allows agents to place bets on arbitrary boolean combinations of events, enabling them to more closely achieve their optimal risk exposure, and enabling the market as a whole to more closely achieve the social optimum. The tradeoff for allowing such expressivity is in the complexity of the agents\" and auctioneer\"s optimization problems. We develop and motivate the concept of a compound securities market, presenting the framework through a series of formal definitions and examples. We then analyze in detail the auctioneer\"s matching problem. We show that, with n events, the matching problem is co-NP-complete in the divisible case and \u03a3p 2-complete in the indivisible case. We show that the latter hardness result holds even under severe language restrictions on bids. With log n events, the problem is polynomial in the divisible case and NP-complete in the indivisible case. We briefly discuss matching algorithms and tractable special cases.", "id": "88", "src": "betting boolean style a framework for trading in securities based on logical formulas . we develop a framework for trading in compound securities financial instruments that pay off contingent on the outcomes of arbitrary statements in propositional logic . buying or selling securities which can be thought of as betting on or against a particular future outcome allows agents both to hedge risk and to profit ( in expectation ) on subjective predictions . a compound securities market allows agents to place bets on arbitrary boolean combinations of events , enabling them to more closely achieve their optimal risk exposure , and enabling the market as a whole to more closely achieve the social optimum . the tradeoff for allowing such expressivity is in the complexity of the agents and auctioneer s optimization problems . we develop and motivate the concept of a compound securities market , presenting the framework through a series of formal definitions and examples . we then analyze in detail the auctioneer s matching problem . we show that , with n events , the matching problem is co np complete in the divisible case and p 2 complete in the indivisible case . we show that the latter hardness result holds even under severe language restrictions on bids . with log n events , the problem is polynomial in the divisible case and np complete in the indivisible case . we briefly discuss matching algorithms and tractable special cases ."}
{"title": "Combinatorial Agency", "abstract": "Much recent research concerns systems, such as the Internet, whose components are owned and operated by different parties, each with his own selfish goal. The field of Algorithmic Mechanism Design handles the issue of private information held by the different parties in such computational settings. This paper deals with a complementary problem in such settings: handling the hidden actions that are performed by the different parties. Our model is a combinatorial variant of the classical principalagent problem from economic theory. In our setting a principal must motivate a team of strategic agents to exert costly effort on his behalf, but their actions are hidden from him. Our focus is on cases where complex combinations of the efforts of the agents influence the outcome. The principal motivates the agents by offering to them a set of contracts, which together put the agents in an equilibrium point of the induced game. We present formal models for this setting, suggest and embark on an analysis of some basic issues, but leave many questions open.", "id": "89", "src": "combinatorial agency . much recent research concerns systems , such as the internet , whose components are owned and operated by different parties , each with his own selfish goal . the field of algorithmic mechanism design handles the issue of private information held by the different parties in such computational settings . this paper deals with a complementary problem in such settings handling the hidden actions that are performed by the different parties . our model is a combinatorial variant of the classical principalagent problem from economic theory . in our setting a principal must motivate a team of strategic agents to exert costly effort on his behalf , but their actions are hidden from him . our focus is on cases where complex combinations of the efforts of the agents influence the outcome . the principal motivates the agents by offering to them a set of contracts , which together put the agents in an equilibrium point of the induced game . we present formal models for this setting , suggest and embark on an analysis of some basic issues , but leave many questions open ."}
{"title": "Learning From Revealed Preference", "abstract": "A sequence of prices and demands are rationalizable if there exists a concave, continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price. Afriat [1] presented necessary and sufficient conditions for a finite sequence to be rationalizable. Varian [20] and later Blundell et al. [3, 4] continued this line of work studying nonparametric methods to forecasts demand. Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast. The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts. Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable, but that there exist interesting and potentially useful classes that are learnable from finite samples. We also present a learning algorithm that is an adaptation of a new proof of Afriat\"s theorem due to Teo and Vohra [17].", "id": "90", "src": "learning from revealed preference . a sequence of prices and demands are rationalizable if there exists a concave , continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price . afriat 1 presented necessary and sufficient conditions for a finite sequence to be rationalizable . varian <digit> and later blundell et al . 3 , 4 continued this line of work studying nonparametric methods to forecasts demand . their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast . the present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts . our results show that the class of all demand functions has unbounded complexity and therefore is not learnable , but that there exist interesting and potentially useful classes that are learnable from finite samples . we also present a learning algorithm that is an adaptation of a new proof of afriat s theorem due to teo and vohra <digit> ."}
{"title": "Approximately-Strategyproof and Tractable Multi-Unit Auctions", "abstract": "We present an approximately-efficient and approximatelystrategyproof auction mechanism for a single-good multi-unit allocation problem. The bidding language in our auctions allows marginal-decreasing piecewise constant curves. First, we develop a fully polynomial-time approximation scheme for the multi-unit allocation problem, which computes a (1 + )approximation in worst-case time T = O(n3 / ), given n bids each with a constant number of pieces. Second, we embed this approximation scheme within a Vickrey-Clarke-Groves (VCG) mechanism and compute payments to n agents for an asymptotic cost of O(T log n). The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by /(1+ )V , where V is the total surplus in the efficient outcome.", "id": "91", "src": "approximately strategyproof and tractable multi unit auctions . we present an approximately efficient and approximatelystrategyproof auction mechanism for a single good multi unit allocation problem . the bidding language in our auctions allows marginal decreasing piecewise constant curves . first , we develop a fully polynomial time approximation scheme for the multi unit allocation problem , which computes a ( 1 + ) approximation in worst case time t o ( n3 ) , given n bids each with a constant number of pieces . second , we embed this approximation scheme within a vickrey clarke groves ( vcg ) mechanism and compute payments to n agents for an asymptotic cost of o ( t log n ) . the maximal possible gain from manipulation to a bidder in the combined scheme is bounded by ( 1+ ) v , where v is the total surplus in the efficient outcome ."}
{"title": "Budget Optimization in Search-Based Advertising Auctions", "abstract": "Internet search companies sell advertisement slots based on users\" search queries via an auction. While there has been previous work on the auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return (the number of user clicks on their ads) for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywords works well. More precisely, this strategy gets at least a 1 \u2212 1/e fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem.", "id": "92", "src": "budget optimization in search based advertising auctions . internet search companies sell advertisement slots based on users search queries via an auction . while there has been previous work on the auction process and its game theoretic aspects , most of it focuses on the internet company . in this work , we focus on the advertisers , who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return ( the number of user clicks on their ads ) for a given budget . we model the entire process and study this budget optimization problem . while most variants are np hard , we show , perhaps surprisingly , that simply randomizing between two uniform strategies that bid equally on all the keywords works well . more precisely , this strategy gets at least a 1 1 e fraction of the maximum clicks possible . as our preliminary experiments show , such uniform strategies are likely to be practical . we also present inapproximability results , and optimal algorithms for variants of the budget optimization problem ."}
{"title": "Implementation with a Bounded Action Space", "abstract": "While traditional mechanism design typically assumes isomorphism between the agents\" type- and action spaces, in many situations the agents face strict restrictions on their action space due to, e.g., technical, behavioral or regulatory reasons. We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces. Our contribution is threefold. First, we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies, and prove that any multilinear social-choice rule is dominant-strategy implementable with no additional cost. Second, we identify necessary conditions for the optimality of action-bounded mechanisms, and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives. Finally, we prove that for any multilinear social-choice rule, the optimal mechanism with k actions incurs an expected loss of O( 1 k2 ) compared to the optimal mechanisms with unrestricted action spaces. Our results apply to various economic and computational settings, and we demonstrate their applicability to signaling games, public-good models and routing in networks.", "id": "93", "src": "implementation with a bounded action space . while traditional mechanism design typically assumes isomorphism between the agents type and action spaces , in many situations the agents face strict restrictions on their action space due to , e . g . , technical , behavioral or regulatory reasons . we devise a general framework for the study of mechanism design in single parameter environments with restricted action spaces . our contribution is threefold . first , we characterize sufficient conditions under which the information theoretically optimal social choice rule can be implemented in dominant strategies , and prove that any multilinear social choice rule is dominant strategy implementable with no additional cost . second , we identify necessary conditions for the optimality of action bounded mechanisms , and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives . finally , we prove that for any multilinear social choice rule , the optimal mechanism with k actions incurs an expected loss of o ( 1 k2 ) compared to the optimal mechanisms with unrestricted action spaces . our results apply to various economic and computational settings , and we demonstrate their applicability to signaling games , public good models and routing in networks ."}
{"title": "Computing the Optimal Strategy to Commit to\u2217", "abstract": "In multiagent systems, strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously. However, this model is not always realistic. In many settings, one player is able to commit to a strategy before the other player makes a decision. Such models are synonymously referred to as leadership, commitment, or Stackelberg models, and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously. The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models (with the exception of the interest in mechanism design, where the designer is implicitly in a leadership position). In this paper, we study how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies, in both normal-form and Bayesian games. We give both positive results (efficient algorithms) and negative results (NP-hardness results).", "id": "94", "src": "computing the optimal strategy to commit to . in multiagent systems , strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously . however , this model is not always realistic . in many settings , one player is able to commit to a strategy before the other player makes a decision . such models are synonymously referred to as leadership , commitment , or stackelberg models , and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously . the recent surge in interest in computing game theoretic solutions has so far ignored leadership models ( with the exception of the interest in mechanism design , where the designer is implicitly in a leadership position ) . in this paper , we study how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies , in both normal form and bayesian games . we give both positive results ( efficient algorithms ) and negative results ( np hardness results ) ."}
{"title": "Nash Equilibria in Graphical Games on Trees Revisited \u2217", "abstract": "Graphical games have been proposed as a game-theoretic model of large-scale distributed networks of non-cooperative agents. When the number of players is large, and the underlying graph has low degree, they provide a concise way to represent the players\" payoffs. It has recently been shown that the problem of finding Nash equilibria in a general degree-3 graphical game with two actions per player is complete for the complexity class PPAD, indicating that it is unlikely that there is any polynomial-time algorithm for this problem. In this paper, we study the complexity of graphical games with two actions per player on bounded-degree trees. This setting was first considered by Kearns, Littman and Singh, who proposed a dynamic programming-based algorithm that computes all Nash equilibria of such games. The running time of their algorithm is exponential, though approximate equilibria can be computed efficiently. Later, Littman, Kearns and Singh proposed a modification to this algorithm that can find a single Nash equilibrium in polynomial time. We show that this modified algorithm is incorrect - the output is not always a Nash equilibrium. We then propose a new algorithm that is based on the ideas of Kearns et al. and computes all Nash equilibria in quadratic time if the input graph is a path, and in polynomial time if it is an arbitrary graph of maximum degree 2. Moreover, our algorithm can be used to compute Nash equilibria of graphical games on arbitrary trees, but the running time can be exponential, even when the tree has bounded degree. We show that this is inevitable - any algorithm of this type will take exponential time, even on bounded-degree trees with pathwidth 2. It is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1, but we show that finding a Nash equilibrium for a 2-action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is PPAD-complete (so is unlikely to be tractable).", "id": "95", "src": "nash equilibria in graphical games on trees revisited . graphical games have been proposed as a game theoretic model of large scale distributed networks of non cooperative agents . when the number of players is large , and the underlying graph has low degree , they provide a concise way to represent the players payoffs . it has recently been shown that the problem of finding nash equilibria in a general degree 3 graphical game with two actions per player is complete for the complexity class ppad , indicating that it is unlikely that there is any polynomial time algorithm for this problem . in this paper , we study the complexity of graphical games with two actions per player on bounded degree trees . this setting was first considered by kearns , littman and singh , who proposed a dynamic programming based algorithm that computes all nash equilibria of such games . the running time of their algorithm is exponential , though approximate equilibria can be computed efficiently . later , littman , kearns and singh proposed a modification to this algorithm that can find a single nash equilibrium in polynomial time . we show that this modified algorithm is incorrect the output is not always a nash equilibrium . we then propose a new algorithm that is based on the ideas of kearns et al . and computes all nash equilibria in quadratic time if the input graph is a path , and in polynomial time if it is an arbitrary graph of maximum degree 2 . moreover , our algorithm can be used to compute nash equilibria of graphical games on arbitrary trees , but the running time can be exponential , even when the tree has bounded degree . we show that this is inevitable any algorithm of this type will take exponential time , even on bounded degree trees with pathwidth 2 . it is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1 , but we show that finding a nash equilibrium for a 2 action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is ppad complete ( so is unlikely to be tractable ) ."}
{"title": "Revenue Analysis of a Family of Ranking Rules for Keyword Auctions", "abstract": "Keyword auctions lie at the core of the business models of today\"s leading search engines. Advertisers bid for placement alongside search results, and are charged for clicks on their ads. Advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates. We consider a family of ranking rules that contains those typically used to model Yahoo! and Google\"s auction designs as special cases. We find that in general neither of these is necessarily revenue-optimal in equilibrium, and that the choice of ranking rule can be guided by considering the correlation between bidders\" values and click-through rates. We propose a simple approach to determine a revenue-optimal ranking rule within our family, taking into account effects on advertiser satisfaction and user experience. We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword.", "id": "96", "src": "revenue analysis of a family of ranking rules for keyword auctions . keyword auctions lie at the core of the business models of today s leading search engines . advertisers bid for placement alongside search results , and are charged for clicks on their ads . advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates . we consider a family of ranking rules that contains those typically used to model yahoo and google s auction designs as special cases . we find that in general neither of these is necessarily revenue optimal in equilibrium , and that the choice of ranking rule can be guided by considering the correlation between bidders values and click through rates . we propose a simple approach to determine a revenue optimal ranking rule within our family , taking into account effects on advertiser satisfaction and user experience . we illustrate the approach using monte carlo simulations based on distributions fitted to yahoo bid and click through rate data for a high volume keyword ."}
{"title": "The Role of Compatibility in the Diffusion of Technologies Through Social Networks", "abstract": "In many settings, competing technologies - for example, operating systems, instant messenger systems, or document formatscan be seen adopting a limited amount of compatibility with one another; in other words, the difficulty in using multiple technologies is balanced somewhere between the two extremes of impossibility and effortless interoperability. There are a range of reasons why this phenomenon occurs, many of which - based on legal, social, or business considerations - seem to defy concise mathematical models. Despite this, we show that the advantages of limited compatibility can arise in a very simple model of diffusion in social networks, thus offering a basic explanation for this phenomenon in purely strategic terms. Our approach builds on work on the diffusion of innovations in the economics literature, which seeks to model how a new technology A might spread through a social network of individuals who are currently users of technology B. We consider several ways of capturing the compatibility of A and B, focusing primarily on a model in which users can choose to adopt A, adopt B, or - at an extra cost - adopt both A and B. We characterize how the ability of A to spread depends on both its quality relative to B, and also this additional cost of adopting both, and find some surprising non-monotonicity properties in the dependence on these parameters: in some cases, for one technology to survive the introduction of another, the cost of adopting both technologies must be balanced within a narrow, intermediate range. We also extend the framework to the case of multiple technologies, where we find that a simple This work has been supported in part by NSF grants CCF0325453, IIS-0329064, CNS-0403340, and BCS-0537606, a Google Research Grant, a Yahoo! Research Alliance Grant, the Institute for the Social Sciences at Cornell, and the John D. and Catherine T. MacArthur Foundation. model captures the phenomenon of two firms adopting a limited strategic alliance to defend against a new, third technology.", "id": "97", "src": "the role of compatibility in the diffusion of technologies through social networks . in many settings , competing technologies for example , operating systems , instant messenger systems , or document formatscan be seen adopting a limited amount of compatibility with one another in other words , the difficulty in using multiple technologies is balanced somewhere between the two extremes of impossibility and effortless interoperability . there are a range of reasons why this phenomenon occurs , many of which based on legal , social , or business considerations seem to defy concise mathematical models . despite this , we show that the advantages of limited compatibility can arise in a very simple model of diffusion in social networks , thus offering a basic explanation for this phenomenon in purely strategic terms . our approach builds on work on the diffusion of innovations in the economics literature , which seeks to model how a new technology a might spread through a social network of individuals who are currently users of technology b . we consider several ways of capturing the compatibility of a and b , focusing primarily on a model in which users can choose to adopt a , adopt b , or at an extra cost adopt both a and b . we characterize how the ability of a to spread depends on both its quality relative to b , and also this additional cost of adopting both , and find some surprising non monotonicity properties in the dependence on these parameters in some cases , for one technology to survive the introduction of another , the cost of adopting both technologies must be balanced within a narrow , intermediate range . we also extend the framework to the case of multiple technologies , where we find that a simple this work has been supported in part by nsf grants ccf0325453 , iis <digit> , cns <digit> , and bcs <digit> , a google research grant , a yahoo research alliance grant , the institute for the social sciences at cornell , and the john d . and catherine t . macarthur foundation . model captures the phenomenon of two firms adopting a limited strategic alliance to defend against a new , third technology ."}
{"title": "Strong Equilibrium in Cost Sharing Connection Games", "abstract": "In this work we study cost sharing connection games, where each player has a source and sink he would like to connect, and the cost of the edges is either shared equally (fair connection games) or in an arbitrary way (general connection games). We study the graph topologies that guarantee the existence of a strong equilibrium (where no coalition can improve the cost of each of its members) regardless of the specific costs on the edges. Our main existence results are the following: (1) For a single source and sink we show that there is always a strong equilibrium (both for fair and general connection games). (2) For a single source multiple sinks we show that for a series parallel graph a strong equilibrium always exists (both for fair and general connection games). (3) For multi source and sink we show that an extension parallel graph always admits a strong equilibrium in fair connection games. As for the quality of the strong equilibrium we show that in any fair connection games the cost of a strong equilibrium is \u0398(log n) from the optimal solution, where n is the number of players. (This should be contrasted with the \u2126(n) price of anarchy for the same setting.) For single source general connection games and single source single sink fair connection games, we show that a strong equilibrium is always an optimal solution.", "id": "98", "src": "strong equilibrium in cost sharing connection games . in this work we study cost sharing connection games , where each player has a source and sink he would like to connect , and the cost of the edges is either shared equally ( fair connection games ) or in an arbitrary way ( general connection games ) . we study the graph topologies that guarantee the existence of a strong equilibrium ( where no coalition can improve the cost of each of its members ) regardless of the specific costs on the edges . our main existence results are the following ( 1 ) for a single source and sink we show that there is always a strong equilibrium ( both for fair and general connection games ) . ( 2 ) for a single source multiple sinks we show that for a series parallel graph a strong equilibrium always exists ( both for fair and general connection games ) . ( 3 ) for multi source and sink we show that an extension parallel graph always admits a strong equilibrium in fair connection games . as for the quality of the strong equilibrium we show that in any fair connection games the cost of a strong equilibrium is ( log n ) from the optimal solution , where n is the number of players . ( this should be contrasted with the ( n ) price of anarchy for the same setting . ) for single source general connection games and single source single sink fair connection games , we show that a strong equilibrium is always an optimal solution ."}
{"title": "Computation in a Distributed Information Market\u2217", "abstract": "According to economic theory-supported by empirical and laboratory evidence-the equilibrium price of a financial security reflects all of the information regarding the security\"s value. We investigate the computational process on the path toward equilibrium, where information distributed among traders is revealed step-by-step over time and incorporated into the market price. We develop a simplified model of an information market, along with trading strategies, in order to formalize the computational properties of the process. We show that securities whose payoffs cannot be expressed as weighted threshold functions of distributed input bits are not guaranteed to converge to the proper equilibrium predicted by economic theory. On the other hand, securities whose payoffs are threshold functions are guaranteed to converge, for all prior probability distributions. Moreover, these threshold securities converge in at most n rounds, where n is the number of bits of distributed information. We also prove a lower bound, showing a type of threshold security that requires at least n/2 rounds to converge in the worst case.", "id": "99", "src": "computation in a distributed information market . according to economic theory supported by empirical and laboratory evidence the equilibrium price of a financial security reflects all of the information regarding the security s value . we investigate the computational process on the path toward equilibrium , where information distributed among traders is revealed step by step over time and incorporated into the market price . we develop a simplified model of an information market , along with trading strategies , in order to formalize the computational properties of the process . we show that securities whose payoffs cannot be expressed as weighted threshold functions of distributed input bits are not guaranteed to converge to the proper equilibrium predicted by economic theory . on the other hand , securities whose payoffs are threshold functions are guaranteed to converge , for all prior probability distributions . moreover , these threshold securities converge in at most n rounds , where n is the number of bits of distributed information . we also prove a lower bound , showing a type of threshold security that requires at least n 2 rounds to converge in the worst case ."}
