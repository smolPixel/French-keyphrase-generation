	title	sentences	label	language	index
0	Sistemas biométricos multimodales que emplean rasgos audio visuales	﻿  Se observa en la actualidad una tendencia muy marcada al empleo de técnicas biométricas (las que incluyen rostro, huellas digitales, geometría de la mano, iris, patrones de retina, firma y voz, entre los más destacados) en los sistemas de  identificación/autenticación de personas. Todas estas técnicas presentan diferentes grados de singularidad, permanencia, mensurabilidad, desempeño, aceptacción del usuario y robustez. Por otra parte, diferentes investigaciones sobre sistemas biométricos multimodales han demostrado que mediante su empleo es posible mejorar la carencias de cualquier sistema biométrico  unimodal, por lo que desde hace algunos años, proliferan las propuestas para el empleo de dos o más métodos biométricos independientes. En el presente trabajo se analizan aspectos significativos de estos métodos y aproximaciones y se desarrollan  breves reseñas, a partir de la búsqueda bibliográfica. Los aspectos abordados incluyen la fusión de información (poniendo el  acento especialmente en la identificación y verificación audio-visual de personas), y las técnicas adaptativas y no-adaptativas  para la decisión de verificación (utilizando información de habla y de rostro) en condiciones de audio con ruido.  Palabras Claves: biometría, biometría multimodal, seguridad de la información.  1. INTRODUCCIÓN  Se observa en la actualidad una tendencia muy marcada al empleo de técnicas biométricas (las que incluyen  rostro, huellas digitales, geometría de la mano, iris, patrones de retina, firma y voz, entre los más destacados)  en los sistemas de identificación/autenticación de personas. Todas estas técnicas presentan diferentes grados  de singularidad, permanencia, mensurabilidad, desempeño, aceptacción del usuario y robustez [1].   Un sistema de verificación biométrica (o autenticación en la jerga de la seguridad de la información) verifica la identidad de un reclamante en base a atributos biométricos de una persona. Además de las diferentes formas de control de acceso (por ejemplo, control fronterizo, acceso a información), los sistemas  de verificación también resultan de utilidad en trabajos forenses, en los que la tarea consiste en determinar si un muestra biométrica dada pertenece a un determinado sospechoso.   Por otra parte, diferentes investigaciones sobre sistemas biométricos multimodales han demostrado que  mediante su empleo es posible mejorar la carencias de cualquier sistema biométrico unimodal, por lo  que desde hace algunos años, proliferan las propuestas para el empleo de dos o más métodos biométricos independientes:  • combinando evidencia de la verificación de un hablante y el reconocimiento de su rostro [2],  • empleando un esquema de fusión a nivel abstracto denominado “2-from-3-approach”, en el que se  integran rostro, movimiento de labios y habla, que se basa en el principio de que las personas utilizan múltiples indicios para identificar a una persona [3],  • empleando una estrategia de integración que se focaliza en múltiples vistas instantáneas de una  única propiedad biométrica utilizando un framework bayesiano [4],  • combinando datos biométricos (por ejemplo voz grabada) con datos no-biométricos (por ejemplo  una contraseña) [5],  • integrando dentro de un sistema biométrico multimodal rostro, huella digital y habla para realizar  una identificación personal [1].  En el presente trabajo se analizan aspectos significativos de estos métodos y aproximaciones y se desarrollan breves reseñas, a partir de la búsqueda bibliográfica. Los aspectos abordados incluyen la fusión  de información (poniendo el acento especialmente en la identificación y verificación audio-visual de  personas), y las técnicas adaptativas y no-adaptativas para la decisión de verificación (utilizando información de habla y de rostro) en condiciones de ruido de audio.  2. ASPECTOS GENERALES  Tradicionalmente, se han utilizado las contraseñas (seguridad basada en conocimiento) y las tarjetas de  identificación (seguridad basada en token) para restringir el acceso a diferentes tipos de aplicaciones.  Sin embargo, se puede fácilmente quebrar la seguridad en las mismas cuando se divulga una contraseña  a un usuario no autorizado o un impostor roba una credencial. El surgimiento de la biometría en el campo de la identificación/autenticación de personas (seguridad basada en lo que se es) permite resolver los  problemas que debilitan los métodos tradicionales de verificación.   La biometría hace referencia a la identificación (o verificación) automática de una persona (o una identidad reclamada) mediante el empleo de ciertos rasgos fisiológicos o de comportamiento asociados con  la persona. Por esta razón, los sistemas biométricos presentan la ventaja de no poder ser fácilmente robados o compartidos respecto de los métodos tradicionales de seguridad.  Un sistemas de autenticación basado en biometría opera en dos modos:  1. Modo Registración (enrollment), en el que se adquieren los datos biométricos del usuario utilizando un lector biométrico y se almacenan los datos en una base de datos etiquetados con una  identidad del usuario para facilitar la autenticación.  2. Modo Autenticación (authentication), en el que nuevamente se adquieren los datos biométricos  del usuario y el sistema los utiliza para identificar quién es el usuario, o para verificar la identidad  reclamada del usuario; la identificación comprende la comparación de la información biométrica  adquirida contra plantillas correspondientes a todos los usuarios existentes en la base de datos, y la  verificación comprende la comparación sólo con aquellos datos que corresponden a la identidad  reclamada. En consecuencia, la identificación y la verificación son dos problemas diferentes que  tienen sus propias complejidades.  Un sistema biométrico simple tiene cuatro componentes principales:  1. Módulo sensor (sensor module), encargado de la adquisión de los datos biométricos de un individuo.   2. Módulo de extracción de características (feature extraction module), en el cual se procesan los datos adquiridos para extraer los valores de la característica.  3. Módulo de correspondencia (matching module), en el que se comparan los valores de característica contra los almacenados para generar un puntaje de correspondencia.  4. Módulo de toma de decisión (decision-making module), encargado de establecer la identidad del  usuario o de aceptar/rechazar una identidad reclamada en base al puntaje de correspondencia generado en el módulo de correspondencia.  El desempeño de un sistema bimétrico se puede medir mediante reportes FAR -False Acceptance Rate-  y FRR -False Rejection Rate- en diferentes umbrales. Estos dos factores generalmente se representan en  una curva ROC -Receiver Operating Characteristic-; alternativamente, se puede graficar la tasa de  aceptación genuina con respecto a la FAR.   Tanto FAR como FRR se computan generando todos los puntajes de correspondencia genuinos y los de  impostor, y luego se establece un umbral para decidir si aceptar o rechazar una correspondencia. Se obtiene un puntaje de correspondencia genuina cuando se comparan dos vectores de característica que corresponden al mismo individuo, y se obtiene un de correspondencia de impostor cuando se comparan  vectores de característica que corresponden a dos individuos diferentes.  El desempeño de un sistema biométrico se ve fuertemente afectado por la confiabilidad del sensor que  se utilice y los grados de libertad que ofrecen las características extraídas de la señal sensada. Además,  si el rasgo biométrico sensado o medido presenta ruido (por ejemplo, una huella digital con una cicatriz  o un voz alterada por un resfrío), el puntaje de correspondencia resultante que calcule el módulo de correspondencia no será confiable. Dicho de manera simple, el puntaje de correspondencia generado por  una entrada ruidosa posee una amplia variación; este problema se puede resolver mediante la instalación  de múltiples sensores que capturen diferentes rasgos. Se espera que estos sistemas, conocidos como sistemas biométricos multimodales, resulten más confiables debido a la presencia de múltiples porciones  de evidencia; asimismo, estos sistemas son capaces de satisfacer los requerimientos severos de desempeño impuestos por algunas aplicaciones.  Los sistemas biométricos multimodales resuelven el problema de la no-universalidad, ya que es posible  que un subconjunto de usuarios no posea una biometría particular; por ejemplo, el módulo de extracción  de características de huellas digitales puede ser incapaz de extraer características de huellas digitales  asociadas con individuos específicos debido a la pobre calidad de los surcos; en tales circunstancias, resulta útil adquirir múltiples rasgos biométricos para la verificación de una identidad.  Además proveen medidas anti-falsificación, dificultándole las acciones al intruso, que debe falsificar  simultáneamente múltiples rasgos biométricos. Y al solicitarle al usuario la presentación de un subconjunto aleatorio de rasgos biométricos, el sistema asegura que un usuario “vivo” está presente en el punto  de adquisición.  Pero estos sistemas requieren de un esquema de integración para fusionar la información presentada por  las modalidades particulares.   3. RESEÑA SOBRE LA FUSIÓN DE INFORMACIÓN  Hablando en términos amplios, fusión de información  (information fusion) comprende cualquier área  que se ocupa de la utilización de una combinación de diferentes fuentes de información, ya sea para generar una formato representacional o para tomar una decisión. Esto incluye: construcción de consenso,  teoría de decisión en equipo, integración de múltiples sensores, fusión de datos multimodales, combinación de múltiples expertos/clasificadores, detección distribuida, y toma de decisiones distribuida. Los  primeros trabajos sobre la materia aparecen a principios de los años 80. [6,7,8,9].  Cuando se lo analiza desde el punto de vista de la toma de decisiones, existen varios motivos por los que  utilizar fusión de información:  • Utilización de información complementaria (por ejemplo, audio y video) pueden reducir las tasas  de error.  • Empleo de múltiples sensores (es decir, redundancia) puede incrementar la confiabilidad.  • Costo de implementación reducido por el empleo de varios sensores más baratos que un único sensor de costoso.  • Sensores físicamente separados, permitiendo la adquisión de información desde diferentes puntos  de vista.  Las personas emplean a diario la fusión de información; algunos ejemplos que se pueden mencionar  son: el uso de ambos ojos, ver y escuchar el mismo objeto, o ver y escuchar a una persona hablar (lo  cual mejora la inteligibilidad en ambientes ruidosos). Existen diferentes métodos para realizar la fusión  de información, los que se suelen dividir en varias categorías: fusión a nivel de datos de sensor, fusión a  nivel de características, fusión de puntaje, y fusión de decisión.    No obstante, resulta más intuitivo clasificarlos en tres categorías principales:  • Fusión pre-mapeo. La información se combina antes de cualquier empleo de expertos o clasificadores.  • Fusión en medio del mapeo. La información se combina durante el mapeo desde el espacio sensor-data/característica hacia el espacio opinión/decisión.  • Fusión post-mapeo. La información se combina luego del mapeo desde el espacio sensordata/característica hacia el espacio opinión/decisión (en este caso el mapeo se realiza mediante la  combinación de expertos o clasificadores en cada posible decisión).   En la fusión pre-mapeo, existen dos sub-categorías principales:  • Fusión a nivel de datos de sensor.  • Fusión a nivel de característica.  En la fusión post-mapeo, también existen dos sub-categorías principales:  • Fusión de decisión.  • Fusión de opinión, también se la denomina fusión de puntaje.  4. COMPARACIÓN DE MÉTODOS DE FUSIÓN MÁS DIFUNDIDOS EN SISTEMAS MULTIMODALES  Como se sugiere en la literatura (por ejemplo en [10,11]), los sistemas multimodales más difundidos que  hace uso de biometrías múltiples se categorizan en tres arquitecturas de acuerdo a las estrategias utilizadas  para la fusión de información (estas categorías son consistentes con las indicadas para los sistemas de fusión de información generales, que se describen en la sección anterior.):  • Fusión a nivel de extracción de características  • Fusión a nivel de puntaje de correspondencia  • Fusión a nivel de decisión      Los sistemas se clasifican de acuerdo a cuán temprano se combina la información proveniente de los diferentes sensores durante el proceso de autenticación. Las autenticación biométrica es un proceso en cadena [12], como se describe en la figura anterior. A continuación se analizan cada una de las tres arquitecturas y se analizan las actividades de investigación relacionadas con las mismas.    3.1 Fusión en el nivel de extracción de las características  En esta arquitectura, la información se extrae desde diferentes sensores, y se la codifica dentro de un  vector de característica fusionado; luego, se los compara con plantilla almacenada (la que es asímismo  un vector de característica fusionado que se encuentra almacenado en la base de datos) y se le asigna un  puntaje de correspondencia, al igual que en un sistema biométrico unimodal.  Las búsquedas bibliográficas realizadas no revelan la existencia de investigaciones significativas reciente relativas a esta método de fusión, lo que sugiere que se lo prefiere menos que los otros dos métodos.  Esto puede deberse a dos problemas que presenta:  1. los vectores de característica que se deben fusionar pueden ser incompatibles (por ejemplo, debido  a problemas numéricos) o algunos de ellos podrían no estar disponibles (por ejemplo, en casos  donde el usuario no posee todos los identificadores biométricos); en tanto el primero de los problemas se puede solucionar con un diseño más complejo del sistema, lo que conduce a un sistema  muy fuertemente acoplado, el segundo provoca problemas en la registración que ya existen en los  sistemas biométricos unimodales.   2. la generación del puntaje es problemática, ya que aún en el caso de un sistema biométrico unimodal, resulta demasidado dificultoso encontrar un buen clasificador, es decir, generar un puntaje representativo basado en la correspondencia de un vector de característica y los datos de una plantilla; cuando se trata de vectores de característica fusionados de grandes dimensiones, esto es aún  más complicado, ya que la relación entre los diferentes componentes de dicho vector fusionado  puede no ser linear [13].    3.2 Fusión en el nivel de puntaje de correspondencia  En un sistema biométrico multimodal que se construye con esta arquitectura, los vectores de característica se crean independientemente para cada sensor, y luego se comparan con las plantillas almacenadas  en forma separada para cada uno de los rasgos biométricos. En base a la proximidad del vector de carac  terística y la plantilla, cada subsistema calcula su propio puntaje de correspondencia. Finalmente, estos  valores individuales se combinan en un puntaje total que se pasa al módulo de decisión.    El flujo de proceso dentro de un subsistema es el mismo que en un sistema biométrico unimodal, lo que  permite el empleo de algoritmos ya probados para la extracción de características y la determinación de  correspondencia.  Se destacan dos informes de investigación, [11,14], en los que se incorporan en un único sistema de autenticación método de exploración de rostro, verificación de huellas digitales y exploracion de geometría de mano; en los mismos se emplean métodos bien conocidos para cada identificador; luego, se normalizan y combinan los puntajes de correspondencia para las tres modalidades utilizando alguno de los  siguientes métodos:  • Suma ponderada, calcula el promedio ponderado de los puntajes.  • Árbol de decisión, emplea para los diferentes puntajes una secuencia de comparación de umbrales  para tomar una decisión de autenticación.  • Análisis de discriminante linear, transforma los vectores de 3-dimensiones de puntajes en un nuevo sub-espacio, en el que está maximizada la separación entre los puntajes de las clases reclamante verdadero e impostor; los parámetros óptimos para esta transformación se calculan en forma anticipada en base a un conjunto de datos de entrenamiento.  En base a los resultados experimentales, las primeras conclusiones indican que el método de suma ponderada logra el mejor desempeño.  Además, se suman al sistema reglas de aprendizaje: inicialmente, se asignan las mismas ponderaciones  a cada rasgo biométrico, los cuales se modifican luego cada vez que se utilizan, a los fines de minimizar  las tasas de falsos positivos y falsos negativos.   Si bien la novedad de la estrategia que hace uso de ponderaciones específicas del usuario, resulta prometedora su aplicación para hacer frente a problemas de rasgos biométricos no-universales y de la plantilla;  si un usuario no posee cierto identificador biométrico y sólo posee características débiles, es posible  ajustar la ponderación para reducir su influencia.    3.3 Fusión en el nivel de decisión  En esta arquitectura, se toma una decisión de autenticación separada para cada rasgo biométrico; luego,  estas decisiones se combinan en un voto final.    La fusión en el nivel de decisión resulta en una arquitectura de sistema débilmente acoplado, en la que  cada subsistema se ejecuta como un sistema biométrico unimodal, lo que hace que resulte muy atractivo  para los fabricantes que muchas veces lo presentan bajo la denominación de “biometría en capas”, concepto que se encuentra respaldado por la aparición de estándares biométricos tales como BioAPI [15].  Existen variadas estrategias para combinar diferentes decisiones en una decisión de autenticación final,  las que van desde mayoría de votos hasta métodos estadísticos más sofisticados [13].  Tomando como ejemplo a BioNetrix Authentication Suite, se tiene la siguiente combinación de estrategias (en [16] se incluye una lista muy completa de posibles combinaciones alternativas):  • Operador AND, requiere de una decisión positiva de todos los módulos de verificación;  • Operador OR, intenta autenticar al usuario utilizando un rasgo biométrico; si falla, ofrece otro intento con otro módulos de verificación;   • Operador RANDOM, selecciona aleatoriamente un rasgo biométrico; si bien se trata de una idea  muy simplista, hace mucho más difícil engañar al sistema.  La fusión en el nivel de decisión es una etapa muy tardía del proceso de autenticación, por lo que se presume que no presenta el mismo potencial de mejora del desempeño del sistema global como la fusión en  el nivel de puntaje de correspondencia.   5. SISTEMAS QUE EMPLEAN RASGOS AUDIO-VISUALES  A continuación se reseñan brevemente las principales contribuciones realizadas en este campo, tanto en lo  que hace a la identificación como la verificación de identidades de personas. Se distinguen dos categorías  principales de métodos: no-adaptativos y adaptativos. En el primero de los métodos, la contribución de cada experto se establece a priori, mientras que en el segundo, la contribución de al menos un experto varía  de acuerdo a su confiabilidad y capacidad de discriminación en presencia de alguna condición ambiental  (por ejemplo, la contribución de un experto en habla se decrementa cuando baja la SNR -Signal Noise Ratio- del audio).  5.1 Métodos no-adaptativos  La fusión de información de audio y visual se ha aplicado al reconocimiento automatizado de personas  desde las primeras propuestas de sistemas multimodales [17,18,2].   En [17], se combina información de imágenes de rostros y grabaciones de habla empleando fusión de  suma ponderada:   2211 owowf +=   donde o1 y o2 son las opiniones de los expertos de rostro y de habla, respectivamente, con sus correspondientes ponderaciones, w1 y w2. Cada opinión refleja la probabilidad de que un reclamante sea el reclamante verdadero (es decir que una opinión baja sugiere que el reclamante es un impostor, en tanto  que una opinión alta sugiere que el reclamente es el reclamente verdadero). Debido a la restricción sobre  las ponderaciones, 12 1 =∑ =i i w , la ecuación anterior se reduce a:  2111 )1( owowf −+=   La verificación de la decisión se logra estableciendo umbrales de la opinión fusionada. Los resultados obtenidos de EER -Equal Error Rate- al emplear un único experto (habla 3.4%, rostro 3.0%) son significativamente superiores a los que se obtienen con el empleo de ponderaciones óptimas y umbrales (1,5%).  En [18] se combinan las opiniones de un experto de rostro (el que hace uso de características obtenidas a partir de imágenes estáticas frontales) y de un experto de habla, y se emplea el método de producto ponderado:  )1( 21 11 )()( ww oof −×=   Cuando el experto de habla se utiliza solo (es decir, w1=1), se obtiene una tasa de identificación del  51%, mientras que cuando se emplea el experto de rostro solo (es decir, w1=0), se obtiene una tasa de  identificación del 92%; y utilizando una ponderación óptima, la tasa de identificación llega al 95%.  En [2] se emplean para la identificación de personas dos expertos de habla (para características estáticas  y delta) y tres expertos en rostro (para las área de ojos, nariz y boca), utilizando el método de producto  ponderado para la fusión de opiniones, donde las ponderaciones se determinaban en base a una heurística.  Con los expertos estático y dinámico, se obtienen tasas de identificación del 77% y 71%, respectivamente;  combinando los dos expertos de habla, este valor se incrementa al 88%. Con los expertos de rostro, se obtienen tasas de identificación del 80%, 77% y 83%, respectivamente; combinándolos, la tasa se incrementa  al 91%. Cuando se combinan los cinco expertos, la tasa de identificación se incrementa al 98%.  En [3] se emplean tres expertos (de rostro frontal, de imagen dinámica de labios y de habla dependiente  del texto), con un esquema de fusión híbrida en el que intervienen mayoría de votos y fusión de opinión;  dos de los expertos deben acordar respecto de la decisión, y la opinión combinada tiene que exceder un  umbral preestablecido. Este esquema presenta un mejor desempeño que cuando se utilizan dichos expertos en forma individual.  En [19] se emplea un experto de rostro frontal, que proporciona una opinión para cada una de las imágenes; cuando se utilizan múltiples imágenes de una persona para generar múltiples opiniones, éstas se  fusionan mediante diferentes esquemas (entre los que se incluyen un caso especial de fusión por suma  ponderada). De demuestra una reducción en las tasas de errores del 40%, y que las ganancias en el desempeño se tienden a saturar luego de utilizar cinco imágenes; estos resultados sugieren que el uso de  una secuencia de video del rostro, en lugar de una imagen, provee un desempeño superior.  En [20] se intenta proporcionar fundamentos teóricos a los métodos más comunes de fusión, tales como  métodos de suma y producto; sin embargo, los autores admiten que los supuestos utilizados “no son realistas para la mayoría de las aplicaciones”. Los resultados experimentales para la combinación de tres  expertos (dos de rostro -frontal y perfil- y uno de habla dependiente del texto) demuestran que el método  de suma supera al de producto. En [21] se investiga la combinación de información de audio (habla) y  visual (labios) mediante concatenación de vector de característica. A fin de hacer corresponder las tasas  de tramas de ambas características, se extrae la información de habla a una tasa de 30 fps en lugar de los  100 fps tradicionales. En la configuracion dependiente del texto, el proceso de fusión presenta una mejora menor en el desempeño; sin embargo, en la configuración independiente del texto, el desempeño disminuye ligeramente, y se sugiere que el método concatenación de vector de característica es poco fiable.  En [22,23] se emplea una forma de fusión de suma ponderada para combinar dos expertos de opiniones: un  experto en habla dependiente del texto y un experto en labios dependiente del texto. Utilizando una ponderación óptima, la fusión conduce a un mejor desempeño frente respecto del uso de dichos expertos en forma  independiente.  En [24] se utiliza un experto de huellas digitales y un experto de rostro frontal, y se emplea un esquema  de fusión híbrida que comprende fusión de lista ordenada y fusión de opinión: las opiniones del experto  de rostro correspondientes a n identidades se combinan con las opiniones del experto de huella digital  para las identidades correspondientes utilizando una forma del método de producto. Se utiliza este método híbrido a los fines de tener en cuenta la relativa complejidad computacional del experto de huellas  digitales (significativamente más lento). Se demuestra que, en todos los casos testeados, la fusión presenta un mejor desempeño que cuando se emplean cualquier de los expertos solos.  En [25] se propone el uso de un post-clasificador bayesiano para alcanzar la decisión de verificación;  formalmente, la regla se decisión se expresa como:     > = ∏∏ == otherwiseC opopifC class EE N i impii N i trueii 2 1 ,1 ,1 )()( λλ   donde C1 y C2 son las clases reclamante verdadero e impostor, respectivamente, NE es el número de expertos, en tanto que λi,true y λi,imp son, para el i-ésimo experto, los modelos paramétricos de la distribución  de opiniones para el reclamante verdadero y el impostor, respectivamente. Debido a problemas de precisión en una implementación computacional, resulta más conveniente el empleo de una suma en lugar de  series de multiplicaciones, y dado que la función logarítmica es una función monótona creciente, se  puede modificar la regla de decisión de la siguiente manera:     − = ∑∑ == otherwiseC opopifC class EE N i impii N i trueii 2 1 ,1 ,1 )(log)(log λλ   La regla de decisión anterior, en la práctica, se modifica introduciendo un umbral a fin de permitir el  ajuste de FAR y de FRR:     >− = ∑∑ == otherwiseC topopifC class EE N i impii N i trueii 2 1 ,1 ,1 )(log)(log λλ   Además, se utilizan tres expertos, observándose que el uso del clasificador anterior (con distribuciones  Beta) proporciona menores tasas de error que cuando se utilizan los expertos solos.  Los clasificadores que se investigaron son: SVM (Support Vector Machine), clasificador bayasiano (utilizando distribuciones Beta), Discriminante Linear de Fisher, Árbol de Decisión y Percepton Multicapa;  en cuanto a los expertos, se emplearon tres: un experto de rostro frontal, y dos expertos de habla (dependiente e independiente del texto). Se determina que el clasificador SVM y el bayasiano presenta los  mejores resultados.  En [26] también investigan, para la fusión de opinión, varios clasificadores binarios y los métodos de  fusión mayoría de votos y operadores AND y OR (lo que lleva a la categoría de fusión de decisión). Se  utilizan tres expertos: experto de rostro frontal, experto de rostro de perfil y experto de habla independiente del texto. En el caso de fusión de decisión, cada experto actua como un clasificar, que provee una  decisión “dura” en lugar de una opinión. Los clasificadores que se investigan son: Árbol de Decisión,  Percepton Multicapa, clasificador basado en Logistic Regression, clasificador bayasiano utilizando distribuciones gaussianas, Discriminante Linear de Fisher, y varias formas del clasificador k-Nearest  Neighbour. Se determinó que el clasificador basado en Logistic Regression proporciona la tasa de errores más baja y que resulta el más fácil de entrenar.   En [27] se utiliza el método suma ponderada para combinar las opiniones de un experto de habla y un  experto de labios (ambos independientes del texto); el desempeño del primero se disminuye deliberadamente variando las cantidad de ruido blanco en los datos de habla. Los resultados experimentales demuestran que si bien el desempeño del sistema siempre es mejor que cuando se emplea sólo el experto  de habla, el mismo disminuye a medida que se incrementa el nivel de ruido. De acuerdo a los valores de  ponderación (que se seleccionan previamente), el desempeño con altos niveles de ruido son realmente  peores que cuando se utiliza el experto de labios solo.   Se propone un método basado en estadísticas para la selección de las ponderaciones, que da por resultado un buen desempeño bajo condiciones limpias, y nunca cae por debajo del desempeño de un experto  de labios en condiciones ruidosas; sin embargo, el desempeño bajo condiciones ruidosas no fue óptimo.   La ponderación para el experto de habla se calcula de la siguiente manera:  21 2 1 ςς ς + =w   donde   imp impi true truei i NN 2 , 2 , σσς +=   siendo, para el i-ésimo experto, ζi el error estándar de la diferencia entre las medias µi,true y µi,imp de las  opiniones para el reclamos verdadero e impostor, σ2i,true y σ2i,imp las correspodientes varianzas, y Ntrue y  Nimp  el número de opiniones para los reclamos verdadero e impostor, respectivamente.   Se asume que el error estandar representa la indicación relativa de la capacidad de discriminación de un  experto; cuanto menor variación exista en las opiniones para reclamos conocidos, menor será el error estandar; y, en consecuencia, un error estandar bajo indica un mejor desempeño.  En [28] se evalúan variaciones del Multi-Stream Hidden Markov Models -MS-HMMs-, una forma de  fusión en medio del mapeo, en la tarea de identificación de una persona por medios audio-visuales dependientes del texto. El flujo de audio consta de una secuencia de vectores que contienen Mel Frecuency Cepstral Coefficients -MFCCs- [29] y sus deltas [30], en tanto que el flujo de video consta de una secuencia de vectores de característica que describen el contorno de los labios. Debido a la naturaleza de  la implementación MS-HMM, la tasa de tramas de características de video debe concordar con la tasa  de tramas de caracterísiticas de audio. Se realizan pruebas utilizando una pequeña base de datos audiovisuales, las que demuestran que para altos SRNs, el desempeño es comparable con un sistema que sólo  emplea HMM de audio, mientras que con bajos SRNs, el sistema multi-flujo presenta un desempeño  significativamente superior al sistema que sólo utiliza audio y excede al desempeño del sistema que sólo  empleo video. Este trabajo no incluye una comparación con los sistemas que emplean fusión pre-mapeo  o post-mapeo, por ejemplo, utilizando dos expertos diferentes y fusión de opinión.  En [31] se resuelven varias limitaciones existentes en los sistemas MS-HMM previos, permitiendo que  los dos flujos se encuentren desincronizados en el tiempo (debido a que los eventos relacionados con los  flujos pueden comenzar y/o finalizar en puntos diferentes) y que presenten diferentes tasas de tramas. Se  realizan pruebas sobre una pequeña base de datos audio-visual, y se emplean dos flujos de características similares a los descriptos en [28]; se observa que para SNRs relativamente altos, el desempeño es  peor que cuando se emplea un sistema de audio dependiente del texto, mientras que para SNRs menores  se mejora el desempeño (y el sistema resulta más robusto) que un sistema HMM dependiente de texto  que emplea concatenación de vector de característica.  5.2 Métodos adaptativos  En [32] se extiende el trabajo presentado en [27] al proponer un método heurístico para ajustar las ponderaciones; los resultados experimentales muestran que, si bien decrese significativamente el desempeño a medida que se incrementa el nivel de ruido, siempre resulta mejor que utilizar solamente el experto  de habla; sin embargo, se observa que con niveles altos de ruido, el empleo de ponderaciones iguales  (no-adaptativo) ofrece un mejor desempeño. Una desventaja importante del método es que el cálculo de  las ponderaciones demanda encontrar la opinión del experto de habla para todos los reclamos posibles  (es decir, todas las personas registradas en el sistema), limitando de esta manera la solución a sistemas  que poseen un número reducido de clientes debido a consideraciones prácticas (es decir, el tiempo que  demanda verificar un reclamo). Es más, según se describe en [27], se observan limitaciones similares en  ambientes experimentales.  En [33], los autores proponen otra técnica heurística para el ajuste de las ponderaciones; en una configuración dependiente del texto, el sistema presenta un desempeño siempre superior al que se tiene utilizando solamente el experto de labios; sin embargo, en una configuración independiente del texto, bajo  condiciones de SNR bajo, el desempeño fue peor que cuando se utiliza sólo el experto de labios.  La ponderación para el experto de habla se calcula de la siguiente manera:     +    + = 21 1 21 2 1 κκ κ ςς ς w  donde  21 2 ςς ς +  se calcula según la ecuación ya indicada durante la etapa de entrenamiento y  truei impiitrueii i oMoM , ,, )()( µ κ − =    se calcula durante el testeo; para el experto i-ésimo,  2 , 2 , , )()( truei trueii trueii o oM σ µ− =  es la distancia unidimensional cuadrática Mahalanobis entre oi y el modelo de opiniones para los reclamantes verdaderos; además,  µi,true y σ2i,true son, respectivamente, la media y la varianza de las oponiones para reclamantes verdaderos,  los que se determinan durante la fase de entrenamiento.  De manera similar  2 , 2 , , )()( impi impii impii o oM σ µ− =  es la distancia unidimensional cuadrática Mahalanobis entre la  opinión oi y el modelo de opiniones de los impostores; acá, µi,imp y σ2i,imp son la media y la varianza de  las opiniones para impostor, respectivamente, se los determina durante la etapa de entrenamiento.  Bajo condiciones limpias, la distancia entre una opinión dada para un reclamante verdadero y el modelo  de opiniones correspondiente debe ser pequeña; de manera similar, la distancia para un reclamante verdadero y el modelo de opiniones para los impostor debería ser grande. Lo inverso se aplica a una opinión dada para un impostor; por ello, bajo condiciones limpias, κi debe ser grande. Se emplea una evidencia empírica para argumentar que bajo condiciones ruidosas, las distancias deben disminuir y por  ello κi debe también disminuir.  En [34] se propone el siguiente método de ajuste de la ponderación; cada vez que se graba habla, generalmente la declaración está precedida por un breve segmento que sólo contiene ruido ambiental; a partir  de cada declaración de entrenamiento, se utilizan los MFCCs [35,36] obtenidos del segmento de ruido  para construir un GMM de ruido global, λnoise; dado un testeo de habla grabada, se emplean los vectores  de característica MFFC Nnoise { } noiseNiix 1= , representando al segmento de ruido, para estimar la calidad de la  declaración mediante la medición del desajuste respecto de λnoise de la siguiente manera   ∑ = = noiseN i noisei noise xp N q 1 )(log1 λ   Cuanto mayor sea la diferencia entre las condiciones de entrenamiento y de testeo, menor ha de resultar  q; entonces, q se mapea con un valor comprendido en el intervalo [0,1] utilizando una curva sigmoidal,  donde a y b describen la forma de la curva:  [ ])(exp1 1 bqa qmap −−+ =   estos valores se seleccionan en forma manual de tal manera que qmap sea próximo a 1 para declaraciones  de entrenamiento limpias, y próximo a 0 para declaraciones de entranamiento corrompidas artificialmente  con ruido.  Si se asume que el experto de rostro es el primer experto y que el de habla, el segundo, dada una ponderación previa w2,prior para el experto de habla (que se determina sobre datos limpios), la ponderación  adaptada para el experto de habla se calcula de la siguiente manera: w2 = qmap w2,prior . Dado que se está  utilizando un sistema de dos modalidades, la ponderación correspondiente para el experto de rostro se  encuentra utilizando w1 = 1 - w2. Este método de ajuste de ponderaciones se denomina detección de  desajuste.  6. CONCLUSIONES  En la actualidad, existe un fuerte consenso entre los investigadores y la industria que la tecnología multimodal será la piedra angular en el empleo masivo de la biometría en los campos de la identificación/verificación de personas. En este trabajo se han reseñado diferentes métodos de abordaje de sistemas  biométricos multimodales, entre los que se destacan interesantes intentos por atemperar algunos de los  problemas que aún hoy no se ha sido posible eliminar en los sistemas biométricos tradicionales; de estos  intentos, los más promisorios aparentan ser los que utilizan fusión de información en el nivel de puntaje de  correspondencia y que, además, incluyen ponderaciones asociadas a usuarios (o grupos) particulares así  como umbrales tales como los propuestos en [11].   Y como ya se planteara anteriormente, resulta evidente que la adquisición de múltiples identificadores biométricos dificulta significativamente las acciones que debe realizar un impostor para engañar al sistema de  identificación/verificación, ya que debe presentar múltiples muestras coordinadas creadas artificialmente.  Sin embargo, todos estos beneficios no se logran sin algún tipo de cargo, ya que estos sistemas son menos  costosos, y presentan efectos significativos sobre sus usuarios, pudiendo resultando en una baja aceptación, en particular en lo que hace a cuestiones de privacidad y al inconveniente derivado de la adquisición  multinivel de datos.  Muchas de las arquitecturas más prometedoras hoy aún se encuentran en un estadio experimental. Y las  tecnologías ya disponibles poseen arquitecturas multicapas, con un acoplamiento débil entre los diferentes  subsistemas, a tal punto que algunos casos presentan diferentes interfaces de usuario.   Por ello, hoy día la industria y, muy particularmente aquellos actores que aguardan que esta tecnología  aportes significativos que impulsen la masividad de sistemas de información de seguridad crítica (gobiernos, salud, bancos, etc.), demandan de los investigadores y fabricantes la aparición de soluciones verdaderamente integradas y altamente confiables y, que al mismo tiempo, mejoren la facilidad de uso (más allá  del empleo de múltiples identificadores biométricos).	﻿biometría , biometría multimodal , seguridad de la información	es	21580
1	Detección de plagio intrínseco usando la segmentación de texto	﻿La detección de plagio intrínseco utiliza la estilografía para delimitar secciones de un documento, que se sospecha que fueron escritas por un autor diferente. En este trabajo, se analiza si la segmentación de texto es una opción viable como estrategia de descomposición de texto y se implementa un algoritmo simple para la detección de outliers, una de las componentes básicas en este tipo de tareas. Se provee además de un ambiente integrado de ejecución que permite ingresar un texto en inglés, y muestra las secciones de ese texto que exiben un estilo de escritura distinto al autor del texto. El algoritmo de detección fue testeado en el corpus de la Competencia de Detección de Plagio Pan 2009, donde se obtienen resultados comparables a los obtenidos con otros algoritmos representativos del estado del arte en el área. Palabras Claves: detección de plagio intrínseco, estilografía, segmentación de texto, índices de legibilidad. 1. Introducción Cuando se tiene un texto cuya autoría, una persona se atribuye sin serle propia, estamos en presencia de plagio [9]. El plagio está presente en distintas áreas como la música, la política, el procesamiento de imágenes y la literatura [2]. También es un problema que deben enfrentar las empresas ya que la competencia puede plagiar su inovación [1]. Esta actividad, se ve favorecida por la gran cantidad de información disponible hoy en día, y a que la Web ha facilitado su acceso suministrando buscadores, enciclopedias online (Wikipedia), sitios con monografías, etc. En muchos casos, un autor comete plagio de texto por desconocer cómo se debe realizar correctamente la cita y su paráfrasis. En el caso que un escritor utilice una frase exacta, debe colocarla entre comillas y cuando se parafrasea un texto, se debe incorporar la fuente con su cita bibliográca. La detección de plagio no es una tarea sencilla, debido a que un texto se puede reescribir de forma tal que es muy dicultoso encontrar similitudes ente ellos. La detección de plagio puede ser caracterizada como extrínseca e intrínseca. La detección extrínseca usa una colección de documentos como referencia y tiene CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 91 2 Dario G. Funez and Marcelo L. Errecalde como desventaja su gran costo computacional, ya que requiere buscar el plagio en cada uno de los archivos de dicha colección. La detección que analiza un texto sin tener en cuenta un potencial archivo fuente, se denomina intrínseca y es el tipo de detección en que nos centraremos en este trabajo. Este tipo de detección no detecta plagio en el sentido estricto de la palabra, es decir, no suministra el documento de donde se extrajo la información y solamente provee los pasajes de texto que se sospecha de plagio. La simple lectura de un texto, nos puede indicar sobre la presencia de distintos estilos de escritura, y por lo tanto que otros autores constribuyeron en el texto sin mencionarlos. Para la detección intrínseca, en [9] se sugiere seguir las siguientes etapas en secuencia: 1) selección de una estrategia de descomposición del texto, 2) denición del modelo estilográco que comprende la denición de las medidas estilográcas que serán recuperadas del texto y 3) la identicación de los outliers que discrimina las secciones plagiadas de las no plagiadas. Con respecto a la manera en que se va a descomponer el texto, en [9] se considera que la detección de límites por tópicos o segmentación de texto, puede dar buenos resultados pero destacan la dicultad de la misma. La segmentación de texto es utilizada en el procesamiento del lenguaje natural, para textos que no tienen límites estructurales como capítulos, párrafos o secciones [7]. La esencia de este algoritmo es que detecta cambios de vocabulario, los cuales son muy importantes para la detección de plagio. Un cambio abrupto de vocabulario, nos informa sobre un posible estilo de escritura distinto. De acuerdo a nuestro conocimiento, no se han realizado hasta el momento experiencias concretas con este tipo de técnicas en la detección de plagio intrínseco. Nuestra contribución en este trabajo se enfoca en este último aspecto, realizando una primera experiencia en el uso de una estrategia de segmentación particular propuesta por Freddy Choi [4] para la etapa de descomposición de texto. Además, se provee un ambiente de ejecución amigable que soporta todas las etapas involucradas en la detección de plagio intrínseco con este enfoque. Los resultados preliminares obtenidos con el corpus Pan09 muestran que este enfoque es una alternativa competitiva, con respecto a otros algoritmos representativos del estado del arte en el área. El resto del trabajo se organiza de la siguiente manera. En la sección 2 se describe la detección intrínseca de plagio. La sección 3 explica el algoritmo de segmentación de texto empleado en este trabajo. En la sección 4 se detalla la implementación de los módulos que componen el ambiente. En la sección 5 se describen los experimentos realizados y el análisis de los resultados. Finalmente, en la sección 6 se ofrecen las conclusiones y el trabajo futuro. 2. Detección de Plagio Intrínseco Se comete plagio cuando una persona se atribuye como propio un trabajo o idea sin reconocer la propiedad intelectual del autor legítimo [3]. El plagio de texto ocurre cuando un texto utiliza secciones de texto de otro autor, sin proporcionar la fuente [2]. Un caso muy frecuente es el uso del cut-and-paste que CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 92 Detección de plagio intrínseco usando la segmentación de texto 3 incluye un fragmento sin modicaciones, siendo este tipo de plagio el más fácil de detectar. La tarea de detección se diculta cuando se oculta un texto, de tal manera que es difícil encontrar similitud entre ellos. Los trucos más usados son utilizar sinónimos, cambiar el orden de las oraciones, adicionar palabras, etc. Para poder armar que un texto ha incurrido en plagio, se debe proporcionar la sección plagiada y la sección del documento fuente. El detector debe realizar comparaciones del texto sospechoso con todos los posibles documentos. Como este conjunto puede ser impráctico de procesar y existe información que no está disponible en formato digital, es que existen métodos de detección que solamente necesitan del texto a chequear, pero no exhiben la prueba del plagio. Este método, denominado intrínseco, analiza un texto t de un único autor y devuelve las secciones de t redactadas por otros autores [9]. El fundamento del análisis intrínceso es que un escritor mantiene su estilo de escritura en todo el texto. Para tal n se realiza un análisis de estilo, que extrae información que no es tan evidente en el texto [3]. El análisis de estilo se basa en el hecho que cada autor tiene un estilo de escritura personal, que no sufre modicaciones en un texto de su autoría y que es difícil de describir, pero puede ser representado por medidas estilográcas que se expondrán más adelante. En el análisis intrínseco se cuenta con la información de estilo de autor del texto, es decir, un fragmento de texto se lo puede clasicar como perteneciente a él o no. Este es un problema de clasicación de una única clase, En estos problemas, se cuenta con una clase objetivo y todos aquellos objetos que no pertenecen a la clase objetivo son denominados outliers [11]. Para la detección intrínseca, se necesitan denir los siguientes componentes del proceso de vericación de plagio [9]: 1) la estrategia de descomposición, 2) la construcción del modelo de estilo y 3) la identicación de outliers. Cada una de ellas, se describe brevemente en las siguientes subsecciones. 2.1. Estrategia de descomposición La estrategia más sencilla y rápida es dividir el documento en secciones de igual longitud. En [5] se elige un tamaño de entre 40 y 200 palabras para delimitar una sección. Una alternativa, es descomponer el texto empleando algún límite estructural como los capítulos, secciones, párrafos, etc. Estos pueden ser complementados utilizando como limitadores objetos del texto como tablas, notas al pie, referencias bibliográcas etc. Otra posibilidad es dividir el texto por tópicos, utilizando algún algoritmo de segmentación de texto. Esta última alternativa, utilizada en el presente trabajo, se describe con mayor detalle en la sección 3. 2.2. Construcción del modelo de estilo La estilografía es el estudio estadístico de un estilo de escritura [9]. Un escritor utiliza el mismo patrón para la redacción de oraciones y tiene un vocabulario especíco que depende de su formación personal y esto se puede caracterizar con CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 93 4 Dario G. Funez and Marcelo L. Errecalde un modelo. Existen distintas medidas de características lingüisticas para modelar un estilo de escritura que corresponden a alguna de la siguientes categorías [6]: Estadísticas del texto: Se basan en características léxicas, a nivel de palabra y/o caracter. Calculan por ejemplo, las frecuencias de caracteres especiales, como símbolos de puntuación y palabras en el texto. Las siguientes medidas corresponden a esta categoría: • Promedio de la clase de frecuencia de las palabras: Las palabras son agrupadas por clases, determinadas por su frecuencia de aparición en un texto (función f), determinándose la clase de una palabra ? de acuerdo a la siguiente fórmula [6]: c(?) = blog 2 (f(??)/f(?))c, donde ?? es la palabra de mayor frecuencia y b c denotan la función piso. El promedio es calculado teniendo en cuenta la cantidad de las palabras de cada clase. Esta medida contiene información sobre la complejidad y la riqueza del vocabulario y se ha demostrado que es robusta y no depende del tamaño del texto para el cual se aplica. • Función R: Esta medida se relaciona directamente con el vocabulario del escritor y se dene como [2]: R = (100×log(CP ? ))/CP ? 2, donde CP ? es el total de palabras en el texto. • Función K: Tiene el mismo objetivo que la función R, asumiendo en este caso una distribución Poisson en la frecuencia de las palabras, utilizando en este caso la fórmula [1]: K = 104× ( ∑ ∞ i=1 ?2v i −CP ? )/CP ? 2, donde v i denota la cantidad de palabras con frecuencia ?. Características sintáticas: Estas medidas capturan estilos de escritura a nivel de sentencias como por ejemplo el promedio de oraciones cortas o el promedio de oraciones que comienzan con pronombres interrogativo (por ejemplo what). Otro ejemplo es el uso del promedio de sentencias en voz pasiva. Características Part-of-Speech: Comprende la frecuencia de clases de palabras como adjetivos, pronombres, sustantivos, adverbios etc. Características estructurales: Contienen información a nivel de la organización del texto (usos de saludos, longitud de párrafos y/o capítulos, etc.). Indices de legibilidad : Estiman el grado de comprensibilidad requerido para el entendimiento de un texto. La denición de estos índices se basan en distintas medidas calculadas en un fragmento del texto: COr?, la cantidad de oraciones, CS? , el total de sílabas y CL?n es el total de números y letras. Algunos de los índices más empleados son: • Indice de Flesh: Utilizado en textos en general, toma todo el rango de valores entre 0 y 100. Los valores más bajos signican que el texto es más difícil de comprender. La fórmula de Flesh se expresa como: ? F = 206, 3? − (1,01? × CP ? )/COr?− (4 ,6 × CS? )/CP ? . • Indice de Flesh-Kincaid: Este índice, modicación del anterior, es comúnmente aplicado a textos técnicos e indica un grado que estima la cantidad de años (edad) requerida para entender el texto. Su fórmula es: ? ? = (0,3? × CP ? )/COr? + (11,? × CS? )/CP ? − 1?, ? • Índice de Coleman-Liau: Toma valores entre 0,4 y 16,3 y se calcula como: ? C = (?,? × CL?n)/CP ? − (30,0 × COr?)/CP ? − 1?,? CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 94 Detección de plagio intrínseco usando la segmentación de texto 5 2.3. Identicación de outliers En las fases anteriores, se obtiene un texto divido en n secciones ? 1 . . . ? n sobre el que luego se computa, en cada una de ellas, el modelo denido y se produce como salida n vectores de características, cuya dimensión depende de la cantidad de medidas utilizadas [9]. La única información con que se cuenta es el estilo de escritura del escritor en cuestión, se conoce sólo esta clase, planteándose de esta manera un problema de clasicación de una clase. En estos casos se caracteriza los elementos objetivos, de tal manera que se pueda distinguir si un nuevo elemento pertenece o no a dicha clase objetivo. La detección de outliers es un problema con estas características. La tarea en este caso es detectar aquellos objetos que no se asemejan a un conjunto de objetos predeterminados, es decir detectar los outliers [11]. En el contexto de nuestro trabajo, serán considerados como outlier aquellas secciones cuyos vectores presenten una alta variabilidad en las medidas respecto a la clase objetivo. Entre los métodos más difundidos para resolver problemas con estas características, podemos mencionar: Métodos de densidad : Aproximan la función de densidad de probabilidad de la clase objetivo. Se considera a los outliers uniformente distribuidos y la regla de Bayes se puede utilizar para diferenciar objetos outliers de los objetivos. Proveen buenos resultados con tamaños de muestra grandes. Métodos de límite: Tratan de delimitar una región utilizando distancias entre los elementos objetivos. Los outliers son aquellos objetos que no están comprendidos en esa región. Métodos de Reconstrucción: Necesitan conocimiento previo sobre la generación de los elementos objetivos. Los outlier son aquellos objetos que son difícil de reconstruir. Método basado en la MEDA: La MEDA se dene como la mediana del conjunto de las diferencias absolutas entre cada medida ? i y su media ?, en donde el n es la cantidad de medidas y se expresa de la siguiente manera: M??A = med?n? [ ? (? 1 −?), ..., ? (? n −?)]. Se conoce que el 50% de las medidas están contenidas en el siguiente intervalo: [?−M??A, ?+M??A]. Una medida ? i se puede clasicar como outlier si ? i /∈ [?−4,?∗M??A, ?+ 4,? ∗M??A]. Una sección se la clasica como outlier si contiene un número mínimo de medidas outliers. En la siguiente subsección, se denen las medidas de evaluación generalmente empleadas para cuanticar la performance de un detector de plagio. 2.4. Medidas de evaluación Para evaluar el comportamiento de un algoritmo de detección de plagio, se deben computar la precisión (en inglés precision), la cobertura (recall) y la granularidad de las detecciones realizadas. En la denición de estas medidas seguiremos las siguientes convenciones de notación: 1) ? representa una sección plagiada del conjunto S de todas las secciones plagiadas, 2) r denota una sección detectada del conjunto R de detecciones, 3) S R son las secciones plagiadas que han sido CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 95 6 Dario G. Funez and Marcelo L. Errecalde detectadas, 4) | ? i | y | r i | denotan el tamaño (en cantidad de caracteres) de la sección correspondiente y | S | y | R | denotan la cardinalidad de los conjuntos respectivos 1 . Finalmente, α(? i ) es la cantidad de caracteres detectados de ? i , β(r i ) es la cantidad de caracteres plagiados de r i y γ(? i ) es la cantidad de caracteres plagiados detectados de ? i . En base a estos valores, la precisión, cobertura, granularidad y evaluación global (overall), se denen de la siguiente manera 2 : rec? = 1/ | S | ∑ |S| i=1 α(? i )/ | ? i |, prec?on = 1/ | R | ∑ |R| i=1 β(r i )/ | r i |, grnu?r dd = 1/ | S R | ∑ |S R | i=1 γ(? i ), y over? = F/(log 2 (1 + grnu?r dd )) Estas medidas se interpretan de la siguiente manera. La precisión cuantica el porcentaje de detecciones correctas, el recall el porcentaje de plagio detectado, una granularidad cercana a 1 signica que el algoritmo detectará cada plagio a lo sumo una vez. En todos los casos, valores cercanos a 1 indican que el algoritmo de detección tiene buena performance. 3. Segmentación de texto La segmentación de texto divide un texto en unidades con el mismo tópico [4]. La implementación de Freddy Choi es realizada en dos fases sobre el texto completo. En la primer etapa las stops words (artículos, preposiciones, conectores etc.) son removidas ya que no aportan información relevante del texto. La raíz de cada palabra se obtiene mediante un algoritmo de stemming y se almacena su frecuencia en el texto en un vector. Cada oración tiene asociada un vector y la frecuencia de la palabra ? en la oración ? se denota como f i,j . La matriz S resultante de aplicar la similitud coseno a cada par de vectores es llamada matriz de similitud [4]. Dado que no es sencillo determinar los límites de los segmentos directamente sobre S, esta matriz es sometida a un proceso de ranking que obtiene una nueva matriz S′ a partir de S, denominada matriz de rango. Cada elemento (valor) de la matriz S′ resulta de desplazar una máscara (matriz cuadrada) sobre S. Cada valor r en S′ se determina en base al conjunto de valores que cubre la máscara en S (?) y al valor central de la máscara en S (c). La fórmula para obtener r es: r = ?v?ue/ |? |, donde ?v?ue es el número de elementos en ? con menor similitud que c. La última etapa del algoritmo de segmentación, utiliza los valores obtenidos en S′ y aplica un método de clustering divisivo, basado en el algoritmo de maximización de Reynar [8] para detectar los límites de los segmentos. Este algoritmo se basa en el concepto de densidad interna donde, dado un segmento delimitado por las sentencias ? y ? (inclusive); si ? i,j es la suma de los valores rango de las sentencias en el segmento y ? i,j es el área interna que abarca el segmento dada por la fórmula: ? i,j = (? − ? + 1)2. Si B = ? 1 . . . ? m es una lista de m segmentos coherentes y ? ? y ? ? denotan la suma de valores rango y área respectivamente, 1 Información obtenida de: http://www.uni-weimar.de/medien/webis/research/workshopseries/pan10/task1-plagiarism-detection.html. 2 En la evaluación global, ? reere a la tradicional medida ? , la media harmónica de precisión y recall: ? ? 2× (?eiio? × ?e? ? )/(?eiio? + ?e? ? ) CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 96 Detección de plagio intrínseco usando la segmentación de texto 7 correspondiente al segmento ? en B, la densidad interna de B se dene como: ? = ∑ m i=1 ? ? / ∑ m i=1 ? ? . El proceso comienza inicializando B con un único segmento que representa todo el documento. Cada paso del algoritmo separa uno de los segmentos en B y el punto de corte se elige de tal manera que maximiza ?. La cantidad de segmentos m se determina de forma automática y queda establecida cuando el gradiente tiene variaciones inusuales. Si ?(n), es la densidad interna de n segmentos, el gradiente se dene como: δ?(n) = ?(n) −?(n−1). Para un documento con ? límites potenciales, si u,v denotan la media y varianza de δ?(n) con n ∈ 2, . . . , ? + 1, elm queda denido al aplicar el threshold u + ? √ v a δd. A menudo, un valor de ? = 1, ? es utilizado en la práctica.3 4. Arquitectura del ambiente El ambiente está compuesto por los módulos que se muestran en la Figura 1: Figura 1. Módulos que conforman el ambiente Segmentador : Este módulo descompone el texto de entrada en una secuencia de segmentos cohesivos ? i , donde cada ? i puede tener distinta cantidad de oraciones y constituye una unidad indivisible. Utiliza la segmentación de texto explicada en la sección 3 e implementada por la libreria Morphadorner. 4 La salida del segmentador es una secuencia de secciones ? 1 . . . ? n . Modelador estilográco: Este módulo recibe como entrada la secuencia de secciones de texto del segmentador y, para cada sección, se obtienen las medidas estilográcas que componen el modelo. El modelo, en este caso incluye las siguientes medidas: promedio de sentencias pasivas, índices de legibilidad (Kincaid, Flesh y Coleman-Liau), promedio de clase de palabras, riqueza de vocabulario (funciones R y K), promedio de cada clase de palabras (sustantivos, adjetivos, adverbios) y cantidad de símbolos de puntuación. Morphadorner suministra toda la información necesaria sobre las componentes de una oración para calcular las medidas de estilo utilizadas. Estas medidas, ya han sido aplicadas exitosamente en trabajos sobre atribución de autoría [6]. La salida de este módulo es una secuencia de n vectores v i de dimensión M , donde elM es la cantidad total de medidas extraídas del texto. Actualmente, el detector implementado utiliza un total de 24 medidas estilográcas. 3 Restricciones de espacio impiden una explicación más detallada del enfoque descripto en esta sección. El lector interesado puede encontrar en [4] más detalles y ejemplos de los procedimientos involucrados en este método. 4 Morphadorner es una libreria Java para PLN de acceso libre, suministrada por la Universidad de Northwestern. CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 97 8 Dario G. Funez and Marcelo L. Errecalde Identicador de Outliers: Este módulo recibe como entrada los vectores de características de cada sección y devuelve las secciones que se sospecha de plagio. El módulo elige las secciones que no se corresponden con el estilo del autor del texto. Para tal n, se implementó el método de detección de outliers basado en la MEDA, el cual fue fue seleccionado por ser simple, ya que la detección no debe insumir demasiado tiempo. Una sección se la considera un outlier si el 30% de sus medidas es un outlier. Una medida ? i es considerada un outlier, si no se encuentra contenida en el siguiente intervalo: [?−α ∗M??A, ?+α ∗M??A], donde el parámetro α = 1,? fue elegido experimentalmente. Una vez que se dispone de las secciones outliers, las secciones adyacentes se agrupan en una única sección. Figura 2. Ejemplo de ejecución del ambiente. El Ambiente de Ejecución Un ambiente de ejecución dene el contexto en el cual se ejecutan ciertas tareas y, en este trabajo, está compuesto de los módulos denidos previamente. De acuerdo a nuestro conocimiento, no se encuentran disponibles actualmente herramientas de libre acceso que permitan realizar la detección intrínseca, existiendo en cambio herramientas online como Stylysis 5 que tienen características similares al ambiente propuesto, pero sólo muestran las secciones con cambios de estilo, es decir, marca en el texto completo los puntos donde se producen alteraciones en el estilo, pero no aisla las secciones que no se corresponden con el autor del texto como se hace en nuestro caso. El ambiente propuesto en este trabajo, denominado ToolIntrinsec 6 , tiene las características deseables de una interface de usuario amigable, ya que es fácil de usar, muestra los resultados de forma sencilla y es portable por estar codicado en el lenguaje Java. Permite que un usuario pueda comprobar si un texto contiene secciones que se sospecha de plagio y luego se puede buscar la fuente utilizada por otro método (extrínseco). El usuario tiene dos opciones de ingreso de un texto para el análisis de plagio intrínseco. Una de ellas, consiste en especicar el archivo donde se encuentra el texto, y la otra es ingresando directamente el texto en la sección dedicada a tal n. Así, por ejemplo, en la Figura 2 se muestra que en la sección Texto se ha ingresado el texto (traducido al inglés) de una nota sobre El Problema de Escasez de Agua 7 , a cuyo texto se le adicionó otro 5 http://memex2.dsic.upv.es:8080/StylisticAnalysis 6 Disponible en forma gratuita en http://sites.google.com/site/merrecalde/resources para aquellos investigadores que deseen profundizar en el tema. 7 http://www.solociencia.com/ecologia/problematica-global-agua-escasez-agua.htm CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 98 Detección de plagio intrínseco usando la segmentación de texto 9 fragmento (de un autor distinto) sobre el agua 8 , mostrando en la sección Salida una fracción del fragmento utilizado para realizar el plagio articial. 5. Experimentos Para los experimentos se seleccionaron de forma aleatoria una colección de 1000 archivos de un total de 6000 archivos extraídos del corpus Pan09. El corpus es una colección de documentos en inglés del proyecto Gutenberg, que contienen fragmentos plagiados generados articialmente [1]. Para evaluar el comportamiento del algoritmo de detección, éste debe producir como salida un archivo xml con las anotaciones de las detecciones realizadas. Para computar las medidas de evaluación se utilizó el script Pyhton perfmeasure.py suministrado por los organizadores de la competencia, que devuelve la precisión, el recall, la granularidad y el Plagdet score u overall. En la siguiente tabla, se muestran los valores totales obtenidos con el detector propuesto en este trabajo: Precisión Recall Granularidad plag-det 0,1204 0,2430 1,27 0,1361 El único detector intrínseco que compitió en Pan10, obtuvo los siguientes resultados para el corpus Pan09 [10]: Precisión Recall Granularidad plag-det 0,0752 0,1852 1.71 0,0743 En la comparación entre ambos, se observa que el detector implementado supera la performance de este analizador con mejores valores de precisión, recall y granularidad. Por otra parte, los siguientes son los resultados de la competencia Pan09 en la tarea de detección intrínseca: Puesto Precisión Recall Granularidad plag-det 1 0,2321 0,4607 1,3839 0,2462 2 0,1091 0,9437 1,0007 0,1955 3 0,1968 0,2724 1,4524 0,1766 4 0,1036 0,5630 1,7049 0,1219 Como se puede observar en la tabla anterior, la performance del detector es ligeramente superior a los valores obtenidos en el cuarto puesto, con mejores valores de precisión y granularidad. Esto muestra, que si bien los resultados experimentales son aún preliminares, el enfoque propuesto es altamente competitivo con respecto a otros algoritmos representativos del estado del arte en el área, y nos motiva para continuar profundizando y perfeccionando la propuesta. En particular, se ha observado que el detector tiene un mejor comportamiento cuando el archivo tiene muchas secciones plagiadas pero, no presenta un buen desempeño cuando el texto no tiene plagio, ya que muestra secciones con cambio de estilo de manera incorrecta. 8 Disponible en: http://es.wikipedia.org/wiki/Agua CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 99 10 Dario G. Funez and Marcelo L. Errecalde 6. Conclusiones y Trabajo Futuro Los experimentos realizados han brindado evidencia aceptable, de que la segmentación de texto es una buena opción para la descomposición de un texto en el análisis intrínseco y que las medidas de estilo utilizadas son útiles para caracterizar un estilo de escritura. Además, el ambiente que soporta este tipo de técnica, es un aporte interesante para la detección de plagio ya que su facilidad de uso permite a usuarios novatos en el tema realizar sus propias vericaciones de situaciones de plagio. Como trabajo futuro, se planea realizar una comparación entre el algoritmo de segmentación de texto implementado y otros enfoques como el TextTiler creado por Marti Hearst, que también forma parte de la librería Morphadorner. Se podría implementar además, un algoritmo hídrido que combine las dos grandes categorías de detección de plagio. De esta manera, en una primera etapa se podría realizar la detección intrínseca y luego aplicar un enfoque extrínseco. Para mejorar la performance del detector, una posibilidad sería incorporar al modelo estilográco nuevas medidas de estilo y utilizar otro método de detección de outlier más efectivo. Finalmente, y como ya fuera mencionado en la sección anterior, un objetivo inmediato es mejorar el desempeño del detector, en aquellos casos de archivos que no tienen secciones plagiadas.	﻿detección de plagio intrínseco , estilografía , segmentación de texto , índices de legibilidad	es	18580
2	El impacto del sistema de Entrada Salida en la eficiencia energética	﻿ El aumento de la complejidad en las aplicaciones científica actuales,  junto con la constante evolución de los sistemas de cómputo, ha causado que se  requiera una gran exigencia por parte de los sistemas de E/S tanto en  prestaciones como en consumo energético, hecho que se traduce en una gran  demanda de energía por parte de los sistemas de E/S, limitando también la  escalabilidad de estos.  Este artículo propone una metodología novel para caracterizar, analizar y  evaluar el impacto de las aplicaciones intensivas de E/S en la eficiencia  energética. Para ello, se consideran dos niveles del sistema de E/S: el nivel de  dispositivo y el nivel de sistema.  Se ha evaluado caracterizando aplicaciones reales sobre diferentes sistemas,  obteniendo valores de potencia consumida al ejecutar la aplicación muy  cercanos a los valores esperados a partir de la caracterización  del sistema.  A  partir de las evaluaciones realizadas se muestra que la configuración del sistema  afecta a la eficiencia energética..  Keywords: Eficiencia energética, consumo, Sistema de Entrada/Salida, HPC,  Metodología.  1 Introducción  Tanto el consumo como la eficiencia energética han pasado en  muy pocos años de  estar en un segundo plano a convertirse en dos factores clave en los sistemas en  general y en los sistemas de E/S en particular. Esto es debido a diversos factores,  entre los cuales destacamos el impacto medioambiental, la escalabilidad de los  sistemas y el más importante, el coste económico, debido al precio del KW/h                                                             1 Este trabajo ha sido subvencionado por el MICINN-Spain bajo el contrato       TIN2007-64974    CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 271 2     impuesto por la compañía eléctrica y el incremento anual de este. Debido a este  último factor, actualmente cuando un centro de HPC adquiere un sistema de E/S, no  sólo no se sopesa el coste de adquisición inicial del producto, sino el coste que tendrá  el producto durante su ciclo de vida útil.  Debido a la importancia que ha adquirido el tema del consumo, actualmente  podemos encontrar rankings similares al clásico TOP500, pero considerando también  el consumo, como es el caso del Green500, que realiza un ranking de los  supercomputadores que considera la capacidad de cómputo por cada watt consumido  (MFLOPS/watt). Aunque cada vez es más habitual encontrar este tipo de rankings, la  gran mayoría de ellos están orientados al cómputo. Actualmente, no es fácil encontrar  análisis que comparen la energía y la eficiencia energética en los sistemas de E/S.  Considerando una computación eficiente y un rendimiento de la E/S determinado,  es importante tener criterios para seleccionar la configuración que consideremos más  eficiente desde el punto de vista energético. Nos planteamos en primer lugar, ¿Cómo  realizar un diagnóstico energético?, ¿Qué debemos caracterizar?, ¿Cómo analizar el  consumo?, ¿Qué métricas debemos utilizar?, todo esto se hace necesario conocerlo  para poder hacer en el futuro propuestas y ejecución de mejoras de la eficiencia  energética.  En este artículo, presentamos una metodología novel que permite caracterizar,  analizar y evaluar el impacto de las aplicaciones intensivas de E/S en la eficiencia  energética . Esta metodología pretende servir como punto de partida para en un futuro  ser capaces de dimensionar el sistema de E/S proponiendo mejoras en términos de  eficiencia energética.  Este trabajo se estructura de la siguiente manera: la sección 2 presenta los trabajos  relacionados con nuestra investigación, la sección 3 presenta la metodología  propuesta, en la sección 4 se presenta los resultados experimentales y por último en la  sección 5 se expone tanto las conclusiones como el trabajo futuro.  2 Trabajos relacionados  El trabajo presentado en este artículo  está relacionado con investigaciones  pertenecientes al área de: “análisis y caracterización  de la eficiencia energética del  sistema del sistema de E/S”.  En esta área podemos encontrar diferentes trabajos,  como es el propuesto por R.Geng[1], el cual propone una metodología de  caracterización del sistema de E/S considerando los diferentes patrones de acceso y el  escalado de frecuencia de la CPU.  A. Hylick[2],  por su parte, propone un análisis de consumo energético analizando  diferentes parámetros tales como el tipo de operación, el estado energético del disco,  el tamaño de bloque de los datos a transferir y la dependencia de localidad de los  datos. Este trabajo está centrado  básicamente en la caracterización de los  dispositivos.  Por otro lado, P.Sehgal[3], propone un análisis de la energía y la eficiencia  energética, considerando la sintonización de los diferentes parámetros configurados  por defecto en sistema locales de ficheros, soportados por el sistema operativo  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 272 3      LINUX. En este trabajo no se consideran los computadores paralelos con sistemas de  ficheros paralelos o distribuidos  En sistemas de almacenamiento masivo y  redundancia, Y. Dong[4], por su parte,  propone  un análisis acerca de la potencia consumida por las diferentes operaciones de  E/S sobre diferentes niveles RAID.   3 Metodología propuesta  Debido a la complejidad que presenta configurar y dimensionar el sistema de E/S,  se hace necesario el diseño de una metodología que permita evaluar la eficiencia  energética en términos de performance, potencia, energía y eficiencia energética,  procediendo de un modo sistemático y, evitando las configuraciones empíricas  basadas en la experiencia. La figura 1 presenta la metodología propuesta, la cual  consta de 3 etapas claramente diferenciadas: caracterización, análisis de la  configuración y evaluación.    Figura 1: Metodología para evaluación de consumo  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 273 4     3.1 Caracterización   Etapa destinada a caracterizar performance, potencia, energía y eficiencia energética  tanto el nivel de dispositivos como el nivel de sistema. Para ello se utilizan diferentes  benchmarks de E/S y herramientas del sistema operativo.   La etapa de caracterización requiere seleccionar  las métricas que se van a utilizar  y,  caracterizar tanto a nivel de dispositivos como a nivel de sistema de E/S,  considerando los diferentes parámetros que se pueden seleccionar al configurar el  sistema de E/S. La tabla 1 muestra  la información obtenida para cada nivel  caracterizado, así como los parámetros de los diferentes benchmarks  considerados  durante la caracterización y las  métricas utilizadas.  A. Selección de las métricas  En esta sección se detallan las métricas utilizadas en la metodología para:  performance, potencia, energía y eficiencia energética.  La performance del sistema de E/S es normalmente cuantificada en términos de  ancho de banda (MB/s) o IOPS (operaciones por segundo). La medida se obtiene al  ejecutar los benchmarks de E/S, la monitorización y medida la realiza el propio  benchmark  o la utilización de herramientas del sistema operativo.  Para cuantificar  potencia y energía consumida se utiliza el Watt (W), el Julio (J).  En este estudio, los watts son obtenidos directamente del medidor de consumo,  mientras que los julios, son calculados indirectamente a partir de la potencia media  consumida  durante la ejecución del benchmark por el tiempo de ejecución total del  benchmark.  Para el cálculo de la eficiencia energética, L. Liu[5] propuso dos métricas nuevas  para cuantificar la eficiencia energética:  IOPS/Watts y MBPS/Kilowatts. Para este  estudio hemos seleccionado Mbps/watt  y MB/J como métricas de eficiencia  energética (Julio = Watt x segundo). Se ha decidido utilizar los Julios además de los  Watts, debido a que los watts son útiles cuando se desea utilizar la potencia en  función del tiempo. En nuestro caso,  para medir eficiencia energética nos interesa  conocer los megabytes transmitidos por julio, ya que lo que se busca es poder  comparar eficiencia energética, para cada fase significativa de la aplicación.      B. Caracterización a nivel de dispositivo  Consiste en caracterizar tanto discos como sistemas RAID mediante la utilización  de benchmarks de E/S como Iozone[6] y/o Bonnie++[7], los cuales nos permiten  caracterizar el patrón de acceso y tipo de operación para diferentes tamaños de  solicitud. Durante la caracterización de este nivel se han considerado: los patrones de  acceso (secuencial y aleatorio), el tamaño de petición de datos y el tipo de acceso  (bloque o carácter). Además, se caracterizan los diferentes estados de consumo de  energía del dispositivo.    C. Caracterización a nivel de sistema   Consiste en la caracterización  de: librerías de E/S, sistema de ficheros (local, y  global), conexión de almacenamiento (DAS, NAS, SAN, NASD) y buffer caché del  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 274 5      sistema. Se utilizan benchmarks de sistemas de ficheros como Iozone[6] y/o  Bonnie++[7] y benchmarks de librerías de E/S como IOR[8], el cual nos permite  caracterizar diferentes librerías de E/S como POSIX, MPI o HDFS..  Table 1. Caractetización del sistema de E/S    3.2 Análisis de la configuración  El objetivo de esta etapa es seleccionar un conjunto reducido de  configuraciones  significativas para analizar. Para ello se consideran los parámetros configurables en  los sistemas  caracterizados en la etapa anterior y  la aplicación o conjunto de  aplicaciones que se van a ejecutar. Es importante tener en cuenta a la hora  de  seleccionar  las configuraciones que se van a evaluar, las restricciones impuestas por  el usuario, las cuales vendrán en forma de restricciones prestacionales y restricciones  de consumo energético. Como resultado de la etapa, obtendremos un conjunto de  configuraciones seleccionadas que se prevé que cumplen el conjunto de restricciones  impuestas por el usuario.  3.3 Evaluación del impacto  Etapa destinada a evaluar y comparar el impacto de las configuraciones propuestas  en base a las aplicaciones especificadas por el usuario. Para la evaluación de las  configuraciones se utilizan las métricas utilizadas durante la caracterización.  Analizado el impacto, seleccionaremos la configuración del sistema  más eficiente y  que se adapte a las condiciones impuestas por el usuario. En caso de no encontrar  ninguna configuración que cumpla las restricciones impuestas por el usuario, se puede  volver a la etapa de análisis de la configuración para seleccionar nuevas  configuraciones a evaluar.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 275 6     4 Resultados experimentales   En la etapa de caracterización, se muestran los resultados obtenidos al caracterizar   los factores de E/S que tienen influencia en la eficiencia energética desde el punto de  vista de prestaciones y consumo.  4.1 Caracterización del sistema  Los experimentos se han realizado sobre dos sistemas diferentes en la UAB:   Workstation con procesador Pentium 4 a 4.1 GHz, 512 MB de RAM y disco  duro Seagate Barracuda de 80 GB (figura 2.a ilustra las especificaciones del  dispositivo). Sistema de ficheros local ext4 con  una conexión DAS. La  librería de I/O utilizada ha sido MPICH.    Cluster Aohyper formado por 8 nodos AMD Athlon(tm) 64 x2 Dual Core x  GHZ), 2G de RAM por nodo, 150 GB de disco local con sistema de ficheros  ext4, 1 servidor NFS con RAID 1 (2 discos) con 236 GB de capacidad y  RAID 5 (5 discos) con stripe de 256 KB y capacidad de 917GB. Se ha  utilizado un sistema de ficheros global NFS con una conexión NAS. La  librería de I/O utilizada ha sido MPICH    Actualmente se están incorporando a las pruebas dos equipos disponibles en la  UNLP:    Cluster Blade con 16 hojas, cada una de las cuales tiene 2 procesadores  quad  core Intel Xeon e5405 2.0 GHz. Cada hoja tiene 2 Gb RAM (actualmente  expandiéndose a 10 Gb) y el cluster dispone de un bloque de discos de 250 Gb.   4 servidores de altas prestaciones para el despliegue de un Cloud privado, de  los cuales 2 servidores equipados con dos procesadores Intel Xeon E5620  Quad-Core, corriendo a 2.4 GHz con 48 GB de RAM y discos rígidos de 500  GB. Y los otros  2 servidores son hojas Blade con dos procesadores Intel  Xeon E5405 cada una, corriendo a 2.0 GHz con 10 GB de RAM y discos  rígidos de 250 GB.     A la hora de cuantificar el consumo, se ha utilizado el  medidor de consumo Watts  UP PRO, para adquirir las muestras de consumo. Este medidor permite adquirir  muestras cada segundo en tiempo real. El medidor fue conectado en serie a la salida  de la fuente de alimentación del computador o del servidor.  Han sido caracterizados los dos sistemas, a continuación se presentan los factores  obtenidos para la  Workstation.    A. Caracterización del dispositivo  Se han caracterizado el disco y los efectos del control de ahorro de energía del  dispositivo. Para ello, se ha utilizado el benchmark IOR[8], el cual  ha sido ejecutado  2 veces seguidas con un intervalo de 60 segundos. La figura 2.b ilustra el resultado de  la ejecución utilizando control de ahorro de energía y sin utilizarlo, mientras que la  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 276 7      figura 2.c muestra la potencia y energía consumida para los dos tipos de ejecuciones.  Debido a diversos factores como: los cortos períodos de standby, el coste tanto en  tiempo como en potencia de la transacción Spin-Up y los picos obtenidos durante la  transacción Spin-up, causan que el resultado sin utilizar control de ahorro  energía sea  mejor que utilizándolo. Además, se han caracterizado diferentes patrones de acceso  (secuencial y aleatorio)[9].     B. Caracterización a nivel de sistema   Se ha caracterizado la influencia de las librerías de E/S. Se ha caracterizado la  librería de paso de mensajes MPICH, para ello se ha utilizado el benchmark IOR[8]  para 1 core sobre diferentes tamaños de solicitud con un archivo de tamaño de 1GB.     (a)                                        (b)                                                 (c)  Figura 2: Control de energía.  a) Especificaciones del dispositivo.   b) control de energía del dispositivo y  c)Potencia y energía consumida control de energía    El objetivo de esta caracterización era observar la eficiencia energética a medida  que insertamos capas intermedias en el sistema de E/S. Se muestran los resultados de  caracterizar operaciones de lectura (R) y escritura (W) para diferentes tamaños de  bloque. La figura 3 muestra los resultados de la caracterización. Se observan las  siguientes tendencias: tanto el ancho de banda como la eficiencia energética  disminuyen, mientras que la energía consumida incrementa a medida que  aumentamos el tamaño de solicitud. En cuanto a la potencia consumida, para  operaciones de lectura, incrementa hasta 512KB, momento a partir del cual empieza a  decrecer. En cuanto a operaciones de escritura, para un tamaño de solicitud de 512  KB, se observa un pequeño decrecimiento, momento a partir del cual incrementa  hasta 1024 KB e inmediatamente comienza a decrecer para tamaños mayores.   Para mostrar la influencia de la configuración se muestra la caracterización de los  diferentes niveles de caché del sistema, para ello se ha procedido a deshabilitar los  diferentes niveles de caché progresivamente. Se ha considerado la caché de disco,  como parte de la jerarquía de memoria cache del sistema y el buffer cache, como el  buffer de memoria del sistema operativo. Para la caracterización se ha utilizado el  benchmark IOZONE[6] para diferentes tamaños de solicitud con un archivo de  tamaño de 1GB. La figura 4 muestra el resultado de la caracterización.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 277 8                  (a)                                                                (b)  Figura 3: Caracterización a nivel de librería de E/S. a) Performance y potencia. b) Energía y eficiencia  energética    (a)                                                             (b)  Figura 4 : Caracterización niveles caché. a) Perfomance y potencia. b)Energía y eficiencia energética  4.2 Análisis de la caracterización  Para evaluar y comparar el impacto de las configuraciones al ejecutar las   aplicaciones, se seleccionó el Bloque Tridiagonal (BT) de la suite NAS como  aplicación, la cual se ejecutó en los dos sistemas previamente caracterizados.  Para la  workstation se ejecutó la clase B y subtipo Full de la aplicación, la cual escribe un  archivo de 1,5 GB,  mientras que para el clúster se ejecutó la clase C y subtipo FULL  utilizado 16 procesadores, la cual escribe un archivo de 6,5 GB.    La figura 5.b muestra la traza de la aplicación obtenida en la Workstation. Se  observan tres fases representativas de la aplicación: una primera fase de cómputo con  operaciones de escritura discontinuas con un tamaño de solicitud de 128 KB, una  segunda fase intensiva de operaciones de escritura y una tercera fase intensiva de  operaciones de lectura.  Por otro lado, la figura 5.a muestra la potencia consumida  durante la ejecución de la aplicación, después de un período inicial donde el estado  del disco se encontraba en Idle. Se observan dos fases distintas, en la primera fase  observamos un consumo mayor que en la segunda fase, eso es debido a que en la  primera fase, la aplicación está computando y realizando operaciones de escritura,  mientras que en la segunda fase, solamente realiza operaciones de E/S, es decir, no es  intensiva en  cómputo. Además, en la primera fase podemos observar picos en la  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 278 9      potencia consumida, eso es debido al consumo de las operaciones de escritura, que se  añaden al consumo del cómputo.  En cuanto a la arquitectura utilizando el cluster aohyper, la figura 7  muestra la traza  de la aplicación utilizando las configuraciones RAID 1 y RAID 5.  Se observan dos  fases representativas de la aplicación: una primera fase de cómputo con operaciones  de escritura discontinuas con un tamaño de solicitud de 128 KB y una segunda fase  intensiva de operaciones de lectura. En cuanto al consumo, la figura 6.a y 6.b   muestran la potencia consumida durante la ejecución de la aplicación, después de un  período inicial donde el estado del disco se encontraba en Idle para las  configuraciones RAID 1 Y RAID 5. Se observan dos fases significativas, una primera  fase de operaciones de escritura y una segunda fase de operaciones de lectura. Al  tener aislado el nodo de E/S, no tenemos el impacto de consumo para operaciones  intensivas de cómputo, con lo cual los valores  obtenidos son básicamente de  operaciones de escritura y lectura.  (a)                                                                         (b)  Figura 5: Aplicación NAS BT clase B subtipo Full. a) Potencia consumida. B) Traza de la aplicación             La figura 8 ilustra tanto los valores reales obtenidos durante la ejecución de la  aplicación, como los valores de referencia obtenidos mediante la caracterización para   potencia y ancho de banda de las dos arquitecturas utilizadas, para las operaciones de  lectura y escritura con un tamaño de 128KB, con el buffer/cache activado.    Para los valores reales obtenidos en la workstation, se han seleccionado los  valores de la fase intensiva de E/S, es decir, de la segunda fase de la aplicación,  mientras que para seleccionar los valores de referencia, se han seleccionado los  valores de la caracterización para la librería MPI, para un tamaño de operación de 128  KB.  Se observan unos valores muy cercanos tanto en potencia como en ancho de  banda.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 279 10          Figura 6:  Consumo cluster aohyper  configuraciones.                      Figura 7: Traza de la aplicación  a)configuración RAID5 b)configuración RAID1        ddddddddddddddddd          ddddddddd           Para los valores obtenidos de la ejecución en el clúster, puesto que el cómputo está  separado de la E/S, se han seleccionado las fases de cada tipo de operación, mientras  que para seleccionar los valores de referencia, se han seleccionado los valores de la  caracterización de 128 KB para la librería MPI. Se observa valores muy cercanos en  potencia, sin embargo,  para el ancho de banda se observan diferencias significativas,  estas diferencias están causadas por la aplicación, ya que no logra estresar el sistema  de E/S.  La tabla 2 muestra el porcentaje de desviación del consumo entre los valores  reales y los valores de referencia para las diferentes arquitecturas utilizadas. Se  observa un consumo muy cercano entre los valores.          Figura 8: Valores reales y de referencia obtenidos    5 Conclusiones  En este trabajo se propone una metodología novel con el propósito de evaluar la  eficiencia energética del  sistema de E/S, a partir de la caracterización, análisis de la  configuración y evaluación del impacto del sistema de E/S en la eficiencia energética.   Se ha evaluado con la caracterización de aplicaciones reales sobre diferentes sistemas,   obteniéndose valores de potencia consumida por la aplicación muy cercanos a los  valores esperados a partir de la caracterización  del sistema.  A partir de las  evaluaciones realizadas se ha mostrado que la configuración seleccionada del sistema  afecta a la eficiencia energética.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 280 11      Este trabajo pretende servir como punto de partida de nuestra investigación.  Actualmente se está trabajando en aportar un  método que permita identificar las fases  significativas de las aplicaciones en términos de consumo, considerando  configuración del sistema. Asimismo interesa el estudio de aplicaciones  transaccionales que tienen un volumen importante de E/S, para caracterizar el impacto  que tiene la E/S en el consumo.   Para ello, también se está trabajando en proponer métricas que permitan comparar  sistemas considerando las prestaciones con la  eficiencia energética.	﻿eficiencia energética , consumo , sistema de Entrada Salida , HPC	es	18650
3	Tecnologías móviles aplicadas a la educación superior	﻿ Actualmente la educación de posgrado busca el desarrollo de  competencias a partir del aprendizaje autónomo, centrado en el aprendiz. Las  tecnologías de la información y de la comunicación (TICs) facilitan este tipo de  aprendizaje y, en particular, el m-learning promueve experiencias contextuadas  y colaborativas. Sin embargo existen dos tipos de problemáticas en el momento  de implementar las tecnologías móviles en la educación de posgrado. Por un  lado, no están definidas las estrategias de m-learning apropiadas según las  tecnologías móviles disponibles en cada contexto. Por otro lado, la usabilidad  de las aplicaciones de m-learning es limitada debido a la escasez de memoria,  pantalla y procesador de los dispositivos. El objetivo de este trabajo consiste en  determinar qué aspectos deben considerarse en el diseño de programas de mlearning en el nivel de posgrado de las universidades. Los aspectos principales  que se abarcan son los referidos a la caracterización del ecosistema,  modos y  estrategias del m-learning. Además, se consideran aspectos propios del diseño  de aplicaciones móviles que deben ser tenidos en cuenta al desarrollar recursos  de m-learning.   Palabras Claves. M-learning, TICs en educación de posgrado, aplicaciones  móviles para el aprendizaje.   1. Introducción  El e-learning marca una tendencia irreversible en cuanto a modalidades preferidas en  el proceso de enseñanza-aprendizaje-desarrollo de los seres humanos [10]. Esta se  acentúa en el nivel de posgrado, donde se busca un aprendizaje continuo y autónomo  que permita desarrollar competencias profesionales y afianzar cuestiones éticas que  promuevan un mundo sustentable.  Un proceso de e-learning exitoso requiere considerar modelos de interacción  hombre-computadora que mejoren la usabilidad [8] de los sistemas de aprendizaje. A  su vez, estos modelos deben involucrar las tecnologías de la información y  comunicación más utilizadas por los aprendices. Actualmente, los dispositivos  móviles constituyen una de las tecnologías más usadas y presentan ventajas en cuanto  a su portabilidad y a su sensibilidad al contexto. Este tipo de e-learning mediado por  tecnologías móviles se conoce como mobile-learning (m-learning).   CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 620  El m-learning se puede presentar en tres modos diferentes y a su vez cada uno de  ellos involucra diversas estrategias de aprendizaje mediadas por tecnologías móviles.  Los modos son los siguientes [15]:    Recuperación de información. Brindan comunicación en un único sentido,  acceder a información.   Recopilación y análisis de información. Brindan comunicación en ambos  sentidos sentidos, acceso a información y respuestas o envíos de información  a los compañeros o a la profesora).   Comunicación, interacción y colaboración en redes. Brindan comunicación en  las comunidades.  El objetivo de esta propuesta consiste en determinar cuáles son las estrategias de  m-learning apropiados para implementar programas de posgrado en las universidades  argentinas, partiendo de un contexto real de cada institución. Esta publicación  constituye un avance de Tesis de Doctorado en Ciencias Informáticas desarrollada por  una de las autoras en el programa de la Universidad Nacional de La Plata.  El artículo se organiza de la siguiente manera. En el apartado 2 se tratan aspectos  técnicos de las aplicaciones móviles que deben ser tenidos en cuenta en el momento  de su desarrollo. En el apartado 3 se aborda la conceptualización y caracterización del  m-learning. En el punto 4 se proponen un modelo de ecososistema para el m-learning  sus estrategias apropiadas para la educación de posgrado en el ámbito universitario.  Finalmente se presentan las conclusiones.  2. Tecnología móvil  Computación Móvil es un término genérico que describe la habilidad para usar  tecnología sin ataduras, es decir, no conectada físicamente o que pertenece a entornos  remotos o móviles, no estáticos [9,11]. En la actualidad el término ha evolucionado de  forma tal que la Computación Móvil requiere conexión inalámbrica hacia y a través  de Internet o de una red privada. A continuación presenta los aspectos relevantes que  se deben tener en cuenta para optimizar la usabilidad de aplicaciones móviles1.  Los servicios que ofrece la computación móvil se desarrollan y ofrecen desde  diferentes enfoques, según su finalidad. Por ello, se generan metáforas que definen el  grado y tipo de interacción del usuario y la forma de recorrer el espacio. Estas  metáforas se determinan de acuerdo a las historias interactivas y a las ubicaciones  físicas de objetos y usuarios. Estas metáforas pueden ser: búsqueda del tesoro,  rompecabezas, dominó, palabras cruzadas o scrabble, recolectando información. Se  diferencian entre sí de acuerdo al tipo de historia, relación entre las piezas, tipo de  inicio y fin, modo de recorrido, nivel de movilidad y nivel de interacción con el  contenido de la historia [1,6,7].   En cuanto a la arquitectura, un sistema móvil generalmente se basa en una  aplicación cliente que se conecta a un servidor de aplicaciones que se encuentra en  Internet [12]. Este servidor, a su vez, utiliza los servicios de un proveedor de                                                             1 Según la propuesta del grupo especializado de esta temática de la FI-UNLP, Dra. S. Gordillo.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 621  ubicación, un Sistema de Información Geográfico (GIS) y de la información provista  por diversos puntos de interés.  Existen dos formas de posicionamiento para indicar el lugar en que se encuentra  ubicado un elemento relevante para el sistema:   Posicionamiento Geométrico. Es necesario una representación de la tierra  de una manera regular: Datum. Son interpretaciones de posiciones  relativas de la tierra. Por ejemplo, se utiliza latitud y longitud sobre el  globo terráqueo.    Posicionamiento Simbólico. Expresa posiciones en términos de elementos  conocidos del dominio, los cuales no necesariamente poseen relaciones  geográficas entre sí. Por ejemplo, un auto en 1 y 50, el Coliseo en Roma, el  alumno en el aula 5 de la Facultad de Informática.  Para posicionar a un usuario en una aplicación móvil, se necesita además una  representación visual del espacio relevante para el usuario: un plano con las calles, un  plano del edificio, etc. Esto se brinda mediante: imágenes (JPG, GIF) o por modelos  de representación (cartografía: raster o vector).  Cuando se conoce la posición del usuario, se la obtiene utilizando una de las  siguientes técnicas de censado de posiciones: GPS (usa triangulación entre satélites  para generar una posición geométricamiento), sistema de antenas (servicio que  ofrecen las compañías de teléfono, determina una ubicación aproximada), tags  (códigos 2D, datos codificados en la altura y longitud del símbolo, no da una posición  real, es barato dado que solo se necesita un teléfono con cámara y la conexión).  Para el posicionamiento in-door es más apropiado el bluetooh o sistema de  sensores. A diferencia de las anteriores que se usan en escenarios out-door.  Los teléfonos móviles o celulares utilizan, típicamente, tecnologías especialmente  desarrolladas para ese tipo de dispositivos las cuales se han ido clasificando en  diferentes generaciones. En Argentina, actualmente, está vigente la 3° Generación  (3G), caracterizada por la convergencia de voz, datos y acceso inalámbrico a Internet  a mayor velocidad; es apta para aplicaciones multimedia (envío y recepción de  imágenes estáticas y video, servicios de ubicación geográfica, televisión en tiempo  real, juegos, etc.). En países más avanzados como Japón, se utilizan dispositivos de 4ª  Generación y se experimenta con la 5ª.   3. M-learning  Las primeras definiciones de m-learning incluían cualquier actividad de aprendizaje  mediada por un dispositivo móvil. Dentro de esta concepción, se definía al m-learning  como cualquier tipo de aprendizaje que ocurre cuando el alumno no se encuentra en  una ubicación fija predeterminada, o que  sucede cuando el alumno se aprovecha de  las oportunidades de aprendizaje ofrecidas por las tecnologías móviles.  Actualmente, el énfasis se ha movido desde un enfoque basado exclusivamente en  la tecnología hacia el poder de las comunicaciones ambientales y ubicuas. El nuevo  enfoque se centra en el aprendizaje desde el punto de vista del alumno. El aprendizaje  móvil es ahora visto como un medio para mantener a las personas en contacto entre sí  y con las fuentes de información, sin importar dónde se encuentra y al mismo tiempo  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 622  teniendo en cuenta el contexto inmediato del individuo y sus preferencias personales  [5,15].   El m-learning, en el contexto actual, es la capacidad de cualquier persona de  utilizar la tecnología de red móvil para acceder a información relevante o para  almacenar nueva información, con independencia de su ubicación física. Más  precisamente, m-learning es aprendizaje personalizado que une el contexto del  aprendiz con la computación en nube (cloud computing) utilizando un dispositivo  móvil.   El m-learning incluye actualmente una variedad de actores: proveedores de  contenido (desarrollan materiales originales personalizados para que se ejecuten en  los dispositivos móviles), proveedores de conversión de contenidos (toman los  contenidos impresos y digitalizados y los reprograman para su visualización en  dispositivos móviles), proveedores de aplicaciones (empresas de desarrollo de  software que crean aplicaciones específicas para la industria del m-learning),  proveedores de hardware (empresas que diseñan, fabrican y venden los dispositivos  móviles), proveedores de servicios (ofrecen una amplia variedad de servicios  incluyendo la gestión, traducción, consultoría y diseño del proyecto), proveedores de  soluciones (tratan empaquetar la cadena de valor entera del m-learning para  gestionarla del principio al fin).  Cuando el m-learning está diseñado correctamente, puede ser descripto como just  in time, just enough, and just for me.  Los principales beneficios del uso del mlearning son: portabilidad, conectividad en cualquier momento y en cualquier lugar,  acceso flexible y oportuno a los recursos de aprendizaje, inmediatez de la  comunicación, participación y compromiso de los alumnos principalmente de  comunidades dispersas, experiencias de aprendizaje activas. Los investigadores  señalan otros beneficios del m-learning, como el aumento de la alfabetización  informática, mejora en las competencias de comunicación y creación de comunidades,  mejora de la creación identitaria, aprendizaje colaborativo y mayor uso del mentoring  o tutoría. Visto más en profundidad, se identifican otros beneficios potenciales de este  tipo de aprendizaje [15]:   Mejora de la retención: debido a que es justo a tiempo, tarea a mano y  personalizado para el alumno.   Eficiencia: El aprendizaje móvil es muy eficiente debido a la portabilidad  de las fuentes de información proporcionada por conectividad en cualquier  momento y lugar.    Ahorro de costos: los dispositivos móviles necesarios, en la mayoría de los  casos, ya los tienen los usuarios potenciales. También hay ahorro debido a  la reducción de las necesidades de espacio para salón de clases y de viaje  del personal y de los alumnos.    Ahorro de tiempo: el aprendizaje móvil es casi inmediato, no hay  necesidad de programar clases sobre un tema o esperar para una  presentación.    Aumento de la colaboración y de las comunidades: pueden formar una  comunidad de práctica que de soporte todos los participantes con la  información oportuna que sea necesaria.   CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 623   Diseño más granular: El contenido de m-learning, por necesidad, se  formatea de diferente manera, lo que se envía al aprendiz debe ser  producido en pequeñas piezas de información.   Información actualizada: el m-learning es dinámico. Siempre están  disponibles expertos en línea y fuentes actualizadas.    Personalización: El m-learning es individual. Los aprendices seleccionan  las actividades según su background en el momento de su elección.    Integralidad: El aprendizaje móvil es muy amplio. Proporciona eventos de  aprendizaje de muchas fuentes, lo que permite a los aprendices seleccionar  un formato favorito, el método de aprendizaje, o el proveedor de  instrucción.   El sistema de tecnología móvil en red que soporta al m-learning está compuesto  por un conjunto complejo de múltiples formas de movilidad, diversas tecnologías  móviles, diversidad de transportistas, una variedad de estudiantes, una multiplicidad  de contextos de aprendizaje, profesores con todos los niveles de experiencia en mlearning y varios enfoques para el diseño de contenidos para móviles y métodos de  enseñanza. Este complejo sistema constituye un ecosistema de m-learning, formado  por personas inmersas en un contexto cultural particular que usan tecnologías móviles  en una red para acceder o almacenar información como parte de una experiencia de  aprendizaje. Los componentes del ecosistema de m-learning son [15]: dispositivos,  infraestructura, conceptos, contenidos, plataformas, herramientas.  En cuanto a las aplicaciones de m-learning, muchas se han desarrollado hasta la  fecha. Además, es posible tener experiencias de m-learning que utilizan los servicios  de información existentes que no se basan en aplicaciones específicas de aprendizaje.   Actualmente, las aplicaciones de m-learning ponen énfasis en el aprendizaje  centrado en el usuario. Se caracterizan por los siguientes aspectos: movilidad,  ubicuidad, accesibilidad, conectividad, sensibilidad al contexto, individualidad y  creatividad. Sin embargo, la usabilidad sigue manteniéndose en un bajo nivel  comparado con aplicaciones de escritorio.  El m-learning se puede llevar a cabo a través de tres modos y niveles diferentes: a)  recuperación de información, b) recopilación y análisis de información y c)  comunicación, interacción y colaboración en redes. A su vez, dentro de cada uno de  estos niveles, existen distintas estrategias que se pueden implementar mediante las  tecnologías móviles. En el apartado siguiente se definen las que son apropiadas para  la educación de posgrado en Argentina, considerando un ecosistema estándar.  4. El m-learning en la educación de posgrado  La sociedad del conocimiento precisa de la diversidad en los sistemas de educación  superior, dirigidas a diferentes tipos de estudiantes. En este sentido, el uso de sistemas  móviles en el proceso de aprendizaje implica llegar a los estudiantes que trabajan y no  disponen de tiempo para asistir a centros educativos. Esta situación se presenta en la  mayoría de los estudiantes de posgrado, quienes ya poseen un título de grado que los  habilita a desempeñarse en su profesión. Si bien las plataformas de aprendizaje virtual  brindan posibilidades de aprendizaje a distancia, la ubicuidad de los sistemas móviles  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 624  permite que el aprendizaje ocurra en cualquier lugar y en cualquier momento, aún  cuando la persona se está desplazando.  La aplicación de TICs a la enseñanza y el aprendizaje posee un gran potencial para  aumentar el acceso, la calidad y la permanencia. Es por ello que, dado que el uso de  sistemas móviles constituye una tendencia indiscutible, es necesario utilizar esta  tecnología con el propósito de contribuir a la mejora de la calidad educativa  [2,3,4,13].  Se presenta a continuación un estudio sobre los factores que se deberían tener en  cuenta para la implementación del m-learning en la formación de posgrado en las  instituciones universitarias argentinas.  4.1. Justificación y marco normativo  La Conferencia Mundial de Educación Superior realizada en el año 2009 en París,  organizada por la UNESCO, abordó las nuevas dinámicas de la educación superior.  En su declaración hace referencia a la necesidad de incorporar las TICs en el proceso  educativo [14].   Entre las principales cuestiones fijadas en el comunicado de dicha conferencia, se  encuentran estos aspectos:    Diversidad en los sistemas de educación superior,    Formación docente con currículas que proporcionen los conocimientos y  las herramientas necesarios para el siglo XXI. Nuevos abordajes que  incluyan la educación abierta y a distancia e incorporen TICs.   La aplicación de TICs a la enseñanza y el aprendizaje posee un gran  potencial para aumentar el acceso, la calidad y la permanencia.    Los resultados de la investigación científica deberían ser más accesibles a  través de las TICs y los recursos de la Educación a Distancia.   Uso de herramientas y recursos de bibliotecas electrónicas para apoyar la  docencia, el aprendizaje y la investigación.  Desde esta perspectiva, se torna imprescindible la implementación de estrategias de  m-learning en las curriculas de formación superior de las universidades. Sin embargo,  aún no está regulada en forma precisa la implementación de este tipo de programas en  el nivel universitario. El marco normativo está dado por la Resolución Ministerial Nº  1168/97 y por la Resolución Ministerial Nº 1717/04 del Ministerio de Cultura y  Educación.  La RM 1168/97 fija los estándares mínimos de calidad que deben cumplir las  carreras de posgrado en Argentina. De acuerdo con el título que otorgan, el artículo 39º  de la Ley de Educación Superior y el Anexo de la RM 1168/97, se reconocen los  siguientes tipos de carrera de posgrado: Especializaciones, Maestrías y Doctorados.  La RM 1717/04 del Ministerio de Cultura y Educación sostiene que se entiende por  Educación a Distancia a la modalidad educativa no presencial, que propone formas  específicas de mediación de la relación educativa entre los actores del proceso de  enseñanza y de aprendizaje, con referencia a determinado modelo pedagógico. Dicha  mediatización se realiza con la utilización de una gran variedad de recursos,  especialmente, de las tecnologías de la información y redes de comunicación, junto con  la producción de materiales de estudio, poniendo énfasis en el desarrollo de estrategias  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 625  de interacción. También sostiene que se comprenderá por Educación a Distancia a las  propuestas frecuentemente identificadas también como educación o enseñanza  semipresencial, no presencial, abierta, educación asistida, flexible, aprendizaje  electrónico (e-learning), aprendizaje combinado (b-learning), educación virtual,  aprendizaje en red (network learning), aprendizaje o comunicación mediada por  computadora (CMC), cibereducación, teleformación y otras que reúnan las  características mencionadas precedentemente.  En síntesis, la introducción de las TICs en los procesos de enseñanza y aprendizaje  universitario supone un gran desafío en cuanto a cambios en las estrategias  pedagógicas pero sin embargo representa una gran oportunidad para la innovación y la  renovación de las ofertas académicas de estas instituciones. Es evidente que la  irrupción de las TICs rompe la cadena de la presencialidad, la relación privilegiada  entre profesor y estudiante. La educación a distancia ha desarrollado grandes avances  en la educación superior argentina a partir del uso de las TICs [2,13].   4.2. Definición del ecosistema de m-learning para la educación de posgrado   Para implementar estrategias de m-learning a nivel de posgrado, se debería  considerar, entre otras cuestiones, cómo sería el ecosistema m-learning apropiado  para cada institución.   En este sentido, es imprescindible revisar cada uno de los componentes que forman  parte del contexto del Posgrado. Para ello, se utilizan técnicas de recolección de  información tales como observación directa, cuestionarios no estructurados a alumnos  y docentes, visita a laboratorios informáticos, entrevista no estructurada con  responsables de implementación de estrategias de aprendizaje a distancia. A  continuación se presenta la caracterización de un ecosistema de m-learning para la  educación de posgrado, basada en un relevamiento realizado en la Región NOA de  Argentina.  Los componentes del ecosistema y sus caracterizaciones para la educación superior  son:   Los dispositivos móviles que se deberían usar en el ámbito de aprendizaje:  teléfonos móviles, teléfonos inteligentes (Smartphones: Blackberry,  Samsung, Motorola, Nokia), notebooks y netbooks, cámaras digitales,  reproductores MP3, e-readers.   Infraestructura: en Argentina, los smartphones utilizan tecnología 3G,   comunicación móvil de banda ancha. En general no se usan redes privadas,  salvo las ofrecidas por las compañías telefónicas.   Plataformas: la tendencia actual es el uso del SO Android de Google. En  menor cantidad, el SO de Blackberry.    Contenidos: pasaje o adecuación de contenidos instruccionales de elearning a m-learning, desarrollo de aplicaciones para m-learning,  caracterizadas por entregarse en pequeños trozos, de manera que sean  apropiados para las pantallas pequeñas. En cuanto al diseño del contenido  en sí, depende de la oferta específica que se trate, y el tipo de aprendices  que cubra dicha oferta.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 626   Conceptos y contextos: las aplicaciones de m-learning deben estar  preparadas para personas que trabajan con dedicación exclusiva o semi  exclusiva y que deberán desarrollar su aprendizaje en sus momentos libres  o durante su jornada laboral. En cuanto a lugares, el m-learning propicia el  aprendizaje mediante casos reales, con prácticas en el propio lugar de  trabajo. El aprendizaje puede ser tanto indoor como outdoor. También  podría ocurrir sentado, parado, caminando o corriendo. En cuanto a la  edad, las aplicaciones de m-learning deben tener un módulo de seteo de  preferencias de usuario que permita personalizar la aplicación a su perfil.   Mobile Browsers: las aplicaciones de m-learning deberían ser Web dado  que de esa manera se obtienen aplicaciones multiplataformas que puedan  ser corridas en la mayoría de los smartphones y con diversos sistemas  operativos y browsers. Para pruebas se aconseja el más usado: Opera.  4.3. Modos y estrategias de m-learning para la educación de posgrado  Como se mencionó anteriormente, el m-learning se puede llevar a cabo a través de  tres modos y niveles diferentes: a) recuperación de información, b) recopilación y  análisis de información y c) comunicación, interacción y colaboración en redes [15].   En el primer nivel (a), los dispositivos móviles pueden actuar como clientes que  recuperan información de los servidores –donde el servidor es un servidor masivo  alojado en la nueve o una simple marca codificada en un objeto. Esto implica que la  información no es almacenada necesariamente en la memoria del aprendiz pero es  actualizada y usada just in time. Esta es siempre una transacción en una sola dirección:  la información es solicitada y enviada al usuario.   En el otro nivel (b), la información es recopilada por el usuario y enviada a un  servidor para almacenamiento o análisis. Esto invierte la dirección del flujo de  información que generalmente se da en una clase. El uso de dispositivos móviles para  obtener información hace que cada usuario sea un nodo en una red que puede ser  utilizado para conducir una especie de inteligencia colectiva.  En el tercer nivel (c), el aprendizaje se lleva a cabo usando aplicaciones sociales  interactivas que corren en dispositivos móviles. Es en este nivel donde brilla el mlearning puesto que permite el aprendizaje social que otros medios no facilitan,  basándose principalmente en un aprendizaje no formal, en un aprendizaje en grupo que  es donde el individuo aprende el 80% del conocimiento que tiene dentro.  En base al estudio realizado, las estrategias de aprendizaje de m-learning que serían  apropiadas para la educación de posgrado son las que se mencionan en la Tabla 1.  Información detallada sobre cada una de las estrategias puede ser solicitada a las  autoras.  5. Conclusiones   La formación brindada por las instituciones de educación superior debería tanto  responder  como a anticipar las necesidades sociales. Esto implica la provisión de  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 627  formación técnica y vocacional, educación para emprendedores y programas para la  educación a lo largo de toda la vida. Por lo tanto, es muy importante brindar formación  de posgrado que implique capacitación flexible y no solo dentro de los programas de  especialización, maestrías y doctorados.     Tabla 1. Modos y estrategias de m-learning para la educación de posgrado.    Estrategias del Modo  Recuperación de  Información  Estrategias del Modo  Recopilación y  Análisis de Información  Estrategias del Modo  comunicación, interacción  y colaboración en redes  - Canales de medios digitales  en Internet:  o libros electrónicos  o itunes, Youtoube (videos)  o Live Streaming video   o Blip.tv, conferencias de  universidades  o Podcasting    - Suscripciones: requiere  lector Avant Reader, Blog  lines Mobile, Egress, Feeder  Reader, etc.  - Información just in time:  Guías y visitas virtuales  - Bibliotecas que ofrecen  colecciones de libros de  audio,  e-books, filmaciones e  imágenes para móviles  - Información basada en la  ubicación: en base a la  información de ubicación del  aprendiz, se puede agregar  otra información. Usando  realidad aumentada se puede  agregar información usando la  cámara del teléfono  - Mapas y fotos satelitales  - Presentaciones: MS PPoint,  Prezzi, Windows Mobile  Lecture Recorder  - Búsqueda y recuperación de  información digital: Google,  Yahoo, MS Bing  - Etiquetas  - Traducción: Mobile Translator  - Uso de dispositivos de  almacenamiento USB.   - Valoración y evalua-  ción:  o llenar espacios en  blanco, V/F, respuestas  cortas, opción múltiple.   o Uso de la cámara  para verificar la  persona que está  haciendo el test.  o Portafolios   o Otros métodos:  juegos de entrenamiento interactivo móvil,  SecondLife (estrategia,  liderazgo y toma de  decisión bajo presión)  - Documentación en  primera persona: las  producciones pueden  guardarse en un eportfolio como evidencias de las competencias y conocimientos  adquiridos por el  aprendiz  - Seguimiento de  tendencias  - Recolección de datos  para investigaciones.  o Encuestas, cuestionarios, sondeos   o Redes de sensores.  - Información y  mate-riales de  aprendizaje generados  por el usuario.  - Ciencia de redes: estudio de  los impactos que producen  las comunicaciones móviles  en la sociedad. Permite crear  conexiones estrechas con  personas que no están  físicamente cerca. Las  interacciones humanas  pueden darse mediante los  siguientes tipos de  relaciones:   (1) uno-a-uno,   (2) pocos-a-pocos,   (3) uno-a-varios,   (4) varios-a-uno, y   (5) varios-a-varios  - Colaboración, comunidades  - Juegos móviles, simulaciones y mundos virtuales.  Los mundos virtuales son  entornos en los cuales los  personajes, llamados  “avatars”, se mueven y  realizan acciones.  - Mentoring, support, and  cognitive apprenticeships.  - Mensajes de texto  - Multimedia personal  - Medios sociales. Interacción social en red:   o blogs,   o wikis,   o sitios de microblogging  como Facebook, MySpace y  Twitter  o sitios de colección  multimedia, Flickr.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 628    En dicho sentido, los sistemas móviles brindan la posibilidad que los aprendices  utilicen realidad aumentada para aprender cuestiones dentro de sus ámbitos de trabajo  o en los ambientes donde se presentan los fenómenos abordados. Se promueve, de esta  manera, el aprendizaje a partir de las propias experiencias y además, el aprendizaje  colaborativo, dado que estas experiencias se publican o se ponen a disposición del  grupo de estudio.  En síntesis, el m-learning implica una alternativa de innovación en el proceso de  enseñanza-aprendizaje que aprovecha las nuevas modalidades de comunicación de las  personas y las últimas tendencias en las TICs. Además, responde a las necesidades de  la educación de posgrado: educación en cualquier momento y en cualquier lugar  (dada la poca disponibilidad de tiempo de los profesionales que trabajan). También  facilita el aprendizaje a partir de las experiencias “in situ”, el aprendizaje  colaborativo. Promueve el autoaprendizaje, es decir el aprendizaje centrado en el  aprendiz, mediado por tecnologías y apoyado en un rol de instructor tipo “mentor”  (dada la posibilidad de consulta inmediata).  Es importante implementar estrategias del m-learning en la educación superior,  principalmente en la educación de posgrado, abarcando tanto cursos de posgrado de  capacitación específica como programas completos de especialización, maestrías y  doctorados, siempre desde una perspectiva pedagógica constructivista planteada por  Piaget, Vigotsky y Ausubel.   Si bien se han desarrollado investigaciones e implementaciones aisladas de mlearning en las instituciones universitarias argentinas [2,3,4,13], las autoras proponen  la implementación de estrategias sistemáticas que cubran los tres modos de mlearning mencionados, considerando para ello: la definición de un ecosistema  específico para cada institución, las normativas vigentes y las cuestiones técnicas  específicas planteadas para los sistemas móviles.	﻿aplicaciones móviles para el aprendizaje , M learning , TICs en educación de posgrado	es	18718
4	Algunas consideraciones para la transformación de Semántica de Negocios SBVR en el Lenguaje de Ontologías Web OWL2	"﻿Muchos trabajos de investigación han realizando significativos avances en las tecnologías semánticas para la representación de conocimiento, que se han distribuido principalmente en las sociedades científicas y sobre algunos dominios particulares, pero que no han alcanzado hasta el momento al mundo empresarial. La mayor dificultad para acceder a esta tecnología radica en la complejidad de los lenguajes de ontologías propuestos para el usuario final del área de negocios. La especificación OMG SBVR permite detallar este dominio, describiendo la semántica de los conceptos de negocios y reglas de negocio utilizando un lenguaje natural controlado, asequible a esa comunidad. Por otro lado, si bien hay indicios de que las formulaciones SBVR son mapeables al lenguaje OWL, todavía no se han propuesto metodologías completas que permitan esta traducción. El presente trabajo introduce algunos conceptos y problemas que se presentan en la transformación de SBVR a OWL2. 1. Introducción Los avances en la tecnología que permiten representación semántica del conocimiento [4,5] brindan la posibilidad de que las computadoras y los expertos puedan intercambiar semántica en tiempo real, logrando así nuevas capacidades para el procesamiento de la información. Lamentablemente estas tecnologías no han alcanzado aún un grado de madurez que represente beneficios significativos en las actividades diarias de las empresas. Parte de las dificultades que evitan la distribución masiva de estas tecnologías radica en las sofisticadas herramientas y lenguajes utilizados, entre ellos RDF [18] y RDFS [6] (Resource Description Framework y Resource Description Framework Schema) y el Lenguaje de Ontologías Web [3,12,20] OWL (Web Ontology Language), usualmente lejos del interés y fuera del alcance de los expertos del dominio. OMG1 creó el metamodelo “Semánticas de Vocabulario de Negocios y Reglas de Negocios” (SBVR - Semantics of Business Vocabulary and Business Rules [27]) que permite describir conceptos de negocios y reglas de negocios con el uso 1 Object Management Group - http://www.omg.org CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 709 de un lenguaje natural controlado (CNL - Controlled Natural Language [26]), que sí es comprensible y más asequible a los usuarios de negocios. Con SBVR, los expertos de negocios pueden comprender, validar y construir vocabularios y reglas. Como SBVR está basado en lógica formal [27] puede aplicarse y utilizarse para el procesamiento automatizado, pero al no compartir la misma forma de representación, no puede usarse directamente con las tecnologías desarrolladas para la Web Semántica [25]. La transformación de las expresiones desarrolladas en SBVR en OWL permitiría que los usuarios de negocios pudieran describir ontologías utilizando un lenguaje similar al lenguaje empresario diario, y por medio de transformaciones automáticas, probar consistencia de vocabulario y reglas en su empresa aprovechando las aplicaciones desarrolladas para la Web Semántica. La literatura habla de los beneficios de esta transformación [10] suponiendo que la base formal de las formulaciones SBVR es fácilmente mapeable a OWL, sin embargo, a pesar de que existen muchos constructores directamente comparables con los provistos por OWL, otros componentes del lengueje SBVR no comparten esa caracterstica, y con la reciente introducción de OWL2 se amplia el campo de investigación para acercar ambos mundos. En este artículo se introducen algunos conceptos básicos necesarios para considerar la transformación de SBVR en OWL2 analizando información relevante de las especificaciones SBVR y las posibles ontologías en OWL2. El resto del artículo se estructura de la siguiente forma: la sección 2 analiza los trabajos relacionados, las secciones 3 y 4 presentan someramente las tecnologías asociadas a los lenguajes SBVR y OWL2, la sección 5 relata las consideraciones para transformar los principales conceptos de SBVR en OWL2 y la sección 6 presenta conclusiones y trabajo futuro. 2. Trabajos relacionados Se han realizando varios trabajos para la aplicación de ontologías y automatización a partir de las reglas de negocios [29]. La posibilidad de llenar el hueco entre SBVR y la Web Semántica fue presentado por varios autores [10,8,19,25], enfocados tanto en las especificaciones del vocabulario como en la incorporación de reglas al lenguaje OWL. En [21] se sugiere la traducción de SBVR a OWL a través de un lenguaje intermedio, el R2ML (Rule to Markup Language). Todas las investigaciones indican que las transformaciones entre SBVR y OWL son deseables y buscadas, y que ganarán amplia aceptación, sin embargo, todas las aproximaciones aún están en desarrollo. Otras ideas para vincular las tecnologías semánticas y su uso para los expertos de negocios se basan en los Lenguajes Naturales Controlados (CNL - Controlled Natural Languages) para la construcción de las ontologías, con una transformación posterior a los lenguajes de la Web Semántica. Para la generación de estos CNL existen dos aproximaciones enfrentadas: por un lado OWL en forma de lenguaje natural, utilizando directamente ese lenguaje como mapeo[24,16]. Otros grupos científicos han desarrollado sus propios CNL [28,9,23], pero ninguno de CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 710 Figura 1. Significados SBVR ellos ha sido ampliamente aceptado para transformarse en estándar aceptable para usuarios finales y especialistas TI. Asimismo el Proyecto Ontorule (ONTORULE - Ontologies meet Business Rules) 2 que busca conformar ontologías a partir de diferentes fuentes, incluso documentación en lenguaje natural, para poder gestionarlas e implementarlas en aplicaciones de software. 3. SBVR En 2008, el consorcio OMG adoptó como estándar la propuesta SBVR, transformándose en la primer especificación creada con la intención de incorporar lenguaje natural para el modelado. SBVR es parte de la Arquitectura Dirigida por Modelos (MDA - Model Driven Architecture [22]) y representa especificaciones de lógica formal expresadas en lenguaje natural controlado, para ser procesadas por medios automáticos. Si bien el vocabulario y las reglas SBVR se ubican en la capa MDA-CIM, pueden ser mapeados a las capas MDA-PIM y MDA-PSM, que no son incluidas como parte del estándar [26,21]. SBVR define un metamodelo para reglas y vocabularios de negocios, orientados a la descripción del negocio en vez de orientarlo a una implementación técnica, dando un vocabulario a los expertos del dominio para que puedan expresar reglas de negocio. Por ello, el metamodelo se puede analizar a través de sus aspectos básicos: la comunidad : la empresa u organización para la cual se establecen la reglas de negocio, el cuerpo de conocimiento (significados) compartido: el conjunto de conceptos, hechos y reglas que se comparten en la comunidad, la formulación semántica: la forma en la cual se captura el conocimiento compartido, la lógica formal : las bases teóricas que son base para la formulación semántica, a partir de recortes de la lógica de primer orden y otras extensiones lógicas, y la la representación empresarial : representación de la formulación semántica en diferentes formas, que sean aceptadas y utilizadas por los miembros de la comunidad, por ejemplo, a través de lenguaje natural. 2 Ontorule Project - http://ontorule-project.eu/ CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 711 Como particularidad explícitamente separa significado, representación y simbología: el mismo significado puede tener varias representaciones y la misma expresión puede representar varios significados. La figura 1 muestra un subconjunto de los principales elementos del metamodelo. Se pueden encontrar más detalles de SBVR en [27]. Los Significados de SBVR se definen como “aquello que significa una palabra, un signo, una sentencia o una descripción, aquello que alguien intenta expresar o algo que se puede entender”, lo que se puede comprender como “algo percibible o concebible” [27]. 4. OWL2 Figura 2. Axiomas, Entidades e Individuos en OWL2 OWL se constituye como una familia de lenguajes de representación de conocimiento para la formulación de ontologías que surge motivado en las actividades de tecnologías semánticas para la Web, fue desarrollado por el Consorcio W3C3, y se fundamenta en la lógica para la descripción (DL - Description Logic)[2] SHOIN (D) [15]. OWL define tres variantes con niveles de expresividad incrementales: OWLLite (variante de DL-SHIF(D) [2,12,7]) para jerarquias de clasificación con restricciones simples, OWL-DL (casi equivalente a DL-SHOIN (D) [15,12]), para proveer máxima expresividad posible pero con decidibilidad y OWL-Full, unión de la sintaxis OWL y RDF/S, pero indecidible. A pesar de que OWL es muy conveniente para el procesamiento en máquina, no parece conveniente para modelar ontologías o construirlas a partir de lenguaje natural [16]. 3 The World Wide Web Consortium (W3C) - http://www.w3.org CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 712 OWL2, recomendación W3C de 2009, extiende a OWL con un conjunto de características que permiten técnicas y semánticas mejor comprendidas, útiles para los constructores de herramientas. Basado en DL-SROIQ(D) [13], es totalmente compatible con OWL1. Presenta tres perfiles o fragmentos [20], como restricciones sintácticas con mejores complejidades computacionales: OWL2 EL: para ontologías con gran cantidad de propiedades y clases (base en DL-EL+ +[1]), con baja complejidad computacional; OWL2 QL: para ontologías con un gran número de instancias y búsqueda de respuestas como razonamiento y OWL2 RL: para aplicaciones que requieren razonamiento escalabe sin sacrificar mucho poder expresivo. Puede implementarse utilizando lenguajes de regla estándares junto a razonadores basados en reglas, se inspira en Programas de Descripción Lógica (DLP - Description Logic Programs) [11] y se define estableciendo restricciones en la estructura de las ontologías OWL2. Figura 3. Expresiones de Clases (ClassExpressions) en OWL2 En la figura 2 se muestran los principales conceptos OWL2 que corresponden a significados SBVR, que pueden modelarse utilizando “Clases” (owl:Class) y la subclase “Expresión de Clase” (owl:ClassExpression). 5. Consideraciones En un empresa es imposible tener conocimiento de todos los conceptos y hechos que afectan al negocio, una empresa puede tener conocimiento completo de algunas partes de su especialidad, y conocimiento incompleto sobre otras partes. Por ello, en la práctica, la presunciones de Mundo Abierto (OWA - Open World Assumption) y Mundo Cerrado (CWA - Closed World Assumption) son aplicables por igual, con OWA por defecto, y aplicar una “clausura local” para “partes específicas” de ese conocimiento empresarial. Por ejemplo, una empresa debe conocer los nombre de todos los empleados, sobre este conocimiento aplica CWA, CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 713 mientras que es posible desconocer el número telefónico de todos los clientes, por lo que la ausencia de ese conocimiento no determina falsedad, presunción OWA. Las DLs base de los lenguajes OWL no son suficientemente expresivas como para modelar toda la semántica de los vocabularios de negocios, por lo que resulta necesario ampliar esa formalización a través de la incorporación de reglas utilizando extensiones lógicas en combinación. El Lenguaje de Reglas de la Web Semántica (SWRL - Semantic Web Rule Language [14]) es una extensión de OWL con reglas en verdadera Propiedad 5.1. Transformaciones de SBVR a OWL Las especificaciones SBVR sugieren que se pueden aparear algunos conceptos SBVR con conceptos OWL [27], pero la lista es incompleta, ya que OWL cubre sólo una parte de SBVR, porque su representación de reglas es muy limitada. Se analizan a continuación ejemplos de transformaciones de los Conceptos Tipo Objeto, Tipo Hecho y Cuantificadores SBVR en ontologías OWL, y se discuten algunas restricciones de la tranducción. 5.2. Tipo Objeto SBVR Un “Tipo Objeto” SBVR (Object Type) se define como un Concepto Sustantivo (noun concept) que clasifica items en base a propiedades comunes, su mapeo correspondería a la Clase OWL2 (Class), ya que Clase se comprende como un conjunto de entidades. Los Tipo Objeto SBVR que sólo tienen una única designación pueden traducirse en Clases OWL (Class) y el valor de la expresión de esta designación se puede transformar a Class IRI. En caso contrario, el Tipo Objeto SBVR se traduce en varias Clases, que se construyen como sinónimos y que se definen con el axioma “Clases Equivalentes OWL” (EquivalentClasses). Para reflejar las designaciones preferidas, característica utilizada diariamente en las actividades de la empresa [17], pueden agregarse “Comentarios” (Comment) a todas las Clases Equivalentes, consignando su naturaleza de “Designación Preferida” y “Designación No Preferida” o “Designación Prohibida”. De hecho, esta simple estrategia de distinción de designaciones heredada del vocabulario SBVR, también puede utilizarse para los Tipos Hecho y las formulaciones de Reglas de Negocio utilizando los Conceptos OWL2, y extenderse para las “Propiedades de Objetos” (owl:ObjectProperty) y “Propiedades de Datos” (owl:DataProperty). 5.3. Tipo Hecho El “Tipo Hecho” SBVR (Fact Type) indica un tipo de relación entre Conceptos Sustantivos (noun concepts) o una característica de un Concepto Sustantivo. Se analizan los Tipo Hecho Asociativo, Propiedad y Especialización (Categorización). CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 714 Un “Tipo Hecho Asociativo” (Associative Fact Type) representa una relación entre conceptos y es el Tipo Hecho más habitual. Cuando tiene dos roles, se transforma en OWL2 usando la “Propiedad de Objeto” (owl:ObjectProperty). Por ejemplo, la “fábrica de autos fabrica modelo de auto” se transforma en la expresión OWL2: <owl:ObjectProperty rdf:ID=""fabrica""> <rdfs:domain rdf:resource=""#Fabrica_Autos""/> <rdfs:range rdf:resource=""#Modelo_Auto""/> </owl:ObjectProperty> En el ejemplo anterior asumimos que el Rol del Tipo Hecho (Fact Type Role) coincide con el Tipo de Objeto que está jugando ese rol. Sin embargo, los Roles del Tipo Hecho pueden ser tipos de conceptos separados, por ejemplo, “el modelo del auto es fabricado por un fabricante”, con “fabricante” del Tipo de Objeto “Organización”. Esta simple característica dificulta la representación en la ontología OWL2, porque los roles de ontologías se refieren a los Tipos Hechos Asociativos en vez de a los roles SBVR. Una alternativa de mapeo es la representación utilizando los axiomas “Subpropiedad de Objeto” y “Cadena de Propiedad de Objeto” (SubObjectPropertyOf y ObjectPropertyChain). La expresión axiomática “SubObjectPropertyOf(ObjectPropertyChain (OPE 1 ... OPEn ) OPE )” establece que, si un individuo x se conecta con una secuencia de expresiones de propiedad de objeto “OPE 1 ... OPEn” con un individuo y, entonces x también está conectado con y por la expresión propiedad de objeto OPE. En nuestro caso, para representar el rol “fabricante”, definidas las clases “Organización”, “Fabrica Autos” y “Fabricante” usamos los siguientes axiomas: <owl:ObjectProperty rdf:ID=""fabrica""> (...) <rdf:Description rdf:about=""fabricante""> <owl:propertyChainAxiom rdf:parseType=""Collection""> <owl:ObjectProperty rdf:about=""es_fabricado_por""/> <owl:ObjectProperty rdf:about=""es_rol_de""/> </owl:propertyChainAxiom> </rdf:Description> Cuando se especifique una instancia en la que “Autolatina” del tipo “Organización”, “Ford” del tipo “Auto” y la propiedad “Ford es fabricado por Autolatina”, el razonador ontológico derivará la propiedad objeto “fabricante”. El Tipo Hecho “Es propiedad de” (Is Property Of ) define una cualidad o característica esencial de un Concepto Sustantivo dado y se identifica usualmente con la expresión “tiene” (en forma pasiva “es propiedad de”). En este Tipo Hecho, los roles se definen sobre un tipo de objeto que es un concepto elemental (número, entero o texto), por lo que se transforma como “Propiedad de Dato OWL” (DataProperty), en el que el primer rol se transforma en “Dominio” (de la Clase OWL) y el segundo en el “Rango” (owl Datatype) de la propiedad. Por ejemplo, “la fábrica de autos tiene nombre” y “la fábrica de autos tiene nacionalidad”, se transforma en la expresión OWL: CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 715 <owl:Class rdf:ID=""Fabrica_Autos""> (...) </owl:Class> <owl:DatatypeProperty rdf:ID=""nombre""> <rdfs:domain rdf:resource=""#Fabrica_Autos"" /> <rdfs:range rdf:resource=""&xsd;string""/> </owl:DatatypeProperty> <owl:DatatypeProperty rdf:ID=""nacionalidad""> <rdfs:domain rdf:resource=""#Fabrica_Autos"" /> <rdfs:range rdf:resource=""&xsd;string""/> </owl:DatatypeProperty> Finalmente, el “Tipo Hecho Especialización (Categorización)” (Specialization (Categorization) Fact Type) representa relaciones entre dos tipos de objetos (conceptos sustantivos): el más general y el más específico. Categorización SBVR se puede transformar al concepto “Sub-clase OWL” (SubClassOf ): <owl:Class rdf:ID=""Escuela""> <rdfs:subClassOf rdf:resource=""#Organizacion"" /> (...) 5.4. Transformación de Cuantificadores Casi todos los cuantificadores presentes en SBVR pueden transformarse directamente como restricciones OWL. El “cuantificador universal”puede transformarse en la restricción OWL “Todos los Valores” (owl:allValuesFrom, ObjectAllValuesFrom o DataAllValuesFrom), el “cuantificador existencial” es un caso especial del cuantificador “al menos n”, con n=1, que se mapea a la restricción OWL “Cardinalidad Mínima” (owl:minCardinality 1 ) o a la restricción “Algunos Valores” (owl:someValuesFrom). Las cuantificaciones expresadas como “exactamente n”, “exactamente 1”, “al menos n” o “como máximo n” tienen el mismo tratamiento con las restricciones OWL apropiadas. 5.5. Problemas en la Transformación Se presentan dificultades al momento de transformar los Tipos de Hecho Asociativos con multiplicidad de roles, ya que OWL sólo maneja propiedades binarias. Podría solucionarse incorporando una nueva Clase para la relación, o utilizar listas como argumentos en una relación. Aunque son sugerencias válidas, esas transformaciones oscurecen la semántica de la ontología expresada en lenguaje natural, cuando se intenta construir Preguntas o Reglas de Negocio: no resultan adecuadas para el uso propuesto, la comprensión de los expertos del dominio. Es necesario realizar mayores análisis para resolver este problema, ya que incluso los Tipos de Hecho Asociativo con multiplicidad de roles no se abordan con precisión en la especificación de SBVR. Otra dificultad presente muchas veces en la práctica, surge porque los Tipos Objetos tienen “varias jerarquias de generalizaciones’ ’, en base a distintos CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 716 criterios. En SBVR estos conjuntos de generalización se representan con “Esquemas de Categorización” (generalizaciones disjuntas incompletas) y “Esquemas de Segmentación” (disjuntas completas) que no tienen su correlato en OWL2. 6. Conclusiones y trabajo futuro SBVR, que fue propuesto como estándar y tiene su propio lenguaje natural controlado, puede utilizarse inicialmente como lenguaje para la creación de ontologías. Aunque se hace referencia a su naturaleza formal, aún no existen métodos o herramientas que realicen la transformación al lenguaje de ontologías OWL. La recomendación OWL2 provee mayores capacidades para expresar semántica del dominio de negocios, por lo que es necesario continuar avanzando en su estudio. En el presente trabajo se realizaron propuestas de transformaciones de algunos significados SBVR y se muestran algunas dificultades. Se continua con la investigación para mejorar las alternativas presentadas en la gestión de sinónimos y los roles de los Tipos Hecho. Se continuará avanzando sobre alternativas de incorporación de reglas al lenguaje OWL para combinar presunciones OWA y CWA necesarias en la dinámica diaria empresarial, atendiendo las posibles descripciones en el metalenguaje SBVR."	﻿especificación OMG SBVR , conocimiento , tecnologías semánticas , lenguajes de ontologías	es	18727
5	Aplicación de Redes bayesianas usando Weka	﻿ Este trabajo tiene el objetivo de exponer la aplicación de una técnica de  minería de datos como las Redes Bayesianas,  aplicadas a la resolución de una  problemática relacionada del campo de la ingeniería como lo es el mantenimiento  correctivo. En él se expone cual es la problemática y el porqué de la elección de  esta técnica para la clasificación de ocurrencias y cuales son los resultados  obtenidos de aplicar esta técnica de minería de datos usando software Weka.    Keywords: Minería de Datos, Técnicas de clasificación, Redes Bayesianas,  Aprendizaje automático, Weka.   1 Introducción  La minería de datos es un proceso que tiene como objetivo descubrir y extraer  información relevante de base de datos o de otras fuentes de almacenamiento de  datos, facilitando la identificación de patrones, tendencias  como así también  develar hechos anormales que pueden estar sucediendo.  Esto permite el aprovechamiento  del valor de la información para que los  directivos tengan un mejor conocimiento de su negocio y poder tomar decisiones  más confiables.  Este trabajo tiene como objetivo la aplicación de una técnica de minería de datos  dentro del grupo de técnicas de clasificación como lo es las redes bayesianas que  será aplicada a una problemática perteneciente al ámbito de la industria, en una  problemática muy frecuente como lo es el tema del mantenimiento correctivo de  los maquinarias involucradas con el proceso productivo.  La problemática expresada por la empresa es la preocupación por la frecuencia  en que se reportan órdenes de mantenimiento correctivo de las áreas  involucradas en el proceso productivo.   Para develar cual pueden ser las causas de este comportamiento, como ya se ha  expuesto aplicaremos la técnica de redes bayesianas y el clasificador  seleccionado es NaivesBayes usando el software Weka.   El eje del estudio y análisis se centra en determinar cuáles son los posibles  factores que tienen una mayor incidencia en los reportes de órdenes de  mantenimiento correctivos generados en la empresa.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 939 1.1 Redes Bayesianas: Fundamentación teórica.  El avance importante que ha tenido el campo de la tecnología y el abaratamiento de  costos ha traído como consecuencia un aumento significativo en la cantidad de datos  que son almacenados en muchas ocasiones en diferentes formatos.  La minería de datos es un mecanismo que nos permite facilitar  la búsqueda de  información valiosa en grandes volúmenes de datos. Este trabajo tiene como objetivo  aplicar una técnica predictiva al campo de la industria como lo es el mantenimiento.  La redes bayesianas es una técnica que pertenece al grupo de las técnicas de  clasificación y consiste en un modelo gráfico que utiliza arcos para formar una gráfica  acíclica y es aplicado en aquellas situaciones en que la incertidumbre se asocia con un  resultado que se puede expresar en términos de probabilidad. Es decir los nodos del  grafo representan variables, los arcos representan dependencia condicional y una  distribución de probabilidad.   [1] En un comienzo, estos modelos eran construidos a mano basados en un  conocimiento experto, pero en la actualidad se han investigado técnicas para aprender  de dichos datos, tanto la estructura como los parámetros asociados al modelo.      Fig. 1. Ejemplo de red bayesiana.  Esta técnica busca determinar relaciones causales que expliquen un fenómeno y es  aplicado en aquellos casos que son de  carácter predictivo.   [2] Es decir el razonamiento probabilístico o propagación de probabilidades consiste  en difundir los efectos de la evidencia por medio de la red para conocer la  probabilidad a posteriori de las variables. Es decir a determinadas variables  (conocidas) se les otorga una probabilidad y en base a esto se obtiene una  probabilidad posterior.  Hay ciertos conceptos básicos que están asociados a esta técnica como:    Grafo: Par de conjuntos  G=(X, L) donde X es un conjunto finito de elementos  (nodos) y L es un conjunto de arcos.  Arco: subconjunto de pares ordenados.  Grafo Dirigido: Par ordenado G=(X, L) donde X es el conjunto de nodos y L conjunto  de arcos.  Grafo Acíclico: grafo que no tiene ciclos.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 940   Una red bayesiana G define una distribución de probabilidad conjunta única sobre U  dada por:                                   PB (X1, X2,…, Xn)= π PB (Xi| π Xi)                                          (1)  [3] Cualquier sistema de clasificación de patrones se basa en lo siguiente: dado un  conjunto de datos (que dividiremos en dos conjuntos de entrenamiento y de test)  representados por pares <atributo, valor>, el problema consiste en encontrar una  función f(x) (llamada hipótesis) que clasifique dichos ejemplos.  La idea de usar el teorema de Bayes en cualquier problema de aprendizaje automático  es que podemos estimar las probabilidades a posteriori de cualquier hipótesis  consistente con el conjunto de datos de entrenamiento para así seleccionar la hipótesis  más probable.   Para estimar estas probabilidades se han propuesto numerosos clasificadores  bayesianos.  [2] Un clasificador en general suministra una función que clasifica una instancia  especificada por una serie de características o atributos, en una o en diferentes clases  predefinidas.  En general los clasificadores bayesianos son ampliamente utilizados debido a que  presentan ciertas características:  o Son simples de construir y comprender.  o El proceso de inducción a partir de los mismos es veloz y sencillo.  o Robusto en cuanto considera atributos irrelevantes.  o Considera una importante cantidad de atributos para generar la predicción final.    Un clasificador bayesiano puede ser tomando en cuenta como un caso particular de  una red bayesiana, en la que hay una variable que cumple el rol de la clase y los  demás variables son consideradas atributos. La estructura va a depender  fundamentalmente del tipo de clasificador.  Los clasificadores de redes bayesianas son:  Clasificador Bayesiano Simple (Naives Bayes classifier, NBC): permite obtener la  probabilidad posterior de cada clase Ci, usando la regla de Bayes.  Este clasificador asume que los atributos son independientes entre sí dada la clase, así  que la probabilidad se puede obtener por el producto de las probabilidades  condicionales individuales de cada atributo.  La representación gráfica puede darse como una red bayesiana en forma de estrella.  Es decir un nodo raíz, que representa la variable de la clase y que está conectada a los  atributos.  Este clasificador bayesiano tiene extensiones y el fundamento de su uso es cuando se  disponen de atributos que son dependientes.  Una manera de considerar este dependencia es extendiendo la estructura básica de  NBC, incorporando arcos a dichos nodos.  Las posibilidades básicas son:   o TAN: clasificador bayesiano simple aumentado con un árbol.  o BAN: clasificador bayesiano simple aumentado con una red.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 941       Fig. 2. Extensiones del clasificador bayesiano simple: (A) TAN, (B) BAN.  Redes bayesianas dinámicas: este clasificador permite representar el estado de las  variables en un cierto momento de tiempo. En el caso de existir la necesidad de  representar estos procesos dinámicos existe esta extensión conocida como red  bayesiana dinámica (RBD).  Para las redes bayesianas dinámicas, generalmente se hacen las siguientes  suposiciones:  o Proceso markoviano: el estado actual solo depende del estado anterior.  o Proceso estacionario en el tiempo: las probabilidades condicionales en el modelo  no se alteran con el tiempo.    El aprendizaje de las redes bayesianas consiste en inferir un modelo, estructuras y  parámetros, a partir de los datos, que puede ser agrupado en dos aspectos:  Aprendizaje Estructural: obtiene la estructura de  la red bayesiana (o topología de red)  tomando como punto de partida una base de datos, es decir las relaciones de  dependencia entre las variables involucradas.  De acuerdo al tipo de estructura, podemos dividir los métodos de aprendizaje  estructural en: Aprendizaje de árboles, Aprendizaje poliarboles, Aprendizaje de redes  multiconectadas.  Aprendizaje Paramétrico: dada una estructura, obtiene las probabilidades asociadas.  El requisito fundamental para llevar a cabo la tarea de aprendizaje de redes bayesianas  es disponer de una base de datos  en la que este detallado el valor de cada variable en  cada uno de los casos.  En el caso de nuestra situación problemática considerada la elección de esta técnica  de minería de datos se fundamenta en que en base a una red ya construida, y dados los  valores concretos de algunas variables de una instancia, podrían tratar de estimar se  los valores de otras variables de la misma instancia aplicando razonamiento  probabilístico.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 942 1.2 Metodología  La propuesta metodológica de trabajo consiste en el uso de una herramienta para el  aprendizaje automático denominada Weka.  Esta herramienta es de distribución libre, desarrollada en Java y permite la  implementación de técnicas de clasificación, agrupamiento y asociación; como así  también realizar tareas de preprocesado y filtrado de datos.  A continuación se detallan las etapas llevadas a cabo para el análisis y estudio de la  problemática expuesta anteriormente:    o Selección de las variables significativas: En todos los estudios que se aplican  técnicas de minería de datos, no siempre todas las características o atributos  disponibles son realmente relevantes para obtener un modelo de conocimiento.  Nuestro caso no es la excepción, el problema se enfoca en saber cuál es la  probabilidad de que exista reportes de mantenimiento en una determinada maquinaria  dadas ciertas condiciones (variables).   Es por ello que nos ayudaremos del software Weka para no tener en cuenta  ciertas  variables usando los filtros establecidos en la sección de Preprocesamiento.  A continuación se detallan cuales han sido las variables consideradas para nuestro  estudio:  Tabla 1.  Variables consideradas para la aplicación del algoritmo BayesNet.  Atributo Descripción  fallo Este atributo corresponde a la clase, que permitirá  conocer que atributos inciden en la probabilidad de  ocurrir un fallo en una maquinaria. El tipo de datos es  booleano.  Fallo humano Representa si el fallo de una maquinaria es  responsabilidad humana o no. El tipo de datos es  booleano.  Turno Indica en que turno de trabajo se ha reportado el fallo.  Este tipo de dato es nominal.  Mantenimiento  Programado  Significa que en el momento que se reporta el fallo,  existe la planificación de un mantenimiento  programado. El tipo de dato es booleano.  Área Indica el área de la empresa en la que se reporta el fallo.  Este tipo de dato es nominal.    Es importante mencionar que la información histórica reportada por la empresa  relacionada con la registración de órdenes de mantenimiento correctivo, se encuentra  almacenada en formato de planilla de cálculo (Excel).  Además para reducir el conjunto de atributos seleccionados hemos usado un Filtro de  Weka que se encuentra en la categoría de Atributos denominado “Remove”, que  permite borrar un conjunto de atributos especificados en el archivo de datos.    CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 943 o Transformación de los datos en el formato adecuado: En nuestro caso la fuente de  datos obtenida se encuentra almacenada en una planilla de cálculo; el software que  utilizamos es Weka por lo tanto es necesario que los datos tengan el formato  adecuado para ser procesados por la herramienta. El formato adecuado de archivo  .arff y la estructura del mismo se ilustra con el siguiente ejemplo:    (A)@relation mantenimiento    (B)  @attribute fallo {yes,no}  @attribute turno{manana,tarde,noche}  @attribute falloHumano{TRUE,FALSE}  @attribute mantenimientoProg{TRUE,FALSE}  @attribute area{Produccion,Embalaje}    (C)  @data  yes,tarde,TRUE,TRUE,Produccion  no,manana,FALSE,TRUE,Embalaje  yes,noche,FALSE,TRUE,Produccion  no,manana,FALSE,FALSE,Produccion  yes,noche,TRUE,TRUE,Produccion  no,manana,FALSE,TRUE,Embalaje     (A): En esta sección se define el nombre de la relación.  (B): Se especifican los atributos de la relación y el tipo de dato.  (C): Es la sección de datos propiamente dicha.    o Tratamiento de los valores faltantes en algunos atributos: Es importante una vez  importado los datos a  la herramienta realizar un análisis de cuál es el nivel de valores  faltantes en las variables consideradas, ya que es un factor importante que puede  llegar a influir en el modelo de predicción a obtener.  Weka nos ofrece una herramienta para el tratamiento de valores faltantes, que consiste  en eliminar todas las instancias con valores nulos que es la que hemos considerado.    Weka permite aplicar una gran diversidad de filtros sobre los datos, permitiendo  realizar transformaciones sobre ellos de todo tipo. En este caso hemos seleccionado  un filtro que se encuentra en la categoría de Atributos cuya denominación es  “ReplaceMissingValues”, que permite reemplazar todos los valores indefinidos por la  moda en el caso de que sea el atributo nominal, como nuestro caso.  o Selección del algoritmo a utilizar: Para poder usar algún algoritmo perteneciente a  la familia de los clasificadores bayesianos, se encuentra en la ruta  weka.classifiers.bayes.NaiveBayes.   En este estudio se ha decidido elegir  el algoritmo BayesNet. En la figura 3 se puede  visualizar cuales son los parámetros que se pueden configurar para la aplicación de la  técnica.    CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 944  Fig. 3. Parámetros a configurar para la aplicación del algoritmo.  Antes de ejecutar el algoritmo se ha seleccionado una de las opciones de test: Crossvalidation: esto permite realizar la evaluación mediante la técnica de validación  cruzada; permitiendo establecer el número de muestras a utilizar.  Luego especificamos del conjunto de atributos seleccionados el que será considerado  como clase principal, que será Fallo {yes/no}.Una vez que se ejecuta el algoritmo se  visualiza en la ventana de salida con la siguiente información:      Fig. 4. Ventana del clasificador de salida.  Weka nos brinda la posibilidad de visualizar el gráfico del árbol generado a partir de  la clasificación.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 945  Fig. 5. Árbol de clasificación generado por Weka.  o Interpretación del modelo: consiste en analizar cuál es la probabilidad de las  variables consideradas, respecto a la variable considerada como clase. Este aspecto se  detallará más en profundidad en la sección de Resultados obtenidos y esperados.  o Validación del modelo de predicción: En esta etapa se consideran ciertos  parámetros (que se detallan más adelante), para validar cual es el nivel de confianza  del modelo de clasificación obtenido.    1.4 Resultados y Conclusiones  Una vez obtenido el árbol de clasificación, es necesario poder evaluar la calidad o  nivel de confianza del  mismo. La métrica Kappa Statistic es considerada para tal fin.   El Kappa Statistic es una medida que permite medir el nivel de predicción respecto a  la variable considerada como clase.   En los resultados obtenidos en este estudio se da para cada variable (cada nodo del  árbol) el nivel de probabilidad de la misma respecto a la clase principal. Por ejemplo:  Si seleccionamos el atributo área en el árbol, nos muestra información que se resume  a continuación:      Fig. 6. Tabla de distribución de probabilidad para la variable área.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 946 De acuerdo a los resultados que figuran en la tabla para la variable área podemos  inferir que si el área es Producción la probabilidad que se produzca un reporte de fallo  en una maquinaria es del 69% y en el caso de tratarse del área de Embalaje será del  31%.  Realizando este procedimiento con todas las variables consideradas se concluye que:  o Área de producción en el turno de la tarde hay una probabilidad del 74% que se  reporte un fallo en una maquinaria del sector.  o En el área de producción existe una probabilidad del 70% que los reportes de  fallos a maquinaria se deben por falla humana o error en la operación.  Hay una clara tendencia de que la mayor frecuencia de reporte de mantenimiento se  origina en el área de Producción básicamente en el turno tarde, con lo que se agrega  un porcentaje alto de los reporte se deben a fallos humanos o error en la operación.  Por lo cual se sugieren las siguientes recomendaciones:  o Capacitación relacionada con el manejo y funcionamiento de las maquinarias,  orientada al sector de Producción específicamente a los operarios del turno tarde.  o Analizar las condiciones ambientales y físicas del sector de producción como  ubicación, luminosidad y posibles ruidos que puedan interferir en el trabajo.   Estos factores, que no están relacionados directamente con el trabajo, pero se ha  comprobado que afectan a la productividad con la probabilidad de generar malas  operaciones en las maquinarias.  o Analizar las condiciones ambientales actuales de las maquinarias del área de  producción como luminosidad, nivel de humedad, instalaciones eléctricas sean las  adecuadas. Es posible que esto puede estar afectando mal funcionamiento en las  misma generando reportes de mantenimiento.    En este trabajo se ha planteado una propuesta metodológica de un caso práctico de  minería de datos usando redes bayesianas. Una vez generado el modelo se han  propuesto una serie de recomendaciones con el objetivo de identificar las variables  que tienen un mayor nivel de incidencia en la problemática expuesta.  Los métodos bayesianos realizan un aporte desde el punto de vista cuantitativo, es  decir da una medida probabilística de las variables consideradas en una determinada  situación. Esta es una de las diferencias fundamentales del uso de las redes bayesianas  respecto a otros métodos como redes neuronales y los arboles de decisión, que no  ofrecen una medida cuantitativa de la clasificación.	﻿técnicas de clasificación , redes bayesianas , aprendizaje automático , Weka	es	18750
6	Compartir información de gobierno Beneficios	"﻿ Compartir Información (CI) permite el intercambio e integración de  información entre distintas dependencias de una organización, entre empresas o  agencias de gobierno. El compartir información permite aumentar la eficiencia  – evitando la duplicación de procesos que actualizan los mismos datos; la  calidad de los procesos y servicios – eliminando errores por datos  inconsistentes, y la transparencia – facilitando el acceso a la información.  Debido a los cambios tecnológicos, organizacionales, institucionales y de  entorno que se requieren para implementar iniciativas de CI en gobierno, tales  iniciativas requieren la definición de casos de negocios convincentes al  momento de estudiar la factibilidad de las mismas. En este trabajo  se presentan  los resultados de las tareas de investigación realizadas para estudiar los  beneficios de CI en gobierno. La investigación basada en recolección de datos  secundarios, relevó iniciativas de CI implementadas en varios países.  En base a  los datos recolectados, se presenta un estudio exhaustivo de los beneficios de CI  en gobierno, con una propuesta para clasificación de beneficios. La mayor  contribución de este trabajo, es proveer una lista detallada de beneficios  factibles al CI y una clasificación de los mismos. Estos resultados sirven de  referencia para justificar la implementación de iniciativas de CI.  Keywords: Compartir Información, Gobierno Electrónico, Beneficios,  Barreras, Riesgos  1   Introducción  La integración de información es considerada una de las más significativas formas  de cambiar la estructura y la función de las organizaciones. En la más simple  conceptualización, la integración de información permite a los gerentes trabajar al  mismo tiempo, con la misma información procedente de múltiples fuentes. Tiene el  potencial para apoyar la transformación de las estructuras de organización y canales  de comunicación entre los múltiples organismos que trabajan en diferentes lugares.                                                              CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1294 El Gobierno electrónico es una herramienta que permite obtener un mejor resultado  de las políticas, una mayor calidad de los servicios públicos, eficiencia en los  procesos de gobierno, un uso eficaz de fondos públicos, aumento de la participación  ciudadana, etc. Existen diferentes modelos de madurez para evaluar el nivel de  desarrollo de Gobierno electrónico. Todos los modelos coinciden que, en el nivel más  alto de madurez, los organismos gubernamentales son capaces de compartir  información.    El compartir y la integración de la información es un desafío relativamente nuevo  para los organismos públicos. Las estructuras tradicionales de gobierno han  organizado la captura, uso y manejo de información a lo largo de líneas de agencia.  La superación de estos programas profundamente arraigados y los ''silos""' de  información son un desafío particular por parte de las agencias de cara a perseguir los  beneficios de la información integrada.     Compartir Información (CI) se define como el intercambio de información, que  permite el acceso a la información de otros organismos. El intercambio de  información y la integración puede ayudar a las agencias del gobierno para prestar  mejores servicios públicos y para resolver los problemas públicos críticos a través de  facilitar la colaboración interinstitucional.    CI ayuda también a transformar el desarrollo de políticas, el diseño y ejecución de  programas y los servicios gubernamentales en todos los niveles del gobierno.    Su función es la de compartir información, articulando  los principios que ayudará  a las agencias a compartir información para beneficiar tanto la economía como la vida  de los ciudadanos. La consideración exhaustiva de los siguientes aspectos es  importante en la implementación de CI para Gobierno:     Beneficios - valor agregado que brinda a los procesos de negocio, de servicio  y a los clientes. Son las consecuencias útiles de la utilización de CI.   Barreras - obstáculos que pueden ocurrir durante la implementación.   Riesgos - valor negativo que puede ocasionar un perjuicio a los procesos, a  los servicios y/o clientes.    En este trabajo se presenta el análisis y desarrollo de los beneficios alcanzados a  partir de la implementación de CI. Se propone una clasificación de los mismos. La  importancia de la clasificación está asociada con los diversos aspectos a considerar en  la interoperabilidad, como es el desarrollo del proyecto y diseño del framework. [7]   2   Trabajos Previos  Hace varios años se publicaron dos de los más influyentes modelos teóricos de  intercambio de información en gobierno. El primero publicado por Dawes en 1996,  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1295 describe un ciclo de aprendizaje de agencias gubernamentales involucradas en la  práctica del intercambio de información (CI).    Dawes [3] señala que la integración de información, así como el intercambio de  información, ofrece a las organizaciones una mayor capacidad para compartir  información a través de fronteras organizativas, para descubrir patrones e  interacciones, y para tomar decisiones mejor informadas sobre la base de datos más  completa.    Dawes [3] clasifica los beneficios que puede brindar cualquier iniciativa de  integración de la información o compartir información (CI), en tres categorías:  técnicos, organizacional y políticos.     Beneficios técnicos son aquellos relacionados con el procesamiento de datos y el  manejo de la información.   Beneficios organizacionales están relacionados a la solución de problemas de todo  el organismo o a la mejora de las capacidades de la organización.    Beneficios políticos podría incluir una mejor apreciación de los objetivos de la  política de todo el gobierno, mayor rendición de cuentas públicas, la información  pública más completa, la planificación integrada y la prestación de servicios son  algunos ejemplos de este tipo de beneficios.    La clasificación (técnicos, organizacionales y políticos) propuesta por Dawes  también se puede aplicar a las barreras y riesgos. Estos dos últimos tópicos están fuera  del alcance de este trabajo.    Estevez y otros [5] partieron del estudio del modelo conceptual para el intercambio  de información del gobierno, de la integración de conocidos frameworks teóricos, y  del uso de frameworks de interoperabilidad de la información. La característica más  relevante del modelo propuesto GISF, es la de agrupar los conceptos en base a los tres  estados de madurez de Landsbergen y Wolken, mientras que los clasifica de acuerdo a  cuatro dimensiones - de la organización, entre organizaciones, políticos y técnicos.     El modelo GISF destaca las áreas específicas que deben abordarse en el desarrollo,  como las políticas para las áreas protegidas. Aquellos aspectos que afectan a la  aplicación de las políticas como sociales y políticos y las relaciones de poder entre las  organizaciones. Mejora la claridad conceptual de intercambio de información en  cuanto al contexto del gobierno, relacionados con las iniciativas del gobierno como  interoperabilidad. Identifica las áreas de interés y ejemplos de iniciativas concretas  CIF (CI Gobierno) para los responsables políticos y administradores públicos. Una  serie de problemas de investigación interesantes relacionados con el intercambio de  información transfronteriza, puede ser identificado a través del framework.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1296 3   Clasificación IEPP-BBR  Los beneficios, barreras y riesgos son factores que influyen en la integración de la  información. Estos factores se los puede catalogar desde distintas aspectos. En la  propuesta de Dawes [3], los aspectos considerados son técnicos, organizacionales y  políticos.     Inicialmente se pueden considerar los beneficios de CI  como las calidades de un  producto. En cualquier disciplina de ingeniería, en este caso particular, aplicado a la  ingeniería de software, construir un producto de alta calidad es el objetivo a alcanzar.  Haciendo un paralelismo entre las disciplinas, se propone la siguiente  clasificación de  los beneficios:      ciudadanía y  gobierno (internos y externos)   producto y proceso   elementales y derivados    Los  beneficios externos (ciudadanía) están asociados con los beneficios de los  usuarios. En este caso, los ciudadanos, empresas, oficinas gubernamentales. En  cuanto a los beneficios internos (gobierno), están asociados a las bondades que se  introducen para cada una de las agencias en el desarrollo de sus actividades.    Además se pueden considerar los beneficios desde el punto de vista del producto.  En este caso, directamente relacionado con la calidad de los servicios que ofrecen  cada una de las agencias de gobierno. Desde el punto de vista del proceso,  relacionados con la forma de llevar  a cabo cada una de las etapas correspondientes de  los servicios. Para este último aspecto es importante considerar que los procesos  deben analizarse y reformularse para lograr eficiencia y eficacia en la entrega de los  mismos.    El producto está comprendido por programas, datos y documentación. El proceso  es el marco de trabajo de tareas a realizar para desarrollar software de alta calidad.  Las áreas claves del proceso forman la base del control de gestión de proyectos del  software y establecen el contexto en el que se aplican los métodos técnicos, se  obtienen productos del trabajo (modelos, documentos, datos, informes, formularios,  etc.), se establecen hitos, se asegura la calidad y el cambio se gestiona  adecuadamente.    Los beneficios elementales son aquellos que se obtienen directamente de la  implementación, y derivados son aquellos que se alcanzan a partir de los beneficios  elementales.    La idea de esta propuesta es clasificar a los beneficios considerando varias vistas.     Vista 1. Comenzando con la clasificación propuesta por Dawes. La primera vista  identifica los beneficios en técnicos, organizacionales y políticos.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1297  Vista 2. Este aspecto identifica a los beneficios en elementales y derivados. Los  beneficios derivados son logrados a partir de los beneficios elementales.   Vista 3.  Los beneficios se dividen en internos y externos. Los internos están  asociados con las agencias gubernamentales y los externos con los ciudadanos.   Vista 4. Este lado considera a los beneficios asociados al producto o proceso.   Vista transversal. Algunos beneficios son universales (globales), esto es, cubren  todos los aspectos y son esenciales a cualquier desarrollo, como por ejemplo:  eficiencia, eficacia, respuesta.    En la figura 1 se muestran las vistas 1 a 4. La cara de enfrente del cubo muestra la  Vista 1, el punto de partida de este estudio. En la cara superior, se muestra la Vista 2,  la misma identifica a los beneficios en elementales y derivados. En la cara de la  derecha se muestra la Vista 3 y por último en la cara de la izquierda la Vista 4.  Dependiendo del beneficio y de la organización, un mismo beneficio puede  pertenecer a diferentes opciones de las vistas. En la sección 4, se presenta una lista de  beneficios y ejemplos mostrando las relaciones existentes entre las vistas.         Fig. 1. Vistas  4   Beneficios  En la actualidad, existen varios países que tienen implementados frameworks de  desarrollo con diferentes niveles de maduración con respecto a Gobierno Electrónico  y CI. Frameworks de algunos países referentes relacionados con estos conceptos son:  Australia, Nueva Zelanda, Reino Unido, Estonia y EEUU.  Del estudio y análisis de  estas implementaciones de CI en Gobierno Electrónico, como por ejemplo [7], [1],  [2], [10], [8], etc., a continuación se presenta una lista consolidada de beneficios:    [Be1.] Reducción de los costos (recopilación de la información, gestión de  información, utilización de la información, compartición de la  infraestructura).  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1298 [Be2.] Mejora para la toma de decisiones de los procesos políticos y de  negocios.  [Be3.] Mejora la puntualidad, coherencia, calidad de las respuestas.  [Be4.] Mejor y mayor  rendición de cuentas públicas.  [Be5.] Mejora la transparencia.  [Be6.] Incorporación de valor añadido para el gobierno por la reutilización  de información existente.  [Be7.] Diseño de métodos integrados y colaborativos para la prestación de  servicios.  [Be8.] Mejora la seguridad nacional.  [Be9.] Mejora de la competitividad nacional.  [Be10.] Reducción de la burocracia.  [Be11.] Reducción de la complejidad e inconsistencias.  [Be12.] Promoción de acceso a medios con información de mejor calidad.  [Be13.] Obtención de información comparable.  [Be14.] Mejora de los servicios de emergencia y de salud.  [Be15.] Mejora la comunicación entre los organismos del gobierno y afines.  [Be16.] Los servicios públicos son provistos dónde son más necesitados.  [Be17.] Facilita al público el acceso a diferentes servicios de gobierno entre  los distintos niveles de gobierno.  [Be18.] Estimula la consistencia en los enfoques.  [Be19.] Estimula la construcción de sistemas, conocimiento y experiencias  reutilizables a partir de una agencia a otra.  [Be20.] Promueve mejores estándares y la compartición de recursos técnicos.  [Be21.] Promueve la mejora en la coordinación.  [Be22.] Eficacia.  [Be23.] Eficiencia.  [Be24.] Respuesta.  [Be25.] Mejora de la eficiencia en las tareas de procesamiento masivo y las  operaciones de la administración pública.  [Be26.] Contribuye con la mejora de la productividad empresarial mediante la  simplificación administrativa.  [Be27.] Mejora la confianza entre el gobierno y los ciudadanos.    A partir de la lista consolidada de beneficios, se detalla en la Tabla 1 la  clasificación de los mismos de acuerdo a la Vista 3, que los identifica como internos  (gobierno) y externos (ciudadanía).  De las vistas presentadas, se seleccionó la  clasificación de internos y externos, ya que se considera que presenta una mayor  relevancia al momento de la toma de decisión en la implementación de CI en  Gobierno Electrónico.    Internos Externos   [Be1.] Reducción de los costos.   [Be2.] Mejora para la toma de decisiones de  los procesos políticos y de negocios.    [Be3.] Mejora la puntualidad, coherencia,  calidad de las respuestas.  [Be3.] Mejora la puntualidad de las  respuestas.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1299 Internos Externos    [Be4.] Mejor y mayor rendición de cuentas  públicas.   [Be5.] Mejora la transparencia.  [Be6.] Incorporación de valor añadido por la  reutilización de información existente.     [Be7.] Diseño de métodos integrados y  colaborativos para la prestación de servicios.    [Be8.] Mejora la seguridad nacional.   [Be9.] Mejora de la competitividad nacional.    [Be10.] Reducción de la burocracia.  [Be11.] Reducción de la complejidad e  inconsistencias.  [Be11.] Reducción de la complejidad e  inconsistencias.  [Be12.] Promoción de acceso a medios con  información de mejor calidad.  [Be12.] Promoción de acceso a medios con  información de mejor calidad.  [Be13.] Obtención de información  comparable.     [Be14.] Mejora de los servicios de  emergencia y de salud.  [Be15.] Mejora la comunicación entre los  sectores del gobierno y afines.     [Be16.] Los servicios públicos son provistos  dónde son más necesitados.   [Be17.] Facilita al público el acceso a  diferentes servicios de gobierno entre los  distintos niveles de gobierno.  [Be18.] Estimula la consistencia en los  enfoques.    [Be19.] Estimula la construcción de  sistemas, conocimiento y experiencias  reutilizables a partir de una agencia a otra.    [Be20.]  Promueve mejores estándares y  la  compartición de recursos técnicos.      [Be21.] Promueve la mejora en la  coordinación.    [Be22. – Be24.] son beneficios transversales.  [Be25.] Mejoras de la eficiencia en las tareas  de procesamiento masivo y las operaciones  de la administración pública.     [Be26.] Contribuye con la mejora de la  productividad empresarial mediante la  simplificación administrativa.  [Be27.] Mejora la confianza entre el  gobierno y los ciudadanos.    Tabla 1. Internos y Externos    El «beneficio de reducción de costos» (Be1.), es una bondad deseable para la  agencia u organización, según la Vista 1 de la clasificación, corresponde a la opción  organizacional. La reducción de los costos es una meta que promueve el análisis y  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1300  estudio para el desarrollo de un proyecto, por ende se lo considera un logro elemental  y también está asociada con la vista interna. Este beneficio agrupa un conjunto de  elementos. Considerando la reducción del costo de recopilación de información, se  puede asociar como perteneciente al proceso.     El «diseño de métodos integrados y colaborativos para la prestación del  servicio» (Be7.) es un beneficio técnico, que también está asociado al proceso y es  elemental.     El beneficio «reducir la complejidad e inconsistencias» (Be11.) es un derivado  del beneficio (Be7.), siendo además un beneficio de tipo organizacional y político,  que está vinculado tanto con las agencias como con la ciudadanía (interno y externo).     El beneficio «mejora la transparencia» (Be5.) es un beneficio político, que está  asociado con la ciudadanía (externo) y es derivado. A su vez se infiere como  consecuencia de (Be7.) y (Be11.)     Elementales Derivados   Internos Externos Internos Externos  Organizacionales [Be1.]         [Be10.]      [Be15.]  (Prod.)       [Be11.]   Políticos          [Be5.]  Técnicos [Be7.] (Proc.)     [Be6.]     Tabla 2. Ejemplos de Estudio   La Tabla 2 muestra los ejemplos mas representativos según nuestro criterio,  vinculando las vistas con las clasificaciones presentadas en este trabajo.   5   Conclusiones  La integración de la información brinda beneficios. Estos beneficios obtenidos pueden  diferir de una organización a otra, de acuerdo con características de los proyectos  específicos. Sin embargo, hay cierto tipo de beneficios que pueden esperarse en casi  cualquier integración de la información, o la iniciativa del intercambio de  información.    En este trabajo se realizó un estudio riguroso de los beneficios obtenidos a partir de  CI implementados en varios países. De este estudio, uno de los aportes más relevantes  es la lista exhaustiva y consolidada de beneficios obtenida. La importancia de esta  lista es que nos permitió agrupar y clasificar a los beneficios en función de tópicos  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1301 útiles para la implementación de CI. La clasificación propuesta en vistas, destaca  distintos aspectos a tener en cuenta al momento de determinar el estudio de  factibilidad, la toma de decisiones, el proceso de desarrollo (análisis, construcción,  producción), la evaluación y monitoreo de los mismos en la implementación de CI en  las agencias de gobierno. Las vistas facilitan el entendimiento del aporte de los  beneficios en los diferentes puntos de vista evaluados.    Además de los beneficios, es necesario identificar las barreras y los riesgos que  pueden ocurrir por la implementación de la integración de la información.  Comprender los beneficios y los objetivos de la integración, son necesarios  para   identificar barreras, y a partir de ellas, desarrollar estrategias para superarlas.     En futuros trabajos se complementará con la incorporación del estudio detallado de  los conceptos de barreras y riesgos."	﻿riesgos , compartir información , barreras , beneficios , gobierno electrónico	es	18786
7	Gobernabilidad Electrónica	﻿os para Dispositivos Móviles (GECODIMO)  Rocío A. Rodríguez, Daniel A. Giulianelli, Artemisa Trigueros,   Pablo M. Vera, Isabel B. Marko    Universidad Nacional de La Matanza1  Departamento de Ingeniería e Investigaciones Tecnológicas  Buenos Aires, Argentina  {rrodri, dgiulian, artemisa, pablovera, imarko}@unlam.edu.ar  Resumen. En el presente paper se muestran las características de GECODIMO,  un software que permitirá a los municipios poder contar con un gestor de  contenidos para dispositivos móviles, que puede ser implementado sin costo  alguno por los gobiernos municipales ya que fue desarrollado con herramientas  open-source. Dicho sistema ofrecerá una vía adicional de comunicación en el  entorno G2C (Gobierno-Ciudadanos) y G2V (Gobierno-Visitantes)  permitiendo, a través de los dispositivos móviles, contar con información del  municipio.  Palabras clave: Open Source, Municipios, Gobernabilidad Electrónica, eServicios  1 Introducción  1.1  Gobernabilidad Electrónica  Dado el gran crecimiento de la telefonía móvil en Argentina debido a su rápida  adopción por parte de la sociedad, por su facilidad de uso y disponibilidad continua,   es necesario que los organismos gubernamentales, dentro de sus estrategias  comunicacionales, provean servicios e información adicionales [2] [3] a sus  ciudadanos y visitantes a través de dispositivos móviles. Dichas estrategias se  encuentran incluidas dentro de los conceptos de Gobernabilidad Electrónica y su  subconjunto Gobernabilidad Móvil.  El concepto de Gobernabilidad electrónica es definido por la UNESCO como “el  uso de las tecnologías de la información y la comunicación por parte del sector  público con el objetivo de mejorar el suministro de información y el servicio  proporcionado. De esta manera, se trata de estimular la participación ciudadana en el  proceso de toma de decisiones, haciendo que el gobierno sea más responsable,                                                             1  El presente artículo describe lo realizado en el área de Gobernabilidad Electrónica por un  grupo de investigación de la Universidad que cuenta con financiamiento de la CIC (Comisión  de Investigaciones Científicas).  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1314 transparente y eficaz”, siendo sus campos de aplicación: e-Administración, eServicios y e-Democracia. Los e-Servicios son una de las aplicaciones más utilizadas  de la Gobernabilidad Electrónica, éstos tienen como objetivo mejorar la provisión de  información y ofrecer a los ciudadanos el acceso a los servicios  públicos. También  están incluidas las informaciones sobre direcciones, horarios de atención al público,  requisitos, eventos, espectáculos, transporte público, bolsa de trabajo, políticas de  empleo, licitaciones, mapas, lugares de interés turístico, etc.   Por otra parte, los entornos de comunicación derivados de la Gobernabilidad  Electrónica son: G2A (Gobierno a Administración), G2E (Gobierno a Empleados),  G2B (Gobierno a Empresas), G2C (Gobierno a Ciudadano) y G2V (Gobierno a  Visitantes).  En este trabajo serán considerados:  1. G2C: Dentro de esta clasificación están comprendidos todos aquellos  servicios, información, alertas e interacción que el gobierno, en especial   local, ofrece al ciudadano de su distrito.    2. G2V: Comprende toda información y servicios previstos para las personas  que visitan la localidad ya sea los propios ciudadanos locales o de otros  lugares, en plan de turismo, así como para aquellas que han concurrido a  ésta por motivos de trabajo, salud, estudio, etc.    El concepto de m-Government fue definido por Kushchu y Kuscu  (2003),  “como  una estrategia y su implementación incorporando todos las clases de tecnología  inalámbrica y móvil, servicios, aplicaciones y dispositivos para mejorar los beneficios  de las partes involucradas en el e-Government incluyendo ciudadanos, negocios y  todos los organismos gubernamentales”[6]  1.2 Crecimiento de la Telefonía Celular  La telefonía celular ha constituido una verdadera revolución en las comunicaciones de  los habitantes de Argentina, basada principalmente en la posibilidad de utilización “en  cualquier momento, en cualquier lugar” (anytime, anywhere)”. Numerosos estudios  estadísticos nacionales (INDEC [4], CNC [1]) e internacionales (ITU) [5] muestran su  continuo crecimiento en la Argentina y el mundo, como muestra la Figura 1.      Figura 1: Evolución de la cantidad de teléfonos móviles, fijos y conexiones de banda  ancha. (Fuente INDEC y CNC) [4] [1].  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1315 La figura 1 representa los celulares con rombos, los teléfonos fijos con cuadrados y  banda ancha con triángulos, permitiendo observar que la cantidad de celulares supera  ampliamente a los demás medios de comunicación.   1.3 Relevamiento de sitios web gubernamentales  Durante el año 2010 y el presente año, el equipo de investigación realizó  relevamientos a sitios web gubernamentales argentinos buscando e-Servicios  ofrecidos a través de dispositivos móviles. El relevamiento está compuesto por un  total de 100 sitios web gubernamentales, entre los cuales se incluyen 80 sitios web  municipales, ya que los gobiernos locales son considerados los más cercanos a los  ciudadanos y los que pueden proveer información más precisa a los visitantes. A  partir de dicho relevamiento fue posible observar que sólo 30% de estos sitios anuncia  la existencia de servicios ofrecidos a través de dispositivos móviles.   2 Construcción de GECODIMO  El objetivo del presente proyecto de investigación es disponer de un gestor de  contenido que permita, mediante un sistema de carga, administrar información  organizada por categorías (ej: noticias, información, direcciones, horarios, avisos,  lugares de interés turístico, etc.). Este gestor tendrá como finalidad la de generar, con  la información cargada en forma dinámica, un portal móvil de consulta.  El sistema estará conformado por un sub-sistema de backend que permitirá la  carga y otro para la configuración del portal móvil. El portal generado permitirá  consultar información por medio de dispositivos móviles.   El sistema contará con las siguientes características:   Sistema de configuración totalmente web   Crear categorías y subcategorías en formar jerárquica   Ordenar y modificar las categorías   Permitir inhabilitar y eliminar categorías    Cargar contenidos con fecha de validez   Administración de imágenes    El presente trabajo deberá estar desarrollado con herramientas open source para  permitir la implementación del mismo en cualquier ambiente sin costos de licencias.     El sistema cuenta con dos actores principales:    1. Administradores: quienes utilizarán el sistema de backend para configurar el  sistema, crear categorías, cargar contenidos: noticias, avisos, etc.     2. Usuarios Móviles: quienes podrán acceder desde un teléfono celular y  visualizar las categorías y sus respectivos  contenidos.    CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1316 2.3 Usuario Administrador  Los administradores contarán con los siguientes módulos, disponibles desde un  servidor web:    1. Administrar Categorías: Se contará con una pantalla que permitirá crear,  modificar, eliminar e inactivar categorías. La pantalla será realizada en forma de  árbol para permitir la relación jerárquica de las categorías. Se podrán crear N  niveles.     Al hacer clic con el botón derecho sobre una rama u hoja del árbol se  desplegará un conjunto de opciones que incluyen:    1.1 Crear hijo: permitirá crear una nueva categoría dependiente de la  seleccionada.    1.2 Activar Categoría: permitirá volver a activar una categoría desactivada.     1.3 Desactivar Categoría: permitirá desactivar una categoría. Al desactivar  una categoría la misma será visualizada en forma distinta al resto, con  un ícono que indicará que esta desactivada. Una categoría desactivada  no permitirá relacionar noticias a la misma ni crearle nuevos hijos. Si se  desactiva una categoría padre, se desactivarán en forma automática  todos los sus hijos.    1.4 Modificar categoría: permitirá modificar el nombre de una categoría  existente.    1.5 Eliminar categoría: permitirá eliminar en forma definitiva una  categoría siempre y cuando la misma no tuviere hijos ni contenidos  relacionados.  Todas estas opciones además de estar disponibles mediante el botón derecho del  mouse sobre la opción, estarán disponibles también mediante iconos en una barra  superior o lateral.  Esta pantalla deberá disponer de filtros sobre el árbol, los filtros serán: (1)  Categorías activas, inactivas o todas (2) Descripción.  Se podrán además reordenar las categorías, para ello deberán incorporarse dos  botones al costado izquierdo del árbol uno con una flecha hacia arriba y otro con una  flecha hacia abajo. Estas flechas permitirán que la opción seleccionada suba o baje en  el árbol, siempre dentro de su misma categoría. Es decir, que si seleccionamos un hijo  dentro de una categoría éste va a subir o bajar hasta el lugar definido por el  administrador, no pudiendo pasar de nivel en forma automática.  La cantidad de hijos dentro de una categoría estará limitada con un parámetro  configurable. Los hijos de una categoría deberán ser del mismo tipo, es decir si una  categoría tiene noticias, está a su vez no podrá incluir otras subcategorías, que no sean  noticias.  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1317   2. Administrar Contenidos: Se mostrará una pantalla con el listado completo de  los contenidos dados de alta en el sistema, este listado deberá recuperarse en  forma paginada desde la base de datos a fin de agilizar la carga del mismo. Se  dispondrá de los siguientes filtros sobre el listado de contenidos combinables  entre sí: (1) Fecha de publicación: desde … hasta (2) Fecha de vigencia: desde …  hasta (3) Árbol de categorías con selección múltiple en distintos niveles de la  jerarquía (4) Ver todos, publicados y no vigentes (5)Titulo (buscará títulos que  contengan un texto ingresado en un cuadro de texto)  La grilla mostrará: (1) Fecha de publicación; (2) Titulo; (3) Estado, por  ejemplo: publicado, no vigente; (4) Categoría: mostrará el nivel completo de la  misma en forma combinada, por ejemplo Noticias-Comunales-Obras, si se tratara  de una noticias ubicada en el tercer nivel de una jerarquía de categorías.    2.1   ABM de Contenidos (Crear, Eliminar y Modificar Contenidos): Se  mostrará un editor que permita escribir el texto, darle formato, insertar  imágenes, tablas, etc, de acuerdo al estándar chtml.  La pantalla de edición de contenidos contará con un árbol de categoría para  poder relacionar el contenido con la categoría de la cual depende. El  contenido debe tener una fecha de publicación y opcionalmente una fecha que  indica hasta cuando este contenido será visible en el sistema.  Al insertar una imagen se deberá validar que la misma cumpla con ciertas  limitaciones de tamaño, resolución y peso que serán configurables mediante  una tabla de parámetros de la aplicación.    3. Administrar Usuarios: Se dispondrá de la posibilidad de crear y modificar  usuarios para que accedan al módulo de administración, por cada usuario se  configurará el nombre utilizado para el login en el sistema y la clave de acceso  que utilizará el mismo. Será posible: Crear, Modificar, Desactivar, Activar y  Eliminar usuarios.  A su vez se dispondrá de un checkbox que exigirá al usuario que ingrese su clave  cuando se loguee en el sistema.    Todas las acciones descriptas anteriormente que pueden ser realizadas por el  usuario administrador se modelan a continuación en la figura 2, por medio de casos de  uso.                         CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1318                                                                                     Figura 2. Diagrama de Casos de Uso para el Usuario Administrador    A continuación se muestran las pantallas del sistema web generado para los  usuarios administradores. La figura 3 muestra la pantalla de logueo por medio de la  cual uno de los usuarios administradores podrá ingresar al sistema para los usuarios  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1319 móviles el sistema debe ser liviano privilegiando el texto por sobre las imágenes, de  esta forma el costo para poder acceder desde un celular y cargar la página será menor.  Por el contrario para el usuario administrador se ha puesto énfasis en el uso de íconos  gráficos los cuales permitan hacer más intuitivo el uso del software. Tal como puede  observarse en la Figura 4 y 5.      Figura 3. Pantalla de Logueo      Figura 4. Menú Principal                                      Figura 5. Alta, Baja y Modificación de Usuarios    CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1320 La pantalla mostrada en la figura 6 permite administrar las categorías del sistema  y la figura 7 contenidos.      Figura 6. Administrar de Categorías para la carga de Contenidos                                                    Figura 7. Pantalla de Administración de Contenidos  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1321 2.4 Usuario Móvil  El usuario móvil contará con los siguientes módulos:  1. Navegar entre las categorías: El usuario al ingresar al portal móvil deberá  visualizar las categorías configuradas, para ello verá un listado con el primer  nivel de categorías el cual podrá ir navegando con su teléfono. Al seleccionar una  categoría se desplegarán sus hijos y así sucesivamente hasta que el usuario llegue  al contenido que le interesa. Poe ejemplo, cuando seleccione una noticia se  deberá visualizar la misma (ver Visualización de Noticias).  El menú de navegación debe ser sencillo para facilitar su navegación con el  celular: una opción debajo de la otra resaltándose la opción seleccionada. Deberá  disp onerse de una opción de regresar a la categoría anterior y a su vez deberá  mostrarse el camino recorrido hasta llegar al nivel actual, dicho camino deberá  contener links para volver de forma directa a cada uno de esos niveles.    2. Visualización de Noticias: Al ingresar a un contenido (Ej: noticia) se deberá  generar el código chtml correspondiente a la misma permitiendo visualizar en  forma correcta, tablas, imágenes, etc. En la parte superior deberá mostrarse: (1)  Categoría a la cual pertenece - camino completo con links a cada uno- (2) Fecha  de publicación de la noticia (3) Título (4) Contenido  Debajo se deberá contar con una opción para regresar al menú anterior.    En la figura 4 se muestra el diagrama de casos de uso para el usuario móvil (ej:  Noticias).                                  Figura 4. Diagrama de Casos de Uso para el Usuario Móvil  CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1322 3 Conclusiones y trabajo futuro  Dadas las posibilidades que ofrece actualmente la tecnología en cuanto a desarrollo de  software open source, que no requiere la adquisición de licencias para su uso, así  como la amplísima difusión de los dispositivos móviles y su facilidad de uso, es hora  de disponer de una herramienta que permita a los sitios web gubernamentales ofrecer  a sus ciudadanos y visitantes la posibilidad de ejercer Gobernabilidad Electrónica  Móvil. GECODIMO, fue pensado como dicha herramienta, desarrollada por el equipo  de investigación de la Universidad Nacional de La Matanza como contribución a  los  municipios para ser implementado en diversos municipios siendo el propio personal  de los municipios quién  cuente con la posibilidad de generar y administrar sus  contenidos sin tener que tercerizar esta tarea. Así podrán ofrecer más y mejores  servicios sin costo alguno.    Actualmente el sistema se ha desarrollado en cuanto a la parte de backend (sistema  web para el administrador), encontrándose en etapas de desarrollo la parte del sistema  móvil destinado a los usuarios. En primera medida el sistema será implementado en  un servidor web de la Universidad Nacional de La Matanza y una vez probado será  implementado en el Municipio de Morón, sin costo alguno. Se prevee que el sistema  luego pueda ser implementado en otros municipios.	﻿municipios , source , gobernabilidad electrónica , e servicios	es	18788
8	Diseño implementación y uso de un framework para el desarrollo de sistemas de inferencia borrosos flexibles	﻿ Este trabajo presenta el desarrollo de un framework, construído a  partir de técnicas de la programación orientada a objetos, que permite crear  sistemas de inferencia borrosos (SIB) de tipo Mamdani flexibles ya que el   Fuzzy Logic Toolbox de MATLAB, que es quizás la herramienta más usada a  tal efecto, presenta una serie de limitaciones tales como no poder usar diferentes  instancias de un mismo operador borroso en un SIB, no permitir elegir  diferentes métodos de implicación para reglas distintas ni tampoco permite  elegir el método de agregación y defuzzificación para cada variable lingüística  de salida, en caso de ser necesario. Se presenta, además de la arquitectura,  detalles de la implementación y un ejemplo de su uso.  Keywords: diseño – implementación - framework - sistema – inferencia –  borroso - flexible.  1   Introducción    Este trabajo presenta el desarrollo de un framework, construído a partir de técnicas  de programación orientada a objetos, que permite crear sistemas de inferencia  borrosos flexibles (SIB) de tipo Mamdani. El mismo permite cubrir las limitaciones  presentes en programas comerciales tales como el Fuzzy Logic Toolbox del software  MATLAB1. Este último permite diseñar sistemas de inferencia borrosos (SIB) de  manera fácil y rápida. También posee un conjunto de interfaces gráficas que ayudan  al usuario en las distintas etapas de la construcción de un sistema de inferencia  borroso [1] pero presenta, no obstante, una serie de limitaciones tales como: a) No es  posible usar distintas instancias (por ej. : min y prod son 2 posibles instancias del  operador and) de un mismo operador lógico en un SIB, b) No es posible seleccionar  un método de implicación para una regla distinto al elegido para las reglas restantes,  c) No es posible que cada variable lingüística de salida tenga un método de                                                             1 MATLAB ® es un producto de The MathWorks, http://www.mathworks.com  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 1 agregación diferente al de otras variables de salida de un mismo SIB y d) No es  posible usar diferentes métodos de defuzzificación para las distintas variables  lingüísticas de salida.     A partir de las limitaciones antes citadas, se presentará un framework que supera  tales inconvenientes y cuyo desarrollo ha formado parte de una tesis doctoral [2] en la  que se analizaron diferentes metodologías para la evaluación de la calidad de vida a  partir de modelos basados en índices e indicadores.     En función de los objetivos propuestos se presentará a continuación una  introducción a los conceptos básicos sobre los sistemas de inferencia borrosos, la  metodología de desarrollo utilizada para la construcción del framework, un ejemplo   de su uso y los resultados obtenidos.    2 Conceptos básicos sobre los sistemas de inferencia borrosos    Los sistemas de inferencia borrosos transforman un conjunto de valores de entrada  en un conjunto de valores de salida por medio de reglas. El proceso que realiza esta  tarea se denomina proceso de inferencia y consta de 5 etapas [1], [3]:    1. Fuzzificar las entradas: Los valores de entrada son usados para evaluar con que  grado satisfacen a c/u de sus valores lingüísticos (conjuntos borrosos) asociados en  cada regla usando para ello las funciones de pertenencia. La entrada es siempre un  valor en el universo de discurso de la variable, en tanto que la salida es un número  entre 0 y 1 que indica el grado de pertenencia a un determinado valor lingüístico.    2. Aplicar el operador borroso: Si el antecedente tiene más de un operando  entonces se aplican los operadores borrosos necesarios para obtener un único número  que representa el grado con que se satisfizo el antecedente de la regla. Esta etapa  recibe como entrada un conjunto de 2 o más grados de pertenencia a conjuntos y  retorna un único número.    3. Aplicar el método de implicación: Antes de aplicar el método de implicación se  debe determinar el peso de la regla. El proceso de implicación recibe como entrada un  número (el grado con que se satisfizo el antecedente) y la salida es un conjunto  borroso truncado.     4. Agregación de las salidas: La entrada del proceso de agregación es la lista de  conjuntos borrosos truncados retornados por el método de implicación para cada  regla. La salida del proceso de agregación es un conjunto borroso para cada variable  lingüística de salida.    5. Defuzzificación: La entrada para el proceso de defuzzificación es  un conjunto  borroso (el conjunto borroso agregado producido en el paso anterior) y la salida es un  simple número.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 2 Puede observarse un ejemplo del proceso de inferencia completo en la figura 1. El  ejemplo, de carácter ilustrativo, estima la calidad de vida a partir de 2 variables (salud  y dinero) usando un sistema de inferencia borroso. El mismo es una adaptación del  expuesto a modo de ejemplo en la tesis doctoral de Ferraro [4]. La elección de este  ejemplo tiene 2 motivos principales: a)Sirve para ilustrar de manera sencilla el  proceso de inferencia borroso y b) Como tiene 2 variables de entradas (salud y dinero)  y 1 de salida (calidad de vida), es posible mostrar gráficamente las salidas en función  de las 2 entradas.      Figura 1. Proceso de inferencia de un sistema de inferencia borroso.    La figura 1 muestra el proceso de inferencia para un par de valores (2500, 30)  correspondientes al par de variables dinero y salud respectivamente. No obstante, en  la Figura 2 se puede observar los resultados obtenidos para cualquier combinación de  valores de entrada mediante un gráfico 3D.    Se observa de los resultados obtenidos que el máximo nivel de calidad de vida se  halla cuando la variable salud alcanza su mínimo valor (0 un/ml., el cual expresa una  salud inmejorable) y dinero toma el valor máximo posible (5000 $/mes). En éste caso,  el valor de calidad de vida obtenido usando el método MOM es de 10, como era de  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 3 esperar. Por el mismo motivo se tiene que el sistema devuelve el valor 0 para la  combinación (0, 300) usando el método MOM de defuzzificación.     Para mostrar la flexibilidad del framework desarrollado se aplicaron los siguientes  cambios a las 2 primeras reglas del ejemplo anterior:   Se cambió la instancia del operador “y”, que era representado por la operación  “min” en todas las reglas, por la operación “prod” en las reglas 1 y 2. También el  método de implicación que era “min” para todas las reglas fue reemplazado por el  método “prod” en las reglas 1 y 2. El resultado es el que se observa en la figura 3.      Figura 2 (izquierda). Superficie que representa la calidad de vida en función de las variables  salud y dinero. Figura 3 (derecha). Superficie que representa la calidad de vida en función de  las variables salud y dinero luego de los cambios realizados.    3   Metodología  El framework desarrollado tiene un alto grado de flexibilidad dado que permite el  uso de distintos tipos de operadores lógicos, métodos de implicación, métodos de  agregación y de defuzzificación en un mismo sistema de inferencia borroso. El mismo  fue desarrollado haciendo uso de técnicas de la programación orientada a objetos.     Son conocidas las ventajas que esto supone, y en este caso particular la  combinación de múltiples patrones de diseño con el objeto de formar un framework  general proporciona una mayor reusabilidad, un mayor grado de abstracción y alcanza  una muy buena genericidad. Se han empleado en múltiples ocasiones los patrones  Strategy e Interpreter [5], [6] y se han aprovechado las ventajas de la herencia de  objetos para aumentar la reusabilidad y, al mismo tiempo, distinguir entre objetos  similares pero no idénticos.     Se muestra en la figura 4 el diagrama de clases completo del framework propuesto.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 4   Figura 4. Diagrama de clases del framework para construir sistemas de inferencia borrosos  flexibles.  4 Ejemplo de uso del framework para el desarrollo de sistemas de  inferencia borrosos flexibles    A continuación veremos un ejemplo de aplicación del framework. Se trata de  analizar la calidad del Servicio Educación (más específicamente los establecimientos  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 5 de educación primaria básica-EPB-) en la región del Gran La Plata, Argentina, a partir  del análisis de las cualidades relacionadas a las variables edilicias, energéticas y  productivas las cuales se detallan a continuación [7] :     a- Características edilicias: Se refiere a las características dimensionales de los  establecimientos. Las superficies aparecen subdivididas en función de las actividades  predominantes dentro de cada establecimiento (aulas, sectores administrativos,  servicios auxiliares y de apoyo, etc.). Incluye la cantidad de pisos del edificio, la  superficie destinada a aulas en relación a la superficie cubierta mínima del  establecimiento y la superficie del terreno; altura promedio y el volumen.     A partir de un análisis particularizado se puede establecer un área estándar en  función de las superficies actuales. Para el caso del área pedagógica (aulas) de EPB,  se considera una capacidad aconsejable de 1.25m² de aula por alumno, considerando a  su vez la superficie cubierta mínima total del establecimiento de 4.40 m²/alumno. O  sea que la relación m²/alumno de aula en relación a la superficie cubierta mínima total  debe ser del orden de 0.28 (superficie aula / superficie cubierta mínima total). Esto  permite mantener el universo en una franja de superficies estándares a los efectos de  identificar todos aquellos establecimientos por encima y por debajo de dicha relación.  De esta manera, se construyen perfiles de extremos (máximos y mínimos) y  estrategias de mejoramiento.     En este caso se consideró una relación media, aquella que no sea ni inferior ni  superior a un 10% de la relación mínima (0,28). La Figura 5 muestra las funciones de  pertenencia asociadas a los valores lingüísticos de la cualidad características edilicias  con respecto a la relación m²/alumno.    b- Datos energéticos ambientales: Se refiere a vectores energéticos de consumo que  intervienen en el funcionamiento del establecimiento en un periodo determinado y a  las emisiones, discriminación y cuantificación de contaminantes aéreos en función de  la fuente utilizada y la complejidad del establecimiento.    En el marco del análisis establecido en un modelo de calidad de vida urbana [9] y a  partir de la información disponible, se tomaron los datos de estudios realizados en un  trabajo anterior [8] en los cuales se estimó un consumo de 317,14 kWh/alumno/año.  Considerando los valores promedios y el desvío estándar, podemos determinar los  establecimientos cuyos consumos están en el intervalo [promedio-desvío,  promedio+desvío] y los que están fuera del mismo. Los establecimientos con  consumos insuficientes refieren a importantes deficiencias en la climatización y  fundamentalmente en la iluminación ya que los mismos registran en los planos de  trabajo niveles por debajo del 2% exigido por norma2. En cuanto a los casos con  consumos excesivos corresponden a establecimientos con altos niveles de  climatización y sin una regulación eficiente en sus sistemas de iluminación (no  aprovechamiento de la luz natural). La Figura 6 muestra las funciones de pertenencia                                                             2 Criterios y Normativa Básica de Arquitectura Escolar.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 6 asociadas a los valores lingüísticos de la cualidad Datos energético- ambientales con  respecto a la relación kWh/alumno.    c- Datos de Producción: Se refiere a todos aquellos datos ligados a la producción de  cada establecimiento de la red de educación. Involucra los requerimientos  pedagógicos, buscando alcanzar la mayor racionalización y optimización de los  recursos disponibles. También comprende la planta funcional del establecimiento y  personal contratado. La organización del edificio escolar debe responder al proyecto  institucional, a los requerimientos pedagógicos y a las pautas socio-culturales de los  usuarios, adaptándose a las diversas características regionales, cumpliendo con las  superficies mínimas y las exigencias cualitativas tecnológicas.    Según los criterios y normativa básica de arquitectura escolar, se considera para las  aulas del EPB una capacidad máxima de 36 alumnos por sección, siendo la capacidad  aconsejable de 30 alumnos. Por lo tanto, una situación óptima sería la relación  docente-alumno 1/30, la máxima admisible 1/36 y la menor admisible 1/13  (superando el máximo admisible –36 alumnos- el mínimo surge a partir de división de  36 alumnos/2). La Figura 7 muestra las funciones de pertenencia asociada a los  valores lingüísticos de la cualidad Datos de producción con respecto a la relación  docente/alumno para el caso educación.    El sistema de inferencia borroso, por lo tanto, estará compuesto por 3 variables  lingüísticas de entrada (Características edilicias, Datos energético-ambientales y  Datos de producción) y una de salida (Eficiencia). La figura 8 muestra las funciones  de pertenencia de los valores lingüísticos (mala, insuficiente, aceptable y óptima)  asociados a la variable lingüística “Eficiencia”.    Las funciones de pertenencia asociadas a los valores lingüísticos “Bajo”, “Medio”  y “Alto” asociados a la variable lingüística “Características edilicias” se muestran en  la figura 5. La primera y la última han sido definidas como de tipo trapezoidal con  parámetros [0, 0, 0.25, 0.31] y [0.25, 0.31, 0.4, 0.4] respectivamente, en tanto que la  correspondiente al valor lingüístico “Medio” es de tipo triangular con parámetros  [0.25, 0.28, 0.31].    Figura 5. Funciones de pertenencia de los valores lingüísticos “Bajo”, “Medio” y “Alto”  asociados a la variable lingüística “Características edilicias”.    Las funciones de pertenencia asociadas a los valores lingüísticos “Insuficiente”,  “Promedio” y “Alto” asociados a la variable lingüística “Datos energéticoCACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 7 ambientales” se muestran en la figura 6. La primera y la última han sido definidas  como de tipo trapezoidal con parámetros [0, 0, 50, 600] y [46.42, 587.68, 700 700]  respectivamente, en tanto que la correspondiente al valor lingüístico “Medio” es de  tipo triangular con parámetros [46.42, 317.14, 587.86].    Figura 6. Funciones de pertenencia de los valores lingüísticos “Insuficiente”, “Promedio” y  “Alto” asociados a la variable lingüística “Datos energético-ambientales”.    Las funciones de pertenencia asociadas a los valores lingüísticos “Bajo”,  “Aconsejable” y “Alto” asociados a la variable lingüística “Datos de producción” se  muestran en la figura 7. La primera y la última han sido definidas como de tipo  trapezoidal con parámetros [0, 0, 18, 36] y [18, 36, 46, 46] respectivamente, en tanto  que la correspondiente al valor lingüístico “Medio” es de tipo triangular con  parámetros [18, 30, 36].    Figura 7. Funciones de pertenencia de los valores lingüísticos “Bajo”, “Aconsejable” y “Alto”  asociados a la variable lingüística “Datos de producción”.    La variable lingüística “Eficiencia” tiene asociados 4 valores lingüísticos: “Mala”,  “Insuficiente”, “Aceptable” y “Óptima”. Estos últimos se representan con funciones  de pertenencia de tipo triangular con parámetros [0, 0, 16.66], [16.66, 33.33, 50], [50,  66.66, 83.32] y [83.32, 100, 100] respectivamente (Figura 8).    Figura 8. Funciones de pertenencia de los valores lingüísticos “Óptima”, “Aceptable”,  “Insuficiente” y “Mala” asociados a la variable lingüística “Eficiencia”.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 8 Se definieron 27 reglas, dado que cada una de las 3 variables lingüísticas de  entrada posee 3 valores lingüísticos asociados. Todas las reglas tienen peso igual a 1.  Se muestran a continuación, a modo de ejemplo, 2 de las 27 reglas establecidas.    Regla 1:  Si Características edilicias=Baja y Datos energético-ambientales=Insuficiente  y  Datos de producción=bajo entonces Eficiencia=Mala.    Regla 27:  Si Características edilicias=Medio y Datos energético-ambientales=Promedio y  Datos de producción=Aconsejable entonces Eficiencia=Óptima.    El operador “y” fue el “min” y el método de implicación fue “min” para todas las  reglas. Como método de agregación se usó el “max” y como método de  defuzificación se utilizó el “MOM”.    A continuación se muestra la eficiencia obtenida en función de 2 variables de  entrada (la restante se fijó en su valor ideal). El primer gráfico (Figura 9) relaciona los  Datos energético-ambientales con las Características edilicias, tomando con valor fijo  los Datos de Producción edilicia (1/30 docente/alumno). El gráfico siguiente (Figura  10) relaciona los Datos de Producción edilicia con los Datos energético-ambientales  fijando el valor de las Características Edilicia (0.28 superficie aula/superficie cubierta  mínima total). Por último se fija el valor de los Datos energético-ambientales (317.14  kWh/alumno/año) y se analizan las variables Datos de Producción edilicia y  Características edilicias (Figura 11). Esto nos permite visualizar el comportamiento de  las variables y sus interrelaciones.        Figura 9 (izquierda). Superficie que representa la eficiencia en función de los Datos  energético- ambientales y las Características edilicias, tomando con valor fijo los Datos de  Producción edilicia (1/30 docente/alumno). Figura 10 (centro). Superficie que representa la  eficiencia en función de los Datos de Producción edilicia y los Datos energético-ambientales,  tomando con valor fijo el valor de las Características edilicias (0.28 superficie aula/superficie  cubierta mínima total). Figura 11 (derecha). Superficie que representa la eficiencia en función  de las variables Datos de Producción edilicia y Características edilicias manteniendo fijo el  valor de los Datos energético-ambientales (317.14 kWh/alumno/año).    Se observa de los resultados obtenidos que el máximo nivel de eficiencia (óptima) se  halla cuando las variables Características edilicias, Datos energético-ambientales y  Datos de producción alcanzan los valores 0.28 (medio), 317.14 (promedio) y 30  (aconsejable) respectivamente (Figura 11). En éste caso, el valor de la eficiencia  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 9 obtenido usando el método MOM es de 10, como era de esperar. Por el mismo motivo  se tiene que el sistema devuelve el valor 0 para la combinación (0, 0, 0) usando el  método MOM de defuzificación.   5 Conclusiones  El framework propuesto para el desarrollo de SIB flexibles ha superado las  limitaciones señaladas al comienzo permitiendo asociar distintos métodos a las  distintas instancias de un mismo operador lógico, cada regla puede tener el método de  implicación más adecuado y todas las variables lingüísticas de salida podrán contar  con los métodos de agregación y defuzificación que mejor representen el fenómeno a  modelizar. Estos aportes se traducen en un mayor grado de flexibilidad a la hora de  construir un sistema de inferencia borroso.	﻿borroso , diseño , implementación , sistema , inferencia , flexible	es	18804
9	Condicionales DEVS en la coordinación de contratos sensibles al contexto para los DHD	﻿para los DHD  Alejandro R. Sartorio 1,2 Guillermo L. Rodríguez1 1 Centro Internacional Franco Argentino de Ciencias de la Información y de Sistemas, CIFASIS (CONICET-UNR-UPCAM), Bv. 27 de febrero 210 bis, 2000 Rosario, Argentina 2 Centro de Altos Estudios en Tecnología Informática, Sede Rosario, Universidad Abierta Interamericana, Ov. Lagos 944, 2000 Rosario, Argentina sartorio@cifasis-conicet.gov.ar, guille@fceia.unr.edu.ar Resumen.  Teniendo en cuenta los aspectos tecnológicos  y de diseño de  los mecanismos para la inyección de las propiedades de coordinación  de contratos sensibles al contextos en los Dispositivos Hipermediales  Dinámicos  (DHD),  se  propuso  la  creación  de  un  nuevo condicional  (Condicional DEVS) que permita establecer valores de verdad a partir  de  la  inferencia  de  un  modelo  de  eventos  discretos  (DEVS)  que  representa una métrica para determinar el grado de interactividad que se  produce para cada participación en los DHD. A través de un caso de uso  se muestra la implementación del modelo de integración propuesto que  permite incluir nueva información de contexto a las reglas teniendo en  cuenta solamente los coeficientes de las métricas intervinientes. Palabras  Clave:  Dispositivo  Hipermedial  Dinámico  –  Coordinación  de  Contratos – Métricas – Eventos Discretos – TIC. 1   Introducción El actual  contexto físico-virtual  que  se construye  a  partir  de  la  utilización  de  las  Tecnologías  de  la  Información  y  Comunicación  (TIC)  posibilita  a  los  sujetos  ser  partícipes de redes sociotécnicas conformadas por una multiplicidad de componentes  y  relaciones,  que  se  configuran  y  reconfiguran  por  las  diversas  interacciones  en  función  de  una  gran  diversidad  de  requerimientos.  En  este  sentido,  el  Programa  interdisciplinario  de  I+D+T  “Dispositivos  Hipermediales  Dinámicos”  (DHD)  [1],  radicado en CIFASIS (CONICET-UNR-UPCAM), estudia la complejidad evidente de  las mencionadas redes, integrando aportes de diversas disciplinas como informática,  educación, ingeniería, psicología y antropología, entre otras. Se  conceptualiza  como  Dispositivo  Hipermedial  Dinámico  -DHD-  a  la  red  heterogénea [2] conformada por la conjunción de tecnologías y aspectos sociales que  posibilitan a los sujetos realizar acciones en interacción responsable con el otro para  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 243 investigar,  aprender,  dialogar,  confrontar,  componer,  evaluar,  bajo la modalidad de  taller  físico-virtual,  utilizando  la  potencialidad  comunicacional,  transformadora  y  abierta  de  lo  hipermedial,  regulados  según  el  caso,  por  una  “coordinación  de  contratos” [3]. En  la  implementación  y  optimización  de  un  DHD  para  la  producción  y  diseminación de conocimiento, los mecanismos de medición y evaluación suponen  una de las actividades principales para el análisis y el aseguramiento de la calidad de  lo que se desarrolla a través de los mismos. Los procesos de medición son fundamentales dado que permiten cuantificar un  conjunto de características deseadas acerca de un aspecto específico de algún ente en  particular, proveyendo una visión más o menos detallada de su estado o condición.  Por  su  parte,  la  evaluación  interpreta  los  valores  obtenidos  en  la  medición.  Para  dichos procesos de medición y evaluación es necesario obtener datos cuantitativos, a  partir de métricas de atributos de entes y la posterior interpretación de la medida a  partir de indicadores [4]. Funcionalmente el DHD es conceptualizado como sistema complejo [5], en el  cual los Participantes (P) a través del intercambio analítico y de producción de textos  mediatizados en diversos tipos de formatos digitales, construyen las posibilidades y  limitaciones de la mediación interdisciplinaria responsable en su área de incumbencia,  siendo  deseable  que  se  pueda  observar  un  paulatino  cambio  en  su  situación  contextual.  Al  constatar  que  la  característica  primordial  del  DHD,  es  que  las  interacciones se deben a la ocurrencia asincrónica de eventos, hemos optado por el  modelado  con  DEVS,  Discrete  Event  System  specification  [6],  considerándose  además la gran adaptación del formalismo para modelizar sistemas complejos, y su  simplicidad y eficiencia en la implementación de simulaciones. Tecnológicamente  el  DHD  está  provista  por  un  agregado  de  una  pieza  de  software para la inyección de propiedades de coordinación de contratos sensibles al  contexto [7]. Esta propiedad se logra a través de la implementación de contratos [8]  con mencanizmos de coordinación y componentes de sistemas context aware [9]. La utilización de reglas es parte esencial en la implementación de las acciones de  los contratos y las tareas de coordinación. A su vez, las reglas están compuestas por  los condicionales donde se centra parte de la lógica de adaptación que se requiere en  los DHD. Algunos condicionales implementados requieren de mecanismos externos  que colaboren en la composición de sus valores de verdad [10]. En este trabajo se define un nuevo tipo de condicional, denominado Condicional  DEVS, para la inclusión de valores de verdad que puedan ser inferidos por medio de  la  implementación  de  un  conjunto  de  métricas  flexibles  que  contemplan  las  principales características de las interacciones de los participantes de los DHD. Tras  esta introducción, en la sección 2 se identifican los elementos de los DHD en relación  directa con los Condicionales DEVS. Luego, en la sección 3 se presenta un modelo de  integración para el funcionamiento en una herramienta del framework SAKAI (para  la justificación de dicha elección ver [7]). En la sección 4 se describen algunas de las  características principales de las métricas y un caso de uso concreto. Para finalizar, se  presentan las conclusiones y consideraciones generales. CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 244 2. Aspecto tecnológico de los DHD En esta sección se describen aspectos tecnológicos y componentes de los DHD que  intervienen en el modelo de integración entre las métricas DEVS y el framework de  nuestra propuesta. De esta manera, se solucionan los requerimientos sobre adaptación  dinámica  mediante  la  construcción  de  un  modelo  de  contrato  orientado  a  la  implementación de servicios sensibles al contexto. El  uso  de  contratos  parte  de  la  noción  de  Programación  por  Contrato  (”Programming by Contract”) de Meyer [8] basada en la metáfora de que un elemento  de un sistema de software colabora con otro, manteniendo obligaciones y beneficios  mutuos. En nuestro dominio de aplicación consideraremos que un objeto cliente y un  objeto  servidor  “acuerdan”  a  través  de  un  contrato,  -representado  con  un  nuevo  objeto-, que el objeto servidor satisfaga el pedido del cliente, y al mismo tiempo el  cliente  cumpla  con  las  condiciones  impuestas  por  el  proveedor.  A  su  vez  las  decisiones de comportamiento partirán de los condicionales de las acciones de los  contratos. Como ejemplo  de  la  aplicación  de  la  idea  de  Meyer  en  nuestro  dominio  de  sistemas  e-learning  planteamos  la  situación  en  que  un  usuario (cliente)  utiliza un  servicio de edición de mensajes (servidor) a través de un contrato que garantizará las  siguientes  condiciones: el  usuario  debe  poder  editar  aquellos  mensajes  que  tiene  autorización según su perfil  (obligación del  proveedor y  beneficio  del  cliente);  el  proveedor debe tener acceso a la información del perfil del usuario (obligación del  cliente y beneficio del proveedor). A partir  de  la  conceptualización  de  contratos  según  Meyer  se  propone  una  extensión  por  medio  del  agregado  de  nuevas  componentes  para  instrumentar  mecanismos  que  permitan  ejecutar  acciones  dependiendo  del  contexto.  En  aplicaciones  sensibles  al  contexto [9],  el  contexto (o  información de  contexto)  es  definido como la información que puede ser usada para caracterizar la situación de  una entidad más allá de los atributos que la definen. En nuestro caso, una entidad es  un usuario  (alumno,  docente,  etc.),  lugar  (aula,  biblioteca,  sala  de  consulta,  etc.),  recurso  (impresora,  fax,  etc.),  u  objeto  (examen,  trabajo  práctico,  etc.)  que  se  comunica con otra entidad a través del contrato. En [2] se propone una especificación del concepto de contexto partiendo de las  consideraciones de Dourish [11] y adaptadas al dominio e-learning, que será la que  consideraremos en este trabajo. Contexto es todo tipo de información que pueda ser  censada  y  procesada,  a  través  de  la  aplicación  e-learning,  que  caracterizan  a  un  usuario  o  entorno,  por  ejemplo:  intervenciones  en  los  foros,  promedios  de  notas,  habilidades, niveles de conocimientos, máquinas (direcciones ip) conectadas, nivel de  intervención  en  los  foros,  cantidad  de  usuarios  conectados,  fechas  y  horarios,  estadísticas sobre cursos, etc. En términos generales, la coordinación de contratos es una conexión establecida  entre  un grupo de objetos  influidas  por  condicionales  que representan parte  de la  lógica de adaptación, aunque en este trabajo se consideran sólo dos objetos: un cliente  y un servidor. Cuando un objeto cliente efectúa una llamada a un objeto servidor (ej.,  el servicio de edición de la herramienta Foro), el contrato “intercepta” la llamada y  establece una nueva relación teniendo en cuenta el contexto del objeto cliente, el del  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 245 objeto servidor, e información relevante adquirida y representada como contexto del  entorno  [1].  En  este  trabajo  en  los  condicionales  de  las  reglas  se  representarán  diferente  tipo  de  información  de  contexto  con  distinto  grado  de  representación  y  abstracción, donde se requieren mecanismos de inferencias basados en la recolección,  representación y simulación. A continuación  se  brindarán  detalles  sobre  algunas  de  los  componentes  y  relaciones esenciales para la integración de este modelo con el framework utilizado y  con los módulos que instrumentan la coordinación de contratos. Fig. 1. Modelo de elementos y relaciones de los DEVS condicionales. Un contrato que siga las ideas de Meyer contiene toda la información sobre los  servicios que utilizarán los clientes. Para incorporar sensibilidad al contexto nuestros  contratos deberán tener referencias sobre algún tipo de información de contexto para  su utilización. En el diagrama de relaciones entre entidades mostrado en la Figura 1 se  describen los elementos que componen el concepto de contrato sensible al contexto  donde se tiene participación de los condicionales DEVS. La figura comienza con la  representación  de  un  contrato  según  Meyer  donde  se  caracterizan  los  principales  elementos que lo componen (pre-condiciones, acciones, pos-condiciones). La flechas  salientes  de  la  zona  gris  indican  los  dos  tipos  de  relaciones  (acción-servicio  e  invariante-contexto)  que  se  debe  instrumentar  para  incorporar  un  mecanismo que  provea a los contratos de la característica de sensibilidad al contexto. En la porción  derecha  de  la  Figura  1  aparecen  las  entidades  necesarias  para  obtener  contratos  sensibles  al  contexto.  A continuación  se  explica  cada  uno  de  los  elementos  y  su  relación con los condicionales de las acciones. - Servicios: En esta componente se representan los elementos necesarios para la  identificación y clasificación de los servicios que pueden formar parte de las acciones  de los contratos. Por ejemplo, nombre del servicio, identificadores, alcance, propósito,  etc. En este caso existe una relación indirecta con el condicional DEVS establecida  por la relación ejecutar entre la acción del contrato y el servicio.  -  Comportamiento:  El  comportamiento  de  un  servicio  se  logra  a  partir  de  combinar  operaciones  y  eventos  que  son  representadas  con  las  componentes  Operaciones y  Eventos. De la misma manera el servicio puede ser implementado a  través del uso de eventos, representados con el componente Eventos, que puede lanzar  operaciones del componente Operaciones. Por ejemplo, de acuerdo con los roles (ej.,  alumno,  instructor,  docente,  etc.)  asignados  a  un  usuario  de  una  herramienta  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 246 involucrado en un determinado contexto del entorno (ej., si está en un espacio Foro) y  del  usuario  (ej.,  si  tiene  permiso  de  moderador),  la  componente  Servicios brinda  distintas funcionalidades (ej., editar un mensaje), que son instrumentadas por medio  de operaciones concretas (ej.,  guardar un mensaje en una tabla) y/o a través de la  publicación o subscripción de eventos. Aquí se establece una relación directa con el  Condicional  DEVS  teniendo  en  cuenta  todas  las  acciones  que  dependan  de  valoraciones influidas por el contexto, representadas por la simulación a través de un  modelos DEVS. -  Parámetros  Context-Aware:  Se  denomina  Parámetros  Context-Aware a  la  representación de la información de contexto que forma parte de los parámetros de  entrada de las funciones y métodos exportados por los servicios, estableciendo de esta  manera  una  relación  entre  el  componente  Servicios y  el  componente  Parámetros   Contex-Aware.  Existe  una  relación  isomórfica  entre  los  valores  usados  en  los  Condicionales DEVS y los elementos del conjunto de Parámetros Context-Aware. -  Contexto: Para nuestro modelo este tipo de información es utilizada de dos  maneras diferentes: en primer lugar para la asignación de los valores que toman los  Parámetros Context-Aware;  en segundo lugar esta  información puede ser utilizada  para  definir  los  invariantes  que  se  representan  en  los  contratos.  Nuevamente  se  establece una relación indirecta entre los Condicionales DEVS y el contexto mediada  por su representación como elemento de los Parámetros Context-Aware. 2.1 Condicionales para la coordinación de contratos Ahora  a  tavés  de  un  diagrama  UML  se  definen  las  clases  utilizadas  en  la  implementación de los Condicionales DEVS dentro de las  reglas de los contractos,  donde  se  mantienen  las  propiedades  e  influencias  (relaciones  entre  elementos  conceptuales) descriptas en la figura 1. La  figura 2 describe los  elementos  y relaciones  relevantes  en  la  creación  de  condicionales inferidos por métricas de interacción (sección 4) implementadas en un  modelo de simulación DEVS integrado (sección 3). Fig. 2. Elementos y relaciones relevantes en la creación de los condicionales. Partiendo  de  una  de  las  propiedades  de  las  reglas  de  los  contratos  sobre  la  posibilidad  de  definir  comportamientos  a  través  de  parámetros  context-aware  e  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 247 inducidos por reglas donde se designan partes de las acciones del contrato. De esta  manera, las reglas forman parte de un mecanismos de agregación encargado de la  composición  de  diferentes  tipos  de  condicionales,  en  los  que  se  encuentran  una  familia  de  condicionales  (Condicionales  DEVS)  conectados  a  los  métodos  que  implementan  las  métricas  definidas  particularmente  para  la  simulación  de  interacciones (sección 4). Las otras dos familias de condicionales representadas por  las clases  Mcondicionales y  MDcondicionales se comportan de manera similares  teniendo en cuenta el mismo modelo de integración propuesto [10]. A continuación se describen los aspectos principales que se tuvieron en cuenta en  la integración de los anteriores sistemas de coordinación de contratos sensibles  al  contexto y extensiones de condicionales para la aplicación de un nuevo sistema de  métricas de interacciones mediante un modelo de simulación DEVS.  3. Modelo conceptual de integración Para implementar la invocación de métricas mediante métodos correctos, propusimos  desde la  perspectiva del  rediseño e implementación computacional,  un modelo de  integración de muy bajo costo, sin cambios sustanciales ni en la arquitectura original  ni en el código de la implementación dentro de la aplicación modificada en el proceso  de inyección de las propiedades de coordinación de contratos sensibles al contexto  [3]. El  modelo conceptual  de métrica pertenece al  Modelo INCAMI (Information  Need,  Concept  model,  Attribute,  Metric  and  Indicador:  Información  relevante,  Modelo  Conceptual,  Atributos,  Métricas  e  Indicadores)  [12].  INCAMI  es  un  framework  organizacional,  orientado  a  la  medición  y  evaluación  que  permite  economizar  consistentemente,  no  sólo  metadata  de  métricas  e  indicadores,  sino  también valores mensurables en contextos físicos. Por  medio  de  un  diagrama  UML,  se  representa  un  modelo  general  de  integración,  teniendo  en  cuenta  experiencias  vinculadas  al  agregado  de  nuevas  componentes  en determinadas implementaciones  resueltas  para sistemas e-learning  similares  al  diseño  del  framework  [2].  La  integración  se  produce  mediante  la  conexión de las reglas, a través de sus condicionales, con una métrica representada  con un método. A su vez,  la métrica es interpretada por un modelo DEVS diseñado  para devolver valores de simulación [13]. En la  figura 3 se puede observar  lo correspondiente a  cada una de las  áreas  mencionadas,  representadas  con  colores  diferentes.  Además,  se  muestra  que  la  principal componente para lograr la integración está representada por la incorporación  de una relación de agregación entre la componente Contrato y la entidad Método. Los  condicionales  de  las  reglas  de  los  contratos  son  invocados  (mediante  un  método  explícito  relacionado  con  la  noción  de  los  Condicionales  DEVS,  por  ejemplo,  getForum_theme) por medio de un mecanismo de callback que permite la correcta  invocación de la métrica. La  primera fase de la misma corresponde a la definición y especificación de  requerimientos. Este módulo trata con la definición de la necesidad de información  (es decir, el foco de la evaluación) y el diseño de los requerimientos no funcionales,  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 248 que servirán como guías para las actividades posteriores de medición y evaluación. Fig. 3. Modelo de integración para contractos, métricas y modelo DEVS Tomamos  como punto  de  partida  la  descripción  del  Dispositivo  Hipermedial  Dinámico [14]. De esta manera se desprende que la información necesaria en nuestro  caso es función de las interacciones de los participantes, las cuales estarán definidas  por:  id del  participante,  rol del  mismo,  Paquete  Hipermedial  (PH)  sobre  el  cual  participa,  tipo  de  PH,  herramienta  sobre  la  cual  realiza  la  interacción,  tipo  de  herramienta, servicio con el cual interactúa y el tiempo, día y hora de la interacción  [13]. El  principal  objetivo  de  la  implementación  de  la  evaluación  global  permite  mayores  niveles  de  flexibilización  para  los  valores  de  los  indicadores  globales  y  parciales, a partir de los valores de indicadores elementales utilizando el modelo de  agrupamiento obtenido para efectuar el cálculo. En este proceso, dichos valores deben  ser acordados y consensuados por expertos con experiencia en el uso de este tipo de  sistemas.  El  seteo  de  los  coeficientes  serán  establecido  a  través  de  la  interface  setcoeficient, de la clase  Modelo DEVS (figura 3). En cada caso, el valor resultado  brinda una medida sobre el grado de interactividad de la participación. El responsable  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 249 de la evaluación pueda dar valor a los diversos coeficientes subrayando aquel atributo  que considere más importante en el proceso. En cada caso, el valor resultado brinda  una medida sobre el grado de interactividad de la participación. Este valor se obtiene  a través de la interfase  getInteracción,  que toma como argumento en este caso un  número entero que identifica la herramienta. Por último mencionamos que la interfase  setParametros queda reservada para  posibilitar diferentes relaciones algebraicas dentro de la métrica potenciando el nivel  de expresión de la misma. La interpretación y manipulación de los resultados de interacciones resueltos en  el  Modelo  DEVS es  manipulado  por  una  herramienta  representada  por  la  clase  Herramienta.  A su  vez  la  herramienta  es  la  encargada  de  brindar  la  información  necesaria sobre los parámetros que necesita la clase  Método que es utilizada como  argumento de la función  setParámetro.  El método getValorCondición representa los  valores de verdad del condicional que formará parte de la regla explícita representada  por el método reglas de la clase Contrato. Técnicamente la  Herramienta es una aplicación que respeta la arquitectura del  framework colaborativo SAKAI [7], utilizando los servicios base para el acceso a la  base de datos. Por otro lado,  permite la aplicación de una función transferencia que  transforma  dichos  datos  teniendo  en  cuenta  un  archivo  de  parametrización.  En  nuestros desarrollos se utilizó iBbatis (http://ibatis.apache.org/) para el acceso a datos  y  XML  DOM  [http://msdn.microsoft.com/enus/library/ms764730%28VS.85%29.aspx]  Parser  para  la  parametrización  de  la  función transferencia.  Los  demás  componentes  tecnológicos  que  complementan  el  desarrollo cumplen los estándares del framework, en este caso se utilizaron Servlets y  Beans teniendo en cuenta el acceso a los servicios base del framework que permitan  el registro de la aplicación como herramienta. 4  Implementación en un caso de uso A continuación se describe un caso de uso para ejemplificar las distintas etapas que se  deben cumplimentar como usuario para la activación de las propiedades que brindan  los  Condicionales DEVS  dentro de los contratos sensibles al contexto (sección 2).  Además, se brindarán detalles funcionales sobre el uso de la herramienta Sakai que  implementa la conexión entre los métodos de la métrica y el modelo de simulación  DEVS, manteniendo la perspectiva de un usuario final. Se comienza con el diseño de las reglas de los contratos y sus correspondientes  condicionales DEVS. Para esta etapa tomaremos como referencia el esquema para el  diseño de contrato propuesto en UWATc en la última etapa del diseño de procesos elearning Web [10]. 4.1 Representación de los Condicionales DEVS a través de UWATc En UWATc se brinda un diagrama de representación de contrato, donde se describen  todos los datos que lo instancian. Cada tipo de dato y valor, pertenece a un elemento  del metamodelo de la figura 1. CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 250 Teniendo en cuenta el diagrama siguiente, en primer lugar (item 1) se identifican  los  objetos  participantes  en  el  contrato;  en  dicho  ejemplo  DiscussionAction y  UserAction hacen referencia a dos clases reales perteneciente a la implementación de  la  herramienta  Foro  y  Usuarios  de  una  aplicación,  respectivamente.  Luego,  se  identifican  los  nombres  de  los  parámetros  context-aware  significativos  para  el  contrato,  alineados en la misma columna del objeto que lo comparte (item 2).  En  Servicios  (item  2)  deben  ser  representados  los  métodos  del  objeto,  que  al  ser  ejecutados,  provocan  la  intervención  del  contrato.  Para  este  ejemplo  initState y  getIdentifier son ejecutados cuando un usuario ingresa a la herramienta Foro y las  posteriores  funcionalidades  (servicios)  disponibles  dependen  de  la  ejecución  del  contrato Edición. Las siguientes filas (item 2) se refieren a las pre y post-condiciones  que se deben cumplir en la ejecución del contrato. Por último se explicitan las reglas de coordinación (item 3). Siguiendo con el  ejemplo, en la parte del condicional u.contexto =' l1; p1; docente; r1; c1; IT1; IPH1'  verifica si el contexto del usuario u está compuesto por la locación l1, tienen el perfil  p1,  es  un  docente,  cumple  el  rol  r1  y  pertenece  a  la  categoría  c1  (este  tipo  de  representación de contexto se encuentra desarrollado en [2]). Además, los últimos dos  valores IT1 y IPH1, representan el nivel de interacciones que un determinado usuario  y de una herramienta particular, respectivamente. A diferencia de los otros valores,  IT1 y  IPH1 serán comparados con la resultante de la aplicación de una métrica de  interacción a través de un elemento externo con interfaz para comunicarse a través de  los Condicionales DEVS. Contrato: Edición 1 Participantes: d:DiscussionAction u:UserAction 2 Param. c-a:Servicios:Pre-Cond:Pos-Cond: state, portlet, rundata,contextinitState()existe < contexto >modifica < contexto > contextidentifier, identifiergetIdentifier()existe < contexto > 3 Reglas : Si  u.contexto=’p1;d;r1;c1;IT1,IPH1’entonce s   d.showMessage(data,string) Condicionales 4 DEVS Valores Coeficientes según métrica IT1 C1..4:  valores de serviciosB1..4: valores de las herramientas , donde Bi = C1*C2*C3*C4 IPH1 PH: valor para el PH, donde PH=B1*B2*B3*B4 5 MD - 6 M - 7 Comentario Del Item 4: Especificación de las Métricas: Herramienta – PH – DHD [13]. CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 251 En el  item 4 se  describe la  forma de representar  en el  diagrama que dichos  valores pertenecen al tipo de Condicional DEVS y la definición de los coeficientes  que formarán parte de la métrica y serán alcanzado por los valores de los parámetros  de las interfaces de los métodos que la representan. En cuanto a la acción de la regla de coordinación, continuando con el mismo  ejemplo, en el item 3 se induce  la ejecución del método showMessage del objeto d  (DiscussionAction). El final del diagrama está dedicado a comentarios generales; cada  comentario debe ir acompañado con el número al que hace referencia. De esta manera, a través de la extensión del diagrama de contrato de la etapa 4  en  el  proceso  de  diseño  de  los  Pe-lrn  [10]  se  logra  describir  los  principales  componentes  que  se  tienen  en  cuenta  en  el  diseño  e  implementación  de  los  Condicionales DEVS  para los casos de uso similares al presentado. 4.2 Ejemplo de ejecución de la métrica en PowerDEVS Atendiendo a lo expuesto, seguidamente se implementan lo explicado en el entorno  PowerDEVS  [15]  teniendo  en  cuenta  el  mismo  caso  de  uso  introducido  en  esta  sección.  En  la  figura  3,  la  clase  Modelo  DEVS contiene  las  interfaces  que  son  implementadas a través de la herramienta PowerDEVS para la interpretación de las  métricas presentes en el comentario del diagrama de contrato. Cada  métrica  directa  tiene  asociado  un  método  de  medición  claramente  especificado. Las colecciones de datos son capturados desde la bases de datos (en este  caso MySQL). Luego los datos son formateados para posibilitar su lectura desde el  entorno. En la figura 4 se muestran, a manera de ejemplo, los resultados obtenidos de  Nivel de Interactividad para cada participación a través del tiempo, en los meses de  Noviembre-Diciembre para un curso seleccionado en el 2009, del Campus Virtual de  la Universidad Nacional de Rosario (http://www.campusvirtualunr.edu.ar). Fig. 4. Resultados obtenidos en el entorno PowerDEVS. Cabe mencionar que los resultados obtenidos son los globales del DHD, pero que  sin  embargo  se  disponen  a  su  vez,  los  valores  parciales  tanto  a  nivel  Paquete  Hipermedial,  como a nivel  Herramienta  individual,  (por  cuestiones  de espacio no  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 252 mostramos aquí dichos gráficos). Los mismos se exportan a un archivo que relaciona  el número de participación, con su nivel de interactividad. El  resultado  de  este  análisis  es  considerado  una  información  de  contexto,  resignificando una característica del comportamiento de los participantes y atendiendo  a la posibilidad de usar  la información de interactividad como parámetro contextaware  de  los  contratos  [3].  Podremos  entonces,  establecer  un  lazo  de  retroalimentación  entre  las  prácticas  efectuadas  en  los  entornos  colaborativos,  informadas en el Registro de Actividad y las acciones que devengan de los contratos. 5. Conclusiones En este trabajo se fundamentó la posibilidad de extender las propiedades expresivas  de las reglas de coordinación de contratos de los DHD a partir de los resultados de un  mecanismo externo. La propuesta de integración expuesta sigue respetando las líneas  de diseño e implementación establecidas por el modelo de los DHD.  Ahora  se  tiene  un  nuevo  mecanismo  para  la  escritura  de  las  reglas  de  los  contratos  que  permitirá  ahorrar  esfuerzo  en  el  diseño  de  los  condicionales  que  verifiquen información de contexto parametrizado. Además, se evita la necesidad de  tener conocimiento sobre las leyes (diseño) de las métricas y su implementación, con  sólo tener información sobre la definición de los coeficientes que participan en la  métrica.	﻿métricas , dispositivo hipermedial dinámico , coordinación de contratos , TIC	es	18926
10	Análisis y diseño preliminar del sistema integral para la asignatura Trabajo Final de Aplicación	﻿ La asignatura Trabajo Final de Aplicación de la carrera de  Licenciatura en Sistemas de Información, de la Facultad de Ciencias Exactas y  Naturales y Agrimensura es el espacio curricular en el cual se generan los  proyectos o planes de trabajo de fin de carrera. Su objetivo general es completar  la formación académica y profesional de los alumnos, posibilitando la  integración y utilización de los conocimientos adquiridos durante sus años de  estudio para la resolución de problemas de índole profesional o científico. En  este trabajo se describen los resultados preliminares obtenidos en el análisis y  diseño de un sistema de información orientado a gestionar los datos y producir  información oportuna en el contexto de la mencionada asignatura.  Palabras claves: Trabajo Final de Aplicación, carrera de informática, análisis y  diseño de sistemas de información.  1 Introducción  La Universidad Nacional del Nordeste promueve actividades de docencia,  investigación, extensión y transferencia, enfatizando aquellas orientadas al medio  social, cultural y económico en la cual se encuentra inserta con el fin de contribuir al  desarrollo local y/o regional.  Que algo esté socialmente construido implica que se generalice dentro de esa  comunidad, una idea, un concepto, una percepción de algo, que es parte de su realidad  y que de alguna manera condiciona o constituye una variable más a considerar dentro  de su desarrollo [2]. Por ello, el construccionismo social es un elemento al que se  puede remitir en el contexto de producción de proyectos y productos de tesinas o  trabajos finales de graduación.  La asignatura Trabajo Final de Aplicación (TFA) de las carreras Licenciatura en  Sistemas y Licenciatura en Sistemas de Información, de la Facultad de Ciencias  Exactas y Naturales y Agrimensura es el espacio curricular en el cual se generan los  proyectos o planes de fin de carrera.   Su objetivo general es completar la formación académica y profesional de los  alumnos, posibilitando la integración y utilización de los conocimientos adquiridos  durante sus años de estudio para la resolución de problemas de índole profesional o  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 343 científico ([3] y [4]). Por lo expuesto, se considera que el TFA constituye el espacio  académico ideal para plasmar los conocimientos adquiridos en las mencionadas  carreras, favorecer la formación de los futuros egresados de acuerdo a los  requerimientos del mundo del trabajo y promocionar y constituir el nicho para la  elaboración de productos en el marco de éstas actividades en que la Universidad es el  principal generador [5]. Es decir, requiere del alumno la integración, reproducción,  adecuación y establecimiento de relaciones de los conocimientos aprendidos a fin de  brindar una solución informática a un caso de estudio.   En el diseño y desarrollo del proyecto de fin de carrera, se diferencian los  siguientes momentos: i) un primer momento, corresponde al diseño del plan o  proyecto, ii) el segundo momento, está asociado a la generación del producto definido  en el plan. Generalmente en carreras de sistemas los productos pueden ser un  software, modelo o prototipo, acompañado de un informe. iii) Finalmente, un tercer  momento corresponde a la preparación y defensa del TFA ante el tribunal designado.   En Mariño y Herrmann [5] se describió la función principal del plantel docente,  orientada al asesoramiento, seguimiento y tutorización en el diseño y desarrollo del  plan de trabajo y las condiciones a cumplir por el alumno, para regularizar la  asignatura.   De lo expuesto se deriva que los actores involucrados en esta etapa de la carrera  son: i) profesor(es) orientador(es)-alumno, ii) cátedra-alumno, iii) cátedraprofesor(es) orientador(es), iv) cátedra-miembros del tribunal examinador, v) alumnomiembros del tribunal examinador, vi) profesor(es) orientador(es)-miembros del  tribunal examinador.  En este artículo se describen los primeros resultados obtenidos en el análisis y  diseño de un sistema de información orientado a gestionar los datos y producir  información oportuna en el contexto de la mencionada asignatura.   2 Marco metodológico  Este trabajo se encuadró en un estudio descriptivo. Se coincide con [1] en que el  “análisis bibliográfico tiene su mayor intensidad en los comienzos”. Este primer  momento aborda la recopilación, revisión y estudio de metodologías y ciclos de vida  de la ingeniería del software.   En una segunda etapa, el énfasis se estableció en el trabajo de campo, representado  por la metodología propuesta y un exhaustivo análisis y diseño del sistema. Entre  estas dos etapas se refleja un proceso dialéctico descripto por Samaja [7] y como lo  exponen Guisen et al. [1] “nos llevará a un nuevo conocimiento científico”.  La metodología es de tipo cualitativa. Se basó en la observación participante en  escenarios originales, entrevistas abiertas y luego focalizadas; consulta de  documentos de la asignatura, por ejemplo, el reglamento para el desarrollo del TFA y  otros recursos destinados a los alumnos; como así también en el empleo de algunas  notaciones gráficas como los diagramas de casos de uso.   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 344 3 Resultados   En esta sección se sintetizan las primeras fases del análisis y diseño del sistema de  información orientado a la gestión integral de la asignatura Trabajo Final de  Aplicación.  Los resultados que se exponen en esta sección se basaron en la adaptación y  aplicación de la metodología propuesta en Pressman [6]. Esta metodología incluye  cuatro actividades genéricas que son comunicación y planeación, modelado, construcción y despliegue. A continuación, se expone la metodología propuesta, detallando las fases  consideradas para las dos primeras actividades genéricas: i) comunicación y  planeación y ii) modelado. Las actividades genéricas denominadas como construcción  y despliegue serán abordadas en detalle en un próximo trabajo.  3.1   Actividad de Comunicación y Planeación.  Esta actividad se corresponde con la primera etapa e incluye las siguientes acciones o  fases:  Definir el objetivo. A partir de la comunicación con los docentes de la asignatura  Trabajo Final de Aplicación se define el objetivo de la aplicación a desarrollar. A  modo sintético, el objetivo del sistema es simplificar la gestión de la asignatura en  cuanto a alumnos, exámenes, proyectos de trabajos finales, etc. También se contempla  como propósito la publicación de los resúmenes sintéticos de los trabajos finales una  vez defendidos y aprobados.  Definir el equipo de trabajo. El equipo de trabajo está compuesto por dos  estudiantes avanzados de la carrera y los docentes de la asignatura. Los docentes  proporcionan los requerimientos y participan en la prueba de los prototipos, aportando  las consideraciones necesarias para guiar el desarrollo. Parte de la etapa de análisis,  así como el diseño de datos, son llevados a cabo en conjunto por ambos alumnos.   Obtener una imagen global. La importancia del Sistema de administración de la  asignatura, está dada por la necesidad de automatizar la gestión académica en cuanto  a alumnos, exámenes, recursos, proyectos de trabajo final, etc. y por otra parte  difundir los datos más generales de los TFA realizados y defendidos por los alumnos  de años anteriores, para que sirvan de base en la elección de nuevos temas de  investigación, tanto para alumnos de ésta como de otras universidades. Es importante  aclarar que no se publica el contenido de los informes producidos por los alumnos,  sino sólo el resumen sintético.   Definir el alcance de la aplicación. En esta fase se han obtenido los principales  requerimientos a implementar mediante el sistema. Pueden resumirse como: i)  Autogestión de alumnos en cuanto a sus datos personales y proyectos. ii)  Administración centralizada de toda la información referente a la asignatura por parte  de los docentes. iii) Presentar información general de los TFA, alumnos, profesionales  orientadores, etc. iv) Permitir la publicación de los resúmenes sintéticos de los  trabajos finales de la carrera. v) Proporcionar facilidades de búsqueda para los TFA  de acuerdo a los criterios más importantes. vi) Facilitar la descarga de archivos en  formato PDF con los datos principales de los trabajos. vii) Generar estadísticas.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 345 Definir los tipos de usuario y su jerarquía. A partir de la obtención de los  requerimientos es posible identificar los tipos de usuarios, llamados a menudo actores,  y formar una jerarquía de acuerdo a la funcionalidad que ofrecerá la aplicación para  cada uno de ellos. Como se ilustra en la Figura Nº 1, el sistema interactúa con  diferentes roles de usuarios que poseen distintos permisos según su perfil. De esta  manera el sistema controla el acceso de cada usuario y permite realizar actividades de  acuerdo a su rol. Todos los usuarios deben autenticarse para acceder al sistema,  excepto el Usuario General, que sólo puede acceder a datos públicos de los trabajos.  El actor denominado “Usuario” representa una categoría general de usuario y los  demás actores, excepto Alumno y Consultor General, heredan atributos de este. Los  diferentes actores que interactúan con el sistema de información integral de la  asignatura son:   Administrador: es el perfil de usuario que posee todos los privilegios de los demás  actores, y es el único con permisos para definir otros usuarios.   Gestor de proyectos: es el perfil de usuario con permisos para consultar y  actualizar cualquier información relativa a los proyectos de trabajos finales.   Gestor de alumnos: un usuario con este perfil tiene permisos para acceder y  actualizar los datos personales de los alumnos. Una de las funciones disponibles  para este perfil consiste en validar las pre-inscripciones a la cátedra efectuadas por  los alumnos.   Gestor de exámenes: es el perfil de usuario con permisos para acceder y actualizar  la información relativa a los exámenes. Algunos ejemplos de las funciones  disponibles para este perfil de usuario son: definir fechas de exámenes, definir  resultados de exámenes y prácticos de un alumno.   Consultor de TFA: una vez iniciada la sesión especificando su nombre de usuario y  contraseña, puede acceder a todos los datos de los trabajos finales. Este rol  corresponderá normalmente a los docentes de la facultad.    Gestor de recursos: un usuario con este perfil tiene permisos para publicar  cualquier material que resulte útil para el taller de la asignatura, de manera que  estén disponibles para los alumnos en condiciones de cursar la asignatura.   Usuario General: es aquel que accede a través de la Web sin contar con una cuenta  en el sistema. Sólo puede consultar los datos públicos de los trabajos.   Alumno: se corresponde con un alumno de la asignatura. Es él quien tiene la  responsabilidad de pre-inscribirse y registrar los datos del anteproyecto de su  trabajo final.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 346 Fig. 1. Actores (tipos de usuarios) considerados en el Sistema de Administración de la  Asignatura.  3.2   Actividad de Modelado  Esta actividad incluye las fases de análisis y diseño de datos. El objetivo es hacer un  análisis más detallado del sistema, incluyendo adaptaciones de algunos diagramas  UML, generalmente utilizados en Análisis Orientado a Objetos.  Fase de Análisis. Se desarrollan diagramas para facilitar la comunicación en el  equipo, el entendimiento del problema, y la construcción del sistema. Particularmente  se trabaja con diagramas de casos de uso y de secuencia, los que se describen a  continuación.  Construcción del diagrama de casos de uso. Un diagrama de casos de uso  representa la funcionalidad provista por el sistema a los usuarios externos. Está  compuesto por actores, nodos de caso de uso, relaciones y la representación de los  límites del sistema. La Figura 2 muestra el diagrama de casos de uso del sistema de  administración de la cátedra a nivel general, mientras que la Figura 3 presenta un  diagrama con mayor nivel de detalle.  Construcción de los diagramas de secuencia. Un diagrama de secuencia  representa una secuencia de intercambio de mensajes en el tiempo, entre varios  objetos para lograr un comportamiento particular. Permite comprender el flujo de  mensajes y de eventos que ocurren en un cierto proceso o colaboración. En las  Figuras 4 a 6 se ilustran los diagramas de secuencia que describen las funciones más  relevantes que el sistema ofrece a los alumnos, como: los escenarios de autenticación,  registro del formulario del proyecto de trabajo final y consulta de datos personales de  un alumno.  El Consultor de TFA dispone de funciones similares que el Usuario General en cuanto  a búsqueda y consulta de trabajos defendidos. La diferencia es que el primero obtiene  acceso a los datos restringidos del TFA. Se requiere realizar un proceso de log-in.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 347 Fig. 2. Diagrama de casos de uso del Sistema de Administración de la Asignatura. Fig. 3. Diagrama detallado de casos de uso del sistema.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 348 Fig. 4. Diagrama de secuencia que describe el escenario de autenticación de un alumno.  Fig. 5. Diagrama de secuencia que describe el  caso de uso “Registrar formulario”.  En la Figura 7 se expone la interacción existente para la funcionalidad de “Buscar  TFA” por parte del Usuario General. La Figura 8 presenta el diagrama de secuencia  del caso de uso “Consultar datos públicos de un TFA”    Fase de Diseño de datos. Considerando las necesidades de la asignatura, se diseñó  el modelo de datos.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 349 Fig. 6. Diagrama de secuencia que describe el caso de uso “Consultar proyectos personales”.  Fig. 7. Diagrama de secuencia que describe el caso de uso “Buscar TFA”  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 350 Fig. 8. Diagrama de secuencia que describe el caso de uso “Consultar datos públicos de un  TFA”.  Actualmente, se trabaja en las tercera y cuarta etapas del sistema de información  propuesto. En referencia a la Etapa 3: Actividad de Construcción o Producción, se han  implementado las funciones orientadas a la autogestión del alumno, como la  inscripción a  la asignatura, registro del formulario de TFA, consulta y modificación  de datos personales, entre otras. Recientemente se inició la implementación de las  funciones específicas de la gestión de proyectos de trabajos finales y del catálogo de  los trabajos finales ya defendidos, que permitirá publicar los resúmenes de los mismos  en la Web. La Figura 9 ilustra el diagrama de despliegue del sistema de gestión,  representando la red de elementos a ser procesados y la configuración de los  componentes de software sobre cada dispositivo físico.   4 Conclusiones  Se expusieron los avances de una de las líneas de trabajo del proyecto en ejecución en  la asignatura “Trabajo Final de Aplicación”. Se considera que su pronta puesta en  ejecución, apoyará a los docentes de la asignatura, los profesores orientadores y los  alumnos en la gestión y el control de éstas producciones integradoras de los  conocimientos adquiridos por los alumnos en su transcurso por las aulas de Educación  Superior.     CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 351 Fig. 9. Diagrama de despliegue del sistema	﻿análisis y diseño de sistemas de información , Trabajo Final de Aplicación , carrera de informática	es	19113
11	Modelo de secuenciamiento y navegación para la personalización de los aprendizajes en ILIAS	﻿ Uno de los riesgos que existen al utilizar plataformas de formación y  estándares para el e-learning es caer en enfoques centrados en la tecnología,  donde docentes y alumnos se encuentran limitados por las posibilidades que  ofrece la solución tecnológica que le da soporte. El estándar SCORM 20041  incorpora características que hacen posible al desarrollador controlar el  comportamiento del Objeto de Aprendizaje describiendo la estructura del  contenido y las reglas de secuenciamiento y navegación. En tiempo de  ejecución, mediante el RTE2 y siguiendo un protocolo de comunicación, el  Learning Management System (LMS) gestiona información de la interacción  del alumno con el contenido. Esto permite ofrecer al estudiante alternativas  que se adapten a sus necesidades instruccionales en forma automatizada. En  este artículo se describe, mediante un ejemplo, la aplicación del Modelo de  Secuenciamiento y  Navegación para personalizar el aprendizaje de los  estudiantes en la plataforma ILIAS.  Keywords: Secuenciamiento y Navegación, personalización del aprendizaje,  ILIAS, SCORM 2004, OA.   1   Introduction  Una plataforma para la formación on-line basada en SCORM contiene aplicaciones  que proporcionan al diseñador y a los usuarios una serie de facilidades para distribuir  y gestionar contenidos de aprendizaje, registrando el progreso de los estudiantes y su  interacción con los contenidos educativos.  Bajo la denominación SCORM la iniciativa ADL (Advanced Distributed  Learning)3 reúne contribuciones de organismos como IEEE e IMS y establece  especificaciones para el desarrollo, empaquetamiento y distribución de material  educativo.    SCORM se compone de tres grandes apartados: el modelo de agregación de datos  (CAM), el entorno de ejecución (RTE) y Modelo de Secuenciamiento y Navegación                                                              1 SCORM:  Sharable Content Object Reference Model- Disponible en http://www.scorm.com/  2 RTE: Run Time Environment (Entorno en Tiempo de Ejecución)  3 ADL (Advanced Distributed Learning) Disponible en http://www.adlnet.org/Pages/Default.aspx   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 395 (SN). Cada uno de ellos resuelve un aspecto concreto en lo que refiere a creación y  distribución de contenido.  El RTE aporta un mecanismo que permite al contenido comunicarse con la  plataforma, mediante un protocolo de comunicación.   La normativa SCORM define un conjunto de valores (DATAMODEL) que se  pueden almacenar en la base de datos del servidor LAMP (Linux-Apache-MySqlPHP). Por cada instanciación Contenido-Alumno, el Learning Managment System  (LMS) guarda la información correspondiente. A partir de la lectura de estos valores  y estableciendo pre y pos requisitos entre los OA se pueden definir diferentes  secuencias de aprendizaje a seguir por los alumnos matriculados en un determinado  curso.  SCORM 2004 presenta algunas mejoras respecto de SCORM 1.2 en lo que  respecta las especificaciones de secuenciamiento y navegación. Esto permite  controlar la entrega de contenido al alumno, a partir del resultado obtenido en la  actividad que se ha completado previamente. De este modo, un estudiante puede  pasar a la siguiente actividad si alcanzó los objetivos fijados, caso contrario podría  continuar en la misma o realizar otra actividad complementaria, hasta alcanzar los  objetivos establecidos.  El material educativo para la formación on-line se diseña a partir de unidades  didácticas denominadas Objetos de Aprendizaje (OAs). Derivados del paradigma de  la Programación Orientada a Objetos, los OAs constituyen un nuevo concepto a tener  en cuenta en la creación de contenidos y actividades digitales con propósito  educativo.  Si bien existen diversas concepciones acerca de qué son los OA’s, los principales  referentes los definen como:  “Objeto didáctico es cualquier recurso digital que pueda ser reutilizado como  soporte para el aprendizaje” (Wiley) [1]  “Unidad didáctica independiente y autocontenida predispuesta para su  reutilización en diversos contextos educativos” (Polsani) [2]  Entre las características que los OA debieran presentar para ser considerados tales,  Longmire [3] tiene en cuenta las siguientes:  - Modular, autocontenido y portable.  - No secuencial.  - Satisfacción de un único objetivo didáctico.  - Orientado a un público amplio.  - Coherente y unitario dentro de un esquema predeterminado, posible  de ser meta-etiquetado.  - Reutilizable sin alterar sus valores esenciales, ni su contenido.  Es decir, si se respetan los requerimientos técnicos para el desarrollo de los OA  mediante la utilización de estándares los materiales didácticos producidos podrán ser  reutilizados en diferentes contextos, accedidos desde diferentes plataformas y  adaptarse a los cambios tecnológicos sin ser rediseñados.   Ilias[4] es una plataforma para la formación on-line de código abierto que brinda  conformidad con el estándar SCORM. Para este caso de prueba se utiliza la  plataforma www.evirtual.unsl.edu.ar instalada a partir de Ilias versión 4.0.0   Para su comunicación este trabajo se estructura de la siguiente manera: introduce  el marco teórico y una breve descripción de los supuestos básicos acerca del estándar  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 396 SCORM 2004. Continúa con el desarrollo del material didáctico en las etapas de  diseño, construcción de los objetos de la unidad didáctia  y testeo de los mismos en la  plataforma Ilias. Finaliza con conclusiones y posibles derivaciones en futuros  trabajos.   1.1 Marco teórico  La idea del constructivismo trajo como resultados avances importantes en el  entendimiento de cómo funciona el desarrollo cognitivo en las personas. La conexión  entre la tecnología y el aprendizaje no es un hecho circunstancial. Las aulas  tradicionales resultan en muchos casos pobres para el soporte de la enseñanza. Las  nuevas tecnologías, si son utilizadas de manera efectiva, habilitan nuevas maneras  para enseñar que coinciden mucho más con la manera en que las personas aprenden.   En la interacción de los estudiantes con las nuevas tecnologías, se pueden observar  los resultados que muestran investigaciones relacionadas con el desarrollo cognitivo  y el constructivismo, donde la conclusión ha sido que el aprendizaje es más efectivo  cuando están presentes cuatro características fundamentales: compromiso activo,  participación en grupo, interacción frecuente, retroalimentación y conexiones con el  contexto del mundo real [5].  Las aplicaciones de las nuevas tecnologías deben servir para desarrollar independencia en el alumno, quien toma un papel activo para solucionar problemas,  comunicarse efectivamente, analizar información y diseñar soluciones. En síntesis,  un alumno autónomo, capaz de “aprender a aprender”.  Esta perspectiva de enseñanza y aprendizaje está en íntima relación con una  postura constructivista, que demanda nuevos roles para alumnos y docentes.   Conocer no es copiar el objeto sino actuar sobre él y transformarlo según  esquemas existentes, sabiendo que en la interacción el objeto también modificará los  esquemas del sujeto. Piaget [6] afirma que “para conocer los objetos, el sujeto tiene  que actuar sobre ellos, y, por consiguiente, transformarlos”   Corresponde al docente el rol de guía del proceso de aprendizaje. En su propuesta  didáctica deberá organizar y secuenciar el material didáctico de acuerdo a las  características de los estudiantes. El orden en que los contenidos son presentados no  es indiferente para el aprendizaje. Zapata Ros [7] propone tres técnicas para  secuenciarlos: basada en el análisis de los contenidos, basada en el análisis de la tarea  y en la teoría de la elaboración.  Seleccionamos la última técnica, que integra elementos de las dos anteriores en un  esquema que basa la secuenciación de los contenidos de enseñanza en el siguiente  principio: “Los contenidos de enseñanza tienen que ordenarse de manera que los  elementos más simples y generales ocupen el primer lugar, incorporando después, de  manera progresiva, los elementos más complejos y detallados”.  Las nuevas tecnologías ofrecen la capacidad de interacción entre los estudiantes, y  le permiten decidir la secuencia de información por seguir, establecer el ritmo,  cantidad y profundización de la información que desea. La secuenciación de  contenidos, de tareas y de actividades deben permitir un acercamiento progresivo  desde una situación inicial hasta los objetivos de aprendizaje propuestos, teniendo en  cuenta las particularidades de los destinatarios y su propio ritmo de aprendizaje.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 397 2.2 Modelo de Secuenciamiento y Navegación SCORM 2004  SCORM 2004 provee especificaciones técnicas (derivadas de IMS) para describir la  estructura del SCO y las reglas de secuenciamiento y de navegación asociadas:    2.2.1 Estructura del SCO  La estructura del SCO recibe el nombre de “árbol de actividades”.   Un árbol de actividades se compone de actividades de secuenciamiento simples  (hojas) o compuestas (clúster o conjunto de actividades).   A una actividad de secuenciamiento se le puede asociar uno o más objetivos  locales (que permanecen sólo durante su ejecución) o globales (perduran aunque la  ejecución finalice y pueden ser leídos y escritos por cualquier actividad, manejándose  como variables globales durante la ejecución del SCO). Si bien no es obligación  definir estos tipos de objetivos, es necesario hacerlo en el caso de que se desee  modificar el comportamiento de secuenciación del SCO.     2.2.2Reglas de Secuenciamiento y de Navegación:  Sobre cada una de las actividades del árbol se definen las reglas de  secuenciamiento y de navegación. Éstas se dividen en varias categorías entre  las que  destacamos, dada su mayor factibilidad de uso para el secuenciamiento, las  siguientes: [8]    • Modo de control del secuenciamiento Sequencing Control Modes:  Determina el tipo de navegación que se le permitirá realizar al  usuario. Para una libre navegación por los contenidos se deben habilitar  los botones anterior  y siguiente.  • Limitar la elección de los Controles Constrain Choice Controls:  Restringe las actividades que el usuario puede seleccionar de la tabla de  contenido.   • Reglas de Secuenciación Sequencing Rules: Especifican las condiciones  que determinarán que actividades estarán disponibles para ser ejecutadas  y que actividades deberían ejecutarse próximamente.   • Condiciones Límite Limit Conditions: Proporcionan límites en el número  de veces que las actividades se pueden intentar.   • Rollup Reglas Rollup Rules: Especifican las condiciones que  determinarán el estado de la actividad padre, en base a las actividades  hijo.  • Controles de Rollup Rollup Controls: Determinan qué actividades  participarán en el proceso de roll-up  y cómo se califica  este estado en  relación con otras actividades.   • Objetivos:  Proporcionan una manera de seguir el estado o la situación de  cada una de las actividades y compartir este estado entre actividades.     Tanto la estructura del contenido como las reglas de secuenciación quedan  representadas en el archivo manifiesto del curso (Manifiest.xml)    2.2.3 Comportamiento del árbol de actividades en tiempo de ejecución  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 398   El proceso de secuenciamiento se produce siempre que un curso o SCO se inicia,  o cuando el alumno realiza una solicitud de navegación a través de interfaz de  usuario del LMS.   Cada actividad tiene, en tiempo de ejecución, dos conjuntos de datos asociados:   • Datos de Seguimiento (tracking data): tienen relevancia para las actividades  hojas. Almacena información referente al estado actual de la actividad  (completed, incompleted, unknown, known, entre otros) que se modifica en  tiempo de ejecución de acuerdo a los valores establecidos en las variables  del modelo CMI, definido por el AICC [9]. Por ejemplo la variable  “cmi.completion_status” afectará el estado actual de la actividad.   También se almacena información referida al estado actual de los objetivos  asociados a la actividad (satisfied, unsatisfied) que se actualizan en base al  valor representado en la variable “cmi.success_status”.  • Definición de secuenciamiento (sequencing definition): conjunto de reglas  de secuenciamiento (precondiciones, poscondiciones) que definen cómo  deben ser secuenciadas las actividades hijo y reglas de roll up que permiten  determinar los datos de seguimiento asociadas a una actividad padre (de tipo  cluster).   2. Desarrollo  El caso que se presenta como objeto de estudio en este trabajo corresponde al  desarrollo de la Unidad Temática: “Búsquedas de información en la Web”. Esta  unidad se había trabajado anteriormente, investigando sólo el uso de precondiciones  disponible para los SCORM 1.2 y SCORM 2004 [11]  Para diseñar e implementar la unidad temática, a partir del estándar SCORM 2004,  versión 1.3 se utilizan las siguientes herramientas:  • El programa Reload Versión 2.5.5[10], que permite la construcción del  árbol de actividades, el establecimiento de objetivos locales y globales y la  definición de las reglas de secuenciamiento y navegación.  • La programación del objeto, mediante la inclusión de las rutinas provistas  por ADL SCORM, tales como APIWrapper.js y courseFunctions.js,  modificadas para lograr el comportamiento esperado del objeto, en tiempo  de ejecución, de modo tal que sea posible la interacción con los datos  asociados a las actividades  Este desarrollo se realizó en varias etapas, teniendo en cuenta los supuestos  pedagógicos referenciados en el marco teórico, los lineamientos correspondientes al  modelo de secuenciamiento y navegación de SCORM 2004 y considerando el uso de  estándares para el e-learning en la construcción de los OA y en los aspectos  relacionados a la interacción alumno-contenido. A continuación se describen las  etapas mencionadas.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 399 2.1 Etapa I: Diseño del árbol de actividades y establecimiento de condiciones de  secuenciaciamiento.   Una vez seleccionados los contenidos a trabajar se identificaron las actividades, sus  contenidos asociados y las condiciones de secuenciación. Esto dio origen a la  secuencia de ejecución de actividades que se indica en la Tabla 1.  Tabla 1. Diseño de las actividades del curso y condiciones de secuenciación  Actividad Condiciones de  Secuenciación  Contenidos  1. Búsquedas de información (Raíz del árbol)  1.1 – Presentación del  Curso  -  Guía didáctica y  planificación del curso.   1.2 – Introducción a  los buscadores  -  La WWW y los Buscadores  de Información  1.3 – Búsquedas  simples y avanzadas  Requiere 1.2 Tips para búsquedas simples  y avanzadas.   1.4  – Práctica con  búsquedas simples y  avanzadas  Requiere 1.3    Ejercitación mediante  búsquedas simples y  avanzadas.  1.5 – Evaluación Requiere 1.4 (el test  podrá ser realizado  dos veces como  máximo)  Test de evaluación en base a  un tema de investigación  asignado.  1.6 Práctica Extra Requiere 1.5 (en caso  que la puntación sea  menor al 70%)  Práctica de refuerzo en  búsquedas simples y  avanzadas.  El árbol de actividades (compuesto sólo por hojas) queda conformado como se  indica en la Fig. 1:                Fig. 1. Árbol de Actividades   Las secuencias de aprendizaje posibles se determinan en tiempo de ejecución, de  acuerdo a los resultados devueltos por las actividades y a las condiciones de  secuenciación establecidas entre ellas.   Es así que, en principio, las actividades 1.1 y 1.2 se encuentran habilitadas,  pudiendo ejecutarse en cualquier orden, incluso la actividad 1.1 podría no ejecutarse  y esto no alteraría el secuenciamiento. La actividad 1.3 podrá ejecutarse siempre que  1  1.1 1.2 1.3 1.4 1.5 1.6  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 400 el alumno haya visitado la actividad 1.2. Este proceso se repite para las actividades  1.4 y 1.5, que requieren de la ejecución exitosa de la actividad anterior. Y la  actividad 1.6 será habilitada para su ejecución, siempre que el resultado de la  actividad 1.5 sea menor que el 70%.  En resumen, y de acuerdo a lo anterior, las secuencias posibles son:  - 1.1à 1.2 à1.3 à1.4 à1.5  - 1.1à 1.2 à1.3 à1.4 à1.5à1.6à1.5  - 1.2 à1.3 à1.4 à1.5  - 1.2 à1.3 à1.4 à1.5à1.6à1.5  - 1.2 à1.1à1.3 à1.4 à1.5  - 1.2 à1.1à1.3 à1.4 à1.5à1.6à1.5  - 1.2 à1.3 à1.1à1.4 à1.5  - 1.2 à1.3 à1.1à1.4 à1.5à1.6à1.5  - 1.2 à1.3 à1.4 à1.1à1.5  - 1.2 à1.3 à1.4 à1.1à1.5à1.6à1.5  - 1.2 à1.3 à1.4 à1.5à1.1  - 1.2 à1.3 à1.4 à1.5à1.1à1.6à1.5  - 1.2 à1.3 à1.4 à1.5à1.6à1.1à1.5  2.2 Etapa 2: Implementación del árbol de actividades y las reglas de  secuenciamiento  La Tabla 2 muestra las reglas de secuenciamiento definidas para el árbol de  actividades anteriormente mostrado.   Tabla 2. Reglas de secuenciamiento y navegación  Actividad  Objetivos/  Reglas de secuenciamiento y navegación/   Modos de control   1  Modos de control de ejecución habilitados: Choice, Choice Exit,  Flow, Use Current Attempt Objective Info y Use Current Attempt  Progress Info   1.1  Objetivo Local: obj_pres   Modos de control de ejecución habilitados:Choice, Choice Exit,  Use Current Attempt Objective Info y Use Current Attempt  Progress Info   1.2  Objetivo Local: obj_introd  Modos de control de ejecución habilitados: Idem 1.1  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 401 1.3  Objetivo Local: obj_tbusq  Objetivo Global Referenciado: obj_introd   Pre-condición:  If (referenced objective is NOT satisfied) O  (referenced objectivés status is NOT known) then  Disable me4  Modos de control de ejecución habilitados: Idem 1.1  1.4   Objetivo Local: obj_pbusq  Objetivo Global Referenciado: obj_tbusq  Pre-condición: Idem 1.3  Modos de control de ejecución habilitados: Idem 1.1  1.5   Evaluación  Objetivo Local: obj_ebusq  Objetivo Global Referenciado: obj_pbusq  Pre-condición: Idem 1.35  Post-condicion:If (referenced objective is NOT satisfied) Y  (attempst allowable has been exceeded) then  Exit all6  Modos de control de ejecución habilitados: Idem 1.1  1.6 Práctica  Extra  Objetivo Local: obj_pextra   Objetivo Global Referenciado: obj_ebusq  Pre-condición: Ídem 1.37  Modos de control de ejecución habilitados: Idem 1.1  2.3 Etapa 3: prueba del OA en ILIAS  El objeto es importado en Ilias como SCORM 2004 3° Edición. Una vez subido a la  plataforma y al inicio de su ejecución, aparece la siguiente ventana (Fig. 2)    Fig. 2 Inicio de la Ejecución de la secuencia de aprendizaje                                                              4 La actividad aparecerá de forma visible pero no podrá ser ejecutada si el estado del objetivo  referenciado no está satisfecho (el alumno no leyó todo el contenido asociado a la actividad)  o es desconocido (el alumno todavía no ha ejecutado la actividad).  5 El estado del objetivo referenciado estará satisfecho si el resultado de la evaluación es >=70.  6 La actividad no podrá ejecutarse y finalizará con su ejecución, si el estado del objetivo  referenciado no está satisfecho (el resultado de la evaluación no es >= 70) y si los intentos  de ejecución han excedido la cantidad de intentos disponibles.  7 La actividad aparecerá de forma visible pero no podrá ser ejecutada, si el estado del objetivo  referenciado ha sido satisfecho.   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 402 De acuerdo a las secuencias de aprendizaje definidas en el apartado anterior, las  actividades 1.1 y 1.2 se encuentran habilitadas para su ejecución (en cualquier  orden).  La ejecución de la actividad 1.4 depende del estado de la actividad 1.3. En el caso  de ser ésta satisfecha, muestra el árbol de navegación tal como aparece en la Fig. 3                      Fig. 3 Inicio de ejecución de la actividad 1.4  Continuando con la secuencia de navegación para el OA, y al momento de  finalizar la actividad 1.5, la actividad 1.6 figura deshabilitada, dado que el estado del  test ha sido “superado”. Esto se muestra en la Fig. 4    Fig. 4 Ejecución de la actividad 1.5  3 Conclusiones y trabajos futuros   La utilización del Modelo de Secuenciamiento y Navegación especificado por  SCORM permite diseñar propuestas de formación más complejas, pero a la vez más  flexibles, personalizando la ejecución de nuevas actividades de aprendizaje para cada  alumno en particular, según el estado de avance y la satisfacción de los objetivos  propuestos. Si bien lograr estas características implica un esfuerzo significativo para  el desarrollador de material educativo, supone beneficios en la calidad de las  propuestas didácticas y por consiguiente en la construcción de los aprendizajes por  parte de los alumnos.  Como limitaciones al uso de este modelo podemos mencionar las características  de las plataformas de formación on-line. Entre las que dan soporte al estándar no  todas permiten el uso del mismo en todas sus funcionalidades, y existen diferencias  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 403 significativas entre las que brindan conformidad a SCORM 1.2 y SCORM 2004 en  sus dos versiones: 1.3 y 1.4.  Ilias, en su versión 4.0.0 utilizada en la experiencia que aquí se relata, permite el  uso del Modelo de Secuenciamiento y Navegación y mantiene los datos obtenidos en  tiempo de ejecución, siempre y cuando se realice la adecuada programación de los  OA y se incluyan las instrucciones requeridas en el manifiesto del curso.  Quedan como trabajos futuros realizar las pruebas con los OA creados para esta  Unidad Temática en otras plataformas de código abierto y evaluar si es factible la  personalización de los recorridos formativos en tiempo de ejecución, tal como se  comprueba en ILIAS.	﻿ILIAS , secuenciamiento y navegación , personalización del aprendizaje , SCORM 2004 , OA	es	19136
12	Integrando UML y DSL en el enfoque MDA	"﻿ En algunos trabajos académicos surge la disyuntiva de utilizar UML  (Unified Modeling Language) ó DSL (Domain Specific Lenguage) para  modelar un determinado artefacto. UML es un lenguaje de propósito general el  cual en un nivel de abstracción elevado resulta de gran aplicabilidad, pero  cuando se comienza a bajar dicho nivel de abstracción y se requiere comenzar a  modelar características propias de un dominio, UML debe ser adaptado. Es  posible adaptar a UML generando un perfil propio para dicho dominio pero esta  actividad resulta compleja y en algunos dominios son muy pocos los elementos  y diagramas existentes que son directamente aplicables y por lo tanto es  necesario realizar una gran cantidad de extensiones para lograr modelar el  dominio. En cambio DSL es un lenguaje más simple de aplicar a un dominio  específico. En este trabajo se presenta una propuesta que permite dentro del  enfoque MDA (Model-Driven Architecture) utilizar UML y DSL en distintos  niveles de abstracción y generar mediante transformaciones el código fuente de  una determinada aplicación.  Keywords: Modelado, MDA, UML, DSL, WAP   1   Introducción  Actualmente los sistemas son muy disimiles unos de otros,  es por ello que al modelar  un sistema que pertenezca a un determinado dominio UML [4] resulta ser muy amplio  y complejo de adaptarse a las características particulares de dicho dominio. Al  momento de modelar el sistema, será necesario analizar el vocabulario de UML  (simbología e incluso diagramas que pueden ser aplicados) y extender el lenguaje por  ejemplo por medio de estereotipos y crear un profile que permita modelar las  características no nomencladas. DSL [5] ha sido creado con la idea de poder modelar  características particulares de dominios.   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 514 En este artículo se propone modelar un sistema con el enfoque MDA [7], [8]  utilizando a UML como lenguaje de modelado que permite analizar al sistema desde  un punto de abstracción alto y a DSL en un nivel más bajo de abstracción y más  cercano a la codificación específica en una determinada plataforma.   MDA es un enfoque ampliamente aceptado para el desarrollo de sistemas de  software complejos. “Es una iniciativa del OMG (Object Management Group), que  representa un nuevo paradigma de desarrollo de software donde los modelos guían  todo el proceso de desarrollo…” [4]  MDA propone el uso de modelos en todas las fases de desarrollo, desde la  especificación y análisis hasta la implementación. La transformación de modelos es la  base de MDA; comenzando por un modelo independiente de la plataforma el objetivo  es lograr, en cada paso, modelos más específicos. “Los modelos son creados en  diferentes niveles de abstracción separando los aspectos del negocio de los detalles  técnicos de la solución de software que se va a implementar. Básicamente tres  diferentes tipos de modelos son construidos, un modelo que contiene las  especificaciones de negocio, un modelo de alto nivel de la plataforma y uno que  incluye los detalles técnicos de la plataforma destino” [3]   “MDA fue establecida como una arquitectura para el desarrollo de aplicaciones;  tiene como objetivo proporcionar una solución para los cambios de negocio y de  tecnología, permitiendo construir aplicaciones independientes de la implementación;  representa un nuevo paradigma en donde se utilizan modelos del sistema, a distinto  nivel de abstracción, para guiar todo el proceso de desarrollo” [6] . A continuación se  definen cada uno de los modelos de MDA tal como se muestra en la figura 1[7]:  • CIM: Es un modelo independiente de lo computacional. No muestra detalles de la  estructura de un sistema. También suele ser denominado modelo de dominio, y  para su especificación se utiliza un vocabulario que es familiar a los practicantes  del dominio en cuestión. Se focaliza en el contexto  El CIM juega un rol importante al unir la brecha entre:  o aquellos que son expertos en el dominio y sus requerimientos  o aquellos que son expertos en el diseño y construcción de artefactos  En este nivel se representa exactamente qué es lo que se espera que el sistema  haga, pero oculta la información de la tecnología o como será implementada.   • PIM: Es un modelo independiente de la plataforma. Esto se puede lograr a través  de un modelado que no esté enfocado a una determinada plataforma sino que  realice una implementación abstracta de los detalles técnicos necesarios para su  construcción.   • PSM: Es un modelo específico de la plataforma. Combina las especificaciones  del PIM con los detalles que indican como ese sistema utiliza un tipo particular  de plataforma.                   Fig. 1. Esquema MDA CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 515 Fig.2. Aplicando UML y DSL al enfoque MDA  UML DSL CIM PIM  PSM 1  Java  …  PSM n  C#  CODIGO  1  CODIGO  ….  CODIGO  n  2   Integración de UML y DSL en el enfoque MDA  Este trabajo se enfoca en utilizar dos lenguajes de modelado (UML-DSL)  aprovechando las ventajas de ambos aplicándolos a distintas etapas del modelado.   • UML  - Lenguaje de propósito general  • DSL  -  Lenguaje de propósito específico  Los lenguajes de dominio específico, como por ejemplo DSL, son una alternativa a  UML para modelar aplicaciones. A diferencia de UML no tienen estructuras  generales sino que para modelar cada tipo de aplicación se debe definir un DSL  específico con las entidades que se necesiten modelar. Esto hace que el lenguaje sea  más acotado y específico.   UML al ser de propósito general se vale de estereotipos y profiles para poder  adaptarse lo más posible a dominios específicos mientras que un DSL nace  específicamente para dicho dominio. Los DSL al ser más acotados son más propicios  para la generación de código.   Es posible utilizar UML para modelar la aplicación de forma genérica (CIM) e  independiente de la plataforma (PIM) y tener un DSL de más bajo nivel ya  dependiente de la plataforma (PSM) que permita de forma más sencilla la generación  de código. Por lo tanto se podrían tener varios PSM modelados en DSL para cada  plataforma sobre la cual se desee generar código (ver figura 2).                              ""DSL eleva el nivel de abstracción más allá de los lenguajes de programación  actuales a través de la especificación de la solución, utilizando directamente  conceptos de dominio del problema. El código fuente es generado desde este nivel de  especificación. Esta automatización es posible porque, ambos el lenguaje y  generadores se ajustan los requerimientos de una sola compañía y un dominio""[2]  Para ello se persiguen los siguientes objetivos:   1. Modelar una aplicación bajo el esquema de MDA (Model Driven  Architecture);   2. Utilizar UML para modelar el CIM y el PIM;   3. Utilizar DSL para modelar el PSM  4.  Desarrollar una herramienta que permita generar automáticamente código  a partir de cada PSM construido.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 516 La metodología está compuesta por cuatro etapas:  1. Generar el CIM: Para este modelo se ha elegido el diagrama de casos de  usos de UML ya que permite ver un bosquejo general de la aplicación  y  los principales requerimientos del sistema.  2. Generar el PIM: Se realizará por cada uno de los casos de usos del CIM  un diagrama de actividades en el cual se mostrará la funcionalidad interna.  Solo se tomarán aquellos casos de uso que realice en forma directa el  usuario ya que para los que dependen de otros, su funcionalidad va a estar  incluida en diagrama de actividades correspondiente.  Estos dos primeros pasos se realizan mediante diagramas propios de UML  tal como se indicaba en la figura 2.  3. Generar el PSM: Para cada uno de los diagramas de actividades se  realiza un DSL enfocado a la plataforma en la cual se quiere desarrollar la  aplicación.  4. Generar el Código Fuente: Cada una de las construcciones de DSL  genera una porción de código especifico a la plataforma elegida, de esta  forma al estar las construcciones relacionadas entre sí, generarán un  código fuente bastante rico, el cual reducirá el trabajo de programación  para obtener el producto final.  3   Modelado de una aplicación  3.1 Plataforma de desarrollo  Como plataforma de la aplicación se ha optado por generar páginas web enfocadas a  teléfonos celulares. Estas páginas se basan en un lenguaje denominado WML  (wireless markup language) que está diseñado especialmente para dispositivos  pequeños, con memoria y capacidades limitadas de procesamiento. Este lenguaje fue  diseñado con el objetivo de ser liviano para transferir poca información por la red de  datos reduciendo tiempos de espera y costos.  Similar a HTML, WML dispone de una serie de tags que son interpretados por el  browser del cliente para mostrar la información.  A continuación se detallan algunos de los tags de WML, especialmente aquellos  que luego serán incorporados al modelo DSL que se construye en el ítem 3.2.  • Páginas: son los archivos físicos .wml dentro de los cuales tienen un header que  identifica al tipo de documento WML para que sea correctamente interpretado  por los browser. El contenido de las páginas debe ser colocado dentro de los tags  <wml> </wml>  • Cards (tarjetas): representan el conjunto de datos que serán mostrados en la  pantalla del teléfono al mismo tiempo. Una misma página puede contener más de  una tarjeta y navegar entre ellas sin necesidad de enviar información por la red,  simplemente cambiando la vista en el browser.  • Listas de Selección: son listas de opciones que puede seleccionar el usuario. Al  momento de seleccionar una opción se dispara un evento asociado y una variable  guarda el valor de la opción seleccionada. Para definir una lista de selección se  usa el tag <select> y para cada una de las opciones el tag <option>.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 517 • Controles de ingreso de datos: WML soporta el ingreso de caracteres  alfanuméricos mediante el tag <input>, el contenido de este control es ingresado  por el usuario y es almacenado en una variable relacionada con el control cuyo  nombre se define con el atributo name.  • Acciones: las acciones se representan entre los tags <do></do> Los atributos más  importantes que contiene la etiqueta <do> son: type="""",label="""" y name="""".  1. type="""". indica sobre qué botón del navegador se aplica la acción, las más  comunes son ""accept"",""prev"" y ""help"".  2. label="""". Texto que aparece asociado a la acción.  3. name="""". Nombre de la acción, es imprescindible si vamos a asignar más de  una acción a un mismo tipo.  El contenido de la etiqueta <do> es la tarea que se realizará al seleccionar esa  acción, y estas tareas pueden ser:  1. <go href=""""/> Permite ir a la dirección indicada en el atributo href.  2. <prev/> Permite ir a la tarjeta anterior, en la historia del navegador.  3. <noop/> Es una acción que no realiza nada.  4. <refresh>...</refresh> Refresca el contenido de la tarjeta actual, volviéndola  a pedir al servidor.  Para mayor información sobre WAP se recomienda consultar [12] y [10]    3.2   Modelado   A continuación se describe el problema a modelar. Se trata de un sistema WAP  destinado a reparto domiciliario (por ejemplo una empresa de agua mineral que  distribuye además otros artículos). Cada empleado tiene la ruta que realizará en un  determinado día precargada en su dispositivo móvil. Para simplificar el ejemplo se  modelarán las siguientes funcionalidades: Visualización de la ruta: El sistema permite  visualizar la ruta en la cual están señaladas las distintas paradas necesarias para visitar  a cada cliente; Toma de pedido: En cada domicilio un cliente realiza su pedido, para  lo cual es necesario consultar la disponibilidad de dichos productos en el camión.  1. Generar el CIM: En la figura 3 se muestra el modelado por casos de uso.                      Fig. 3. Modelado UML – Casos de Uso    2. Generar el PIM: Para cada caso de uso que posea el sistema se realiza un  diagrama de actividades. En el caso de esta aplicación hay un caso de uso  principal “Visualización de ruta” del cual se extiende “Toma de Pedido” caso que  utiliza “Chequear Stock disponible”. Por ello se realiza un solo diagrama de  actividades planteado para el caso de uso principal lo que desencadenará en el  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 518 mismo diagrama la necesidad de incluir el modelado de las funcionalidades de  los otros dos casos de uso  (ver figura 4).                                        Fig.4. Diagrama de Actividades de un caso de uso  3. Generar el PSM: Para el PSM se debe definir el DSL específico para la  plataforma en la que queremos implementar la aplicación. El DSL construido  (ver figura 5) contiene los objetos presentados en la tabla 1 que están directamente  relacionados con los componentes de WML explicados en el ítem 3.1.  Tabla 1. Objetos de DSL  Página    Representa un archivo físico que representa a la pagina wml    Tarjeta    Representa los cards de wml    ListaSelección    Permite definir una lista de opciones para que el usuario pueda elegir  una de ellas. Este objeto tendrá, aparte del título, tres atributos:   • La función que recupera los datos de la lista   • El nombre de la variable interna donde almacenará la opción  seleccionada   • El evento que se disparará al seleccionar una opción.    Input  Permite definir un ingreso de datos alfanumérico. Para ello se informa  el texto mostrado al usuario y el nombre de la variable interna que  contendrá el dato ingresado.   Acción  Se relacionan con el tag DO de WML, los cuales representan la  interacción del usuario. Para las acciones se especifica el texto a  mostrar y una URL que permite generar la solicitud sobre el servidor  WEB ejemplo: www.mipage.aspx?par=confirm.  Descripcion: GoAction=urlpage?par =action   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 519                                                                             Fig.5. Modelo DSL  4. Generar el código fuente: En base a los elementos que posee el modelado de DSL  mostrado en la figura 4 se puede generar automáticamente código para cada uno  de ellos, en este ejemplo el modelo cuenta con 5 elementos distintos presentados  previamente en la tabla 1. En la figura 6 se muestra una porción de código  generada automáticamente para cada uno de estos elementos.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 520                                                                                                 Automáticamente se crea en el servidor una función que permite generar  dinámicamente las listas de selección. Se muestra a continuación el código generado:    <do type =""accept"" name=""doNombre""  label=""Descripcion"">     <go href=”urlpage?par=action""/>    </do>  <p>  <input type=""text"" name=""VarTextBoxId""  title=""Descripción de Texto""/>  </p>  <card id=""TarjetaID""  title=""Titulo de tarjeta"">      </card>   1  2  2  2  4  5  <#GenerateSelectOptionToOtherCard(“ID”,ObtenerDatosSelect(),strCardId)#>  <?xml version=""1.0""?>  <!DOCTYPE wml PUBLIC ""//WAPFORUM//DTD WML 1.2//EN"">  <wml>       </wml>  1  3  Fig.6. Generación automática de código  <#GenerateSelectOptionToOtherPage(“ID”,ObtenerDatosSelect(),PickURLAction)#>  3  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 521 Public Function GenerateSelectOptionToOtherPage(ByVal Id  As  string,byref tblSource as DataTable, byval URLPick as string) As  String          Dim strSelect As String = ""<select>""          For Each dtRow As DataRow In tblWapFormatedTable.Rows              strSelect+= ""<option value=’” + dtRow[Id] + ”’  onPick=’” + urlPick + “?” + Id + “=” + dtRow[Id] + ”’>"" +  dtRow(""Description"") + + ""</option>""          Next          strSelect += ""</select>""          Return strSelect   End Function  5  Conclusiones   En el presente artículo se plantea que UML y DSL no son  excluyentes sino que  pueden complementarse y aplicarse al enfoque de MDA, utilizándose UML en un  nivel mayor de abstracción y DSL en un nivel más cercano a la implementación.  OMG propone el enfoque MDA y a su vez también el lenguaje UML. Microsoft  lanzó DSL como respuesta al modelado de dominios específicos. Quienes utilizaban  el enfoque MDA por añadidura utilizaban UML. Sin embargo cuando se intenta  adaptar UML a dominios específicos en algunos casos sólo se toma como base una  pequeña parte de UML, resultando necesario agregar muchos elementos nuevos con  semántica propia haciendo que UML pierda su gran ventaja que es la estandarización.  Para reducir esfuerzos de adaptación resulta conveniente usar DSL, ya que nace  explícitamente para ser aplicado a un domino en particular pero con la desventaja de  no tener un estándar en símbolos y diagramas de modelado.  Por lo tanto en  los  modelos de alto nivel cuando se quiere tener un diagrama que sirva para comunicar  las funcionalidades del sistema mediante un estándar que pueda ser fácilmente  entendido por las distintas partes resulta conveniente usar UML pero al llevar el  modelo a bajo nivel conviene utilizar un lenguaje adaptable al dominio en particular  del cual fácilmente pueden aplicarse transformaciones para llegar al código fuente en  forma automática derivándolo de los modelos.   Era inevitable comenzar a considerar las ventajas de ambos lenguajes y que estos  pudieran complementarse y no verse como alternativas independientes. En algunos  papers (por ejemplo [9] [11]) se presenta una aplicación de DSL en el modelo MDA  para evitar extender UML a un dominio particular. En este trabajo se presenta el  modelado de una aplicación que se toma a modo de ejemplo simplemente a fin de  mostrar que ambos lenguajes pueden coexistir y ser aplicados al enfoque MDA."	﻿MDA , UML , DSL , WAP	es	19201
13	Evaluación de una variante de control de acceso al medio inalámbrico para tiempo real basada en 802 11e	﻿ Las tecnologías inalámbricas son una buena elección para trabajar en  ambientes industriales, donde es necesario interconectar sistemas móviles o  bien se desea evitar el cableado de sensores y controladores en planta. Sin  embargo estas tecnologías presentan problemas de confiabilidad y temporizado  inherentes a las características de los canales de radio, a los mecanismos de  acceso al medio, etc. El estándar 802.11e provee dos alternativas de acceso al  medio (EDCA y HCCA) con cuatro niveles de Calidad de Servicio (QoS)  diferenciados. Este trabajo propone un mecanismo de control de acceso al  medio, denominado WRTMAC (Control de Acceso al Medio Inalámbrico para  Tiempo Real), desarrollado a partir del esquema EDCA del estándar 802.11e,  optimizado empleando clases de prioridades. El manejo de los espacios entre  tramas para arbitraje (AIFS) fue modificado a fin de que el tiempo de respuesta  de la red sea predecible. Esto provee un mecanismo de control de acceso al  medio (MAC) libre de colisiones en redes inalámbricas. Se presenta además  una comparativa de WRTMAC con respecto a EDCA, simulado en ns-2.  Keywords: WRTMAC, LAN inalámbrica, Prioridades, RIFS, EDCA,  Determinístico, ns-2.  Abstract. Wireless technologies are a good choice for work in industrial  environments, where it is necessary to interconnect mobile systems or it is  wanted to avoid sensors and controllers wiring. However, these technologies  present reliability and timing problems inherent to radio channels, medium  access mechanisms, etc. The standard 802.11e provides two types of medium  access (EDCA and HCCA) by differentiating traffic into four Access  Categories (ACs). This paper proposes a mechanism for medium access control,  so-called WRTMAC (Wireless Real Time Medium Access Control), developed  from the EDCA scheme of 802.11e and an optimization method using priority  classes. The handling of the arbitration inter frame spaces (AIFS) has been  modified in order to make deterministic the medium access. This provides a  free-collision MAC mechanism in a wireless environment. Also, it is presented  a comparative between WRTMAC and EDCA, simulated in ns-2.  Keywords: WRTMAC, Wireless LAN, Priorities, RIFS, EDCA, Deterministic,  n-s2.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 774 1   Introducción  La reducción de tiempo y costos de instalación, mantenimiento y modificación del  cableado, evitando el impacto de ambientes agresivos sobre cables y conectores son  algunos de los beneficios de emplear tecnología inalámbrica en un ambiente  industrial. Las aplicaciones de control industrial que involucran sistemas móviles [1],  también se benefician por esta tecnología. Sin embargo, los medios inalámbricos  presentan desafíos como lo son los problemas de un canal de radio frecuencia (RF), la  movilidad de algunas estaciones, la incertidumbre en el tiempo de acceso al medio de  ciertos protocolos, etc.  Si bien existen diversas opciones de conectividad inalámbrica, éste trabajo se ha  desarrollado en base a las redes inalámbricas de área local (WLAN) basadas en el  estándar IEEE 802.11e.   El protocolo de Control de Acceso al Medio (MAC) es decisivo en el rendimiento  de una red [2]. El mecanismo MAC de 802.11e [3] emplea el mecanismo CSMA/CA  (“Carrier Sense Multiple Access with Collision-Avoidance”) para lograr el acceso al  medio, soportando calidad de servicio (QoS) para diferenciar distintos tipos de tráfico.  Para ello propone dos mecanismos de QoS: Acceso al Canal en Forma Distribuida  Mejorada (EDCA) y Acceso al Canal Controlado por Función de Coordinación  Híbrida (HCCA). EDCA [4], diferencia cuatro Categorías de Acceso (AC) priorizadas  [5]. EDCA mejora la tasa de transferencia y el tiempo de respuesta con respecto a la  norma 802.11 original, aunque la reducida cantidad de AC limita la diferenciación de  tráfico con restricciones temporales [6].   El presente trabajo, basado en los conceptos de EDCA definidos por 802.11e,  propone usar tantas AC como nodos y/o mensajes existan en la red [7], a fin de lograr  un tiempo de acceso al medio predecible. El método propuesto ha sido denominado  WRTMAC: “Control de Acceso al Medio Inalámbrico para Tiempo Real”.  2   EDCA en WLAN 802.11e  EDCA es un esquema distribuido de control de acceso al medio, basado en  CSMA/CA. El estándar 802.11e introduce el modo EDCA  (Fig. 1), que propone un  mecanismo diferenciado de QoS con cuatro AC: AC_BK (Background) para niveles  de prioridad más bajos (1 y 2), AC_BE (Best Effort) para los siguientes (0 y 3),  AC_VI (Video) para las prioridades 4 y 5 y AC_VO (Voice) para las más altas (6 y  7). De acuerdo a su prioridad, una trama será ubicada en una de esas cuatro categorías  (ACi). EDCA puede operar en dos modos: uno consiste en transmitir la trama de datos  cuando se obtiene el acceso al medio; en el otro, previo al envío de datos se  intercambian tramas RTS/CTS, a fin de evitar colisiones con nodos ocultos [8]. El  presente trabajo está basado en la primera.  Como es difícil detectar colisiones en un medio inalámbrico, se controla el acceso  al canal mediante Espacios de Tiempo entre Tramas (IFS). Cuando una estación  detecta el medio libre, debe esperar durante un tiempo IFS Distribuido (DIFS)  posterior a la transmisión anterior (Fig. 1). Luego, dependiendo de la categoría a la  que pertenece la trama, debe esperar un tiempo específico de IFS de arbitraje (AIFS).   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 775 Después de sensar el medio libre durante un tiempo AIFSi, la estación debe esperar  durante la ventana de retroceso (BW: backoff-window), cuya duración es una  cantidad aleatoria de ranuras de tiempo (ST: slot–time), entre cero y un máximo igual  a CW–1. CW es la ventana de contención, que tiene una duración mínima CWmín, y se  va duplicando luego de cada colisión, hasta un máximo CWmáx. Si el contador BW  llega a cero estando el medio libre, comienza la transmisión. Si otro ocupó el medio  antes de que BW llegue a cero, se suspende la cuenta hasta que el medio vuelva a  estar libre durante DIFS. Si BW llega a cero en dos o más nodos al mismo tiempo, se  producirá una colisión. Luego de un tiempo IFS Corto (SIFS) posterior a la correcta  recepción de una trama, la estación receptora envía ACK (Fig. 1). Si la estación  transmisora no recibe ACK, asume que pudo haber una colisión y es necesario  retransmitir. La ocurrencia de colisiones causa indeterminación en el tiempo  requerido para concretar una transmisión.  Cada AC tiene valores específicos de AIFSi, CWmín y CWmáx [3] [9].    Fig. 1. Esquema básico EDCA  A mayor prioridad, AIFS es menor, aumentando la probabilidad de acceder al  canal. Debido a que en una misma AC pueden coexistir tramas de distintos nodos, no  se descarta la ocurrencia de colisiones.  En el presente trabajo se propone un esquema de acceso al medio libre de  colisiones, que garantice el tiempo de respuesta (definido como el tiempo transcurrido  entre el pedido de transmisión y la recepción del ACK). Se establecen tantas AC  como tipos de mensajes estén previstos en la red, se elimina la ventana de contención  (CW) y se le asigna a cada tipo de mensaje un AIFSi distinto. El tiempo de espera  previo a una transmisión es igual a DIFS más el AIFSi correspondiente.  3   WRTMAC: una variante para WLAN 802.11e en Tiempo Real  El objetivo básico propuesto para WRTMAC es asegurar la latencia máxima para la  transmisión de una trama, por lo tanto será necesario quitar los elementos  probabilísticos de EDCA. WRTMAC introduce variantes sobre EDCA para permitir  alcanzar el comportamiento determinístico buscado. Se han establecido las siguientes  pautas:  • Todas las estaciones son capaces de escucharse entre sí (no hay nodos ocultos).  • La lógica del mecanismo MAC debe evitar la ocurrencia de colisiones.  • Se considera un ambiente libre de ruido.   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 776 • A cada tipo de trama le corresponde una determinada prioridad, conocida desde el  instante inicial y distinta a cualquier otra (a la manera del bus CAN) [10].  • La prioridad es un valor numérico comprendido entre cero para la máxima y un  cierto número positivo N para la mínima. La cantidad total de prioridades es  función del número de tipos de mensajes que se van a manejar en el contexto de la  aplicación. Este esquema no admite otras estaciones que no sean del tipo  WRTMAC.  • Habiendo dos o más requerimientos simultáneos, siempre se debe transmitir la  trama de mayor prioridad.  La Fig. 2 presenta los aspectos básicos de WRTMAC. Cuando una estación tiene  una trama para enviar, espera que el medio permanezca inactivo durante un cierto  tiempo denominado “Espacio Entre Tramas de Tiempo Real” (RIFS: Real-Time InterFrame Spacing) y luego inicia la transmisión. Si durante la espera el medio es  ocupado, la misma se aborta y deberá reiniciarse cuando el medio vuelva a quedar  ocioso.    Fig. 2. Esquema básico de WRTMAC  El concepto central de WRTMAC es que cada mensaje tiene asociado un RIFS  constante y diferente al de cualquier otro. Su duración es inversamente proporcional a  la prioridad que representa. Se denomina RIFSi al tiempo de espera (backoff)  correspondiente al mensaje de prioridad i.  El uso de distintos tiempos de arbitraje RIFSi evita la ocurrencia de colisiones y  brinda determinismo, al asegurar que en caso de competencia, el acceso al medio sea  obtenido por el mensaje de mayor prioridad.  En la Fig. 3 se muestra el ordenamiento de tres tramas de prioridades 2, 3 y 4, que  compiten por el acceso al medio. Los tres nodos inician el procedimiento de espera,  pero como RIFS2 tiene la menor duración se abortan los intentos de Trama3 y Trama4.  Éstos se reinician luego de finalizado el ciclo de Trama2.  La duración de RIFSi se calcula en función de los valores de DIFS y ST fijados por  la norma correspondiente a la capa física (PHY), según la siguiente fórmula:  STiDIFSRIFSi ∗+=  (1)  Los valores de SIFS, DIFS y ST dependen de la variante de capa física utilizada.  Por ejemplo: para 802,11b (11 Mbps), SIFS = 10 µs , DIFS = 50 µs y ST= 20 µs.  Se denomina Ci al ciclo de una transmisión de prioridad i, compuesto por RIFSi,  SIFS y los tiempos de transmisión de las tramas i (tTRAMAi) y ACK (tACK):  ACKTRAMAiii tSIFStRIFSC +++=  (2)    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 777   Fig. 3. Ordenamiento de transmisiones según la prioridad de los mensajes  El ACK tiene la finalidad de indicarle a la entidad MAC del transmisor que la  trama enviada llegó a destino. En general, en caso de no recibirse el ACK no se  efectúa una retransmisión, sino que se notifica a las capas superiores que la  transmisión ha fallado. Estas deben tomar la decisión respecto a que acciones  corresponde ejecutar, ya que conocen la lógica y las restricciones temporales de la  aplicación. WRTMAC sólo se encarga de brindar un servicio de comunicación  determinístico en cuanto a la latencia máxima.  Se puede observar que WRTMAC permite implementar un esquema de tiempo real  del tipo Prioridades Monotónicas Crecientes (PMC) [11], asignando prioridades a los  mensajes en orden inverso a sus períodos. Conociendo tTRAMAi para todos los mensajes  de un determinado sistema de tiempo real y asumiendo que los mismos son  periódicos, se puede establecer el mínimo período posible entre requerimientos de  transmisión (Ti) para un dado mensaje mi, en función de todos los demás mj de mayor  prioridad que mi (siendo j<i). Adaptando la clásica fórmula que se utiliza para  analizar la diagramabilidad de un conjunto de tareas periódicas de tiempo real sobre  un procesador [12], el mínimo período posible para un mensaje de prioridad i es:  i ij j j i i CCT T T + ⎥⎥⎥ ⎤ ⎢⎢⎢ ⎡≥ ∑ <∀  (3)  Donde: Ti , Tj : Período de mensajes de prioridad i y j.     Ci , Cj : Duración del ciclo de transmisión de mensajes de prioridad i y j.    El ejemplo de la Fig. 3 corresponde a mensajes con períodos T2 ≤ T3 ≤ T4.  Cabe destacar que el fin de una transmisión es el evento usado por cada nodo para  reiniciar el temporizado y mantener el sincronismo. Por lo tanto, a fin de evitar  prolongados intervalos de silencio, el nodo que tenga asignada la transmisión de la  trama de menor prioridad (RIFSN) siempre debe efectuar una transmisión. Si al  finalizar su temporizador RIFSN no tiene un requerimiento pendiente, de todos modos  deberá enviar una trama vacía (“dummy”), a fin de ocupar el medio y permitir que  todos los nodos se sincronicen con el fin de la misma.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 778 Cada nodo debe mantener el temporizado de la actividad en el medio, incluso  cuando no tenga requerimientos de transmisión, porque hasta un instante previo al  vencimiento de su RIFS puede recibir uno y despacharlo en el ciclo actual.  3.1 Inversión de prioridad  Se denomina inversión de prioridad a la situación en que la transmisión de una trama  debe esperar hasta la finalización de otra de menor prioridad. La Fig. 4 muestra el  requerimiento casi simultáneo de Trama2 y Trama3. Como el requerimiento de  Trama2 se produjo un instante después de vencido RIFS2, su transmisión debe esperar  al próximo ciclo.    Fig. 4. Ejemplo de inversión de prioridad. Trama3 es transmitida antes que Trama2  Como el requerimiento de Trama3 llegó antes del vencimiento de RIFS3, la misma  es transmitida y completa su ciclo C3. De esta forma, Trama2 estuvo bloqueada  durante un tiempo B2, cuyo valor máximo es B2 = C3 – RIFS2. Considerando todas las  tramas de prioridad inferior a 2, el bloqueo máximo de Trama2 será: B2 = máx(Cj) –  RIFS2, para cualquier j > 2.  En general, para cualquier trama de prioridad i, el tiempo de bloqueo por inversión  de prioridad es:  ijRIFSCmáxB iji >∀−= )(  (5)  Por lo tanto, la fórmula (3) se extiende de la siguiente manera:  ii ij j j i i BCCT T T ++ ⎥⎥⎥ ⎤ ⎢⎢⎢ ⎡≥ ∑ <∀  (6)  3.2 Clases de prioridades  El rendimiento de WRTMAC tiende a degradarse cuando aumenta el número de  mensajes, porque cada mensaje adicionado incrementa el valor de RIFSN en un ST.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 779 Aunque el modelo propuesto se basa en el uso de prioridades diferentes para  arbitrar el acceso al medio, bajo ciertas condiciones sería factible que dos o más  mensajes usen un mismo RIFS, siempre que ello no provoque colisiones.  La idea es que, como los mensajes originados en un mismo nodo no pueden  colisionar entre si, se agrupen dos o más mensajes en una misma AC.   En el caso más favorable bastaría un único RIFS por nodo, si esto fuera suficiente  para satisfacer las restricciones de tiempo real. Por lo general, cada nodo usará una  cierta cantidad de RIFS, cada uno representando una “clase de prioridad”, según el  concepto de diagramabilidad de tareas con número limitado de prioridades [13] [14].  Por lo tanto, se utilizarían diferentes RIFS para distinguir prioridades de acceso  entre clases, mientras que para planificar las transmisiones dentro de la clase se podría  emplear una estrategia basada en PMC o bien Rueda Cíclica (RC) (“Round Robin”).  Las fórmulas definidas para analizar el modelo básico, se deben adaptar al nuevo  esquema de operación. Primero se debería definir la clase de acceso a la que pertenece  la trama i-ésima, AC(i), cuyos rango de valores va desde cero (prioridad más alta)  hasta N (la más baja). Luego el tiempo de retroceso (backoff) para una trama que  pertenece a la clase k-ésima, será:  STkDIFSRIFSk ∗+=  (7)  La fórmula (7), para la i-ésima trama, se puede expresar de una manera más  conveniente como:  STiACDIFSRIFS iAC ∗+= )()(  (8)  Un ejemplo puede ayudar a entender mejor la expresión (8). Suponiendo que hay  seis tramas a transmitir: F0, F1, ..., F5, con períodos T0 < T1 <....< T5, y se dispone de  tres nodos N0, N1 y N2. Si los requerimientos de tiempo real se pueden resolver con  tres ACs (RIFS0 < RIFS1 < RIFS2), la solución podría ser:    AC (0) = AC (5) = 0 (RIFS0) AC (1) = AC (4) = 1 (RIFS1) AC (2) = AC (3) = 2 (RIFS2)    Como se puede observar, F1 puede ser bloqueada F5, ya que AC(5) < AC(1), sin  embargo esto se puede aceptar si se satisfacen las restricciones de tiempo. De otra  forma, será necesario agregar más clases de prioridad.  Reemplazando el tiempo de “backoff” de (8) en (2), el tiempo de transmisión de la  i-ésima trama es:  ACKFRAMEiiACi tSIFStRIFSC +++= )(  (9)  Para determinar el período más corto posible para la i-ésima trama, se deben  computar las transmisiones de las clases de mayor prioridad que AC(i) y otras  transmisiones de la propia clase. La planificación dentro de la clase podría estar  basada en PMC o RC. En este trabajo solo se desarrollará la alternativa PMC.  Para PMC dentro de la clase, el mínimo período factible para la i-ésima trama será:  ii iACjAC ij j j i iACjAC j j j i i BCCT T C T T T ++⎥⎥⎥ ⎤ ⎢⎢⎢ ⎡+⎥⎥⎥ ⎤ ⎢⎢⎢ ⎡≥ ∑∑ =<∀<∀ )()( |)()(|   (10)  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 780 En (10), el primer término computa las tramas pertenecientes a las clases de  prioridades más altas que i, mientras que el segundo término considera el tiempo  asignado a todas las tareas de prioridad mayor que i, ordenadas por PMC, dentro de la  clase. El último término tiene en cuenta la inversión de prioridad causada por  mensajes pertenecientes a la propia clase o a clases de menor prioridad.  4.   Evaluación del desempeño  La motivación que ha impulsado el desarrollo de WRTMAC es su aplicación en  sistemas de control industrial, que por lo general utilizan mensajes periódicos de  pequeño tamaño. Por lo tanto, se ha planteado un escenario que permita evaluar las  prestaciones de una red de este tipo, en función de la cantidad de mensajes, su  tamaño, período y la forma de agrupamiento por nodos, con el fin de determinar el  mínimo período de tiempo necesario para el mensaje de prioridad i. Los valores  obtenidos para WRTMAC se compararon para el mismo escenario con dispositivos  EDCA, simulados sobre ns-2 [15].  En este escenario se ha planteado evaluar la red para la transmisión de un  determinado conjunto de mensajes de igual tamaño y período. A los fines del ejemplo  se ha considerado la capa física de 802.11b a 11 Mbps con preámbulo largo (192 µs),  mensajes que transportan 50 bytes (más los 36 bytes de encabezado) y ACK de 14  bytes. Para realizar la evaluación de WRTMAC, se calcularon los períodos mínimos  para mensajes con prioridades diferentes (Fórmula 6) y agrupados en clases de a  cuatro mensajes cada una (Fórmula 10). Los mismos se presentan en Tabla 1 y Fig. 5,  junto con los resultados de las simulaciones para EDCA, a fin de poder compararlos.  Tabla 1. Mínimo período según cantidad y tamaño de mensajes, con prioridades individuales y  agrupados de a cuatro mensajes por clase, para dispositivos WRTMAC y EDCA   Una prioridad por mensaje 4 mensajes por clase  N° de  mensajes  50 bytes  WRTMAC  50 bytes  EDCA  50 bytes  WRTMAC  50 bytes  EDCA  8 5,16 7 4,68 6,56  16 11,13 13,15 9,21 13,84  32 26,92 28,47 19,24 26,24  64 73,86 62,51 43,14 51    En la Tabla 1 se observa una tasa de crecimiento significativa del período mínimo  a medida que la cantidad de mensajes aumenta. En el caso de no utilizar clases, se  puede notar un mejor rendimiento de WRTMAC hasta una cierta cantidad de  mensajes y luego se degrada debido a los RIFS crecientes.  WRTMAC presenta una mejora muy notoria cuando se agrupan los mensajes en  clases. En este ensayo se agruparon de a cuatro mensajes por clase. Por ejemplo para  una red WRTMAC con 64 mensajes de 50 bytes cada uno, el período mínimo  agrupando en clases, se reduce más de 40% respecto al modelo sin clases. Si  comparamos con dispositivos EDCA en condiciones similares, hay una mejora  aproximada del 15%.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 781  Fig. 5. Grafico comparativo entre EDCA y WRTMAC sin y con clase.  Cabe tener en cuenta que a los fines de esta evaluación, se han considerado todos  los mensajes de igual tamaño y período. Con mensajes heterogéneos, será necesario  determinar el agrupamiento más conveniente.   De todas formas, el escenario analizado representa un caso hipotético que nos  permite concluir que resulta necesario establecer una metodología para determinar el  agrupamiento óptimo en función de los requerimientos específicos de cada sistema.  5.   Conclusiones  WRTMAC (Control de Acceso al Medio Inalámbrico para Tiempo Real) es una  propuesta para implementar un mecanismo MAC basado en el esquema EDCA de la  norma 802.11e, que incluye una operación libre de colisiones. Su finalidad es lograr  un mecanismo de acceso al medio inalámbrico distribuido, que asegure un tiempo de  acceso predecible. Para ello se propone modificar el mecanismo EDCA, generando  tantas prioridades (llamadas clases de acceso en EDCA) como dispositivos y/o  mensajes conformen la red, a fin de hacerlo adecuado para aplicaciones industriales  de tiempo real.  Se mostró que WRTMAC sería apto para implementar un esquema de tiempo real,  al poder acotar el mínimo período entre requerimientos de transmisión.  Al evaluar el desempeño sobre patrones de tráficos típicos en redes de aplicación  industrial, se pudo observar que WRTMAC presenta un comportamiento  determinístico de buen rendimiento. Se determinaron las limitaciones que surgen al  aumentar el número de prioridades y también como un agrupamiento de mensajes en  clases de prioridad, permitiría mejorar notablemente su desempeño, reduciendo  mismo tiempo la complejidad.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 782 En futuros trabajos se intentarán desarrollar estrategias para implementar  WRTMAC sobre dispositivos estándar del tipo EDCA.	﻿EDCA , WRTMAC , LAN inalámbrica , prioridades , RIFS , determinístico , ns 2 , wireless LAN , priorities , deterministic	es	19347
14	Diseño de una aplicación web embebida para el control de la carga útil de un satélite geoestacionario	"﻿ Este trabajo presenta una solución real de diseño e implementación de una aplicación de control del payload de un satélite  geoestacionario construida como un ambiente web embebido operando en una plataforma de microcontroladores Rabbit. La misma  permite enviar comandos y leer telemetría de la carga útil durante la integración al modelo de ingeniería y vuelo. Se plantea en primer  lugar el diseño del hardware que provee al sistema de las interfaces adecuadas para llevar a cabo la conectividad entre el payload y el  módulo. El resultado final es un dispositivo capaz de realizar el monitoreo y control, a través de una red Ethernet, de distintas variables  de estado, proveyendo al usuario de una interfaz gráfica que puede ser ejecutada en cualquier terminal remota con conexión a Internet.  Keywords: Carga útil, satélite, web, sistemas embebidos, telecomandos, telemetría, geoestacionario.  Introducción  En el proceso de construcción de un satélite uno de los principales objetivos es lograr comandar y conocer el  estado de distintas variables de la carga útil (payload) durante la integración con los modelos de ingeniería y   vuelo. En el proyecto referido en este trabajo surgió la necesidad de explorar las diversas alternativas que  permitirían implementar conectividad TCP/IP (Transmission Control Protocol/Internet Protocol), proveer el  acceso a los subsistemas mediante un servicio Web y gestionar, en una capa más baja del stack del  protocolo, el hardware subyacente de la plataforma por medio de interfaces estrictamente definidas de la  carga útil. El módulo encargado de comandar el payload es la unidad de interfaz de distribución de carga útil  (PIDU por su nombre en inglés Payload Interface Distribution Unit), en este diseño se busca sustituir el  PIDU de vuelo en la etapa de integración del payload al modelo de ingeniería del satélite.  En tecnología satelital se crea con éste propósito un equipo eléctrico de soporte en tierra (EGSE por sus  siglas en inglés Electrical Ground Support Equipment) el cual sirve de apoyo durante la fase de integración  y ensayo de cada uno de los subsistemas del satélite, tanto en el modelo de ingeniería, como en el modelo de  vuelo. En este trabajo se describe el diseño de un equipo de ésta índole que sirve de apoyo en la fase de  integración del payload de un satélite geoestacionario el que  ha sido especificado para trabajar en banda Ku  con canales de 36MHz y 72MHz de ancho de banda, como así también con un cierto número de  entradas/salidas digitales y analógicas. Uno de los desafíos que debe resolver exitosamente el diseño  propuesto es la implementación de la solución mediante herramientas estandar y que permita entregar desde  una plataforma embebida una GUI (GUI por las siglas en inglés Graphical User Interface) flexible para su  operación. Los alcances del proyecto requieren que este deba ser realizado bajo estrictos y restrictivos  requerimientos de costo, calendarios y calidad claramente especificados.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 980     Desarrollo del módulo y aplicación embebida  En base a las interfaces del PIDU y a las necesidades de los usuarios en la fase de integración, es que se  desprenden los requerimientos de la plataforma embebida, principalmente en lo que respecta al hardware de  la misma. Para el desarrollo se eligió una plataforma RCM3365  [R01] basada en microprocesadores  Rabbit[R03], la que cumple con los requisitos de cantidad de entradas/salidas, conectividad Ethernet, stack  TCP/IP integrado y prestaciones de espacio en memoria adecuadas para este desarrollo.  Una vez seleccionada la plataforma, se desarrollaron los restantes componentes de hardware y software,  actividades que involucraron:  • Diseño del la plataforma de hardware de acuerdo con los requerimientos de interfaces del fabricante  del payload.  • Diseño y fabricación del circuito impreso, listados de materiales y documentación del desarrollo.  • Poblado de la placa (630 componentes) y soldadura por olas y horno de termo-fusión.  • Diseño e implementación de la aplicación embebida en la plataforma seleccionada (RCM3365)  mediante Dynamic C para el envío de telecomandos y recepción de telemetría a través de la  plataforma de hardware con el payload.  • Implementación de un servicio Web donde la información a desplegar se genere utilizando el  formato HTML y se despliegue utilizando RabbitWeb[R02].  • Definición del protocolo de comunicación para una aplicación remota, usando TCP/IP.  Definición de interfaces y requerimientos de diseño  De acuerdo con las especificaciones de la misión serán necesarios los siguientes tipos de interfaces:    • Low Level Commands (LLC) – Envío de comandos digitales   •  Digital Relay (DR) –   Lectura de Telemetría digital   • Analog Ouput (AN) – Lectura de Telemetría analógica  • Temperature Sensing (PT) – Lectura de telemetría analógica  Estrategia de diseño  En la Tabla 1 se especifican las interfaces LLC, para el caso de DR se corresponden entradas digitales TTL y  el rango de las lecturas analógicas es de 0 a 5V.    Tabla 1 Especificaciones de interfaces LLC  Teniendo en cuenta los requerimientos de diseño y las especificaciones del payload la plataforma de  hardware debe contar con 16 salidas LLC, 6 entradas DR, 2 AN y 2 PT. El envío de comandos es vía  interfaces LLC y la lectura de telemetría discreta mediante interfaces DR, por lo tanto asociamos  telecomandos (TC) con LLC y telemetría (TM) con DR.    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 981 Diseño de una aplicación Web embebida para el control de la carga útil de un satélite geoestacionario           Figura 1 Diagrama de arquitectura general de interfaces del PIDU  La arquitectura de interfaces es la descripta en la Figura 1 donde se puede apreciar un diagrama del PIDU,  para el cual, la plataforma embebida debe conectarse como un usuario del payload.  A partir del diseño de alto nivel del satélite se define el entorno de integración así la configuración base de  los EGSE. La arquitectura seleccionada utiliza armarios de 20” resistentes a las vibraciones conteniendo una  PC industrial, una pantalla de 17” y de la electrónica adicional involucrada en cada subsistema a ser  integrado. El software que corre en la PC tiene como propósito controlar la electrónica del EGSE para que se  vincule con cada subsistema. Surge entonces la necesidad de contar con una GUI  con la que puedan  configurarse parámetros establecidos durante la etapa de integración y ensayo.  El enfoque convencional es implementar este diseño mediante una FPGA, pero para satisfacer los  requerimientos de conectividad con la plataforma de simulación (SIMPLAT por sus siglas en inglés  Simulation Platform) es mandatorio disponer de un stack Ethernet en 10Mbps.  Como estrategia de diseño todos los módulos EGSE se comunicarán con el SIMPLAT por medio de TCP/IP,  con lo cual es evidente la importancia de seleccionar una plataforma que cuente con soporte para este  protocolo. Una posible solución explorada en el diseño explicado en este trabajo es recurrir a una interfaz  basada en Web donde la información se codifique en HTML. Se aprecia la flexibilidad de configuración de  la misma y por la facilidad de acceso remoto desde cualquier PC conectada al ambiente de desarrollo del  satélite.  En este marco, considerando restricciones en los tiempos del proyecto, disponibilidad de herramientas de  desarrollo y costos, la elección de un microprocesador de la familia Rabbit proporciona la flexibilidad  necesaria puesto que esta provee capacidad de conexión TCP/IP, programación en una versión propietaria de  C (Dynamic C) y abundantes recursos de desarrollo.   El diseño de interfaces con el payload, LLC, DR, AN y PT es, por tratarse de un satélite geoestacionario,  propietario de la empresa proveedora de la carga útil. Está implementado con interfaces muy robustas y  resistentes a fallas. Esos mismos atributos por otra parte hacen que sea imposible encontrar en el mercado  hardware de control que se adapte para los requerimientos de la misión. Fue  necesario entonces diseñar un  hardware de interfaces a nivel electrónico específico que contemplara al mismo tiempo conectividad con el  usuario y con el SIMPLAT.   Selección de la plataforma embebida RCM3365    CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 982     De acuerdo con las prestaciones de diseño de hardware, y la posibilidad de que el módulo a ser desarrollado  fuera usado en la integración de otros subsistemas del satélite como  potencia, control de actitud, navegación  y otros, se tuvieron en cuenta la disponibilidad de un número importante de entradas/salidas LLC, DR  adicionales para expansión futura.  En el otro extremo las características de comunicación TCP/IP servicio web y la posibilidad de acceso vía  Telnet, requieren cantidades de memoria significativas lo que  resulta clave al momento de elección de la   plataforma de desarrollo. Conjugando espacio en memoria (512K programa, 512K datos), capacidad de  almacenamiento masivo y la cantidad de E/S provista por 52 líneas, se seleccionó para el diseño el  microprocesador  RCM3365 de la familia Rabbit.  La plataforma seleccionada soluciona varios problemas de diseño de hardware al proveer una capa física  Ethernet, tolerancia al régimen de alimentación y memoria incorporada en dispositivo [R03].  Al seleccionar una plataforma embebida basada en RCM3365 se facilita, además, la solución de  restricciones de implementación puesto que el módulo de desarrollo se monta en un zócalo de interfaz. De  esta forma se puede trabajar de manera simultánea a la puesta en marcha del hardware para posteriormente  ser integrado en la plataforma final.  Otro problema resuelto con la elección,  es solucionar la capa física de Ethernet, puesto que el módulo  cuenta con un conector RJ45 y el hardware asociado para operar con este protocolo.  Diseño del Hardware de interfaz  Una vez finalizada la etapa de definición de requerimientos de diseño se pudo avanzar con la elaboración de  los planos esquemáticos-eléctricos, del hardware de interfaz que hace de nexo entre la plataforma embebida  RCM3365 y las interfaces de la carga útil.   El  diseño fue sometido a dos procesos de revisión por estar involucrado  directamente con un subsistema de  un satélite y de manera de realizar la detección de defectos lo más temprano posible en el ciclo de vida del  proyecto. Los procesos de revisión típicos son la revisión preliminar de diseño (PDR por Preliminary Design  Review) y revisión crítica de diseño (CDR por Critical Design Review). Una vez establecida la línea de base  de los requerimientos de diseño, se realiza la PDR con el objetivo de saber si se han comprendido esos  requerimientos y se está en condiciones de avanzar con la fase de diseño de detalle. En este análisis, se  presenta el sistema como un diagrama en bloques.  Finalizada la PDR comienza la etapa de desarrollo y antes de fabricar la placa se realiza la CDR donde se  estudia a nivel de circuitos y en detalle cada una de las interfaces involucradas y los posibles modos de fallo.  En este análisis se presentan planos esquemáticos muy detallados de cada circuito.  Por último se realiza un análisis independiente de modos de falla (FMEA Failure mode and effects analysis).  En caso de detectar riesgos de que el fallo de un componente se propague y dañe el hardware de vuelo, se  identifica la mitigación a realizar mediante estrategias de protección como interfaces opto-acopladas o de  aislamiento galvánico.  Se utilizan en la implementación, prácticas de Ingeniería de Software [C04,P01] consistentes con la complejidad  del desarrollo realizado, la criticidad de los entregables y la necesidad de alcanzar estrictos requerimientos  de calidad.  Diseñado el esquemático se desarrolla el diseño del circuito impreso (PCB) teniendo en cuenta aspectos de     Interferencia y compatibilidad electromagnética  (EMI/EMC) e integridad de las señales entre otros.   CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 983 Diseño de una aplicación Web embebida para el control de la carga útil de un satélite geoestacionario       Diseño e implementación de la aplicación embebida   Para la programación embebida se utilizaron los recursos de desarrollo provistos por el lenguaje Dynamic C  propietaria de la plataforma RCM3365. Este lenguaje es un derivado de C adecuado y mejorado para los  requerimientos de los microprocesadores de la familia Rabbit. El fabricante de los módulos entrega junto con  el kit de desarrollo de la plataforma, el software para programación [R05] y el cable de conexión con la PC  utilizada como ambiente de desarrollo.  Una vez finalizada la puesta en marcha a nivel electrónico de la plataforma de interfaz de hardware se  comienza a trabajar en el componente de software. En primera medida se validan todas las entradas/salidas  digitales, LLC y DR, comprobando que el software sea capaz de comandar y leer todos los puertos del  microprocesador que habían sido asignados a las distintas interfaces del PIDU. Este proceso se simplifica  notablemente aplicando una técnica de “stubs” [C04] (funciones simuladas no operativas) donde pequeños  segmentos de código son aplicados a verificar un aspecto en particular.  A continuación se presenta en el Ejemplo 1, la programación que muestra como se escribe y lee un puerto en  Dynamic C [C01] para un módulo RCM3365.        Los canales de entrada analógicos AN y PT provienen de un conversor analógico-digital (ADC Analog To  Digital Converter) de Analog Devices AD7993AR [C03] que posee una interfaz serial I2C [I01].  El lenguaje de programación Dynamic C incluye una interfaz de desarrollo API [D01] llamada I2C.LIB [R04],  que maneja los aspectos genéricos de la interfaz I2C y proporciona funciones simplificadas [C02] para que el  diseñador utilice el módulo RCM3365 como un Master del BUS según lo mostrado en la Figura 2.    Figura 2 Modulo de arquitectura Rabbit como master de Bus I2C  Los principales elementos del API proporcionado por la librería pueden ser vistos en la Tabla 2. Una vez  corroborado el correcto funcionamiento de todas las entradas/salidas del la plataforma embebida, LLC, DR,  AN y PT, se pone el énfasis en la interfaz con el usuario.  El software Dynamic C provee un entorno de  trabajo, para el cual, una de sus principales ventajas es poder realizar una depuración del programa paso a  paso, esto constituye una herramienta muy importante a la hora de realizar cualquier implementación en una  plataforma embebida de este tipo.    main()  {  BitWrPortI(PDDDR, &PDDDRShadow,0,1);  // switch, input  BitWrPortI(PDDDR, &PDDDRShadow,1,0);  // LED, output  BitWrPortI(PDDR, &PDDRShadow,1,0);   // apaga LED  while(1){   if(!BitRdPortI(PDDR,1))      BitWrPortI(PDDR,&PDDRShadow,0,0); /* Prende el LED DS1 */              else              BitWrPortI(PDDR,&PDDRShadow,1,0); /* Apaga el LED DS1 */              }  }  Ejemplo 1 Lectura de puerto en Dynamic C  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 984                                    Tabla 2 Principales elementos del API de gestión de bus I2C  Implementación de un servicio Web utilizando Rabbit Web  El servidor HTTP es el encargado de resolver las solicitudes GET y POST direccionada a la dirección IP y  puerto donde está escuchando el servidor [H01]; las mismas son respondidas mediante páginas HTML  generadas dinámicamente. De esta forma un usuario puede entre otras facilidades:  • Configurar distintos parámetros de funcionamiento como dirección IP y usuario del dispositivo  • Obtener registros parciales de las muestras de temperatura y valores analógicos de tensión  propiciados ambos por el ADC.  • Obtener valores de telemetría DR del payload.  • Enviar comandos LLC al payload.                                      • Inicialización de los pines  de I2C  i2c_init() // Configura los pines SCL y SDA como salidas de colector abierto y constantes de retardo.  • Envío de la condición de Comienzo  i2c_start_tx() // Inicializa la transmisión mediante el envío de un comienzo (start)  i2c_startw_tx() // Inicializa la transmisión mediante el envío de un Start (S)  // Inserta un delay después del pulso S  • Envío de un Byte de datos  i2c_write_char() // Envía 8 bits al dispositivo esclavo  i2c_wr_wait() // Reintenta escribir una variable char hasta que el esclavo responde  • Escucha de un ACK  i2c_check_ack() // Chequea si un dispositivo esclavo pone un bajo en SCL   • Recepción de un Byte de datos  i2c_read_char() //Lee 8 bits de datos de un dispositivo esclavo  • Envío de un ACK  i2c_send_ack() // Envía una secuencia ACK al esclavo.  i2c_send_nak() // Envía una secuencia NAK  • Envío de una condición de parada  i2c_stop_tx() // Envío de P (STOP) al esclavo  #define TCPCONFIG 0  #define USE_ETHERNET  1  #define MY_IP_ADDRESS   ""192.168.1.54""  #define MY_NETMASK      ""255.255.255.0""  #define MY_GATEWAY  ""192.168.1.1""  #ximport ""index.html""     index_html  #ximport ""rabbit1.gif""    rabbit1_gif  #memmap xmem  #use ""dcrtcp.lib""  #use ""http.lib""  const HttpType http_types[] =  {   { "".html"", ""text/html"", NULL},           // html     { "".gif"", ""image/gif"", NULL} };  const static HttpSpec http_flashspec[] =  { { HTTPSPEC_FILE,  ""/"",              index_html,    NULL, 0, NULL, NULL},     { HTTPSPEC_FILE,  ""/index.html"",    index_html,    NULL, 0, NULL, NULL},     { HTTPSPEC_FILE,  ""/rabbit1.gif"",   rabbit1_gif,   NULL, 0, NULL, NULL} };  main()  {sock_init();   http_init();     while(1){      http_handler(); } }  Ejemplo 2 Implementación de HTML en RCM3365  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 985 Diseño de una aplicación Web embebida para el control de la carga útil de un satélite geoestacionario       En el Ejemplo 2 se presenta de manera simplificada la estructura del software en Dynamic C implementada  para montar un servicio web en un módulo RCM3365.  Para el desarrollo de la página HTML se utilizó el software Macromedia DreamWeaver 8; en realidad  cualquier editor puede ser utilizado con éste propósito puesto que el diseño de las páginas es relativamente  sencillo y el volumen de información desplegada bajo.     Figura 3 Ejemplo de interfaz Web embebida  El principal uso de este servicio es proveer una interfaz de usuario que se capaz vincular las variables leídas  desde el bus I2C del ADC provenientes de los canales AN y PT, y procesar las mismas para que puedan ser  mostradas al usuario en formato de página HTML.   Implementación en RabbitWeb  RabbitWeb es una facilidad que ofrece Dynamic C, con la cual se puede crear una interfaz web para los  dispositivos de la familia Rabbit. La principal ventaja respecto a servidores HTTP convencionales consiste  en la eliminación de la necesidad de utilizar programación CGI [G01]. El resultado obtenido es una página  Web que puede ser accedida desde cualquier PC visible mediante ruteos TCP/IP de manera que permita al  usuario enviar comandos LLC y leer telemetría DR y analógica. En la Figura 3 puede apreciarse el aspecto  final de la página que relaciona al usuario con la electrónica del payload en la etapa de integración del  modelo de ingeniería. Como fuere mencionado anteriormente, la recolección de telemetría y el envío de  telecomandos debe ser llevado a cabo por el SIMPLAT que emula la aviónica del satélite y no su carga útil.  Al realizarlo es necesario implementar un protocolo de comunicación entre éste y la plataforma embebida  que replique la que tendrá luego en la plataforma de ingeniería y de vuelo.  Protocolo de comunicación para una aplicación remota, vía TCP/IP.  El protocolo de comunicación entre la aplicación embebida y el simulador de plataforma se implementa   sobre un socket TCP/IP, en el campo de datos del segmento, donde la plataforma RCM3365 operará como  cliente. El  control de flujo propio de TCP/IP garantiza una entrega de paquetes en orden y libres de errores,  lo cual constituye una ventaja para la integridad de la comunicación. El SIMPLAT inicia la conexión con la  plataforma embebida enviando un paquete “Request” (TM o TC) formado por 4 Bytes (32bits), que incluye:  número de secuencia, un identificador de unidad (Payload Nominal o Redundante) y un campo de datos,  donde se especifica el comando “TC Variable” que el SIMPLAT desea enviar o el valor de telemetría “TM  Variable “que se pretende leer. Para una solicitud de telemetría “TM Request”, la plataforma embebida  devuelve un paquete “TM Response” con el mismo número de secuencia con el cual se originó la petición  y  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 986  el campo “TM Value” con un valor de TM DR, AN o PT según lo requerido, de tratarse de una solicitud de  telecomandos “TC Request”, la plataforma, retorna un paquete “TC Response” (eco), tras haber ejecutado el  comando indicado en los campos “LLC Status” y “TC Variable”. En caso de que la conexión no pueda ser  establecida dentro de 10 intentos se asumirá una condición de error en el SIMPLAT. Las principales ventajas   del protocolo bidireccional implementado son simplicidad y robustez. La estructura de los paquetes de datos  del protocolo de comunicación puede ser observada en la  Tabla 3.        Tabla 3 Estructura de los paquetes de datos del protocolo de comunicación Implementado  Cada 500 mSeg el SIMPLAT establecerá una comunicación con la plataforma embebida realizando el envío  de telecomandos (TC Resquest) y/o pedidos de telemetría (TM Request) que estén determinados en ese ciclo.  Se ha implementado en la memoria de programa del módulo RCM3365 una tabla de asignación de  telecomandos y variables de telemetría, que vincula TC y TM con valores hexadecimales. Estos valores son  enviados en el campo “TM Variable” o “TC Variable” de los paquetes “TM Request” y “TC Request”  implementados para este protocolo de comunicación  En la figura 4 se puede observar el funcionamiento del  protocolo.      Figura 4 Funcionamiento del Protocolo de Comunicación Simplat-Plataforma    Si la plataforma embebida encuentra un error en el sub-campo “TM Variable” o “TC Variable” del campo  de datos de un paquete “Request”, devolverá  un paquete “retry” con número de secuencia  0x00 y el campo  de datos con un valor 0x000000, lo cual indicará al SIMPLAT un pedido de retransmisión. Si la plataforma  no devuelve un  número de secuencia esperado por el SIMPLAT, se generará automáticamente desde el  SIMPLAT un reenvío de la solicitud TM o TC según corresponda. Una vez finalizado el período de  comunicación con la plataforma, la misma estará esperando hasta que comience un nuevo ciclo de   Telecomandos y pedidos de Telemetría que esté determinado por parte del SIMPLAT.  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 987 Diseño de una aplicación Web embebida para el control de la carga útil de un satélite geoestacionario      Validación y Verificación  La validación y verificación (V&V) del diseño requiere consideraciones especiales. A nivel de verificación  se procede a revisar unitariamente cada interfaz en particular, los niveles de tensión, corrientes, tiempos de  transitorios, flancos y tolerancias requeridas por el fabricante del payload las que son estrictas.   Para los ensayos a nivel unitario de recepción de comandos y emisión de telemetría mediante “stubs” que  simularon el payload a ser controlado de acuerdo a la especificación de su fabricante. Posteriormente se  realizó la integración con el payload de manera de verificar la recolección de telemetría DR. Se probaron  100 casos de estados de cambios de variables controladas por un operador y su correcto reflejo en la  información HTML proporcionada por la interfaz Web.  Una estrategia similar se utilizó para verificar unitariamente los comandos LLC controlando con  instrumental apropiado que los parámetros eléctricos y de tiempo estuvieran dentro de especificación al  mismo tiempo que se verifica que las lecturas son correctamente reflejadas en la GUI. Los resultados del test  fueron documentados e incluidos en la documentación del sistema.  La integración con la PC industrial se resuelve también con un criterio de verificación unitaria primero para  luego ser validado con mediante ciclos extendidos de mediciones del SIMPLAT, verificando la conectividad  integrada entre la plataforma embebida bajo desarrollo y el simulador de la plataforma.  La integración final del EGSE con el subsistema ha quedado para una etapa posterior del proyecto y no se ha  completado al momento de formular esta publicación.  Resumen del proyecto  La duración total del proyecto implicó aproximadamente un año de trabajo desde la definición de  requerimientos a nivel sistema, hasta la concepción de un entregable como el descripto en este trabajo.   El enfoque utilizado cumple con todas las etapas de un ciclo de vida de tipo iterativo incluyendo  requerimientos, diseño, construcción, etapas de testing, integración de hardware y software y validación del  EGSE.  El esfuerzo total de desarrollo es del orden de 3 personas-año donde algunas etapas fueron  tercerizadas tales como la fabricación de PCB, poblado de la placa, soldadura por horno y por olas, este  diseño constituye una pieza de relevante importancia en la cadena de tests de integración y ensayo de un  satélite geoestacionario.   Conclusiones  Este trabajo refleja la creación de una plataforma embebida como parte del proyecto de desarrollo de un  satélite geoestacionario. Este tipo de desarrollo apela a la sencillez de un entorno HTML/Web para  implementar la interfaz de usuario. El proceso de desarrollo se sostiene en prácticas de ingeniería de  software establecidas tales como ciclo de requerimientos, diseño por etapas, establecimiento de línea de base  de configuración e inspecciones confirmando la utilidad de estas técnicas en el desarrollo de sistemas  embebidos de esta complejidad. Queda confirmada la hipótesis de las ventajas relativas de la plataforma  elegida respecto a un desarrollo funcionalmente equivalente basado en arquitecturas FPGA. Estas ventajas se  ven en aspectos tales como capacidad de integración de hardware y software como una solución integrada  así la posibilidad de actualización flexible del firmware, lo cual transforma al módulo desarrollado en una  estructura tolerante al cambio de requerimientos de diseño. Las aplicaciones de este módulo en las etapas de  CACIC 2010 - XVI CONGRESO ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN                                                 988     integración y ensayo del satélite geoestacionario son diversas, de acuerdo con los requerimientos de cada  subsistema a ser integrado,  razón por la cual, para cada uno de ellos el software embebido en el módulo  RCM3365 deberá ser actualizado y modificado. Este desarrollo se extenderá entonces hasta fines del 2011,  fecha estipulada para finalizar con la integración del modelo de ingeniería del satélite.   Una ventaja del diseño utilizado fue desarrollar en un entorno de programación  amigable, basado en las  estructuras del lenguaje C, lo cual permitió abordar la tecnología con una curva de aprendizaje modesta  comparada a la que hubiera sido necesaria para adquirir los conocimientos de VHDL para haber podido  utilizar una implementación en FPGA.  La disponibilidad de gran cantidad de ejemplos, notas de aplicación y manuales del fabricante sirvieron de  guía en este desarrollo y una importante cantidad de aplicaciones similares en el mundo de los entornos  embebidos indicaban que la alternativa elegida fuera la correcta.   Como aprendizaje se puede mencionar que el compilador  de Dynamic C 9.62 no es totalmente compatible  con ANSI C y si bien puede ser portado entre diferentes microprocesadores de la familia Rabbit, se requiere  un trabajo extra para implementarlo. Usos específicos como RabbitWeb con Z Server  son propietarios y no  son reutilizables en otros microprocesadores  o plataformas embebidas.  Quedará como trabajo a futuro culminar con la integración del payload al modelo de ingeniería y actualizar  el software embebido de cada módulo de acuerdo a los requerimientos de cada subsistema a ser integrado.  La integración final del EGSE con el subsistema será realizada a fines de Julio del corriente año y se prevé  hacer en primera medida una recepción e inspección visual del subsistema montaje en el modelo de  ingeniería del satélite y posterior encendido, apagado del transmisor, receptor. Se busca en primera instancia  verificar las funcionalidades vitales. El plan de integración y ensayo tiene estipulado en la segunda etapa  conectar un simulador de canal, y realizar pruebas específicas, para lo cual la plataforma aquí desarrollada es  de vital importancia para el control de la carga útil."	﻿carga útil , satélite geoestacionario , web , sistemas embebidos , telecomandos , telemetría	es	19378
15	Aplicando redes BPN para determinar áreas no deseadas en quinotos resultados gráficos	﻿El bajo costo de las cámaras digitales hace atractivo el uso de la segmentación de imágenes a color para evaluar la calidad de alimentos como ser los citrus. Para implementar un sistema automático de clasificación se requiere explicitar el conocimiento presente en las personas y caracterizar la variación de color en áreas de similar aceptabilidad atendiendo la profundidad de color con que se capturan las imágenes. En este trabajo evaluaremos algunas redes BPN aplicadas a la diferenciación de áreas con colores deseables de aquellas con colores que corresponden a defectos como ser manchas o áreas no lo suficientemente maduras. Se pondrá énfasis en la aplicabilidad de la técnica y se profundizará en la descripción del espacio de decisión a aprender, los efectos de las representaciones en espacio de color RGB y L∗a∗b∗ y la calidad de los resultados obtenidos. Concluiremos que el enfoque es viable y perfeccionable atendiendo la discretización de las salidas de la red, la variedad de entradas usadas y el interés en disminuir cada tipo de error y la complejidad de la red usada. Palabras claves: segmentación supervisada, segmentación imagen color, clasificación automática fruta, red BPN. 1. Introducción Tanto la agricultura como la industria de alimentos requieren analizar imágenes de productos para realizar tareas de interés económico. Roy Davies en [1] hace un compendio del uso del procesamiento de imágenes en la industria de la alimentación y lo compara con la performance humana. Comúnmente la tarea de determinar la calidad, defectos o enfermedades en ciertos productos es propensa a no mantener la coherencia y sufrir limitaciones propias del accionar humano al ser realizada en forma no automática por un experto empleando solo sus sentidos y experiencia. Uno de los problemas en el procesamiento de imágenes y la visión por computadora es el determinar automáticamente las regiones en una imagen que poseen características de interés y que de su análisis, resalte, diferenciación o relación con el resto que permita derivar alguna conclusión ya sea con o sin intervención humana. Trabajos como el presente en [2] por Jimenez et al. sobre la recolección automática de frutas y en [3] por Sun et al. presentan algunos resultados y problemas a considerar al momento de diferenciar objetos en agricultura y la industria de alimentos. Peterson et al. en [4] analizan el uso de redes neuronales en problemas propios del procesamiento de imágenes. Los perceptrones multicapa resultan atractivos por su aplicación exitosa en la segmentación realizada a nivel de píxel y su facilidad de uso al ser aplicables como cajas negras que aprenden por sí mismas. De estas cajas negras posteriormente se podrá extraer el conocimiento adquirido tanto para hacer una descripción formal como para dar pautas de estudios futuros, tal como se hace en inteligencia de negocio con la extracción de reglas y el análisis estadístico. Nos propusimos aplicar redes BPN (Backprogration Neural Networks) para ubicar áreas con manchas en quinotos como tarea previa a su evaluación de calidad asumiendo que la red puede aprender la manera en que un experto discrimina entre los colores propios de la superficie de un quinoto maduro de excelente calidad, de la de aquellos con manchas, verdes o en mal estado. Hicimos un estudio sobre su eficacia a la hora de separar los colores deseables de los no deseables y complementaremos los resultados tradicionales con un análisis de los patrones a aprender y los errores cometidos. Nuestro trabajo se organiza como sigue: en las secciones 2 y 3 presentaremos el marco teórico utilizado. En la sección 4 describiremos el diseño y el criterio de evaluación de los experimentos realizados; y en la sección 5, sus resultados. Por último, en la sección 6 concluiremos. 2. Segmentación de imágenes Un problema de segmentación implica establecer una partición del conjunto de puntos F que forman una imagen digital para poder realizar tareas de mayor grado de abstracción. Esto es [5], establecer subconjuntos de puntos, regiones de interés o ROIs(Region of Interest) (S1, S2, . . . , Sn) según algún criterio de tal modo que ⋃n i=1 Si = F siempre que Si ⋂ S j = /0 para todo i 6= j. Métodos de segmentación que se reseñan en [6] diferencian en una imagen de nivel de grises aquellos pixels pertenecientes al fondo, que no interesan, de aquellos pertenecientes al primer plano, que sí interesan. Entre ellos, la umbralización o thresholding basa su funcionamiento en clasificar los pixels según sus niveles de gris sean menores o mayores que un valor umbral L. Mayor información sobre métodos para determinar este valor y los resultados que se obtienen al aplicarlos en distintas imágenes de interés puede encontrarse en [7]. 2.1. Segmentación de imágenes a color Un color se acostumbra a describir en un espacio tridimensional con componentes vinculados a la elección de tres colores primarios y un blanco de referencia. Monitores y cámaras de video identifican, en general, los colores en el espacio de color RGB, por ser un espacio que facilita las tareas al momento de reproducir colores mediante la adición de haces rojos, verdes y azules. Numerosos espacios de color se han definido sin lograr que ninguno sea capaz de describir el color con el que se percibe un objeto en cualesquiera condiciones de observación[8].Al estandarizar espacios como el RGB, el XYZ y el L∗a∗b∗ la CIE(International Commission on Illumination) puso énfasis en conseguir representaciones que privilegiaran diferentes aspectos. El espacio de color L∗a∗b∗ busca lograr que la noción de distancia euclídea entre puntos tenga una correspondencia con la noción de diferencia de color percibida por el hombre. Las coordenadas L∗, a∗ y b∗ fueron definidas tal que la claridad, el tono y el croma de un color puedan ser expresados, respectivamente, como L∗, arctan b ∗ a∗ y√ b∗2 +a∗2 , y se pueda lograr un mapeo a un espacio que distinga al color propiamente dicho de su intensidad y pureza, y que sea consistente con la definición del espacio HSI(Hue-Saturation-Intensity)[5]. El uso de estos y otros espacios de color en la segmentación de imágenes color es explorado en [5]. Son cuestiones a tener en cuenta para extender las nociones de segmentación por niveles de gris: el modo en que se va a particionar el espacio de colores y la elección del espacio de color con el que se van a expresar los datos de una imagen expresada en RGB. Señala asimismo que el RGB no es bueno para la segmentación de imágenes, y que modelos como el HSI y el L∗a∗b∗ si bien son no lineales respecto al primero permiten controlar de forma independiente información sobre el color y la intensidad. 3. Redes neuronales BPN Las redes BPN son uno de los modelos usados para la aproximación de funciones y reconocimiento de patrones. Se caracterizan por la agrupación de neuronas en capas, las conexiones hacia adelante y la propagación de errores hacia atrás. Cada neurona realiza una combinación lineal de sus entradas y bias y le aplica una función de transferencia f para generar su salida. La combinación lineal puede expresarse como net = ∑ni=0 wixi donde cada wi representa el peso asignado a cada entrada xi, w0 = bias y x0 = 1 y la función de transferencia puede ser, entre otras, lineal f (net) = net o sigmoidal f (net) = 11+e−net . Las neuronas luego se agrupan en una capa de entrada, una capa de salida y una o más capas ocultas para formar una red neuronal que se utiliza como caja negra. Según [9] puede establecerse cuál es la complejidad máxima de la superficie que separa las clases según la cantidad de neuronas que integran las capas y la cantidad de capas utilizadas. Las BPN, según [10], pueden describirse como funciones que realizan un mapeo F = ℜn → ℜm con n igual a la cantidad de entradas o neuronas de entrada y m, la cantidad de salidas o neuronas de salida; capaz de aproximar tan precisamente como se desee cualquier función real continua o con un número finito de discontinuidades entre dos conjuntos compactos y que puede ser expresado sin perder detalle como un sistema de reglas con lógica difusa. El aprendizaje en la red neuronal se logra ajustando los pesos de la red para minimizar una medida de error escogida como la dada por el promedio en una época de las diferencias al cuadrado entre la salidas obtenidas y las deseadas también conocida como MSE(Mean Squared Error). El ajuste de los pesos puede realizarse utilizando el algoritmo de Levenberg-Marquardt que tiene un comportamiento que varía entre el algoritmo de descenso más pronunciado y el de Gauss-Newton según un parámetro µ manejado por el propio algoritmo y que en general presenta una mejor performance al compararlo con el primero [11,12]. 4. Propuesta de solución 4.1. Metodología Una manera de determinar la presencia de manchas en alguna fruta es definir en qué grado los píxels que componen esa región de la imagen cumplen con un patrón de color deseado. En nuestro caso, consideramos como colores aceptables a aquellos presentes en frutas maduras de buen aspecto; y como colores no aceptables a aquellos presentes en frutas verdes, en mal estado o bien en áreas con manchas visibles. Nuestro enfoque se basó en utilizar una red neuronal que diferencie los colores deseables de aquellos no deseables usando su representación como puntos en un determinado espacio de color. Sobre una red BPN se aplican los componentes de color de un píxel expresados bien sea en el espacio color RGB o bien en el L∗a∗b∗ para obtener un valor continuo centrado entre 0 y 1 que indique de manera creciente su deseabilidad y que luego se discretizó a un nivel 0 o 1 según fuera inferior o superior a algún valor límite de corte para distinguir entre los puntos no aceptables (nivel 0) de los aceptables(nivel 1). Matemáticamente, la entrada a la red es ℜn = colorPíxel = (c1,c2,c3) con ci, el valor de cada componente color. En caso de usar RGB, la terna es (R, G, B) ; y, en caso de usar L∗a∗b∗, (L∗, a∗, b∗). La salida de la red es ℜm = aceptabilidad =ℜ . A fin de obtener los pares de entrenamiento (colorPíxel, aceptabilidad) se evaluaron en forma manual con un 1(deseable) o un 0 (no deseable) los píxeles que forman parte de las imágenes color de las frutas escogidas como ejemplo. La evaluación se realizó tal como lo haría un experto que determina áreas en la fruta que no son aceptables; esto es, a nivel de una única fruta o de regiones en ella y no a nivel global del conjunto de frutas elegidas. Se incluyeron 14 quinotos entre 5 maduros y con algunas manchas pequeñas, 5 verdes y 4 en mal estado de conservación o rechazables por su excesivo contenido de manchas. 4.2. Espacio de decisión a considerar A fin de observar cuál es la complejidad del problema planteado y dar respuestas a ciertos interrogantes derivados de los resultados presentamos un resumen de los datos presentes en el conjunto de entrenamiento. En el cuadro 1 se pone en evidencia la cantidad de ejemplos obtenidos con la poca cantidad de frutas utilizadas, los colores que hacen al concepto “color aceptable” y la redundancia presente en las muestras utilizadas. Del conjunto de pares de entrenamiento obtenidos se tabulan por cada clase a diferenciar la cantidad de ejemplos presentados y la cantidad de colores encontrados en cada una de ellas. La figura 1 muestra cuáles son los colores que forman parte del conjunto de entrenamiento particularizando las diferencias de representación proporcionadas por los espacios RGB y L∗a∗b∗, como así también ubica en ambos espacios los colores que entendemos son aceptables y aquellos que no los son . Resulta llamativa la presencia de colores similares (que se aprecian mejor observándolos en el monitor) y, en ciertos casos, iguales en las clases supuestamente excluyentes de colores. De hecho, colores aceptables como los presentes en las áreas de sombra de frutas maduras estaban también presentes como colores no aceptables en frutas en mal estado o con excesiva cantidad de manchas. La figura 2 resalta la forma que debería adoptar una superficie de decisión que separe las clases de colores. Tanto la figura 1 como la 2 muestran las clases con una agrupación principal de puntos caracterizada gráficamente porque existen puntos que no pueden unirse con una línea recta que esté contenida en esa región, como así también puntos aislados. A priori se puede observar que el espacio de color L∗a∗b∗ minimiza la complejidad de una posible superficie de separación entre clases al poder expresarla con menor cantidad de vértices. Tipo de píxel Total ejemplos Colores presentes Aceptable 237.030 25.948 No aceptable 481.900 59.215 Cuadro 1: Patrones de entrenamiento 4.3. Redes neuronales a evaluar y parámetros de entrenamiento Se probaron redes neuronales con capas ocultas sigmoidales puesto que poseen derivadas elevadas en el origen de coordenadas, y luego de una evaluación previa, se observó que finalizaban el entrenamiento con menor error; y con capa de salida lineal para mejorar la distribución de los valores de salida y evitar su concentración en los extremos con una sola neurona. En la capa de entrada se utilizó una función identidad con tres neuronas. Redes de una y dos capas ocultas fueron probadas. La primer capa oculta con 20, 40 o 60 neuronas. La segunda capa, de existir, con 5 neuronas. En el entrenamiento se usó el algoritmo Levenberg-Marquardt y una división aleatoria del conjunto de entrenamiento para evitar sobreajuste de datos con 60% para entrenamiento y 20% para validación con los parámetros que siguen: Épocas: 50 Factor de aprendizaje: 0,025 µ máximo: 1×1010 Gradiente mínimo: 1×10−10 Épocas con incremento del error de validación: 6 Medida de performance: MSE 50 100 150 R 50 100 150 G 0 50 100 B (a) Colores aceptables en RGB 20 40 60L 0 20a 20 40 60 b (b) Colores aceptables en L∗a∗b∗. 50 100 150 200 250 R 50 100 150 200 250 G 0 100 200 B (c) Color no aceptables en RGB 20 40 60 80 100 L -20 0 20a 0 20 40 60 b (d) Colores no aceptables en L∗a∗b∗ Figura 1: Colores de entrada presentes en los pares de entrenamiento en distintos espacios de color según la clase de píxel a la que corresponden. 50 100 150 200 250R 50 100 150 200 250 G 0 50 100 B (a) En RGB 20 40 60 80 100 L -20 0 20 a0 20 40 60 b (b) En L∗a∗b∗ Aceptables No aceptables Figura 2: Colores a diferenciar. (a) Quinotos seleccionados. De arriba a abajo: quinotos verdes, maduros, en mal estado o con manchas Aceptables No aceptables (b) Clasificación deseada Figura 3: Imágenes de evaluación 5. Resultados 5.1. Criterio de evaluación La eficacia de la propuesta para detectar áreas con manchas se evaluó en dos etapas. En la primera etapa se comparó en forma general las distintas configuraciones de redes BPN junto a los espacios color sobre los que trabajaban. El aprendizaje efectuado se evaluó según la performance obtenida al finalizar el entrenamiento. En la segunda etapa, atendiendo que la medida de performance es un promedio, se evaluaron los resultados proporcionados por una red neuronal y el proceso de binarización sobre un conjunto (colorPíxel, aceptabilidad) diferente al empleado en el entrenamiento. Se dispuso de prototipos de quinotos verdes, maduros y en mal estado cuyas imágenes y clasificación deseada(realizada de la misma forma que para obtener el conjunto de entrenamiento) están presentes en las figuras 3a y 3b. Por cada clase de quinotos se obtuvieron los errores de clasificación y las probabilidades de obtener determinados valores en las salidas. 5.2. Performance de las redes neuronales al finalizar entrenamiento El cuadro 2 presenta los parámetros usados para determinar el fin de entrenamiento en cada red al momento de finalizar la época 50. En todos los casos la finalización se produjo por finalización de épocas estipuladas y no por error de validación. Las redes neuronales con una capa oculta(cuadro 2a) presentan peores rendimientos que aquellas con dos capas ocultas(cuadro 2b). En ambos casos puede verse que hay una leve diferencia en el rendimiento según la cantidad de neuronas en la primer capa oculta: a mayor cantidad, mejores resultados. Cuando se utilizan datos expresados en L∗a∗b∗se obtienen resultados con una pequeña mejoría respecto a cuando se usa RGB. neuronas en 1er capa oculta L∗a∗b∗ RGB 20 0,0447 (0,06; 0,1) 0,0463 (0,29; 1) 40 0,0421 (0,05; 1) 0,0455 (0,19; 1) 60 0,0409 (0,07; 1) 0,0427 (0,03; 1) (a) 1 capa oculta neuronas en 1er capa oculta L∗a∗b∗ RGB 20 0,0370 (0,95; 0,01) 0,0400 (2,86; 0,1) 40 0,0369 (0,28; 0,1) 0,0396 (0,344; 1) 60 0,0367 (1,149; 0,01) 0,0387 (0,76; 1) (b) 2 capas ocultas Cuadro 2: Variables de control de entrenamiento a su finalización. Datos en forma per f ormance(gradiente, µ) 5.3. Resultados al utilizar la red 3-60-5-1 con colores expresados en L∗a∗b∗ Por presentar el menor error de aprendizaje al finalizar el entranamiento se escogió la red 3-60-5-1 con entradas expresadas en espacio L∗a∗b∗. Salida de la red Los datos presentes en las figuras 4a y 4b permiten diferenciar entre los quinotos maduros, y aquellos verdes o con manchas. Los quinotos maduros tienen salidas predominantemente altas; en tanto aquellos verdes o en mal estado, predominantemente bajas. En la figura 4a se forman picos en torno a los valores 0 y 1. En los quinotos maduros una proporción considerable de píxeles no aceptables presentan salidas elevadas. Clasificación Usando un valor de corte de 0,68 para garantizar que un 90% de los puntos clasificados manualmente como aceptables en los quinotos maduros sean clasificados como aceptables se obtiene la clasificación y los errores de clasificación presentes en la figura 5b y en el cuadro 5a. Son muy llamativos la presencia de falsos no aceptables en los bordes de los quinotos maduros y el error cometido en la clasificación de los píxeles no aceptables en los quinotos maduros. Palabras comunes Tanto a la salida de la red como en la imagen de clasificación se distinguen defectos presentes en las frutas y reflejos de luz clasificadas como áreas no deseables. Una ampliación de un sector con manchas, como el presente en la figura 6, revela que en gran cantidad de ellas existe al menos un pixel identificado correctamente como no aceptable. -0.5 0.0 0.5 1.0 1.5 0.1 0.2 0.3 0.4 0.5 Quinotos verdes -0.5 0.0 0.5 1.0 1.5 0.1 0.2 0.3 0.4 0.5 Quinotos maduros -0.5 0.0 0.5 1.0 1.5 0.1 0.2 0.3 0.4 0.5 Quinotos en mal estado Aceptables No aceptables (a) Probabilidad salida red neuronal según clase de quinoto y salida deseada -0,05 1 (b) Figura 4: Salida red neuronal Obtenido/ Deseado Aceptables No aceptables Aceptables - No aceptables 5,8% 94,2% Quinotos verdes Obtenido/ Deseado Aceptables No aceptables Aceptables 90,5% 9,5% No aceptables 57,9% 42,1% Quinotos maduros Obtenido/ Deseado Aceptables No aceptables Aceptables - No aceptables 3,6% 96,4% Quinotos en mal estado (a) Errores de clasificación Aceptables No aceptables Falso aceptable Falso no aceptable (b) Figura 5: Clasificación de los píxeles con un valor de corte igual a 0,68 Figura 6: Ampliación de las figuras 3a, 4b y 5b sobre un quinoto maduro 6. Conclusiones Atendiendo a la relativa arbitrariedad con la que se separaron los colores aceptables de aquellos no aceptables, es posible determinar la ubicación de manchas en las frutas y decidir en términos generales si una fruta es aceptable o no usando redes BPN. Siempre que existe una diferencia significativa de color, la clasificación es acertada. La presencia de píxels incorrectamente clasificados en las áreas de sombra en frutas maduras puede explicarse por el hecho que esos colores están presentes en mayor medida en áreas no aceptables pertenecientes a frutas en mal estado. La distribución de salidas en las frutas maduras indica que pueden obtenerse otros resultados según los valores de corte empleados, siempre haciendo concesiones según la importancia de los errores a ignorar. De los resultados obtenidos y la agrupación de los colores en las entradas puede suponerse que con redes neuronales más pequeñas aplicadas sucesivamente con criterios de decisión menos complejos sobre menor variedad de entradas se generarían representaciones más compactas y con mejores resultados. Se podría pensar en la aplicación sucesiva de redes explotando la similitud de colores tal que cada etapa diferencie las áreas maduras de las verdes, las maduras de las pasadas, las sin manchas de las con manchas, y entre las manchas, por ejemplo, aquellas negras de las blanquecinas. Restaría profundizar en el uso de la teoría de segmentación aplicada sobre la imagen en nivel de grises construida con las salidas de la red, la calidad de resultados empleando distintos espacios color, el empleo de lógica difusa para analizar las salidas y la calidad de salidas obtenibles según las dimensiones de la red.	﻿red BPN , segmentación supervisada , segmentación imagen color , clasificación automática fruta	es	20885
16	Simuladores para afianzar conceptos de teoría de colas	﻿ Se describen simuladores que abordan un caso de estudio de la teoría  de colas, que incorporados al EVEA de la asignatura Modelos y Simulación de  la FACENA (UNNE), apoyan el proceso de enseñanza–aprendizaje de los  alumnos en la modelización de problemas abstraídos de situaciones reales. El  trabajo se compone de cuatro secciones. En la primera sección se sintetiza el  marco institucional y el estado del arte en el que se encuadra el trabajo. La  segunda resume la propuesta metodológica diseñada ad-hoc como una natural  consecuencia de la experiencia en estos tipos de software y en la dirección de  trabajos finales de carrera en la temática. En la tercera sección se describen  casos de estudios ejemplificadores y las funcionalidades de los productos  generados. Finalmente, se mencionan las conclusiones y futuras líneas de  trabajo.  Palabras clave: educación superior, modelos y simulación, entornos de  enseñanza-aprendizaje, simuladores, modelos de colas.  Abstr act. Simulators to address a case study of the theory of queues are  described, which incorporated into the EVEA of the course Modeling and  Simulation of the FACENA (UNNE), support the learning process of students  in modeling of problem abstracted from real situations. The work consists of  four sections. The first section synthesizes the institutional framework and the  state of the art at which the article belongs. The second summarizes the  methodology designed ad-hoc as a natural consequence of experience in these  types of software and in the management of late-career work in the field. The  third section describes exemplary case studies and features of products. Finally,  the conclusions and future lines of work are mentioned.  Keywords: higher education, models and simulation, teaching-learning  environments, simulators, queue models.    1   Introducción  Las Tecnologías de Información y de Comunicación (TICs) son una de las aliadas  fundamentales de la educación que caracteriza a este siglo. Como lo señalan [6], su  aplicación en la Educación Superior constituyen un modelo complementario de la  docencia presencial.  La asignatura Modelos y Simulación, contexto en donde se encuadra el presente  trabajo, pertenece al Plan de estudios de la Carrera de Licenciatura en Sistemas de  Información, de la Facultad de Ciencias Exactas y Naturales y Agrimensura -  Universidad Nacional del Nordeste (FACENA - UNNE). Esta asignatura nació con la  puesta en marcha de la Carrera de Licenciatura en Sistemas (plan anterior) en el año  1988, y tuvo siempre el carácter de optativa.    Los entornos virtuales de enseñanza y aprendizaje (EVEA) son aplicaciones  informáticas desarrolladas con  fines  pedagógicos [4]. En trabajos previos ([12] y  [15]) se describieron las funcionalidades del EVEA diseñado y desarrollado para ésta  asignatura el cual evolucionó desde el año 1999. En otros trabajos, se detallaron  componentes de software creados para la modelización y simulación de casos de  estudios, algunos de ellos abstraídos de situaciones reales ([1], [2], [8], [9], [10], [11],  [12] y [13]) accesibles desde éste EVEA.   Estos entornos o Los EVEA, como expresan [4], “fueron  diseñados  con  el   propósito  de facilitar la comunicación pedagógica entre los participantes en un  proceso educativo, fundamentalmente docentes y alumnos, sea éste completamente a  distancia, presencial, o de naturaleza mixta”. En la cátedra Modelos y Simulación,  desde el año 2005, se aplica la modalidad de aprendizaje combinado o blended  learning ([14] y [15]), así como las estrategias didácticas aplicadas a fin de  cumplimentar la planificación prevista.   La simulación de un sistema es la operación de un modelo como una  representación del sistema. Este modelo puede sujetarse a manipulaciones que en la  realidad serían imposibles de realizar, demasiado costosas o imprácticas. En la  modelización y simulación se deben disponer de rutinas generadoras de variables  aleatorias con distribuciones específicas, como por ejemplo, exponencial, normal, etc.  Este procedimiento es realizado en dos fases. La primera consiste en generar una  secuencia de números pseudoaleatorios distribuidos uniformemente entre 0 y 1.  Luego esta secuencia es transformada para obtener los valores aleatorios de las  distribuciones deseadas, los cuales conforman las muestras artificiales.   En este trabajo se descr ibe un caso de estudio de modelos de colas para  apoyar  el proceso de enseñanza–aprendizaje de los alumnos en la modelización  de problemas abstraídos de situaciones reales.   El trabajo se compone de cuatro secciones. En la primera sección se sintetiza el  marco institucional y el estado del arte en el que se encuadra el trabajo. La segunda  resume la propuesta metodológica diseñada ad-hoc como una natural consecuencia de   la experiencia en estos tipos de software y en la dirección de trabajos finales de  carrera en la temática. La tercera sección describe casos de estudio ejemplificadores y  las funcionalidades de los productos educativos generados. Finalmente, se mencionan  las conclusiones y futuras líneas de trabajo.    2   Metodología   En esta sección se expone la metodología propuesta diseñada ad-hoc, para el diseño y  construcción de software de enseñanza-aprendizaje aplicable en el ámbito de la  asignatura Modelos y Simulación. La misma consta de las siguientes etapas [16]:  - Estudio de factibilidad. Consiste en una estimación de recursos necesarios y  escenarios posibles. Permite establecer claramente los límites del entorno virtual  y su integración con otros entornos similares aplicables en la asignatura.   Primeramente, como paso fundamental y previo a la etapa de selección de la  herramienta, se observaron las necesidades del sistema y qué aplicabilidad  tendría, para luego acotar más el espectro que definiría los posibles lenguajes o  herramientas que serían utilizados a tal efecto. Las necesidades requeridas por el  sistema a desarrollar son de tipo educativo con el objetivo de desarrollar uno o  varios complementos para apoyar el proceso de aprendizaje de la asignatura  Modelos y Simulación.   - Definición de los destinatar ios. Al diseñar un software un interrogante muy  importante que se debe plantear es: ¿Quiénes utilizarán el software a diseñar? Los  destinatarios de este software interactivo son los alumnos de la asignatura  Modelos y Simulación de la carrera de Licenciatura en Sistemas de Información  de la FACENA - UNNE. Realizada la delimitación geográfica, se puede decir  que el software podrá ser utilizado en los laboratorios de la institución como así  también en los domicilios de los alumnos, convirtiéndose de esta manera en una  herramienta de apoyo fuera del horario del cursado de la asignatura.   - Identificación de los requer imientos. En esta etapa de la construcción de los  materiales instruccionales interactivos, se establece de manera clara y precisa el  conjunto de requisitos que debe satisfacer el software. Desde el punto de vista del  rendimiento, éste debe generar series de números pseudoaleatorios y muestras  artificiales en lapsos muy breves de tiempo. Para brindar una visión más  clarificadora de los requerimientos del sistema se recurre a técnicas de modelado  UML (Unified Modeling Language) [5].   - Definición de la arquitectura general o infraestructura. Desde el punto de  vista de la arquitectura o infraestructura sobre la cual se ejecuta el software, en  general se requiere una computadora con sistema operativo. En el caso de que los  procedimientos sean desarrollados en algún lenguaje específico, se requerirá del  mismo.   - Selección del medio de distr ibución. Se deben tener en cuenta las características  del desarrollo, la forma de ejecución y tamaño de los archivos a la hora de decidir  el medio de distribución.   - Análisis del entorno vir tual. Luego de realizar el estudio de los aspectos  fundamentales del software educativo, se logra una visión más clara de las  funcionalidades o características que debe presentar, a fin de cumplimentar  requerimientos didácticos-pedagógicos.   - Diseño del entorno vir tual. Se contemplan características como: i)  Interactividad, ii) Integración de contenidos en múltiples formatos, iii) Definición  del objetivo de implementación. En el diseño de las interfaces se deben  considerar la navegabilidad, accesibilidad y comunicación, y su especificación en  el desarrollo de entornos virtuales de enseñanza-aprendizaje.  - Selección y evaluación de her ramientas. El análisis de las herramientas de  software permite obtener una visión mas concreta de las funcionalidades y  características más importantes, que permiten lograr un enfoque más sencillo y  práctico de los problemas de simulación abordados. Para la construcción de los  simuladores se seleccionan una diversidad de herramientas de programación  como Flash, MatLab, Visual Basic, Java, Mathematica, Octave, entre otros. Estos  desarrollos, luego son integrados en un sitio web construido empleando el  lenguaje HTML, atendiendo a su fácil acceso desde diversos medios, ya sea vía  web o en un dispositivo digital como cd-rom o dvd-rom. Estas herramientas  constituyen una plataforma que garantiza mínimamente la homogeneidad y el  funcionamiento en diversos equipos y no requieren grandes recursos de  hardware.  - Selección y preparación de contenidos. Los contenidos ([3], [7], [18], [20], [21]  y [22]) incorporados al entorno virtual tienen como finalidad facilitar y/o  complementar el desarrollo de las clases presenciales de la asignatura,   - Desar rollo del entorno vir tual. Se elabora una versión preliminar, orientada a  comunicar la visión esperada en el producto final. Esta etapa comprende la  realización de las siguientes tareas: i) Diseño de las interfases, ii) Desarrollo de  las interfaces, iii) Definición de funcionalidades del entorno.   - Integración de contenidos. Consiste en la incorporación de los contenidos y  elementos en las interfaces desarrolladas.  - Validaciones. Finalizado el desarrollo, se verifica el correcto funcionamiento del  sistema y el acceso a los contenidos. Con respecto al funcionamiento se  comprueba: i) Mapa de navegación. Buena estructuración que permite acceder  bien a los contenidos, actividades, niveles y prestaciones en general. ii) Sistema  de navegación. Entorno transparente que permite al usuario tener el control. iii)  Velocidad entre el usuario y el programa (animaciones, lectura de datos, etc.). iv)  Ejecución de los programas incluidos para actuar como simuladores de lo  problemas abordados.  3   Simuladores en la teor ía de colas   Uno de los propósitos fundamentales de la asignatura es incorporar en los  estudiantes conocimientos de una manera más atractiva y familiar por intermedio de  un software educativo y transformándolos de meros receptores en productores de  conocimientos.   Por otra parte, en [19] se establece una diferenciación en “clases” o “categorías” de  trabajo experimental, según la línea de las asignaturas correspondientes, donde una de  las mismas corresponde a “Utilización de simuladores, ambientes o bibliotecas ya  desarrolladas”. Es decir, los modelos de simulación propuestos en estos software  pueden emplearse como bibliotecas y además como base o prototipo para la  simulación de otros problemas basados en casos reales, previas adaptaciones  requeridas.   El empleo de estas herramientas, permite a los estudiantes afianzar los  conocimientos adquiridos a medida que avanzan en la lectura y estudio de los  contenidos teóricos, y efectuar auto-evaluaciones del aprendizaje de manera continua.  Desde el punto de vista funcional, estos software permiten al alumno: i) Emplear  las computadoras en el tratamiento de problemas reales; ii) Disponer de una  herramienta complementaria para afianzar conocimientos de modelos de simulación;  iii) Ejecutar aplicaciones animadas generadoras de series aleatorias y muestras  artificiales para la implementación de los modelos. iv) Implementar procedimientos  interactivos que simulen modelos. v) Experimentar con diferentes ejercicios que  simulen problemas reales. vi) Repasar conceptos fundamentales de la asignatura.   La teor ía de colas o fenómenos de espera puede ser estudiada en numerosos  problemas o situaciones cotidianas como por ejemplo: i) Colas que se forman en las  estaciones de servicio para cargar combustible en los autos. ii) Colas que se forman en  las consultas médicas. iii) colas que se forman en las cajas de un banco.   En cualquier cola, ciertos entes (clientes) llegan a un punto de servicio, se ponen en  una fila, son atendidos en cierto orden cuando el servicio está disponible, y después  salen del sistema.   El análisis de sistemas de colas tiene por objetivo construir un modelo matemático  que contenga todos los elementos presentes en el sistema. A continuación se describe  un simulador orientado a afianzar la teoría de los modelos de cola, partiendo de un  caso de estudio.   Siguiendo a [22], los fenómenos básicos necesarios para diseñar un modelo de  colas son: i) Forma en que los clientes llegan al punto de servicio: aleatoria o  determinística. ii) Forma como se realiza el servicio: aleatoria o determinística. iii)  Modo de elegir los clientes de la fila que espera el servicio: pudiendo optarse por la  norma FIFO (primero que entra, primero que se sirve), o la norma LIFO (último que  entre, primero que se sirve), etc. Cabe aclarar que si hay varios puntos de servicio, la  descripción del fenómeno de espera necesita otras especificaciones.  A continuación se sintetiza un caso de estudio descripto por el siguiente enunciado:  Considérese un fenómeno de espera en el que los tiempos entre llegadas al sistema  son exponenciales de parámetro M, y los tiempos de servicio siguen también una  distribución exponencial de parámetro L. Realizar el proceso de simulación para n  clientes según la norma FIFO, suponiendo que el primer cliente llega al sistema en el  instante t=0.   Generalmente, los modelos de colas asumen que el tiempo entre llegadas y el  tiempo de servicio siguen una distribución exponencial. En consecuencia, estos  modelos asumen que la cantidad de clientes que llegan al sistema por unidad de  tiempo y la cantidad de clientes que el sistema sirve por unidad de tiempo siguen una  distribución de Poisson.   Variables de entrada (exógenas): semilla, modulo, pa: Elemento semilla y  parámetros utilizados por el método de generación de números pseudoaleatorios, en el  procedimiento Serie(). L, M: parámetros de las distribuciones exponenciales para  simular los tiempos entre llegadas y los tiempos de servicio. n: Número de clientes.  Variables de salida (endógenas): S(): Vector de tiempos de servicio. R(): Vector de  tiempos entre llegadas. W(): Vector de tiempos de permanencia en el sistema. T():  Vector de tiempos de llegada. E(): Vector de tiempos de espera en la cola. A():  Vector de tiempos de salida. C(): Vector que contiene el número de cliente en la cola.  O(): Vector de tiempos de ocio del servidor.  Variables intermedias (de estado): i: Contador de la cantidad de clientes (extensión  de la simulación). alea: Número pseudoaleatorio entre 0 y 1 devuelto por el  procedimiento serie(). ww: Variable empleada en el cálculo del tiempo de  permanencia en el sistema del cliente i. tt: Variable empleada en el cálculo del tiempo  de llegada al sistema del cliente i. l1, cont: variables empleadas en el cálculo del  número de clientes en la cola una vez que ha llegado el cliente i.  Definidas las variables entrada (exógenas), de salida (exógenas) y las intermedias  (de estado), se está en condiciones de iniciar la simulación con el artefacto de  software desarrollado.   Se puede expresar el tiempo de permanencia en el sistema del cliente i, que se  denomina W(i), en función de:  i) El tiempo de espera en el sistema del cliente i-1, W(i-1).   ii) El tiempo transcurrido entre las llegadas del cliente i-1 e i, R(i) (i >= 2).   iii) El tiempo de servicio del cliente i, S(i).    Se verifica que:  W(i) = W(i-1) – R(i) + S(i)  si W(i-1) >= R(i) (1)  W(i) = S(i)    si W(i-1) < R(i) (2)    Las igualdades anteriores se plasman en las Figuras 1 y 2.     La Figura 1 indica que el tiempo de espera en el sistema del cliente i es igual al  tiempo de espera en el sistema del cliente i-1, más el tiempo de servicio del cliente i,  menos el tiempo transcurrido entre las llegadas del cliente i-1 e i.     En la Figura 2 se observa que el tiempo de espera en el sistema del cliente i  coincide con el tiempo de servicio del cliente i,  ya que cuando éste llega al sistema, el  cliente i-1 ya no se encuentra en el mismo. Se observa además que el sistema  permanece ocioso el tiempo que transcurre entre la salida del sistema del cliente i-1 y  la llegada al sistema del cliente i.    Se ha desarrollado un software educativo que permite implementar  en  computadora el modelo de colas descr ipto anter iormente, mediante el empleo de  los métodos de las Congruencias para generar las sucesiones de números aleatorios  requeridas.      Fig. 1. Situación en la que el tiempo de espera del cliente i – 1 es mayor que el tiempo que  transcurre entre las llegadas del cliente i-1 e i: W(i-1) >= R(i)       Fig. 2. Situación en la que el tiempo de espera del cliente i – 1 es menor que el tiempo que  transcurre entre las llegadas del cliente i-1 e i: W(i-1) < R(i)     En la Figura 3 se ilustra la interface desde la cual el usuario podrá ingresar los  parámetros de entrada o exógenos y obtener los valores de salida o exógenos.     Nótese que el software desarrollado permite al usuario:   a) Seleccionador el método de generación de números pseudoaleatorios a emplear:  Método Multiplicativo de Congruencias, Método Mixto de Congruencias ó Método  Aditivo de Congruencias.  b) Ingresar diferentes parámetros iniciales para el método de generación de  números pseudoaleatorios seleccionado, y analizar el comportamiento del modelo en  cada caso.   c) Ingresar diferentes parámetros iniciales para el modelo, evaluando los  resultados obtenidos al variar los valores de los mismos.  LL LL  S S  i-1 i  i-1 i  R(i)  W(i-1)  W(i)  S(i)  LL LL  S  S  i-1 i  i-1  i  R(i)  W(i-1)  W(i)  S(i) ocio      Fig. 3. Interface del software que implementa la simulación de un modelo de colas.  4   Conclusiones   Se presenta un software educativo que implementa en computadora un modelo de  colas. El mismo permite la realización de prácticas interactivas por parte de los  alumnos, quienes pueden realizar aprendizajes inductivos y deductivos a través de la  manipulación del mismo.   El alumno tiene la posibilidad de seleccionar los valores iniciales de los parámetros  del modelo y de los métodos de generación de números aleatorios a emplear,  propiciándose de este modo un aprendizaje significativo por descubrimiento.   Además, los estudiantes pueden explorar los elementos del modelo simulado, y ver  los resultados y gráficas del mismo, posibilitando su comprensión. Estudiar estos  conceptos realizando cálculos manuales resultaría prácticamente imposible dado el  número elevado de operaciones necesarias para poder apreciar algún tipo de  resultado. Por tanto, este tipo de software simuladores permiten al estudiante  concentrarse en el análisis de los resultados de la simulación y no en las operaciones  matemáticas necesarias para que éstos aparezcan.  El trabajo descripto está enmarcado en las acciones de docencia, extensión e  investigación impulsadas desde la cátedra Modelos y Simulación [17]. Entre ellas se  pueden mencionar: la incorporación de recursos humanos de grado a fin de afianzar y  propiciar un ámbito de formación continua en temas específicos de la asignatura, la  aplicación de las tecnologías de la información y comunicación plasmadas en  innovaciones pedagógicas (alternativas complementarias para acompañar el proceso  de enseñanza), la elaboración de materiales didácticos en diversos formatos y la  integración de temas abordados en la asignatura con otras disciplinas, otros dominios  del conocimiento y/o la práctica profesional.   El software desarrollado forma parte de un conjunto de aplicaciones desarrolladas  por los docentes de Modelos y Simulación. En el futuro, se prevé continuar en esta  línea a través del desarrollo de software de simulación de modelos matemáticos, que  incluirá prácticas interactivas de los principales estudios de casos deterministas y  estocásticos presentados en el dictado de las clases teórico-prácticas a fin de asegurar  la apropiación de conocimientos específicos.   Se continuará con la construcción de modelos de simulación, como el expuesto en  este trabajo, que realizan la abstracción de problemas o situaciones reales complejas.  Su implementación en el aula universitaria dará lugar a acciones de retroalimentación  por parte de los estudiantes, a quienes se les expondrán estos modelos como casos  ejemplificadores,  y a partir de los cuales podrán desarrollar otros similares aplicando  diferentes lenguajes de programación a elección.   Asimismo, en concordancia con la política institucional de la Universidad y la  Facultad de promover el acceso y el desarrollo de cátedras desde la plataforma  UNNE-Virtual, los productos generados podrán incorporarse como una alternativa  más al espacio virtual asignado a la asignatura Modelos y Simulación.	﻿modelos y simulación , simuladores , educación superior , entornos de enseñanza aprendizaje , modelos de colas	es	20926
17	Definición del diseño orientado a aspectos según el metamodelo de la OMG	"﻿Con la evolución de la Ingeniería de Software, se ha introducido conceptos que llevan a una programación de más alto nivel. Debido a que la Programación Orientada a Objetos (POO) posee aspectos que no pueden encapsularse aumentando la indertependencia entre las clases lo cual no es deseable. El enfoque de nuestro trabajo esta dirigido a la definición de un diseño Orientado a Aspecto (OA) basado en el metamodelo de la OMG, siendo éste un aporte con el fin de agilizar el desarrollo de software con la construcción de una herramienta que genere código OA. Como en la actualidad no hay un estándar en cuanto al diseño OA, proponemos construir el diseño a través del lenguaje estandarizado UML utilizando los mecanismos de extensión que éste provee. Se define un metamodelo en XML con la finalidad de utilizar distintas herramientas ya sea para el modelado como para la generación de código OA. Palabras claves: UML, POO, POA, XML, XQuery, aspecto, perfil, estereotipo, punto de corte, aviso. 1. Introducción La Ingeniería de Software tiene como objetivo construir o mejorar un producto de software. En este trabajo se prestará especial atención en la etapa 3, que tiene como propósito crear una abstracción de la implementación (refinamiento directo del diseño), permitiendo así el uso de tecnologías como la generación de código y la ingeniería de ida y vuelta entre el diseño y la implementación. Por este acercamiento entre el diseño y la implementación es que, el diseño detallado está directamente relacionado con los lenguajes de programación y en esta etapa, se deben construir modelos dependiendo del tipo de programación que se utilice para el desarrollo del software. A continuación se brinda una descripción de los tres pilares fundamentales que intervienen en nuestro trabajo: Programación Orientada a Aspectos (POA). Lenguaje basado en la POO, que provee soporte explícito para tratar los aspectos que entrecruzan y atraviesan todo el sistema (crosscutting concern) y que no pueden ser totalmente separados con las técnicas de POO ya que ésta tiene como principal desventaja la obtención de ejecuciones ineficientes debido a que las unidades de descomposición no van siempre acompañadas de un buen tratamiento de los aspectos tales como sincronización, manejo de errores y manejo de excepciones, administración de memoria, gestión de seguridad entre otros. UML. Lenguaje iniciado por la organización OMG (Object Management Group) que se utiliza para modelar la mayoría de los dominios, pero no todos los dominios son factibles de ser modelados en este lenguaje, para esto UML incluye características de extensión [1]. Cuando se diseña un modelo en UML se puede generar código en un lenguaje específico, de igual manera se puede construir un modelo OA y poder generar código para distintos tipos de lenguajes OA. La ventaja de utilizar un estándar como UML para construir el modelo de diseño, es que hay una gran cantidad de herramientas en el mercado que se pueden aplicar. Éstas no sólo permiten definir estereotipos para el soporte de aspectos sino además guardar los documentos en un formato estandarizado como XML (eXtensible Markup Language) [2, 3, 4], posibilitando una factible construcción de una herramienta que genere código OA siendo ésta independiente de las herramientas utilizadas para el modelado. XQuery. Lenguaje de consulta, desarrollado por el grupo de trabajo en consultas XML de la W3C [5]. Se diseñoó para escribir de manera rápida y compacta consultas sobre bases de datos con formato XML, siendo su principal función proporcionar los medios para extraer y manipular información de documentos XML o de cualquier fuente de datos que pueda ser representada mediante XML. Cada consulta es una expresión que es evaluada y devuelve un resultado [6]. 2. Orientación a Aspectos Los aspectos son propiedades de un software que tienden a atravesar sus principales funcionalidades. Formalmente se define: Un aspecto: es una unidad modular que se disemina por la estructura de otras unidades funcionales. Ellos existen tanto en la etapa de diseño como en la etapa de implementación [7]. Por lo general los aspectos están diseminados por todo el sistema causando el problema de tener código desordenado. La OA tiene como objetivo soportar la separación de dichos aspectos encapsulándolos en módulos en vez de tenerlos dispersos en los componentes del sistema. Para lograr escribir programas OA se utiliza una clase especial de lenguajes llamados Lenguajes OA. Ellos definen la manera de encapsular las funcionalidades que cruzan todo el código. Además, estos lenguajes deben soportar la separación de los aspectos vistos anteriormente. Sin embargo, estos conceptos no son totalmente independientes, y está claro que hay una relación entre los componentes y los aspectos, y que por lo tanto, el código de los componentes y de estas nuevas unidades de programación tienen que interactuar de alguna manera. Para que ambos (aspectos y componentes) se puedan combinar, se necesita una interacción entre el código de los componentes y el código de los aspectos. Esta interacción se logra teniendo puntos en común, conocidos como puntos de enlace, y debe haber algún modo de mezclarlos. El encargado de realizar este proceso de mezcla se lo conoce como tejedor (weaver), ayudado por los puntos de enlace él se encarga de mezclar los diferentes mecanismos de abstracción y composición que aparecen en los lenguajes de aspectos y componentes [8]. 3. Diseño OA UML no es lo suficientemente expresivo, como se menciono en las secciones anteriores, como para representar conceptos específicos de dominios particulares. Por esta razón, UML estándar incluye un mecanismo para extender y adaptar UML a diferentes dominios y plataformas: el Perfil UML (UML Profile). La infraestructura de UML se define a través de una librería [9], ella define un metalenguaje base (metalanguage core) que puede ser reusado para definir metamodelos (incluido UML), además permite personalizar UML por medio de los Perfiles UML. La extensión del perfil permite agregar constructores, propios para un dominio particular, al metamodelo sin modificar el existente. El perfil UML incluye los elementos necesarios para hacer posible la extensión del lenguaje. Dichos elementos son. Estereotipos (stereotypes). Valores etiquetados (tags values). Restricciones (constrains). Un estereotipo permite crear nuevos tipos de bloques de construcción parecidos a los existentes, pero que son específicos a un problema particular, para más detalle ver [1]. Durante su definición, se lleva a cabo su asociación con elementos del metamodelo (clase, operación, etc.). Dentro de los modelos se denota un estereotipo de la siguiente manera: ((stereotype-name)) [10]. Los perfiles UML permiten extender no sólo la sintaxis sino además la semántica UML para modelar elementos de dominios particulares. En este trabajo hacemos uso de la ventaja de los mecanismos de extensión que brinda UML, cómo se muestra en la Figura 1. Figura 1. Ejemplo: Diseño Orientado Aspectos. La Figura 2 muestra la definición de un perfil UML para el modelado de sistema OA, siendo sus componentes principales: Aspect =⇒ estereotipo que modela los aspectos como un tipo particular de clase. Un aspecto comprende un conjunto de características y un conjunto de puntos de corte. PointCut=⇒ estereotipo que modela el punto de corte; actúa como filtro y está definido por un conjunto de puntos de enlace. Realize=⇒ relación entre una clase y un aspecto. Indica que una clase usa uno o más aspectos. WeavedClass=⇒ es la unión de la clase y el/los aspectos que la afectan. Advice=⇒ código adicional que se ejecuta en un punto de corte, especifica una conducta que quiere ejecutar antes de (before), después de (after), o alrededor de (around) un punto de enlace [11]. Figura 2. Definición de perfil. 4. Definición del Diseño O.A. según el metamodelo de la OMG Este trabajo tiene como principal ventaja agilizar la tarea del ingeniero de software, ya que hoy por hoy no existe un estándar para un diseño OA ocasionando que cada ingeniero construya su propio diseño. Para evitar esto proponemos una definición del diseño OA según el metamodelo de la OMG. A continuación el siguiente código muestra el metamodelo en XML que debe cumplir si desea poder utilizar una posible herramienta que genere el código Orientado a Aspectos. El diseño OA mostrado en las secciones anteriores fue construido con la herramienta de modelado MagicDraw, la cual permite guardar el diagrama con formato XML. El siguiente paso fue construir consultas con el lenguaje XQuery [12], para filtrar y transformar el código que se generó con la herramienta de modelado, con el fin de poder construir el metamodelo en XML mencionado anteriormente. Se ha decidido construir un metamodelo en XML con el objetivo de que ambas herramientas puedan interactuar (herrramienta de modelado — herramienta que permite la generación de código OA). Metamodelo en XML <diagram> <elements> <class name=""ComunicadorMensaje"" id=""_12_5_1_24100552_1248559736171_533213_253""> <operations> <operation name=""entregar"" visibility=""public"" id=""_12_5_1_24100552_1248560172906_511909_332""> <parameters> <paramenter name=""mensaje"" type =""String""/> </parameters> </operation> <operation name=""entregar"" visibility=""public"" id=""_12_5_1_24100552_1248560578875_745998_336""> <parameters> <paramenter name=""persona"" type =""String""/> <paramenter name=""mensaje"" type =""String""/> </parameters> </operation> </operations> </class> <class name=""Prueba"" id=""_12_5_1_24100552_1248559741671_25693_273""> <operations> <operation name=""main"" visibility=""public"" id=""_12_5_1_24100552_1248560657046_568832_340""> <parameters> <paramenter name=""args"" type =""String[]""/> </parameters> </operation> </operations> </class> <class name=""Modal"" id=""_12_5_1_24100552_1248559744000_443735_293""> <operations> <operation name=""entregarMensaje"" visibility=""private"" id=""_12_5_1_24100552_1248642587500_34511_122""> </operation> <operation name=""before():entregarMensaje"" visibility=""private"" id=""_12_5_1_24100552_1248642887171_149824_141""> </operation> </operations> </class> <class name=""SaludoJapones"" id=""_12_5_1_24100552_1248642960515_797226_144""> <operations> <operation name=""decirle"" visibility=""private"" id=""_12_5_1_24100552_1248643179437_930981_164""> <parameters> <paramenter name=""persona"" type =""String""/> </parameters> </operation> <operation name=""around(persona:String):void : decirle"" visibility=""private"" id=""_12_5_1_24100552_1248643366281_3602_166""> </operation> </operations> </class> <relations type=""Dependency"" id=""_12_5_1_24100552_1248643920875_823152_357"" supplier=""_12_5_1_24100552_1248559736171_533213_253"" client=""_12_5_1_24100552_1248559741671_25693_273""/> <relations type=""Realization"" id=""_12_5_1_601020a_1248882521671_526011_161"" supplier=""_12_5_1_24100552_1248559744000_443735_293"" client=""_12_5_1_24100552_1248559736171_533213_253""/> <relations type=""Realization"" id=""_12_5_1_601020a_1248882552421_412678_178"" supplier=""_12_5_1_24100552_1248642960515_797226_144"" client=""_12_5_1_24100552_1248559736171_533213_253""/> <aspect> <class id=""_12_5_1_24100552_1248642960515_797226_144""/> <class id=""_12_5_1_24100552_1248559744000_443735_293""/> . </aspect> <pointcut> <operation id=""_12_5_1_24100552_1248643179437_930981_164""/> <operation id=""_12_5_1_24100552_1248642587500_34511_122""/> </pointcut> <advice> <operation id=""_12_5_1_24100552_1248643366281_3602_166""/> <operation id=""_12_5_1_24100552_1248642887171_149824_141""/> </advice> <realize> <relation id=""_12_5_1_601020a_1248882552421_412678_178""/> <relation id=""_12_5_1_601020a_1248882521671_526011_161""/> </realize> </elements> </diagram> 5. Conclusiones Para lograr realizar la construcción del diseño OA se utilizó el mecanismo de extensión que provee UML: el perfil, en particular el uso de estereotipos. Como primera fase se construyó el diseño OA basado en un ejemplo particular. En una segunda fase se llevó a cabo consultas basadas en el lenguaje de consulta XQuery siendo su finalidad filtrar y transformar el diseño OA con formato XML a fin de lograr construir el metamodelo en XML, permitiendo facilitar la tarea del ingeniero de software obteniendo un concenso en el diseño OA, además de lograr una documentación mejorada. Como última fase de nuestro trabajo se realizó la construcción del metamodelo en XML para un diseño OA en particular. Se ha logrado a través de este trabajo mostrar las ventajas de la utilización de herramientas estándar bajo las especificaciones de la OMG que permiten que el diseño sea guardado con formato XML."	﻿XML , UML , POO , POA , XQuery , aspecto , perfil , estereotipo , punto de corte , aviso	es	21039
18	Un framework para evaluación de metodologías ágiles	﻿ Las metodologías de desarrollo ágil se basan fundamentalmente en  la colaboración  con los usuarios de software durante todo el proceso de  desarrollo, la facilidad para adaptar el producto a cambios en requerimientos  y  en la entrega incremental del producto. Basadas en el Manifiesto Ágil, han sido  aceptadas y son utilizadas con éxito en proyectos donde los requerimientos  detallados son inicialmente desconocidos y se van construyendo durante el  proceso de desarrollo a partir de interacciones con los usuarios y de la retroalimentación obtenida a partir de las mismas. En este trabajo se propone un  framework de evaluación para las metodologías ágiles de desarrollo, y se aplica  a dos de ellas – Scrum y eXtreme Programming (XP).   La definición de este  framework cuantitavio es novedosa, especialmente porque permite evaluar en  cuánto las metodologías ágiles satisfacen los principios básicos  definidos por el  Manifiesto Ágil. Su utilización es recomendada al momento de decidir una  metodología a adoptar.   Keywords: Manifiesto Ágil, Metodologías Ágiles, SCRUM, XP  1   Introducción  Tradicionalmente, los procesos de desarrollo de software llevan asociado un  marcado  acento en el control del proceso. Definen actividades, artefactos y documentación a  producir, herramientas y notaciones a ser utilizadas, orden de ejecución de las  actividades, entre otras definiciones. Si bien existen varios procesos de desarrollo –  Proceso Unificado [1], Proceso V [2], etc. , la mayoría de estos procesos se derivan  del Modelo de Cascada propuesto por Boehm [3]. Estos procesos, denominados  tradicionales, han demostrado ser efectivos en proyectos de gran tamaño,  particularmente en lo que respecta a la administración de recursos a utilizar y a la  planificación de los tiempos de desarrollo. Sin embargo, el enfoque propuesto por  estos métodos no resulta el más adecuado para el desarrollo de proyectos donde los  requerimientos del sistema son muy cambiantes, se pide reducir drásticamente los  tiempos de desarrollo y al mismo tiempo producir un producto de alta calidad.   Como alternativa a los métodos tradicionales de desarrollo, surgen las  Metodologías Ágiles. Manteniendo prácticas esenciales de las metodologías  tradicionales, las metodologías ágiles se centran en otras dimensiones del proyecto,  como por ejemplo: la colaboración con los usuarios durante todas las etapas del  proceso de desarrollo, y el desarrollo incremental del software con iteraciones muy  cortas que entregan una solución a medida.  Las prácticas ágiles están especialmente  indicadas para productos cuya definición detallada es difícil de obtener desde el  comienzo, o que si se definiera, tendría menor valor que si el producto se construye  con una retro-alimentación continua durante el proceso de desarrollo.  El objetivo de este trabajo es presentar un marco de evaluación de metodologías  ágiles que permite evaluar en qué medida las metodologías cumplen con los valores  declarados por el Manifiesto Ágil. El marco de evaluación permite tomar una decisión  más informada al momento de seleccionar una de estas metodologías. A modo de  ejemplo, el marco se aplica para las metodologías SCRUM y XP.  El resto de este trabajo se estructura de la siguiente manera. La Sección 2 presenta  el Manifiesto Ágil y algunas de las metodologías ágiles comúnmente utilizadas. La  Sección 3 explica en detalle dos metodologías – SCRUM y XP. A continuación, la  Sección 4 presenta y explica el framework de evaluación, mientras que la Sección 5  muestra su aplicación a las metodologías SCRUM y XP. Finalmente, la Sección 6  presenta comparación con trabajos relacionados, conclusiones y trabajo futuro.   2   Manifiesto Ágil y Metodologías Ágiles de Desarrollo  En febrero de 2001, académicos y expertos de la industria del software se reunieron  en Utha, Estados Unidos, a fin de discutir los valores y principios que facilitarían  desarrollar software más rápidamente y respondiendo a los cambios que surjan a lo  largo del proyecto. La idea era ofrecer una alternativa a los procesos de desarrollo  tradicionales. Como resultado de esta reunión, se creó The Agile Alliance [4], una  organización, sin fines de lucro, dedicada a promover los conceptos relacionados con  el desarrollo ágil de software y ayudar a las organizaciones a adoptar dichos  conceptos. El resultado de esta reunión fue un documento conocido como el  Manifiesto Ágil [5]. El Manifiesto Ágil incluye cuatro postulados y una serie de  principios asociados. Sus postulados son:    1) Valorar al individuo y a las interacciones del equipo de desarrollo por encima  del proceso y las herramientas. Tres premisas sustentan este principio: a) los  integrantes del equipo son el factor principal de éxito de un proyecto; b) es más  importante construir el equipo de trabajo que construir el entorno; y c) es mejor  crear el equipo y que éste configure el entorno en base a sus propias necesidades.  2) Valorar el desarrollo de software que funcione por sobre una documentación  exhaustiva. El principio se basa en la premisa que los documentos no pueden  sustituir ni ofrecer el valor agregado que se logra con la comunicación directa  entre las personas a través de la interacción con los prototipos. Se debe reducir al  mínimo indispensable el uso de documentación que genera trabajo y que no  aporta un valor directo al producto.  3) Valorar la colaboración con el cliente por sobre la negociación contractual. En  el desarrollo ágil el cliente se integra y colabora con el equipo de trabajo como un  integrante más. El contrato en sí no aporta valor al producto, es sólo un  formalismo que establece líneas de responsabilidad entre las partes.  4) Valorar la respuesta al cambio por sobre el seguimiento de un plan. La  evolución rápida y continua deben ser factores inherentes al proceso de  desarrollo. Se debe valorar la capacidad de respuesta ante los cambios por sobre  la capacidad de seguimiento y aseguramiento de planes pre-establecidos.  El ciclo de desarrollo que aplican las Metodologías Ágiles es iterativo e  incremental. Este modelo permite entregar el software en partes pequeñas y  utilizables, conocidas como incrementos. Cada iteración se puede considerar como un  mini-proyecto en el que las actividades de análisis de requerimiento, diseño,  implementación y testing son llevadas a cabo con el fin de producir un subconjunto  del sistema final. El proceso se repite varias veces produciendo un nuevo incremento  en cada ciclo hasta que se elabora el producto completo. Si bien todas las  metodologías ágiles adoptan este ciclo, cada una presenta sus propias características..  Las metodologías ágiles más comúnmente usadas se describen a continuación.    Scrum [6] – Indicada para proyectos con alto ratio de cambio de requerimientos, su  principal característica es la definición de sprints – cada una de las iteraciones del  proceso con una duración máxima de 30 días. El resultado de cada sprint es un  incremento ejecutable que se muestra al cliente. Otra  característica importante son las  reuniones diarias que se llevan a cabo a lo largo del proyecto. Dichas reuniones no  requieren más de 15 minutos del equipo de desarrollo y su objetivo son la  coordinación e integración del producto a entregar.  Cristal Methodologies [7] – Se trata de un conjunto de metodologías para el  desarrollo de software caracterizadas por la valoración a las personas que componen  el equipo de trabajo y la reducción al máximo del número de artefactos producidos.  Enfatiza en esfuerzos para mejorar las habilidades de los integrantes del equipo y para  definir políticas de trabajo en equipo. Las políticas dependerán del tamaño del equipo,  estableciéndose una clasificación por colores, por ejemplo Crystal Clear corresponde  a equipos con 3 a 8 integrantes y Crystal Orange en equipos con 25 a 50 integrantes.  Dynamic Systems Development Method (DSDM) [8] – Cumple con las  características generales de definir un proceso iterativo e incremental. Propone cinco  etapas de desarrollo: Estudio de Viabilidad, Estudio del Negocio, Modelado  Funcional, Diseño y Construcción, e Implementación. La iteración se produce en las  tres últimas etapas, sin embargo prevé retro-alimentación en todas.  Adaptive Software Development (ASD) [9] – Es un proceso iterativo, tolerante a  cambios y orientado a los componentes de software. Define tres etapas para el ciclo  de vida: a) Especulación – se inicia el proyecto y se planifican las características del  software; b) Colaboración – se desarrolla el producto; y c)  Aprendizaje – se revisa la  calidad del producto y se entrega al cliente. La revisión tiene como objetivo aprender  de los errores cometidos y volver a iniciar el ciclo de desarrollo.  Feature-Driven Development (FDD) [10] – Define un proceso iterativo, con  iteraciones cortas de dos semanas como máximo. El ciclo de vida consta de cinco  pasos: a) Desarrollo de un modelo global, b) Construcción de una lista de  funcionalidades, c) Planeación por funcionalidad, d) Diseño por funcionalidad y e)  Construcción por funcionalidad.   Programación Extrema (XP) [11] – Define un proceso iterativo e incremental con  pruebas unitarias continuas y entregas frecuentes. El cliente o un representante del  cliente son integrados al equipo de desarrollo. Recomienda que el desarrollo de las  funciones del producto sea realizado por dos personas en el mismo puesto –  programación por pares. Antes de incorporar nueva funcionalidad, se deben corregir  todos los defectos encontrados. Constantemente, se llevan a cabo pruebas de regresión  a fin de detectar los posibles errores.   3  Scrum y XP – Principios, Actividades, Roles y Prácticas  Las próximas dos secciones presentan en detalle los principios, actividades, roles a  cubrir en los equipos de trabajo y prácticas recomendadas de Scrum y XP.    3.1 Scrum    La metodología respeta el ciclo de vida evolutivo y la entrega incremental. Al  comienzo del proyecto, se identifican los requerimientos funcionales y no funcionales  y se conforma una lista de los mismos llamada product backlog. El product backlog  constituye el artefacto base para medir el avance del proyecto. Las iteraciones,  denominadas sprints entregan partes del producto llamadas builds, que si bien no  incluyen toda la funcionalidad del sistema, constituyen ejecutables operativos.  Cada  iteración comienza con una planificación adaptativa guiada por el cliente y culmina  con una demostración del build al cliente. Cada sprint puede durar como máximo 30  días. En cada sprint, el equipo de desarrollo selecciona del product backlog un  conjunto de ítems de mayor prioridad que se convierte en el objetivo a desarrollar.  La  metodología propone las siguientes tres fases:  1) Fase de Planeamiento – es subdividida en: a) Planeación – se define el equipo del  proyecto, herramientas, el sistema de desarrollo y se crea el product backlog con la  lista de requerimientos conocidos hasta ese momento, se definen prioridades para  los requerimientos y se estima el esfuerzo necesario para llevar a cabo la  implementación de los mismos; y b) Diseño Arquitectónico – se define la  arquitectura del producto que permita implementar los requerimientos definidos.  2) Fase de Desarrollo - es la parte ágil, donde el sistema se desarrolla en sprints.  Cada sprint incluye las fases tradicionales del desarrollo de software –  relevamiento de requerimientos, análisis, diseño, implementación y entrega.  3) Fase de Finalización – incluye integración, testing y documentación. Indica la  implementación de todos los requerimientos, quedando el product backlog vacío y  el sistema listo para entrar en producción.    La metodología propone la creación de equipos de trabajo auto-dirigidos y autoorganizados, aconsejando equipos pequeños que maximizan la comunicación entre  sus integrantes. Dentro del equipo de trabajo, se  identifican roles com o el Scrum  Master – responsable de asegurar que el proyecto se ejecute en base a las prácticas,  valores y reglas de Scrum-; el Dueño del Producto – responsable del proyecto,  administra, controla y mantiene y publica el product backlog-; los Miembros del  Equipo – tienen la autoridad para decidir acerca de las acciones a realizar y  organizarlas de tal manera de alcanzar los objetivos de cada sprint; y el Cliente –  participa de las tareas relacionadas con la lista de requerimientos del producto a  desarrollar, aporta ideas, sugerencias y nuevas necesidades-.  Scrum prevee las siguientes prácticas:  o Reunión de Planeamiento del Sprint – organizada por el Scrum Master, se divide  en dos etapas. En la primera etapa se reúnen los clientes, el dueño del producto y  los miembros del equipo para decidir sobre los objetivos y funcionalidad del  nuevo sprint. La segunda etapa de la reunión se realiza entre el Scrum Master y el  equipo de trabajo y se concentra en cómo el incremento del producto será  implementado durante el proceso.  o Sprint – es una lista de requerimientos seleccionados para ser implementados en la  próxima iteración. Los requerimientos son seleccionados  por el equipo de trabajo,  en conjunto con el Scrum Master y el propietario del producto en la reunión de  planeamiento del sprint. Cuando todos los ítems del sprint se completan, se  entrega una nueva iteración del sistema.  o Reuniones Diarias – son dirigidas por el Scrum Master. Se organizan básicamente  para mantener una revisión constante del avance del proyecto. Los integrantes  responden a tres preguntas: 1) ¿Qué se ha logrado completar desde la última  reunión?, 2) ¿Qué obstáculos o problemas se han detectado ?, y 3) ¿Qué funciones  del backlog planea completar para la próxima reunión?  o Revisión del Sprint - el equipo de trabajo y el Scrum Master presentan los  resultados del sprint al cliente.   o Retrospectiva del Sprint – se realiza al finalizar un product backlog y la revisión  del sprint. El equipo de trabajo revisa el cumplimiento de los objetivos marcados  al inicio del sprint. Se analizarán y se aplicarán los ajustes y cambios necesarios  cuando corresponda, se destacarán los aspectos positivos y se tratará de cambiar  aquellos aspectos negativos, para no repetirlos en el siguiente sprint.  3.2 eXtreme Programing (XP)  XP, formulada por Kent Beck, se diferencia del resto de las metodologías por su  énfasis en la adaptabilidad. La metodología está diseñada para ofrecer el software que  el usuario necesita, en el momento en que lo necesite. El éxito de la metodología se  basa en potenciar las relaciones interpersonales, promoviendo el trabajo en equipo, el  aprendizaje de los desarrolladores, y un ambiente de trabajo cordial. Los cinco  principios básicos de XP incluyen: 1) Simplicidad – simplificar el diseño para agilizar  el desarrollo y facilitar el mantenimiento mediante la refactorización del código; 2)  Comunicación – fomenta la comunicación: escrita – como código autodocumentado y  pruebas unitarias, recomendando documentar el objetivo de clases y la funcionalidad  de métodos; y oral - entre programadores y con el cliente, recomendando que ambas  sean constantes y fluidas; 3) Retro-alimentación – promueve la retro-alimentación  constante del cliente a través de ciclos de entrega cortos y demostraciones de la  funcionalidad entregada; 4) Coraje – para mantener la simplicidad permitiendo diferir  decisiones de diseño, para comunicarse con los demás aún cuando esto permita  exponer la propia falta de conocimiento y para recibir la retro-alimentación durante el  desarrollo; y 5) Respeto – se infunde entre integrantes del equipo – los desarrolladores  no pueden realizar cambios que hagan que las pruebas existentes fallen o que demore  el trabajo de compañeros, y  hacia el trabajo – los miembros del equipo tienen como  principal objetivo lograr un producto de alta calidad con un diseño óptimo.   El proceso de desarrollo consiste de tres etapas:  1) Interacción con el cliente – el cliente interactúa permanentemente con el equipo de  trabajo. Se elimina así la fase inicial de recolección de requerimientos, y éstos se  van incorporando de manera ordenada a lo largo del desarrollo. La metodología  propone utilizar la técnica de Historias de Usuario mediante la cual el usuario  especifica los requerimientos funcionales y no funcionales del producto. Cada  historia debe ser lo suficientemente atómica y comprensible, para que los  desarrolladores puedan implementar los requerimientos durante una iteración.    2) Planificación del Proyecto –  el equipo de trabajo estima el esfuerzo requerido  para la implementación de las Historias de Usuario. Cada historia debe ser  implementada en un período de una a tres semanas. Aquellas historias que  requieran más tiempo se sub-dividen  tratando de que resulten atómicas y puedan  realizarse dentro del plazo máximo.   3) Diseño y Desarrollo de Pruebas – la implementación está conducida por las  pruebas unitarias. Cada vez que se quiere implementar una función, primero se  debe definir el test y luego el código para que lo satisfaga. Una vez que el código  cumple el test exitosamente, se amplía y se continúa. A medida que se van  implementando las Historias de Usuario, se van integrando los pequeños  fragmentos de código. De este modo, se realiza una integración continua, evitando  una integración más costosa al finalizar el proyecto. XP fomenta la programación  por pares, donde el desarrollo es realizado por una pareja de programadores. Las  parejas tienen que ir cambiando de manera periódica, para que el conocimiento sea  adquirido por todo el grupo de desarrollo.   Los roles definidos para los integrantes del equipo incluyen el Programador –  encargado de escribir pruebas unitarias y producir el código-; el Cliente – escribe  Historias de Usuario y pruebas funcionales. Asigna prioridades a las Historias de  Usuario y decide cuáles se implementan en cada iteración-; el Tester – es el  responsable de las pruebas, ayuda al cliente a escribir las pruebas funcionales, las  ejecuta, informa los resultados al resto del equipo y mantiene la herramienta de  soporte que se utiliza para llevar a cabo las  pruebas.; el Encargado de Seguimiento  (tracker) - proporciona retro-alimentación al equipo, verifica el grado de acierto de las  estimaciones y controla el avance del proyecto. Entrenador  (coach) – es responsable  del proceso en su totalidad, guía al equipo de manera que se respeten las prácticas XP  y que el proceso se ejecute correctamente; el Consultor – es un miembro externo al  equipo con conocimientos específicos en algún tema, necesario para resolver  problemas que surjan durante el proyecto; y el Gestor (big boss) – es el vínculo entre  el cliente y los desarrolladores, ayuda a que el equipo trabaje efectivamente. Su  principal tarea es la coordinación.  Entre otras, XP define las siguientes prácticas:  o Juego de Planificación – el equipo estima el esfuerzo requerido para la  implementación de las Historias de Usuario.  o Refactorización – actividad constante de reestructuración de código cuyo objetivo  es remover duplicación de código, mejoras de legibilidad y aumentar la  flexibilidad para facilitar cambios posteriores.  o Programación por pares – el desarrollo lo realiza una pareja de desarrolladores.  o Integración continua – el código es integrado una vez que está disponible.   o Cliente in-situ - el cliente debe estar presente y disponible todo el tiempo.    4   Framework de Evaluación  El framework de evaluación propuesto mide de qué manera las metodologías ágiles  cumplen con los postulados del Manifiesto Ágil descriptos en la Sección 2. A este  efecto, el framework define medidas que satisfacen la Teoría Representacional de la  Medición [12]. Las medidas se definen usando escala de intervalos [13].   El framework provee mediciones para los cuatro postulados presentados en la  Sección 2. Estos postulados (Pi, i=1..4) fueron expresados como la valoración de dos  atributos (Pi.1 y Pi.2). La medida de cada postulado se define como la suma de las  medidas de los atributos relacionados, como se formula a continuación:    m(Pi) = m(Pi.1) + m(Pi.2)     i=1..4    Por ejemplo, el Postulado 1 (P1) - Valorar al individuo y a las interacciones del  equipo de desarrollo por encima del proceso y las herramientas, se mide sumando la  medida de cómo la metodología valora al individuo y a las interacciones del equipo  (P1.1) y la medida de cómo valora al proceso y a las herramientas (P1.2).  El atributo que los principios intentan enfatizar (atributo positivo) se mide en una  escala de 0 a 5, y el otro atributo (atributo negativo) en una escala de -5 a cero. De  este modo, cada principio podría obtener una medida entre -5 – en el caso que ambos  atributos tomen el peor valor (-5 el atributo negativo y 0 el atributo positivo), y 5 – en  el caso que ambos atributos tomen el mejor valor (0 el atributo negativo y 5 el  atributo positivo). Si se obtiene un valor cero o cercano a cero, significa que la  metodología no valora significativamente el atributo positivo por sobre el negativo, en  cuyo caso, el principio del Manifiesto Ágil no se satisface plenamente. El framework,  los atributos y sus medidas se presentan en la Tabla 1.    Tabla 1. Framework de Evaluación de Metodologías Ágiles  P1 Valorar al individuo y las interacciones del equipo por sobre el proceso y las herramientas  P1 .1  Valorar al individuo y las interacciones P 1. 2  Valorar el proceso y  las herramientas  valor descripción valo r  descripción  0 No define roles para individuos -5 Define actividades, entregables,  herramientas de desarrollo y de gestión  1 Clara definición de roles para individuos -3 Define actividades, entregables y  herramientas de desarrollo  2 Clara definición de roles y responsabilidades -2 Define actividades y entregables  3 Clara definición de roles, responsabilidades y  conocimientos técnicos  -1 Define actividades para cada iteración  5 Clara definición de roles, responsabilidades,  conocimientos técnicos e interacciones entre  miembros del equipo de trabajo  0 Define actividades para el proyecto pero no  a nivel de cada iteración  P2 Valorar el desarrollo de software que funcione por sobre una documentación exhaustiva  P2.1 Valorar desarrollo de software que funcione P2.2 Valorar documentación exhaustiva  valor descripción valo r  descripción  0 Generar entregable al finalizar el proyecto -5 Requiere documentación detallada al  comienzo del proyecto  3 Generar entregable con testing satisfactorio al  finalizar cada iteración  -2 Requiere solo documentación necesaria al  comienzo de cada iteración  5 Generar entregable con testing satisfactorio e  integrado con el resto de las funciones al  finalizar cada iteración  0 No requiere documentación para comenzar  a implementar la funcionalidad incluida en  una iteración  P3 Valorar la colaboración con el cliente pro sobre la negociación contractual  P3.1 Valorar la colaboración con el cliente P3.2 Valorar la negociación contractual  valor descripción valo r  descripción  0 Cliente colabora a demanda del equipo -5 Existe contratación detallada al inicio y no  se aceptan cambios  3 Cliente es parte del equipo, responde consultas  y planifica las iteraciones  -2 La contratación exige contemplar cambios  durante el proyecto  5 Cliente es parte del equipo, responde consultas,  planifica iteraciones, y colabora en la escritura  de requerimientos y pruebas  0 El contrato por la construcción del producto  no aporta valor al producto.  P4 Valorar la respuesta al cambio por sobre  el seguimiento de un plan  P4.1 Valorar la respuesta al cambio P4.2 Valorar el seguimiento de un plan  valor descripción valo r  descripción  0 No prevé incorporar cambios durante la  ejecución del proyecto  -5 Define un plan detallado al inicio del  proyecto  1 Prevé introducir sólo cambios de alta prioridad  -3 Define un plan detallado de iteraciones, no  acepta cambios durante una iteración  4 Permite la evolución y el cambio, pero no es  recomendable en la iteración en curso  -2 Define un plan detallado para cada  iteración, que puede ser modificado   5 Permite introducir cambios en la iteración en  curso  0 No define planificación alguna  5   Aplicación del Framework  La aplicación del framework se muestra en la Tabla 2 y se explica debajo.   Postulado P1. Scrum y XP obtienen 5 en el atributo P1.1, ya que ambas  metodologías valoran al individuo, definen roles y responsabilidades y reconocen la  importancia y promueven la capacitación de los integrantes del equipo. Scrum obtiene  -3 en el atributo P1.2 ya que define actividades, entregables y herramientas de  desarrollo; mientras que XP obtiene -2 por que solo define actividades y entregables.  Conclusión: Scrum obtiene 2 puntos y XP 3. Scrum obtiene un menor valor ya que la  metodología define herramientas para el desarrollo. XP satisface P1 mejor que Scrum.    Postulado P2. Scrum obtiene 3 puntos y XP 5 en el atributo P1.2. La diferencia  radica en que XP también considera la integración parcial del software al finalizar  cada iteración. Ambas metodologías se evalúan con un valor -2 para el atributo P2.2  ya que ambas sólo requieren documentación para la iteración planificada. Scrum y XP  obtienen un valor positivo para el principio P2, superando XP a Scrum por 1 punto.  Conclusión: XP satisface P2 mejor que Scrum, ya que requiere que los incrementos  entregados sean integrados en forma continua con el resto de la funciones.  Postulado P3.  Ambas metodologías obtienen el mejor valor en ambos atributos – 5   puntos para P3.1 y 0 punto para P3.2. Ambas consideran al cliente como un miembro  del equipo, que colabora desde la planificación de las iteraciones hasta la escritura de  requerimientos y pruebas funcionales. Ninguna de ellas utiliza la relación contractual  para agregar valor al producto. Conclusión: Ambas satisfacen P3 de manera óptima.   Postulado P4. En el atributo P4.1 Scrum obtiene un valor 4 ya que si bien permite  introducir cambios, no se recomiendan en el sprint en curso. De ser prioritario el  cambio en el sprint en curso, se debe estimar nuevamente el esfuerzo requerido y si es  necesario, quitar tareas del sprint ya planificado. XP obtiene el máximo valor debido a  que los cambios se pueden incorporar durante la iteración. Debido a que un enfoque  similar sucede con la planificación, Scrum obtiene un valor de -3 y XP -2.  Conclusión: XP obtiene 3 puntos y Scrum 1. XP satisface P4 mejor que Scrum.     Tabla 2. Aplicación del Framework a Scrum y XP  Postulados P1 P2 P3 P4  Metodología P1. 1  P1. 2  tot al  P2 .1  P2 .2  tot al  P3 .1  P3 .2  tot al  P4 .1  P4 .2  tota l  Scrum 5 -3 2 4 -2 2 5 0 3 4 -3 1  XP 5 -2 3 5 -2 3 5 0 5 5 -2 3  6  Conclusiones y Trabajo Futuro  Varios trabajos existen en la literatura que comparan metodologías ágiles. En lo que  respecta a nuestro conocimiento, todas ellas se basan en comparaciones cualitativas.  Abrahamsson et al [14] define una lista de palabras claves y evalúa varias  metodologías en base a dicha lista. Las palabras claves incluyen: estado de desarrollo  del método, puntos importantes, características especiales, adopción y el grado de  soporte de la metodología para las actividades tradicionales del proceso de desarrollo.  Iacovelli y Souveyet [15] definen un framework de evaluación en base a cuatro  atributos de alto nivel: capacidad de agilidad, uso, aplicabilidad, y proceso y  productos. Strode [16] define un framework de comparación que incluye los  siguientes atributos: filosofía de la metodología, modelos, técnicas, herramientas,  entregables, práctica y el grado de adaptabilidad a una situación. Visconti y Cook [17]  analizan de qué manera XP y Scrum satisfacen los principios del manifiesto ágil.  Luego de concluir que ninguna de ellas satisface totalmente los principios proponen  una metodología combinando aspectos de ambas.  Todos estos frameworks existentes  son cualitativos. Si bien casi todos, de alguna manera incluyen un análisis sobre la  forma en la cual las metodologías cumplen con el Manifiesto Ágil, lo hacen de  manera cualitativa. La principal contribución de este trabajo es la definición de un  framework de evaluación cuantitativo para evaluar de qué manera las metodologías  ágiles cumplen con los postulados del Manifiesto Ágil. En base a la evaluación se  puede concluir que XP satisface los postulados ágiles mejor que Scrum.   Trabajos a futuro incluyen extender el framework para medir el cumplimiento de los  principios del Manifiesto Ágil, aplicar el framework a otras metodologías ágiles, e  incluir en el framework atributos organizacionales para adopción de las metodologías.	﻿metodologías ágiles , Manifiesto Ágil	es	21086
19	Detección de patrones para la prevención de daños y o averías en la industria automotriz	﻿ La problemática planteada en este artículo es en referencia a los  daños y/o averías que se producen en  automóviles 0 KM desde que salen de  producción hasta que llegan a su destino final, ya que ocasionan una pérdida de  dinero importante para la compañía quien debe hacerse cargo del arreglo de las  averías ocurridas. Detectar e identificar determinados patrones de  comportamiento es de importancia para la estimación de costos y presupuestos  tanto para el cambio de la parte o reparación de la parte dañada o averiada  según corresponda. Además, disponer de este tipo de información también  permite tomar las medidas necesarias para evitar siniestros o preparar las  instalaciones por donde transitan o se estacionan las unidades para reducir el  porcentaje de los mismos.  Palabras claves: Minería de datos. Prevención de averías. Aplicación de  minería de datos en la industria automotriz.  1   Introducción  El gran desarrollo tecnológico de los últimos años ha potenciado el almacenamiento  de grandes volúmenes de información. No solo desde el punto de vista del  “computador personal” sino también desde el punto de vista de la computación como  un conjunto de dispositivos electrónicamente interconectados capaces de soportar el  trasporte de grandes cantidades de información en una dirección determinada con el  propósito de ser almacenada centralizada o descentralizadamente, o bien desde el  punto de vista de redes electrónicas donde desde cada nodo se permite la posibilidad  de cargar información en una o más bases de datos.  Al mayor poder de procesamiento y transporte de información se suman los  avances tecnológicos en materia de base de datos que soportan “grandes” volúmenes  de información de diferentes orígenes y fuentes pudiendo incluir texto, números,  imágenes, video, etc. Estos avances en materia de gran capacidad de almacenamiento  de información han dado lugar a la creación de nuevas tecnologías como la Minería  de Datos.   Conceptualmente la Minería de Datos o Explotación de Datos, como también será  llamada en el presente trabajo, se puede definir como un conjunto de técnicas y  herramientas aplicadas al proceso no trivial de extraer y presentar conocimiento  implícito, previamente desconocido, potencialmente útil y humanamente  comprensible, a partir de grandes conjuntos de datos con motivo de predecir de forma  automatizada tendencias y comportamientos y/o descubrir de forma automatizada  modelos previamente desconocidos [1].  Desde un punto de vista empresarial la minería de datos puede ser definida como  un conjunto de áreas que tiene como propósito la identificación de un conocimiento  obtenido a partir de las bases de datos que aporten un sesgo hacia la toma de  decisiones [2].  Hoy en día realizando las consultas (simplemente navegando los datos)  convencionales a grandes bases de datos no es suficiente para resolver problemas de  negocios ya que es necesario obtener más información de la situación de negocio, por  lo cual se hace necesario seguir una metodología y aplicar los procesos adecuados a la  situación de negocio [3] para así obtener conocimiento y resultados que permitan a las  compañías obtener un beneficio.   En la industria automotriz uno de los temas relevantes en la producción y  distribución de unidades es la detección e identificación de daños y/o averías  producidas a lo largo del circuito que recorren los autos desde que salen de planta de  producción hasta que llegan a su destino final.   Con el crecimiento de la globalización las fábricas de autos han desarrollado  complejas redes de distribución en todo el mundo. Para el aprovechamiento de la  especialización en la producción de los automóviles los fabricantes producen  determinados modelos en determinadas regiones / países. Esta estrategia contribuye a  la optimización de costos de producción pero incrementa la necesidad de mayores  controles en el intercambio de unidades entre las regiones. Todo esto ha generado un  importante aumento en el transporte de vehículos de lugar a lugar, de país a país o de  continente a continente. Durante estos trayectos los vehículos se ven sometidos a  daños, en grado variable, en toda la cadena de suministro.   El objetivo del presente trabajo es la aplicación de la metodología CRISP-DM [4]  para detectar patrones de comportamiento en la producción de daños y/o averías en la  cadena de distribución de automóviles utilizando los procesos planteados en [3]  2   Problema a resolver  En todo el circuito de distribución de automóviles existe la posibilidad y el riesgo de  provocar daños a los automóviles. Entre los puntos a tener en consideración está el de  la responsabilidad de los siniestros lo cual es factible detectar a partir de la  información disponible.   A lo largo de la cadena logística de este circuito se pasa por diversos puntos donde  circulan los autos desde que salen de producción hasta que llegan a su destino.    La información vinculada a los temas de siniestralidad de vehículos en la  actualidad se encuentra almacenada en forma distribuida entre las diferentes empresas  responsables en cada punto de control de cada cadena logística. Actualmente cada  empresa que se dedica a la distribución de automóviles realiza en forma privada y por  separado los estudios referidos a los daños y/o averías de cada unidad automotriz en  el punto donde les corresponde su responsabilidad, ya que estos suceden con mucha  frecuencia y ocasionan un gasto importante a la organización. En resumen se puede  decir que los problemas a resolver son los siguientes:  • Identificar  daños y/o averías de automóviles en: Ingreso y despacho de  puerto, Ingreso y despacho de playa, Bajada y subida de buque, Salida de  planta.  • Determinar  responsabilidad de siniestralidad.  • Imputar siniestros.  • Determinar tipos de averías y daños.  • Determinar partes averiadas y dañadas  El movimiento de unidades es organizado y supervisado logísticamente por la/s  empresa/s encargadas de tomar el auto desde que sale de planta hasta que llega a su  destino final. Los destinos finales pueden ser el mismo país o países distintos.  Las empresas u operadores logísticos responsables directos sobre los autos se  encargan de administrar todo el proceso de traslado y distribución de las unidades, ya  que los fabricantes de automóviles normalmente no son responsables por su  distribución. Una marca puede tener uno o más operadores logísticos en todo el  mundo y este inclusive pueden variar de país en país. El proceso incluye carga y  descarga de vehículos de buques (barcos con capacidad para transportar 3000/4000  unidades), entrada y salida de unidades de puertos (con capacidades de  almacenamiento desde cientos a varios miles de unidades),  carga y descarga de  unidades en bateas (camiones con capacidad de 7, 9 y 11 unidades), entrada y salida  de unidades de patios y playas intermedias (con capacidades de almacenamiento  desde cientos a varios miles de unidades), o inclusive en la salida de los autos de la  planta de producción.  En cada uno de estos puntos: plantas, puertos, playas, patios, etc. para la  realización de las operaciones mencionadas; carga o descarga, ingreso o salida  intervienen diferentes empresas. Es de considerable importancia para los operadores  logísticos saber y controlar que es lo que sucede con los autos en el cambio de  responsabilidad cuando se traspasa una unidad de una empresa a otra (el cambio de  responsabilidad no solo involucra a las empresas que entregan y reciben, sino también  hay empresas de seguros y empresas contratadas por los operadores logísticos que  pueden ser una o varias) debido a que los daños y/o averías que sufren los mismos  deben ser imputados (asignados) a la/s empresa/s que lo/s produjeron. Es frecuente la  tercerización en el desarrollo de las inspecciones o peritajes que es realizado  normalmente en los lugares donde se produce el intercambio de responsabilidad de  una empresa a otra.   Los datos en cada uno de los puntos de control son tomados por personal  especializado. Cabe aclarar que siempre se hace referencia a daños y/o averías no  mecánicas puesto que todo lo referente al funcionamiento mecánico solo es  controlado en fábrica antes de su salida de línea de montaje y la unidad no es  verificada mecánicamente hasta que llega al destino final.  Los procedimientos de inspección y captura de datos son llevados a cabo según  estándares mundiales creados por las marcas y los operadores logísticos. Todos los  estándares de verificación y control de autos están certificados según políticas  internas de cada compañía que participa en la distribución de las unidades o según la  demanda por contrato de la marca correspondiente.  La información tomada en cada punto de control es codificada (usualmente el  código está compuesto por: Área o parte de un auto – Avería – Gravedad) según los  procedimientos y demandas de cada operador logístico. Esta información es  compartida con cada una de las partes que intervienen en el traslado de las unidades  debido a que las mismas deben ser aceptadas por cada responsable. Cada observación  imputada a cada unidad debe ser certificada y aceptada por ambas partes en cada  punto; tanto la parte que recibe como la parte que entrega.  Otro dato importante referido a la condición del auto es la descripción de una  observación realizada por el personal de inspección, por ejemplo controles a la bajada  o subida de buque,  dentro de los buques, a la entrada o salida de un puerto, dentro de  los puertos (estiba), a la salida de una planta de producción, a la entrada o salida de un  patio / playa (stock) comercial o por reparación, controles a la subida o bajada de  bateas (camiones), entre otros. Los circuitos mencionados se pueden visualizar en las  fig. 1 y 2:        Fig 1. Circuito exportación        Fig 2. Circuito importación  3   Solución propuesta  Para llevar adelante este proyecto se utilizó como  metodología CRISP-DM y el  proceso de descubrimiento de reglas de comportamiento en grupos propuesto en  [Britos, 2008] para la detección de patrones de comportamiento y reglas de negocio,  la cual se orienta a la resolución de problemas empresariales motivo por el cual fue  seleccionada para este desarrollo. La metodología CRISP-DM es implementada en  forma progresiva por lo cual resulta conveniente para la aplicación en este trabajo,  debido principalmente a la cantidad de datos disponibles y la estructura de los  mismos. Además su característica inductiva o de búsqueda de patrones puede ayudar  a restringir la cantidad de información útil sobre la que se realiza el análisis de  resultados. La exploración de datos se realiza sobre información disponible desde el  año 2003 al 2008, para un correcto análisis se definen muestras uniformes distribuidas  por semestre. Los atributos específicos necesarios para la minería de datos son:  • Identificador de auto,  • Fecha de operación en la que se detecta el daño,  • Modelo de cada unidad,  • Lugar donde se produce la falla,  • Parte del auto donde se produce la avería,  • Tipo de avería producida en cada unidad,  • Severidad o gravedad de avería producida, y  • Observaciones por cada unidad   Una vez aplicada inducción a los diversos tipos de averías se puedo observar que  le  96,04 % corresponde a faltantes, pintura saltada, rayado o rozado y abollado, a  través de la figura 3 se puede observar la distribución a lo largo de los diversos  periodos:    Fig. 3.  Cantidades por tipo de avería  A través de dicho análisis se pudo observar los distintos lugares en lo que se  producen con mayor frecuencia las averías (fig. 4), siendo la “Subida a Buque en  Puerto de Origen” como el de mayor producción de daños (45%), lugar que pertenece  a dos cambios de responsabilidad, esto es, las averías detectadas en la subida a un  buque son imputables tanto al transporte terrestre como a playa de puerto. Es difícil  estimar un porcentaje representativo de lo que sucedió en la realidad en el punto de  control en cuestión pero esto permite deducir que como línea futura de investigación  se puede trabajar con los datos agrupados en función de los lugares de control y no  por periodos de tiempo como es el estudio actual.        Fig. 4. Lugar de avería        Al realizar el proceso de descubrimiento de reglas de pertenecía a  los grupos  detectados se obtuvo los siguientes resultados (tabla 1):   Tabla 1. Especificación de cada grupo por periodo.  Periodo Grupo Lugar Parte Avería Cant  1 Bajada de Buque Tapizado interno  puerta delantera  izquierda  Rayado -  Rozado  596  2 Retiro de Puerto Cristales de parante  delantero  Abollado 1007  3 Subida Buque Puerto  de Origen  Sistema de  navegación  Proyección 15  4 Ingreso a Puerto Moldura protectora  lateral derecha  Rayado -  Rozado  488  5 Subida Buque Puerto  de Origen  Cable de carga Faltante 582  6 Transferencia ubicación  y línea de carga  Insignia Faltante 99  7 Subida Buque Puerto  de Origen  Llave Abollado 2120  8 Subida Buque Puerto  de Origen  Guardabarro delantero  izquierdo  Rayado -  Rozado  14  Se gu nd o  Se m es tr e  20 03   9 Subida Buque Puerto Guardabarro delantero Rayado - 79  Periodo Grupo Lugar Parte Avería Cant    de Origen    izquierdo Rozado  1 Bajada de Buque Cristales de parante  delantero  Faltante 259  2 Retiro de Puerto Guardabarro trasero  derecho  Faltante 627  3 Bajada de Buque Panel lateral izquierdo Faltante 1961  4 Ingreso a Puerto Tapa posterior de  techo  Pintura saltada 82  5 Bajada de Buque Llanta delantera  derecha  Abollado 1642  6 Transferencia ubicación  y línea de carga  Parrilla delantera Abollado 15  7 Bajada de Buque Llanta delantera  derecha  Faltante 405  Pr im er  s em es tr e  20 04   8 Retiro de Puerto Techo (inclusive  convertible)  Proyección 9  1 Subida a Buque Spoiler delantero Rayado -  Rozado  3  2 Ingreso a Puerto Parante delantero Rayado -  Rozado  15  3 Ingreso a Puerto Alfombra  suplementarias  Proyección 48  4 Ingreso a Puerto Luz de neblina trasera Faltante 54  5 Subida Buque Puerto  de Origen  Giro Delantero  (derecho / izquierdo)  Faltante 448  6 Ingreso a Puerto Retrovisor externo  derecho  Proyección 548  7 Transferencia ubicación  y línea de carga  Moldura protctora  lateral izquierda  Pintura saltada 1  Se gu nd o  se m es tr e  20 04   8 Subida Buque Puerto  de Origen  Otros Faltante 3883  1 Subida a Buque Zócalo interno puerta  delantera izquierda  Rayado -  Rozado  3  2 Subida a Buque Paragolpe trasero /  Spoiler trasero  Rayado -  Rozado  10  3 Ingreso a Puerto Capot Rayado -  Rozado  64  4 Ingreso a Puerto Parabrisa Faltante 43  5 Ingreso a Puerto Luz de neblina trasera Proyección 165  6 Ingreso a Puerto Cubierta delantera  derecha  Faltante 436  7 Bajada de Buque Zócalo derecho Faltante 2342  8 Subida Buque Puerto  de Origen  Escape Faltante 855  Pr im er  s em es tr e  20 05   9 Subida Buque Puerto  de Origen  Luneta trasera /  Cristal puerta trasera  Arrancado -  Roto - Fisurado  1  1 Ingreso a Puerto Techo (inclusive  convertible)  Pintura saltada 67  Se gu nd o  se m e 2 Ingreso a Puerto Otros Abollado 177  Periodo Grupo Lugar Parte Avería Cant  3 Ingreso a Puerto Techo (inclusive  convertible)  Rayado -  Rozado  142  4 Subida a Buque Bolsa de herramientas Proyección 12  5 Ingreso a Puerto Llanta delantera  izquierda  Proyección 111  6 Subida Buque Puerto  de Origen  Guardabarro delantero  derecho  Faltante 2955  7 Bajada de Buque Zócalo derecho Faltante 168    8 Ingreso a Puerto Llanta delantera  derecha  Proyección 249  1 Bajada de Buque Spoiler delantero Rayado -  Rozado  7  2 Retiro de Puerto Paragolpe delantero /  Protector delantero  Rayado -  Rozado  1  3 Bajada de Buque Spoiler delantero Rayado -  Rozado  9  4 Ingreso a Puerto Tapa posterior de  techo  Rayado -  Rozado  35  5 Bajada de Buque Moldura protectora  lateral derecha  Faltante 103  6 Transferencia ubicación  y línea de carga  Puerta trasera  izquierda  Abollado 33  7 Bajada de Buque Tapa acceso gancho  remolque  Abollado 274  Pr im er  s em es tr e  20 06   8 Bajada de Buque Zócalo derecho Faltante 3301  1 Bajada de Buque Spoiler delantero Rayado -  Rozado  1  2 Bajada de Buque Paragolpe delantero /  Protector delantero  Rayado -  Rozado  3  3 Retiro de Puerto Paragolpe delantero /  Protector delantero  Rayado -  Rozado  1  4 Bajada de Buque Panel trasero bajo  baúl  Rayado -  Rozado  14  5 Bajada de Buque Alfombra  suplementarias  Proyección 21  6 Bajada de Buque Parrilla delantera Abollado 67  7 Ingreso a Puerto Guardabarro delantero  derecho  Faltante 250  Se gu nd o  se m es tr e  20 06   8 Transferencia ubicación  y línea de carga  Guardabarro delantero  derecho  Faltante 2271  1 Ingreso a Puerto Paragolpe delantero /  Protector delantero  Pintura saltada 2  2 Transferencia ubicación  y línea de carga  Faldillas para barro Pintura saltada 6  3 Ingreso a Puerto Zócalo izquierdo Abollado 57  4 Transferencia ubicación  y línea de carga  Puerta trasera  izquierda  Rayado -  Rozado  68  Pr im er  s em es tr e  20 07   5 Transferencia ubicación  y línea de carga  Tapizado interno  puerta delantera  izquierda  Rayado -  Rozado  160  Periodo Grupo Lugar Parte Avería Cant  6 Bajada de Buque Capot Rayado -  Rozado  10  7 Subida Buque Puerto  de Origen  Panel lateral derecho Abollado 349    8 Transferencia ubicación  y línea de carga  Zócalo derecho Faltante 2377  1 Bajada de Buque Zócalo izquierdo Rayado -  Rozado  11  2 Transferencia ubicación  y línea de carga  Llanta trasera  izquierda  Rayado -  Rozado  72  3 Ingreso a Puerto Tapizado interno  puerta delantera  izquierda  Proyección 239  4 Transferencia ubicación  y línea de carga  Tapa tanque  combustible  Rayado -  Rozado  233   5 Ingreso a Puerto Escape Rayado -  Rozado  51   6 Transferencia ubicación  y línea de carga  Zócalo derecho Rayado -  Rozado  100  7 Subida Buque Puerto  de Origen  Puerta delantera  izquierda  Abollado 62  8 Subida Buque Puerto  de Origen  Limpiador y lavador  de parabrisas  Abollado 266  Se gu nd o  se m es tr e  20 07   9 Transferencia ubicación  y línea de carga  Limpiador y lavador  de parabrisas  Proyección 3966  1 Transferencia ubicación  y línea de carga  Paragolpe delantero /  Protector delantero  Arrancado -  Roto - Fisurado  1  2 Subida a Buque Paragolpe delantero /  Protector delantero  Arrancado -  Roto - Fisurado  3  3 Subida a Buque Bolsa de herramientas Arrancado -  Roto - Fisurado  12  4 Ingreso a Puerto Giro Delantero  (derecho / izquierdo)  Pintura saltada 44  5 Ingreso a Puerto Paragolpe trasero /  Spoiler trasero  Rayado -  Rozado  255  6 Bajada de Buque Moldura protectora  lateral derecha  Abollado 116  7 Transferencia ubicación  y línea de carga  Llanta delantera  izquierda  Rayado -  Rozado  402  8 Ingreso a Puerto Encendedor Manchado 235  Pr im er  s em es tr e  20 08   9 Transferencia ubicación  y línea de carga  Retrovisor externo  derecho  Faltante 3932  1 Transferencia  ubicación y línea de  carga  Paragolpe delantero /  Protector delantero  Arrancado -  Roto - Fisurado  1  2 Subida a Buque Paragolpe delantero /  Protector delantero  Arrancado -  Roto - Fisurado  3   3 Subida a Buque Bolsa de herramientas Arrancado -  Roto - Fisurado  12  Se gu nd o  se m es tr e  20 08   4 Ingreso a Puerto Giro Delantero Pintura saltada 44  Periodo Grupo Lugar Parte Avería Cant  (derecho / izquierdo)  5 Ingreso a Puerto Paragolpe trasero /  Spoiler trasero  Rayado -  Rozado  255  6 Bajada de Buque Moldura protectora  lateral derecha  Abollado 116  7 Transferencia  ubicación y línea de  carga  Llanta delantera  izquierda  Rayado -  Rozado  402  8 Transferencia  ubicación y línea de  carga  Llanta delantera  izquierda  Rayado -  Rozado  402   9 Ingreso a Puerto Encendedor Manchado 235    El comportamiento demostró un alto grado de concordancia entre los lugares en que  se produce la avería y la parte afectada a lo largo de los distintos periodos. De acuerdo  a la información vertida por el experto los casos presentados pueden ser evitados o  realizar acciones para su minimización, logrando así una disminución en los costos de  reparación.  4   Conclusiones  Podemos observar que la metodología CRISP-DM es apropiada para la explotación  de datos en esta industria.   En cuanto a los resultados obtenidos desde el punto de vista práctico permiten  establecer como patrones de comportamiento los tipos de averías y los modelos  afectados en este nicho de mercado específico (considerando el estándar de  inspección para la toma de datos y los modelos de autos que intervienen en el  proyecto).         La aplicación del proceso de descubrimiento de reglas a grupo es óptima en este  proyecto ya que permite ver el comportamiento de los grupos a los largo de los  periodos estableciéndose situaciones comunes a todos lo que permitirá definir  procesos que minimicen la ocurrencia de averías.	﻿Prevención de averías , Minería de datos , Aplicación de minería de datos en la industria automotriz	es	21204
20	Software interactivo para el aprendizaje de números pseudoaleatorios y pruebas de hipótesis SIANP	"﻿ “Modelos y Simulación"" is an elective subject in the curriculum of the career Licenciatura en  Sistemas de Información from the Facultad de Ciencias Exactas y Naturales y Agrimensura  (Universidad Nacional del Nordeste). The overall objective of the course is to provide a solid  training in management of concepts and techniques used in the simulation of digital processing  systems through mathematical models. This paper describes instructional software developed in  order to accompany the teaching-learning process in this subject. It was conducted with the  incorporation of a Grade human resource. The work consists of four sections. The first section  summarizes the state of the art. The second summarized the methodology adopted in the  development of educational software. The third section describes the functionality of the product.  Finally, identifies concrete results and future lines of work.    Keywords: higher education, models and simulation, instructional software, training of human  resources    Resumen  ""Modelos y Simulación"" es una asignatura optativa en el Plan de estudios de la carrera Licenciatura  en Sistemas de Información de la Facultad de Ciencias Exactas y Naturales y Agrimensura de la  Universidad Nacional del Nordeste. El objetivo general de la asignatura es proporcionar una  formación sólida en el manejo de los conceptos y técnicas utilizados en la simulación de sistemas  mediante el procesamiento digital de modelos matemáticos. Este trabajo describe el desarrollo de un  software instruccional elaborado con el objeto de acompañar el proceso de enseñanza-aprendizaje  en la mencionada asignatura, realizado con la incorporación de un recurso humano de grado. El  trabajo se compone de cuatro secciones. En la primera sección se sintetiza el estado del arte. La  segunda resume la metodología adoptada en la elaboración del software educativo. La tercera  sección describe las funcionalidades del producto. Finalmente, se mencionan los resultados  concretados y, las líneas de trabajo futuras.    Palabras clave: educación superior, modelos y simulación, software instruccional, formación de  recursos humanos.    1. INTRODUCCIÓN     En los últimos tiempos, se asiste a un cambio sustancial en la forma de difusión del conocimiento  que afecta a la propia metodología de investigación y a la construcción de ciencia de la educación  [9].  El salto cualitativo dado por la microinformática en los últimos diez años, así como el desarrollo  espectacular de los nuevos soportes de alta densidad capaces de almacenar en un disco una cantidad  de información inimaginable hace unas décadas, todo ello unido a los avances en la compresión  digital y en las redes ópticas que permiten trasladar y compartir información de todo tipo, textual,  sonora, imagen estática y en movimiento, etc. ha llevado inevitablemente a la progresiva  digitalización de todo tipo de información [1].  La existencia de nuevas tecnologías en el campo de la informática y su amplia difusión, lleva a que  la juventud conozca las posibilidades de éstas por medio de videos, juegos virtuales, chats,  navegación por Internet, planteándose en todo momento la comunicación por medio de imágenes y  de la interacción con ellas con la consiguiente pérdida de la ejercitación de la lectura. Es éste uno de  los motivos por los que el alumno que ingresa a la universidad lo hace con una predisposición y  preparación cada vez menor para afrontar el ritmo universitario, el que tradicionalmente exige  lectura de profusa bibliografía, Comprensión de textos, capacidad de síntesis, integración de  conceptos, etc. Ese mismo alumno sin embargo, presenta gran interés en investigar un tema  navegando por Internet, una obra hipermedial o siguiendo los pasos de un tutorial, por ejemplo.  Se presenta así una rivalidad entre los conocimientos adquiridos fuera de la universidad (con  medios más llamativos que brindan las nuevas tecnologías) y los adquiridos en las clases (con  instrumentos tradicionales y que posiblemente sean menos atractivos y menos motivadores). Lo  expuesto, permite plantear el análisis de la situación desde dos ópticas:  Desde el punto de vista del profesor: se debe considerar que la modernización de la enseñanza es un  proceso continuo y aceptar que la progresiva introducción de los medios informáticos en las  actividades educativas provoca cambios, tanto en la forma de plantear los problemas como en el  modo de resolverlos, debido a que las herramientas disponibles son cada vez más potentes y  versátiles.  JDesde el punto de vista del alumno: se debe tener en cuenta que, especialmente en el ámbito  educativo, las nuevas técnicas comunicacionales requieren un nuevo tipo de alumno: más  preocupado por el proceso que por el producto, preparado para la toma de decisiones y la elección  de su ruta de aprendizaje, en definitiva preparado para el autoaprendizaje.  Por otra parte, a nivel mundial se observa que las universidades están planteando la necesidad de  acortar sus planes curriculares, promoviendo una capacitación permanente por medio de postgrados,  maestrías y doctorados, lo que provoca una reducción en la carga horaria y sin embargo los  conceptos básicos son los mismos de siempre o más. También es importante destacar que las  universidades están creando una nueva propuesta para la sociedad: la realidad virtual, esta propuesta  brinda la posibilidad de contar minuto a minuto con toda la información actualizada posible, poder  contactar sin límites de espacio a los más destacados pensadores para enriquecer las ideas, es decir,  se está generando la necesidad de información veloz, creativa y ordenada que permita acortar la  brecha entre el atraso y la modernidad, entre el futuro y el presente.  La educación para el Siglo XXI, permanente (a lo largo de toda la vida) y abierta (a todas las  personas), inmersa dentro de una sociedad en la que el conocimiento será una de las fuerzas que  harán peso en el balance socio-económico que conlleva el desarrollo (o el subdesarrollo), tendrá  como uno de sus grandes aliados potenciales las Tecnologías de Información y de Comunicación  (TICs). No se puede simplemente ponerle tecnología a la educación para estar a tono con la  sociedad en la que se da, hay que repensarla.  En este escenario y conjugación de realidades, es donde el Software Educativo (SE) se perfila como  la herramienta base de las próximas generaciones de educandos. Esto exige, a su vez, el diseño de  metodologías y herramientas adecuadas para satisfacer los nuevos requerimientos [12].  Se denomina software educativo a todo programa de computación realizado con el objetivo de ser  utilizado como facilitador del proceso de enseñanza y consecuentemente de aprendizaje. Este  software por lo general tiene particularidades tales como facilidad de uso, interactividad y  posibilidad de adaptación a ciertas características de los alumnos [8].  Por lo expuesto anteriormente, este trabajo busca incorporar en los alumnos conocimientos de una  manera más atractiva para ellos y que a la vez familiar por intermedio de un Software Educativo.  En [11] se clasifica el software educativo en: i) Software de Ejercitación; ii) Software Tutorial; iii)  Software de Simulación; iv) Software de Juegos Instruccionales; v) Software Constructivo o  Micromundos.    La asignatura Modelos y Simulación, objeto de estudio del presente trabajo, pertenece al Plan de  estudios de la Carrera de Licenciatura en Sistemas de Información, de la Facultad de Ciencias  Exactas y Naturales y Agrimensura (Universidad Nacional del Nordeste (FACENA - UNNE). Esta  asignatura nació con la puesta en marcha de la Carrera de Licenciatura en Sistemas (Plan anterior)  en el año 1988, y tuvo siempre el carácter de optativa.  En Mariño y López [4] y Mariño y López  [5] se mencionan las estrategias didácticas aplicadas con el objeto de cumplimentar la planificación  prevista. En Mariño y López [6] (2008) se sintetizan las acciones de docencia, investigación y  extensión proyectadas y en ejecución en el marco de la cátedra. Este trabajo se encuadra en ella.    La simulación de un sistema es la operación de un modelo, el cual es una representación del  sistema. Este modelo puede sujetarse a manipulaciones que en la realidad serían imposibles de  realizar, demasiado costosas o imprácticas. Un paso clave en simulación consiste en disponer de  rutinas generadoras de variables aleatorias con distribuciones especificas: exponencial, normal, etc.  Este procedimiento es realizado en dos fases. La primera consiste en generar una secuencia de  números pseudoaleatorios distribuidos uniformemente entre 0 y 1. Luego esta secuencia es  transformada para obtener los valores aleatorios de las distribuciones deseadas. Por tanto, el  principal énfasis en pruebas estadísticas deberá centrarse en  el generador de números  pseudoaleatorios, ya que cualquier deficiencia estadística en la distribución de la variable aleatoria  no uniforme, se deberá exclusivamente a la utilización de un deficiente generador de números  pseudoaleatorios.  El objetivo general de este trabajo es desarrollar un software educativo implementando con  tecnología hipermedia, para transmitir los conceptos de los diferentes métodos de generación  de series de números pseudoaleatorios y pruebas estadísticas para la verificación de los  mismos. Asimismo, se incluyen animaciones digitales de los diagramas de flujo de los  procedimientos correspondientes.  El trabajo se organiza en cuatro secciones. En esta primera sección se sintetizó el estado del arte. La  segunda sección resume la metodología adoptada en la elaboración del software educativo. La  tercera sección describe las funcionalidades del producto. Finalmente, se mencionan los resultados  concretados y las líneas de trabajo futuras.    2 METODOLOGÍA    En esta sección se resume la metodología aplicada en la construcción del material instruccional  interactivo desarrollado en éste trabajo, consistió en las siguientes etapas:   Estudio de factibilidad. Consiste en una estimación de recursos necesarios y escenarios  posibles. Permite establecer claramente los límites del entorno virtual y su integración con otros  entornos similares aplicables en la asignatura.  Primeramente como paso fundamental y previo a  la etapa de selección de la herramienta se debió observar cuales eran las necesidades del sistema  y que aplicabilidad tendría para luego acotar más el espectro que definiría los posibles lenguajes  o herramientas que serian utilizadas a tal efecto. Como se vio en los informes previamente  elaborados y presentados, las necesidades requeridas por el sistema a desarrollar son de tipo  educativo con el objetivo de desarrollar un complemento para la asignatura Modelos y  Simulación.    Definición de los destinatarios. Al diseñar un software un interrogante muy importante que se  debe realizar es: ¿Quiénes utilizarán el software que se va a diseñar?. Los destinatarios de este  Software interactivo son los alumnos de la Asignatura Modelos y Simulación de la carrera de  Licenciatura en Sistemas de Información de la Facultad de Ciencias Exactas y Naturales y  Agrimensura- UNNE. Realizada la delimitación geográfica, se puede decir que el software  podrá ser utilizado en los laboratorios de la institución como así también en los domicilios de  los alumnos, convirtiéndose de esta manera en una herramienta de apoyo fuera del horario del  cursado de la asignatura.    Identificación de los requerimientos. En esta etapa de la construcción de los materiales  instruccionales interactivos se estableció de manera clara y precisa el conjunto de requisitos que  debe satisfacer SIANP. Desde el punto de vista del rendimiento, SIANP debe generar extensas  series de números pseudoaleatorios en lapsos muy breves de tiempo, ya que uno de los objetivos  de este trabajo es evitar lo tediosa e impráctica que se puede volver la generación de series de  números mediante cálculo manual. Para brindar una visión más clarificadora de los  requerimientos del sistema se recurrió a técnicas de modelado UML (Unified Modeling  Language) es el sucesor de la oleada de métodos de análisis y diseño orientados a objetos  (OOA&D) que surgió a finales de la década de 1980 y principios de la siguiente. El UML  unifica, sobre todo, los métodos de Booch, Rumbaugh (OMT) y Jacobson [3]. Se utilizó el  Diagrama de Casos de Uso (Fig. 1), para representar los requerimientos del sistema. Este  diagrama muestra la relación entre los actores y los casos de uso del sistema. Representa la  funcionalidad que ofrece el sistema en lo que se refiere a su interacción externa. A continuación  se sintetizan los casos de uso identificados:    Caso de uso: Generar flujograma. Se inicia cuando el usuario selecciona el método de  generación de números pseudoaleatorios, luego el sistema le brinda la posibilidad de  introducir los parámetros correspondientes el método seleccionado; en caso de que los  parámetros ingresados sean correctos el SIANP genera de una forma muy intuitiva y  progresiva el diagrama de flujos del método elegido. El caso excepcional de este caso de uso  ocurre cuando el usuario ingresa parámetros incorrectos ante lo cual se solicita nuevamente  el ingreso de los parámetros.   Caso de Uso: Mostrar serie. Extiende el caso de uso Generar flujograma; y se inicia con la  finalización de este último, permitiendo al alumno (actor) seleccionar el formato de salida y  ver la serie de números pseudoaleatorios generada mediante el método elegido y con los  parámetros ingresados.    Caso de Uso: Probar Serie. Extiende el caso de uso Mostrar serie; y se inicia con la  finalización de este último, permite al alumno elegir el tipo de prueba a la que someterá a la  serie obtenida y observar el comportamiento estadístico de la misma.   Caso de Uso: Mostrar instructivo. Se inicia cuando el alumno selecciona la opción  “Conocer el  SIANP” y permite ver al usuario el funcionamiento y las diferentes  posibilidades que le brinda este software educativo.   Caso de Uso: Mostrar conceptos. Se inicia cuando el alumno selecciona la opción “Material  de la cátedra Modelos y Simulación”; permitiendo al usuario acceder al contenido de la cátedra.  Figura 1. Diagrama de casos de uso del SIANP     Definición de la arquitectura general o infraestructura. Desde el punto de vista de la  arquitectura o infraestructura sobre la cual se ejecuta el SIANP, éste requiere para ser utilizado  una computadora con sistema operativo Microsoft Windows y Flash player instalado. La  versión de este último debe ser 8 o superior ya que el SIANP fue construido con Adobe  Macromedia 8.   Selección del medio de distribución. Teniendo en cuenta las características del SIANP,  respecto de la forma de ejecución y tamaño, a la hora de decidir el medio por el cual será  distribuido se optó por el CD-ROM.   Análisis del entorno virtual. Luego de realizar el estudio de los aspectos fundamentales del  software educativo, se logró una visión mas clara del entorno que debe presentar el SIANP.   Diseño del entorno virtual. Se contemplaron características como: i) Interactividad, ii)  Integración de contenidos en múltiples formatos, iii) Definición del objetivo de implementación.  En el diseño de las interfaces se consideraron la navegabilidad, accesibilidad y comunicación,  y  su especificación en el diseño y desarrollo de entornos virtuales de enseñanza-aprendizaje.   Selección y evaluación de herramientas. El análisis del software que se llevó a cabo permitió  obtener una visión mas concreta de las funcionalidades y características más importantes que  permitieron llegar hasta el objetivo de este trabajo que es lograr una visión más sencilla y  práctica de las mencionadas técnicas de modelado y simulación. Se evaluaron distintas  herramientas, seleccionándose las que en combinación cumplimentaron los requisitos para el  desarrollo del software. Se eligieron el lenguaje HTML, la variante dinámica del anterior  DHTML, un lenguaje de creación de scripts, un organizador de estilos Web y dos herramientas  de creación de animaciones cada uno con características particulares. Estas herramientas  constituyen una plataforma que garantiza mínimamente la homogeneidad y el funcionamiento  en diversos equipos y no requieren grandes recursos de hardware.   Selección y preparación de contenidos.  Los contenidos incorporados al entorno virtual tienen  como finalidad facilitar y/o complementar el desarrollo de las clases presenciales. Los  contenidos específicos de la asignatura: generadores de números aleatorios y pruebas  estadísticas codificadas se basaron en textos disciplinares. En el primer caso se seleccionaron  los siguientes generadores: i) Métodos de los Cuadrados Centrales de Von Neumann ([2], [7] y  [10]), ii) Método de Fibonacci ([2], [7]  y [10]), iii) Método de las congruencias ([2], [7] y [10]).  Se programaron las siguientes pruebas estadísticas: i) Prueba de Chi Cuadrado X2  [10], ii)  prueba de series ([10]), iii) Prueba de distancia [10].   Desarrollo del entorno virtual. Se desarrolló una versión preliminar, orientada a comunicar la  visión esperada en el producto final. Se realizaron las siguientes tareas: i) Diseño de las  interfases, ii) Desarrollo de las interfaces, iii) Definición de funcionalidades del entorno [13] y  [14].   Integración de contenidos. Consistió en la incorporación de los contenidos y elementos en las  interfaces desarrolladas.   Validaciones. Finalizado el desarrollo, se verificó el correcto funcionamiento y el acceso a los  contenidos. Con respecto al funcionamiento se comprobó: i) Mapa de navegación. Buena  estructuración del mismo que permite acceder bien a los contenidos, actividades, niveles y  prestaciones en general. ii) Sistema de navegación. Entorno transparente que permite al usuario  tener el control. iii) La velocidad entre el usuario y el programa (animaciones, lectura de  datos…) resulta adecuada. iv) Ejecución de los programas incluidos para actuar como  simuladores de lo problemas abordados.    3. PRESENTACION DEL SOFTWARE. SIANP    Este software educativo se caracteriza por su modularidad. Se disponen de seis procedimientos de   generadores de números pseudoaleatorios y de tres pruebas de bondad de ajuste.  Este software contempla las opciones de: i) Seleccionar métodos para generar series  pseudoaleatorias y aplicar tests de hipótesis a las mismas; ii) Conocer las diferentes funcionalidades  del software; iii) Acceder a material de la cátedra de Modelos y Simulación.   Al elegir la metodología de generación de la serie aleatoria, el sistema solicita el ingreso de los  parámetros en un entorno gráfico muy amigable e intuitivo, luego de lo cual se presenta el diagrama  de flujo correspondiente al método seleccionado. Al cabo de esto, se puede también ver la serie  generada y corroborar si ésta presenta o no fenómenos de recurrencia u otro fenómeno que impida a  la serie ser considerada como aleatoria mediante alguna prueba de hipótesis.   En la Figura 2 se ilustra la interfaz de acceso. La figura 3 muestra dos interfaces referentes a la  generación de números pseudoaleatorios aplicando el método de los cuadrados centrales de Von  Neumann. Como se puede observar, al iniciar el procedimiento el software solicita el ingreso de los  parámetros requeridos. Luego de especificados éstos, se da inicio a una simulación que ilustra el  diagrama de flujo que se presenta. Si se presiona en el símbolo de Resultados, es posible visualizar  la serie de números generada en función a los valores de los parámetros especificados y  al  generador seleccionado (Fig. 4). Asimismo, se dispone de la opción de aplicar las pruebas  estadísticas de Chi Cuadrado, de Series y de las Distancias en cada generador programado (Fig. 5).   El software no sólo permite visualizar dinámicamente la generación de series de números  pseudoaleatorios, sino que además posibilita que el alumno realica prácticas interactivas y  comparativas orientadas a seleccionar el generador más óptimo.   Figura 2. Interfaz inicial del software SEMYS    Figura 3. Interfaz inicial y final al seleccionar un método generador de números pseudoaleatorios    Figura 4. Visualización de una serie generada     Figura 5. Interfaces de pruebas estadísticas   Desde el punto de vista funcional, el SIANP permite al alumno: i) Repasar conceptos  fundamentales de la asignatura, tendientes a facilitar el uso de la herramienta. ii) Seleccionar  cualquiera de los métodos de generación de números pseudoaleatorios que se tratan en el cursado de  la asignatura. iii) Ingresar parámetros del método seleccionado. iv) Ver en forma gráfica y dinámica  la construcción del flujograma correspondiente al método seleccionado, el cual puede modificarse  según los parámetros ingresados por el alumno. v) Mostrar la serie de números generada por el  método. vi) Aplicar sobre la serie de números obtenida la prueba de hipótesis que el alumno crea  conveniente.  4. CONCLUSIONES    Se propicia un ámbito de formación continua en temas específicos de la asignatura, aplicación de  las tecnologías de la información y comunicación plasmadas en innovaciones pedagógicas  (alternativas complementarias para acompañar el proceso de enseñaza), elaboración de materiales  didácticos en diversos formatos e integración de temas abordados en la asignatura con otras  disciplinas, otros dominios del conocimiento y/o la práctica profesional.  Se tiene previsto la implementación de este producto en el próximo dictado de la asignatura.  Asimismo, siguiendo la política institucional de la Universidad y la Facultad de promover el acceso  y el desarrollo de cátedras desde la plataforma UNNE-Virtual, se prevé incorporar este recurso  didáctico como una herramienta más disponible desde el espacio virtual asignado a la cátedra  Modelos y Simulación."	﻿modelos y simulación , educación superior , software instruccional , formación de recursos humanos	es	21989
21	Modelo digital para facilitar el logro de competencias cognitivas de orden superior mediante la creación de zonas de desarrollo próximo	"﻿   Los cambios que afectan a las instituciones educativas universitarias ocurren en distintos órdenes y se  refieren a: cambios en el conocimiento (en la generación, gestión y distribución del mismo); cambios socioculturales (relacionados con la globalización); cambios propiciados por las TIC (por el rápido intercambio de  información) y los cambios en la forma de organizar la enseñanza universitaria (propiciados por los nuevos  enfoques de enseñanza y la colaboración informal a través de redes está incorporándose a estructuras sociales  más formales).   Todos ellos afectan directamente a la función que las universidades cumplen en la sociedad y ponen de  manifiesto la necesidad de revisar sus referentes actuales y promover experiencias innovadoras en el proceso  educativo. Así, la innovación, está relacionada con perspectivas de globalidad e implica variaciones en el  currículo, en las formas de ver y pensar las disciplinas, en las estrategias desplegadas, en la forma de  organizar y vincular cada disciplina con otra, etc. La aplicación de las TIC en acciones de formación bajo la  concepción de enseñanza flexible abre diversos frentes de cambio y renovación a considerar. En este sentido,  la propuesta de este trabajo consiste en el diseño de un modelo digital de enseñanza y aprendizaje estratégico  orientado a la adquisición de competencias de orden superior y soportado por recursos digitales que faciliten   el aprendizaje autónomo, flexibilidad e interacción entre estudiante-docente-recursos promoviendo, de esta  manera, las formas de aprender y construir el conocimiento conjuntamente.     Palabras claves: Competencias cognitivas de orden superior, Zona de Desarrollo Próximo, Aprendizaje  autónomo, Aprendizaje colaborativo      ABSTRACT    The changes which affect all educational university institutions happen in diverse order and are referred to:  knowledge changes (in its generation, management and distribution); socio-cultural changes (related to  globalization); changes favored by IT (because of the rapid exchange of information) and changes in the way  to organize university learning (favored by new approaches of learning and the free collaboration through  networks within more formal social structures).  All these directly affect the role that the university fulfills in the society and highlight the necessity for  reviewing their present actors and for promoting new types of experiences in the learning process. Thus,  innovation is related to globalization perspectives and implies variations in the curricula, in the way we  consider and think about the disciplines, in the used strategies, in the way of connecting and organizing each  discipline with respect to the others, etc. The learning activities application in an IT environment, on a  flexible approach of the learning processes, opens diverse lines of change and renovation. In that sense, the  aim of this paper is the design of a digital and strategic model of the learning process aiming to the  acquisition of higher level skills supported by digital resources to help the autonomous learning, the  flexibility and interaction between student-instructor-resources, thus, improving the learning methods and the  construction of a collaborative learning.    Keywords: Higher level cognitive competences, next development zone, autonomous learning, collaborative  learning.    1. INTRODUCCIÓN    El avance científico tecnológico y en particular el de las Tecnologías de la Información y la Comunicación  (TICs) han revolucionado todos los ámbitos de la vida del hombre y han generado la aparición de nuevos  modelos económicos, nuevos modos de comunicación, nuevas formas de funcionamiento de la política,  estado y sociedad, en definitiva una nueva cultura. [3,11,12]  Como parte de la cultura, la educación no quedó ajena a estos cambios. Y a pesar de su fuerte carácter  conservador, gradualmente las TICs han producido un movimiento de transformación que adquiere  características relevantes tanto en el área de gestión administrativa como en el área de gestión académica.  En este marco, las nuevas demandas de la sociedad en materia de conocimiento requieren contar con  profesionales con pensamiento abstracto, lógico y multivariable que sepan responder a las necesidades  actuales. [6,7]  La sociedad del conocimiento en el que nos encontramos inmersos exige y plantea nuevos retos al sistema  educativo, que implican nuevos métodos de trabajo y enseñanza que contribuyan a la formación integral de  los estudiantes y la adquisición de las competencias que les permitan desenvolverse de manera eficaz y  eficiente. De esta afirmación se deduce las connotaciones que adquiere el aprendizaje en su fuerte vínculo  social, pues cada individuo otorga un significado a lo que percibe de acuerdo a su matriz de aprendizaje.   Las instituciones de educación superior se destacan porque juegan un papel relevante por su tradición,  finalidades y funciones en la construcción de la sociedad siempre vinculadas con la producción, transferencia  y difusión del conocimiento respondiendo a los cambios sociales y tecnológicos actuales.   Por esto, la  introducción de las tecnologías de la información y de la comunicación en la universidad ha  generado profundas implicancias en términos de modelos de gestión administrativa y también gestión  curricular.  Por otra parte, se tiene en cuenta que el uso de las nuevas tecnologías en el proceso de enseñanza y de  aprendizaje requiere un cambio pedagógico: la redefinición de las metas de enseñanza y el replanteo de  actividades y estrategias de aprendizaje innovadoras orientadas a mejorar las competencias de los estudiantes  y la calidad educativa [5,11,14,15]. Además teniendo en consideración las posibilidades y potencialidades de  las TICs, tales como el acceso a la información sin condicionantes de tiempo, espacio o cantidad, se requiere  un cambio en el rol de los agentes del proceso educativo. La función del docente, cambia de transmisor de  información a orientador, mediador, facilitador, guía y tutor. A su vez, el rol del alumno se desplaza de  receptor pasivo a agente activo de su propia formación.  Respecto a la docencia universitaria las nuevas competencias tienden a generar procesos de aprendizaje  donde el estudiante se convierte en el constructor de su propio conocimiento, a partir de recursos de  información disponibles, de nuevas estrategias y enfoques de trabajo. Tales cambios afectan también a la  organización de la enseñanza para adaptarse a los modelos de formación mas centrados en el estudiante y en  su trabajo [4,10] utilizando la variable pedagógica ZDP para fundamentar la potencialidad educativa de los  recursos tecnológicos al promover la condición social del aprendizaje [16]. Es decir, que el modelo que se  propone puede influir en la creación de competencias en la medida en que desarrolle operaciones y  habilidades cognoscitivas que se generen a través de la interacción social.  En función a lo expuesto, se propone el desarrollo de un modelo digital pedagógico-didáctico que de soporte  al proceso de enseñanza-aprendizaje con características de flexibilidad para adaptarse a las necesidades del  estudiante superando barreras de tiempo y espacio, y mejorando la comunicación y la interacción de  docentes-alumnos-recursos. [2,8,11,14]  Se propone una estrategia de aprendizaje basada en el uso didáctico de la tecnología web sostenida por  diferentes técnicas fundamentadas en los andamios de recepción, transformación y construcción para  favorecer el desarrollo de habilidades cognitivas superiores, actitudes, valores y conocimientos en los  estudiantes.  El trabajo se organiza como sigue: en la sección 2 se presenta la propuesta completa y se describe el modelo  conceptual, tecnológico y funcional; en la sección 3 se presenta el diseño de implementación del modelo en  un marco concreto de enseñanza. Por último, en la sección 4 se presentan las conclusiones preliminares y las  proyecciones futuras.        2. DESCRIPCIÓN DE LA PROPUESTA    El modelo propuesto se constituye en una herramienta estratégica orientada por una parte, al docente al cual   le provee la posibilidad de mejorar la enseñanza incorporando nuevas estrategias didácticas para favorecer la  creación de ZDP y con ella el andamio de apoyo al proceso educativo. Por otra, al estudiante le facilita la  formación de competencias orientadas al estudio independiente, autónomo, flexible, colaborativo e  interactivo que lo potencia, cambiando su forma de aprender y construir el conocimiento.  En síntesis se pretende innovar la educación a través de nuevas metodologías de acción, formación  seguimiento y evaluación educativas basada en las TICs.    2.1. Modelo Conceptual    Para definir el modelo conceptual de enseñanza- aprendizaje se han considerado tres dimensiones:  disciplinar, pedagógica-didáctica y tecnológica. Estas tres dimensiones se retroalimentan en una  multiplicidad de relaciones transversales y complejas en función del aprendizaje de los estudiantes.  • Dimensión disciplinar. Se define como un conjunto móvil de conceptos, ámbitos, procesos  epistemológicos, teorías y confrontaciones entre éstas, reglas de acción susceptibles de ser descritas  teóricamente y procedimientos específicos que corresponden a un área determinada [14]. Forman parte de  esta dimensión los conocimientos básicos, los métodos de trabajo y validación aceptados, los lenguajes  especializados, las creencias y las formas de autoridad compartidas por los miembros de una determinada  comunidad académica o profesional. Resulta prioritario el reconocimiento previo de los componentes de la  dimensión disciplinar y el análisis detallado de los elementos fundamentales requeridos para realizar con  éxito las operaciones que corresponden a la educación formal, en este caso a la educación superior y a los  objetivos de la formación profesional.  Siguiendo la propuesta de [14], para una adecuada articulación con las demás dimensiones que componen  el modelo, es preciso que el docente tenga diferentes competencias tales como, conocer integralmente y  sistemáticamente la disciplina, su concepción epistemológica, referentes teóricos y científicos que la  caracterizan y otras complementarias como capacidad para trabajar en equipo, una actitud cooperativa con  los demás profesionales y flexibilidad entendida ésta como la apertura en cuanto a la búsqueda de modelos,  métodos de trabajo y técnicas.  • Dimensión pedagógica-didáctica. Se definen en esta dimensión los principales conceptos relacionados   con el proceso educativo centrado en el estudiante.  Actualmente existe una tendencia creciente hacia el constructivismo y el aprendizaje significativo. El  aprendizaje se concibe como la reconstrucción de los esquemas de conocimiento del sujeto a partir de las  experiencias que éste tiene con los objetos (interactividad) y con las personas (intersubjetividad), en  situaciones de interacción que sean significativas de acuerdo con su nivel de desarrollo y los contextos  sociales que le dan sentido.   Un aprendizaje significativo implica una re-estructuración activa de las percepciones, ideas, conceptos y  esquemas que el alumno posee en su estructura cognitiva. En este sentido, el aprendizaje no es una simple  asimilación pasiva de información literal, sino que ello implica reconocer al alumno como sujeto  procesador activo de la información, y el aprendizaje como un fenómeno complejo, sistémico y  organizado, que va más allá de las asociaciones memorísticas.  Desde el constructivismo se reconoce al sujeto como una construcción propia que se va generando como  resultado de la interacción de sus disposiciones internas y su medio ambiente; por lo tanto, su  conocimiento no es una copia de la realidad, sino una construcción que él mismo hace [1].   Por tanto, el constructivismo no se centra en el resultado del aprendizaje, sino en el proceso de  construcción del conocimiento. Para conseguir un aprendizaje significativo se requiere que los alumnos  operen activamente la información a ser aprehendida, pensando y actuando sobre ella para revisarla,  ampliarla y asimilarla.  En una sociedad tan evolucionada como la actual, donde las nuevas tecnologías han impactado  fuertemente, se requiere hacer referencia al aprendizaje innovador definido por [9] desde el ""paradigma de  la complejidad"", como el instrumento para salvar la distancia que media entre la creciente complejidad del  mundo y la capacidad del hombre para hacerle frente. Los rasgos más destacados de este concepto de  aprendizaje es que sea anticipador y participativo.  Por otra parte, el aprendizaje autónomo se apoya en una concepción abierta e interactiva y sitúa a cada  alumno, equipos de alumnos y clase como protagonistas de su aprendizaje. El aprendizaje autónomo   requiere de una concepción de enseñanza que promueva: 1) la construcción de concepciones, esquemas  mentales y estilos docentes abiertos al fomento de la autonomía real de los alumnos; 2) currículum sensible  a la exigencia de autonomía en el aprendizaje; 3) la adecuación de la práctica de enseñanza a las  singularidades de autoaprendizaje de cada alumno y, 4) implementación de acción que implique la  creación de un ambiente educativo sociocomunicativo propiciador de la autonomía de aprendizaje.  Para favorecer la coexistencia de los tipos de aprendizaje mencionados, es necesario crear ambientes  educativos que propicien condiciones favorables de aprendizaje para que los participantes desarrollen  competencias necesarias para responder a los requerimientos actuales de la sociedad [13]. El desarrollo de  competencias lleva a la adquisición de capacidades, habilidades, actitudes y disposiciones en una  perspectiva abierta, anticipadora y participativa, con una visión de futuro, donde cada cual es consciente  del valor de sus aprendizajes y logros en su aplicación concreta.  Martínez [7] señala que es fundamental que los alumnos desarrollen su capacidad de navegar por la  información y de reconstruir la información, en definitiva, “conocimientos y habilidades que hagan posible  la navegación significativa y la construcción de su propia aproximación al conocimiento”.   Por su parte, Vigotsky precisa que dentro de la construcción de las competencias de los estudiantes, la  interacción grupal en equipo y el trabajo colaborativo constituye el motor de desarrollo, siendo que el  aprendizaje es anterior al desarrollo y por tanto, el primero motiva al segundo, despertando una serie de  procesos evolutivos internos capaces de operar cuando el sujeto esta en interacción con su entorno y en  cooperación con sus semejantes.  Con respecto, en esta dimensión se relaciona el trabajo colaborativo con la ZDP de Vigotsky. La ZDP se  define como el espacio entre la capacidad autónoma del alumno y lo que puede realizar mediante apoyos  específicos, por tanto el transito por esa zona deberá contar con la ayuda del profesor y los demás  integrantes del grupo.   Cuando los alumnos trabajan colaborativamente deben negociar las metas, la representación del problema,  como así también el significado de conceptos y los procedimientos involucrados, sin dejar de hacer  explícitos su conocimiento y su pensamiento. Para establecer la comprensión común, los alumnos deben  intercambiar argumentos y negociar significados. En la acción comparativa entre sus propias ideas y la  elaboración de las afirmaciones del otro, los significados pueden modificarse, refinarse o extenderse. Esto  es válido tanto cuando las interacciones se producen entre pares, como cuando interviene la acción  reguladora del docente. El “terreno conceptual” donde ocurren esas interacciones es la ZDP.   Por último, la zona de desarrollo próximo no es algo “preexistente”, sino que es creada en la interacción  entre el estudiante y los coparticipantes de la actividad, incluyendo las herramientas disponibles y las  prácticas seleccionadas; depende de la naturaleza y de la calidad de esa interacción qué tan alto es el límite  superior de la capacidad del aprendiz.   Vigotsky afirma que la significación de la zona, es que ella determina las cotas superior e inferior en la  cual debería ser ubicada la enseñanza, destacando un aspecto esencial de esta interacción: que los  participantes menos capacitados pueden participar en formas de interacción que están más allá de sus  competencias cuando actúan en soledad [16]. Bajo esta concepción, la ZDP no es un atributo del estudiante  aislado, sino del estudiante en relación con la especificidad de un ambiente particular de actividad.  • Dimensión tecnológica. Actualmente las nuevas tecnologías de la información y la comunicación son  imprescindibles para el desarrollo de la educación, desde el punto de vista que pueden facilitar el logro de  metas en el aprendizaje, fortalecen la interacción y la comunicación entre los actores del proceso,   enriquecen el ambiente de trabajo y el desarrollo educativo de los estudiantes. Estas nuevas opciones  tecnológicas deben considerarse como herramientas de apoyo al proceso de enseñanza-aprendizaje y no  como un sustituto del mismo. La plataforma tecnológica que permita la implementación de un modelo de  enseñanza-aprendizaje debe poseer ciertas características. Por un lado están las de carácter netamente  técnico (servidores, redes, cableados, modems, etc.) y por otro los relacionados directamente con la  modalidad elegida para la enseñanza y se entrelazan con las demás dimensiones (posibilidades de  intercomunicación, interacción, formato, etc.).   Algunos de los elementos más importantes a ser considerados en esta dimensión y que tienen estrecha  relación con el diseño de un modelo de enseñanza aprendizaje son:   9 Interacción: referida a la comunicación e intercambio de significados mediante mensajes entre los  alumnos, el profesor, expertos en la materia, otros estudiantes geográficamente y temporalmente  distantes y con los contenidos de los cursos.  9 Flexibilidad: es la capacidad de respuesta y flexibilidad que debe tener el modelo educativo frente a las  distintas necesidades de los estudiantes, y adaptación a la situación personal, compromisos laborales,  familiares y sociales de cada uno, a través de la nueva dimensión espacio temporal que deviene de la  virtualidad.  9 Colaboración: orientado a crear un ambiente colaborativo de trabajo, donde se provea las herramientas  de intercambio, colaboración y coordinación necesarias entre los participantes, sobre la base de una  estrategia pedagógica conducente a incentivar y desarrollar competencias para trabajar en equipo.  9 Accesibilidad: de acuerdo a los principios de diseño universal, es la condición que posibilita entrar,  salir y utilizar los distintos elementos de la plataforma tecnológica permitiendo a los estudiantes  desenvolverse equitativamente y lo más independientemente posible.  9 Usabilidad: relacionada con la facilidad de aprendizaje, eficiencia en el uso, facilidad de memorizar,  tolerante a errores y subjetivamente satisfactorio.    2.2. Modelo Tecnológico    El modelo propuesto será implementado en una plataforma tecnológica MOODLE (Modular ObjectOriented Dynamic Learning, Entorno de Aprendizaje Dinámico Modular Orientado a Objetos).  Se selecciona moodle porque sirve de soporte del proceso educativo y por su  potencialidad para el desarrollo  de modelos pedagógicos-didácticos que fortalezcan la educación presencial.  El uso de moodle ha permitido integrar diferentes herramientas para la comunicación (foros, chats, correo  electrónico); para el aprendizaje autónomo (tareas, consultas, lección, cuestionarios, diario) y para el trabajo  colaborativo en equipo (wikis, blogs, glosarios).    2.3. Modelo Funcional     El modelo tecnológico implementado en moodle está organizado en ocho módulos claramente diferenciados  de acuerdo a las funciones y operaciones proporcionados por cada uno separadamente. En este apartado se  describe las funcionalidades de cada uno de los módulos Presentación, Anuncios, Recursos, Gestión,  Contenidos, Actividades, Evaluación y Comunicación. La representación global del modelo se presenta en la  figura 1, donde se indican los módulos desarrollados y las flechas representan las posibilidades de  interacción entre los mismos.    b HERRAMIENTAS DE  COMUNICACIÓN  DESARROLLO DE  ACTIVIDADES  PRÁCTICAS HERRAMIENTAS  DE GESTIÓN  NOVEDADES  5 7 4 2 3 CONTENIDOS  EDUCATIVOS  6 EVALUACIÓN 8 RECURSOS   1 PRESENTACIÓN                         Figura 1. Representación del modelo funcional   9 Módulo presentación (1): en este módulo se describe en forma general las características del modelo, los  objetivos y alcances del mismo. Este módulo se implementa como la sección de entrada en el modelo.  9 Novedades (2): se utiliza para comunicar o informar a los estudiantes sobre la realización de actividades  propias de la asignatura, divulgación eventos y para el intercambio de ideas y noticias entre los  participantes.   9 Contenidos educativos (3): en este se proporcionan los elementos que representan los contenidos  materiales de la asignatura a desarrollar. Son los distintos tipos de textos, libros, apuntes, presentaciones de  diapositivas para que los alumnos estudien sobre ellos. Otros recursos para mostrar los contenidos  temáticos son los enlaces a páginas web y a distintos documentos (en formato Word o PDF).   Se pueden implementar recursos como libros y lecciones. Un libro permite agrupar contenido textual que  puede ser organizado en capítulos y subcapítulos y cuenta con un índice que posibilita al estudiante  navegar por el contenido. La lección, además de mostrar contenidos, incluye, al final de cada sección,  preguntas de comprobación. Las respuestas a las mismas determinan las posibilidades del estudiante para  continuar con la lección.   En este modelo se implementa como recurso didáctico la lección, por la posibilidad del vincular contenidos  con actividades evaluativos, de manera que el docente guié el aprendizaje en función de las respuestas  obtenidas.  También se usa en este modelo, para la administración de contenidos, los glosarios y wikis. El glosario es  una herramienta útil donde se encuentra el significado de términos y conceptos que pueden ser accedidos  por el estudiante de manera explícita e implícita dentro del texto del curso. Por su parte, un wiki es una  herramienta que permite a los participantes subir comentarios, conceptos, ideas que son accesibles a todos.  Tanto en glosario como el wiki son utilizados como recursos didácticos de aprendizaje colaborativo.  9 Herramientas de comunicación (4): permite que los alumnos puedan comunicarse con el docente y entre  ellos, para construir su propia comunidad de aprendizaje.  Esta posibilidad incrementa la comunicación entre los participantes, saliendo del marco de las actividades  presenciales, sin estar sujetos a horario ni a la ubicación geográfica del participante.  Se implementa para el intercambio de opiniones y comunicación entre los participantes foro y chat. El foro  es utilizado para la discusión de tópicos y para la creación de actividades didácticas colaborativas. El chat  para la aclaración de dudas de manera rápida y la divulgación de noticias y eventos de la asignatura como  los exámenes, las consultas presenciales, la imposición o defensa de trabajos extraclase, entre otros.  9 Herramientas de gestión (5): en este módulo se agrupan las funciones realacionadas con la información  sobre la modalidad de trabajo en la asignatura: objetivos generales y específicos del curso, la programación  de contenidos estructurada por objetos de aprendizaje, actividades de enseñanza-aprendizaje, recursos  humanos y físicos, apoyo tutorial y metodología de evaluación.   Se incluye como recurso de planificación una agenda para describir la programación semanal de diferentes  actividades de aprendizaje.   En este módulo se proporciona un recurso para las inscripciones de los alumnos, y profesores participantes  en el curso y se registran los datos de cada uno para completar un perfil. Además se organizan los grupos  de trabajo, para lo cual se contempla la distribución de los alumnos y los roles con el fin de organizar la  participación en los trabajos y actividades grupales.   9 Seguimiento y evaluación (6): Se ha planificado una evaluación inicial de los alumnos, por medio de un  cuestionario, para relevar el conocimiento o experiencia que los mismos tienen sobe el uso de las  herramientas informáticas y plataformas multimediales similares a la propuesta, determinar su  capacitación, actitud y grado de utilización de las mismas.   En este módulo se recogen los trabajos elaborados por los alumnos en formato digital y que ellos mismo se  encargan de subir al sitio.   Además cuenta cada alumno con la posibilidad de acceder a un diario personal para redactar las  apreciaciones sobre las actividades propuestas y la modalidad de desarrollo. Este diario es revisado por el  docente de manera de recolectar información relevante para mejorar la propuesta.  Por último se incluye la función de consultas de las estadísticas de acceso a los recursos y a las actividades  del curso por cada usuario. De esta manera los docentes evalúan la participación y rendimiento de los  estudiantes que le sirve para orientar el trabajo particularizado a cada grupo de estudiantes.   9 Desarrollo de actividades prácticas (7): este módulo se constituye en un espacio activo y colaborativo  donde el alumno tiene que hacer algo más allá de meramente leer un texto. Se presentan las actividades  prácticas a realizar por parte de los alumnos o grupos de alumnos. Se consignan los debates y discusiones,  resolución de problemas propuestos, redacción de trabajos, etc.  Se implementa como estrategia de apoyo a la resolución de las diferentes actividades las webquests.  Cuando se realizan las diferentes actividades, el modelo proporciona a los estudiantes la posibilidad de  completar en forma colaborativa un glosario técnico que puede ser accedido por todos.   9 Recursos (8): se proporciona los recursos tecnológicos necesarios para la realización de las actividades  propuestas, software específico, de uso general, etc.  Algunas de las interfaces de la plataforma desarrollada se presentan en las figuras 1 y 2.       Figura 1. Interfaces del modelo tecnológico-funcional.      Figura 2. Interface presentación del modelo tecnológico-funcional.    3. IMPLEMENTACIÓN DEL MODELO     En este apartado se describe el procedimiento de implementación del modelo en un contexto educativo de  nivel universitario. Concretamente la experiencia se realiza en una asignatura de la carrera Licenciatura de  Sistemas de Información de la UNSE.  Para comprobar la efectividad del modelo desarrollado, se propone su aplicación para observar lo siguiente:  el nivel de desempeño académico de los estudiantes, según la valoración del profesor y de los propios  estudiantes; nivel de satisfacción de los estudiantes con respecto a la organización y funcionamiento del  modelo propuesto y la calidad y la efectividad de la herramienta como complemento de la modalidad  presencial.  Por último los resultados del experimento serán usados como feedback para mejorar las propiedades del  modelo y optimizar sus posibilidades educativas.      3.1. Definición de Objetivos     Con el uso del modelo se pretende alcanzar objetivos mediante el empleo de competencias básicas para  alcanzar el desarrollo de competencias de orden superior que impliquen poner en juego operaciones de  pensamiento complejo.  Los objetivos son definidos en base a criterios y sirven como parámetros de evaluación de logros alcanzados  al final de la experiencia. A continuación se describen los criterios y objetivos:   Con respecto al desempeño académico de los estudiantes.  1-Alcance de los objetivos conceptuales, procedimentales y actitudinales previstos;   Con relación a la satisfacción de los estudiantes con la organización y funcionamiento del modelo  propuesto.  2-Determinar el grado de adaptación de los estudiantes al trabajo en el modelo digital;  3-Conocer las interacciones entre los estudiantes y entre los estudiantes y el docente;   4-Determinar el nivel  de competencias grupales y colaborativas de los estudiantes    Orientados a valorar la calidad educativa del modelo como complemento de la modalidad  presencial.  5-Establecer la coherencia existente entre los diferentes elementos del proceso:necesidades de  aprendizaje, objetivos, contenidos, actividades, temporalización y sistema de evaluación;  6-Identificar las limitaciones pedagógicas, técnicas y tecnológicas del modelo.    3.2. Descripción de la Metodología Didáctica    La metodología de desarrollo se apoya en la corriente constructivista del aprendizaje, que propone que la  formación es una experiencia personal de construcción autónoma del conocimiento, influenciada por el  contexto social en el que ocurre el aprendizaje. Esta perspectiva enfatiza la formación basada en la  experimentación y la colaboración, para compensar los diferentes niveles de conocimientos y experiencia de  los participantes de un equipo, permitiendo que todos aprendan de todos. Además está centrada en el  estudiante, al entender que éste es el máximo responsable de su propia formación.  El trabajo en el ámbito de la asignatura Arquitectura del Computador es llevado a cabo en forma presencial  (en el aula) combinada con actividades no presenciales a través del uso de la plataforma educativa soportada  por la red. Contempla la realización de actividades presenciales individuales y grupales y tutorías online con  un facilitador. Se prevé una duración de tres semanas.  Se estructura en tres fases secuenciales e interconectadas por medio de los objetivos definidos:   1)- Presentación del modelo: el propósito de esta fase es dar a conocer a los estudiantes la plataforma  educativa desarrollada y el uso de los recursos de aprendizaje incluidos.   2)- Enseñanza de contenidos teóricos conceptuales: se prevé una clase presencial para exponer los conceptos  teóricos específicos y necesarios para el desarrollo de las actividades prácticas.   3)- Puesta en marcha del modelo tecnológico donde se desarrollan las actividades prácticas propuestas  mediante el uso de los recursos pedagógicos- didácticos previstos para tal fin.     3.3. Definición de Instrumentos de Evaluación    En esta etapa se definen los diferentes instrumentos de seguimiento y evaluación de los alumnos tanto  individualmente como su desempeño en el trabajo grupal asignado. Se han diseñado los siguientes  instrumentos de evaluación: Cuestionario para sondeo del conocimiento previo del estudiante y las  expectativas del mismo con respecto a la modalidad de trabajo;  resolución de problemas para la evaluación  parcial formativa individual del estudiante; planillas de seguimiento para valorar el grado de participación en  el trabajo de grupo; y cuestionario al final de la experiencia para recolectar las apreciaciones y opiniones con  respecto a la modalidad de trabajo implementada.  El diario personal que el estudiante completa a medida que se avanza en el curso sirve también como  referencia para mejorar la propuesta.  En la tabla 1, se presenta un esquema  que permite visualizar la correspondencia entre los principales   componentes del proceso educativo: objetivos, competencias, actividades y recursos previstos para los temas  específicos que se desarrollarán siguiendo el modelo propuesto.    Tabla 1. Correspondencia entre los elementos del proceso educativo  OBJETIVOS COMPETENCIAS ACTIVIDADES RECURSOS    • Alcance de los objetivos conceptuales,  procedimentales y actitudinales previstos;  - Comprender el concepto de  direccionamiento a memoria interna  - Diferenciar los modos de  direccionamiento  - Seleccionar el modo de direcciomiento  adecuado a cada situación problemática  - Uso de lenguaje de maquina  - Operar las estructuras de datos mediante  el direccionamiento adecuado   - Valorar la formación teórico conceptual  - Capacidad de análisis y síntesis  de información    - Capacidad de abstracción de  conceptos    - Habilidad para extrapolar  conocimientos teóricos a la  practica    - Capacidad para uso de  vocabulario técnico especifico  Documentos    Glosarios    Wikis    WebQuest        • Determinar el grado de adaptación de los  estudiantes al trabajo en el modelo digital;  - Actitud positiva ante el uso de  recursos digitales    - Capacidad de superar la  resistencia al uso de las  tecnologías    - Habilidad en el manejo de la  información  Diario  personal    Agenda    Correo  electrónico    • Conocer las interacciones entre los  participantes;   - Capacidad de trabajo en grupo    Foros        • Determinar el nivel  de competencias  grupales y colaborativas de los estudiantes     - Capacidad de colaboración entre  pares  Chat    • Establecer la coherencia existente entre los  diferentes elementos del diseño formativo:  necesidades de aprendizaje, objetivos,  contenidos, actividades, metodología,  temporalización y sistema de evaluación.    - Capacidad critica     - Capacidad de generar nuevas  ideas  Correo  electrónico  • Identificar las limitaciones pedagógicas,  técnicas y tecnológicas del modelo.            - Cuestionarios  - Desarrollo conceptual de  temas  - Diseño de programas que  resuelvan situaciones  especificas tales como  creación, lectura y  direccionamiento de  vectores y manejo de  pilas   - Elaboración de glosario  - Construcción de wikis  - Discusión en foro sobre  los objetos de estudio        4. CONCLUSIONES Y TRABAJO FUTURO    La implementación del modelo propuesto implica un cambio del proceso de enseñanza-aprendizaje, donde  los docentes desplazan su papel de difusores del conocimiento y transmisores de información a tutores,  facilitadores, mediadores del aprendizaje. Por otra parte los estudiantes, deben desarrollar competencias que  implican una actitud más activa y comprometida con su propio aprendizaje y del otro.  El modelo propuesto se orienta a alcanzar las siguientes metas:  9 Mejora de la interacción, socialización y comunicación en un ambiente de aprendizaje colaborativo y  participativo guiado y mediado por el docente, cuyo objetivo es la construcción colectiva de  conocimiento.   9 Desarrollo de competencias específicas en la formación universitaria mediante estrategias de aprendizaje  centrada en el estudiante y grupos de estudiantes.   9 Potenciación de las capacidades de los estudiantes para la gestión del conocimiento, en un entorno en  forma colaborativa, constructiva e intersubjetiva.  9 Aprovechamiento de los recursos digitales integrados en el proceso de enseñanza-aprendizaje para  mejorar la colaboración, el intercambio e interactividad comunicacional, el acceso al conocimiento y la  interactividad comunicacional y fortalecer así los resultados del trabajo académico.  El modelo será implementado en el contexto de enseñanza aprendizaje concreto descrito en trabajo con  estudiantes de las carreras afines a los Sistemas de Información e Informática para valorar sus posibilidades  educativas y dar respuesta a interrogantes tales como:   9 ¿El uso del modelo digital incrementa significativamente la capacidad de aprendizaje?,   9 ¿Puede ayudar a los docentes a guiar mejor a los alumnos para la construcción de su conocimiento?,   9 ¿Este tipo de prácticas predispone al trabajo grupal colaborativo en los estudiantes? y finalmente,  9 ¿El modelo digital constituye un andamio que propicia ZDP efectivas para el desarrollo de competencias  cognitivas de orden superior?."	﻿competencias cognitivas de orden superior , zona de desarrollo próximo , aprendizaje autónomo , higher level cognitive competences , next development zone , autonomous learning	es	22007
22	Evaluación en entorno virtual para el ingreso masivo universitario	"﻿   This paper describes the experience of a group of researchers convened by the National Agency for  Promoting Science, technology and innovation (ANPCyT) for the Project for Scientific and  Technological Research Oriented (PICTO) called ""Consolidation of a Red Cross teaching of  Sciences for further development, implementation and monitoring of innovative materials,  EGB3/Polimodal levels, higher university and non-formal education, aiming at improving  education, particularly, improving the educational practice of science education.   The group develops computer software with educational content relevant to the topic Computer  Problem-Solving. Recipients of the product are the initials of the students Degree in Systems  Analysis of the UNSA, which are evaluated through automated colloquia and communicate the  results of these evaluations through the website of the chair, mounted on the platform of education  Virtual Moodle.   This work includes aspects related to developing innovative educational materials and their impact  on students and teachers.    Resumen    Este trabajo relata la experiencia de un grupo de investigadores, convocados a través de la Agencia  Nacional de Promoción Científica, Tecnológica y de Innovación (ANPCyT) para el Proyecto de  Investigación Científica y Tecnológica Orientado (PICTO) denominado “Consolidación de una Red  Multidisciplinar de Enseñanza de las Ciencias para profundizar el desarrollo, la aplicación y el  seguimiento de materiales innovadores, en los niveles EGB3/Polimodal, superior universitario y en  la educación no formal, buscando la mejora de la enseñanza”, particularmente, la mejora de la  práctica educativa de la enseñanza de las ciencias.  El grupo de Informática desarrolla un software educativo con contenidos correspondientes al tema  Resolución de Problemas Computacionales. Los destinatarios del producto son los alumnos  iniciales de la Licenciatura en Análisis de Sistemas de la UNSa, los cuales son evaluados a través de  coloquios automatizados y comunican el resultado de dichas evaluaciones a través del sitio Web de  la cátedra, montado sobre la plataforma de enseñanza virtual Moodle.  El presente trabajo incluye aspectos relativos a la elaboración de los materiales educativos  innovadores y su impacto en alumnos y docentes.     Palabras clave:   NTICs – Nivel Superior – Software Educativo – Entornos virtuales – Resolución de problemas    1 INTRODUCCIÓN    Este trabajo relata la experiencia de un grupo de investigadores, convocados a través de la Agencia  Nacional de Promoción Científica, Tecnológica y de Innovación (ANPCyT) para el Proyecto de  Investigación Científica y Tecnológica Orientado (PICTO) denominado “Consolidación de una Red  Multidisciplinar de Enseñanza de las Ciencias para profundizar el desarrollo, la aplicación y el  seguimiento de materiales innovadores, en los niveles EGB3/Polimodal, superior universitario y en  la educación no formal, buscando la mejora de la enseñanza”, particularmente, la mejora de la  práctica educativa de la enseñanza de las ciencias. La educación hoy se ha visto superada por un  vertiginoso cambio que involucra las NTICs, lo cuál abre el debate sobre el valor social y educativo  de las mismas.    El subgrupo de Informática está integrado en su mayoría, por docentes investigadores de las  cátedras de primer año de la carrera Licenciatura en Análisis de Sistemas. Incluye también un  docente del área pedagógica y especialistas en recursos informáticos aplicados a la Educación, con  vasta experiencia en el desarrollo de software educativo. El equipo reconoce que la problemática de  una cátedra universitaria de ingreso masivo, es multicausal, parte de ella está asociada a la  asignación de escasos recursos humanos, con el consecuente impacto que la relación deficitaria  alumnos/docentes provoca en la calidad educativa. Concretamente, en la asignatura Elementos de  Computación (en adelante EC), primera materia de programación de la carrera, se viene trabajando  en el diseño de estrategias metodológicas que incorporen las NTICs para facilitar y mejorar los  aprendizajes de este numeroso alumnado. Si nos orientamos hacia una educación de calidad para  todos, debemos promover y desarrollar estrategias que contemplen la mayor participación del  estudiante, generando políticas inclusivas que contemplen ritmos y desempeños de aprendizaje  personales e individuales, desde las perspectivas de accesibilidad y aprovechamiento de los recursos  tecnológicos; sin perder de vista la integración de conocimientos disciplinares y sociales en la  práctica educativa.    Naturalmente, integrar esta experiencia a la vida universitaria nos orientó a analizar y reflexionar  sobre la adecuada integración de las NTICs en la práctica educativa universitaria, como una  herramienta más al servicio de objetivos educacionales preestablecidos. Entendemos que la  tecnología debe estar al servicio de la educación, como herramienta que contribuya a la acción  formativa, cuya principal función es la de alcanzar los objetivos propuestos. En este aspecto es que  reconocemos la función fundamental de la evaluación y nos planteamos el lugar que ocupa la  tecnología en esta instancia. Por otra parte, dependiendo de su funcionalidad, distinguimos otras  formas de evaluación aparte de aquella que permite acreditar los aprendizajes de los alumnos.  Interesan también aquellas, no menos importantes, dirigidas a evaluar la propia acción formativa y  consecuentemente, la de los materiales educativos desarrollados sobre un determinado soporte  tecnológico.     En este sentido, cabe hacernos la pregunta: ¿tiene nuestra sociedad local y en particular los  ingresantes a esta carrera universitaria la madurez suficiente como para ser partícipes activos de su  proceso de aprendizaje, atentos a que se requiere de compromiso, iniciativa, interacción de grupos,  trabajo interdisciplinario, trabajo colaborativo y fundamentalmente la modificación de su conducta  en lo que respecta al seguimiento y control reflexivo de su aprendizaje?, ¿los docentes se  comprometen, involucran y acompañan esta propuesta generando canales de participación y  andamiando el proceso de aprendizaje? Creemos que se requiere la modificación de las conductas  personales de todos los actores participantes del proceso de enseñanza y aprendizaje, dirigidas a  formar comportamientos responsables y comprometidos para lograr un aprendizaje eficaz.   1.1 Breve reseña histórica     Desde el año 2003 este equipo ha desarrollado e implementado diversos productos de software  ejercitativo como EC_Soft y SisNum, que abarcan algunos contenidos del programa. En los años  2006 y 2007 se aplicó un software denominado ColoquiosEC que permite al alumno auto evaluarse  en contenidos de Sistemas de Numeración y Algebra de Boole conforme a un calendario  preestablecido por la cátedra. ColoquiosEC surgió como un producto desarrollado por este grupo de  I+D denominado PI (Productos Interactivos).      En simultáneo a la incorporación de materiales educativos informatizados, EC implementó  evaluaciones rápidas o coloquios para medir semanalmente el nivel de los aprendizajes, lo cual  impactó favorablemente en los alumnos, quienes reconocieron su importancia en la definición del  nivel de avance alcanzado. Hasta el año 2005, estas instancias evaluativas se desarrollaban  exclusivamente en forma presencial, asignando para su resolución un breve intervalo de tiempo. La  incorporación de la aplicación ColoquiosEC, versión 1.0, se produce a partir del año 2006 y se  continúa su uso en el 2007.     1.2 Investigación y Desarrollo presente    En el año 2008 se concretó el mantenimiento perfectivo y adaptativo del software ColoquiosEC,  desarrollando su versión 2.0, la cual genera automáticamente un código de resolución de cada  coloquio rendido por el alumno, que se envía a la cátedra a través de la plataforma de enseñanza  virtual Moodle. Posteriormente, otra aplicación decodifica estos códigos de resolución y registra en  la base de datos el resultado obtenido por el alumno. Actualmente se trabaja en el guión multimedia  que permita incorporar un módulo de evaluación correspondiente al tema Resolución de Problemas  Computacionales, contenido central de EC, que corresponde a la unidad 6 del programa,  denominada Introducción a la Programación.     2. OBJETIVO DEL PICTO    Es importante señalar que el objetivo del PICTO es la mejora de la práctica educativa, por lo tanto  debemos orientar a docentes y alumnos a ser usuarios de este tipo de tecnologías, sin perder de vista  el significado social y educativo de las NTICs, en el debate educativo democrático contemporáneo.  El debate y reflexión de los que hacemos posible esta experiencia incluyó el análisis de las  competencias de los docentes para incorporar las NTICs como soporte didáctico, ya que la eficacia  de su utilización depende que se lo haga en los momentos oportunos y de la manera más adecuada.     Este trabajo incluye aspectos relativos a la elaboración de los materiales educativos innovadores y  el diseño de las actividades de seguimiento de los mismos, como así también su impacto en  alumnos, docentes y en la práctica educativa en general.     3. ESTRATEGIA DIDÁCTICA    3.1 Modalidad de enseñanza    La modalidad de enseñanza que hemos adoptado, corresponde a una extended learning o uso de  recursos virtuales como apoyo a las instancias de enseñanza y aprendizaje presencial. Esta  modalidad brinda al alumno la oportunidad de planificar y llevar a cabo su propio proceso de  aprendizaje en un entorno más creativo y menos mecanizado. A los docentes, por su parte, aplicar  metodologías más activas y menos expositivas, y orientar otras modalidades de evaluación a través  del seguimiento.    Particularmente, en la experiencia de EC, los alumnos acreditan conocimientos a través de dos  parciales y promocionan la asignatura aprobando un examen final, todos ellos bajo la modalidad de  evaluación presencial. Sin embargo, las evaluaciones cortas que provee ColoquiosEC previstos bajo  la modalidad a distancia, no están pensados como instancias de acreditación sino de participación.  El programa no brinda al alumno la posibilidad de recuperar el coloquio rendido. Esta decisión de  diseño se sustenta en la intención de modificar drásticamente el objetivo de estas evaluaciones  cortas y la percepción que el alumno tiene de ellas. Creemos que, abandonar la modalidad  tradicional de evaluación y reemplazarla por una evaluación virtual pone el acento en nuestros  principales objetivos, es decir, propiciar el trabajo autónomo, favorecer el entorno de la instancia  evaluativa y fortalecer el seguimiento exigiendo la participación y no solo la aprobación, de manera  que promueva en los alumnos el desarrollo de sus potencialidades de manera integral.    3.2 Evaluación en el entorno virtual    El esquema de la figura 1 explica la interrelación de los elementos que componen la triada básica:  docente, alumno y contenido, dentro del contexto virtual de evaluación de coloquios.         Figura 1: Esquema de interrelación virtual para la evaluación de coloquios    A continuación se describen brevemente los elementos constitutivos.     3.2.1 Docentes    Entendemos al docente como un sujeto enseñante que guía, orienta o conduce el proceso de  enseñanza. Los docentes constituyen uno de los elementos centrales de este proyecto, ya que de  ellos depende adaptarse a los cambios, generando actitudes favorables para la incorporación de las  DOCENTES  U1: Introducción  U5: Introducción a la Programación  U2: Sistemas de Numeración  U3: Arquitectura de la Computadora  U4: Álgebra de Boole  U6: Sistemas Operativos  CONTENIDOS  ALUMNOS  BD  FOROS  CALENDARIO  COLOQUIOS  OTRAS HERRAMIENTAS …  CLAVE DE ACCESO  TAREA  RESULTADOS  comunicación  archivo  PLATAFORMA MOODLE  actividad  NTICs. Necesitan estar formados en distintos teorías que actúen como sustento de base de procesos  epistemológicos y metodológicos que orientan y ordenan su práctica, aunque no de manera  exclusiva. La cátedra de EC está conformada por tres profesores, siete auxiliares docentes y tres  auxiliares alumnos que imparten clases teóricas, prácticas y de consulta bajo la modalidad  presencial. La mayoría de ellos se desempeñan también en los equipos de investigación del PICTO  y de un proyecto acreditado en el Consejo de Investigación de la Universidad Nacional de Salta  (CIUNSa). Los otros investigadores de ambos proyectos que son externos a la cátedra, tienen  acceso al entorno virtual de EC, sin permiso de edición.    3.2.2 Alumnos    Desde la perspectiva teórica son sujetos de aprendizaje, sujetos enseñados. La matrícula anual de  EC correspondiente a los últimos años es de aproximadamente setecientos alumnos. En el año 2008,  se registró una pre-inscripción a la carrera de 569 alumnos, de los cuales 366, es decir un 64%  iniciaron efectivamente el cursado de la asignatura. El desgranamiento observado desde el inicio de  clases hasta el momento de la primera instancia de evaluación presencial, redujo la cifra a 259  estudiantes, es decir, el 71%, confirmando valores históricos de deserción, cercanos al 30%. Al  finalizar el cursado obtuvieron la condición de alumno Regular el 30% de los estudiantes.    3.2.3 Contenidos    Es el saber disciplinar o ""saber enseñado"". El programa de EC incluye seis unidades, siendo  centrales las unidades 2, 4 y 5 que se desarrollan con una carga horaria presencial del 17%, 30% y  44% respectivamente. Es por ello que están resaltadas en el esquema de la figura 1, no sólo por su  importancia intrínseca sino por ser objeto de evaluaciones periódicas a través de la aplicación  ColoquiosEC.     3.2.4 ColoquiosEC    Esta aplicación permite realizar un seguimiento de los alumnos sobre el aprendizaje de temas  abordados en EC mediante evaluaciones rápidas llamadas coloquios. Para acceder a cada coloquio  el alumno dispone de un índice. Una vez elegido el coloquio, se presenta una pantalla en la que se  solicita la clave de acceso. Esta clave tiene el propósito de habilitar el acceso al coloquio en el  momento definido por la cátedra. Los profesores son los encargados de comunicar dicha clave en el  momento oportuno, a partir del cual los alumnos disponen de 72 horas para realizar la evaluación y  devolver el resultado obtenido. Cada estudiante tiene una única oportunidad para resolver el  coloquio seleccionado. La respuesta del ejercicio debe ser elegida cuidadosamente, ya que una vez  marcada no cuenta con opción de cambio e inmediatamente el programa evalúa la solución  propuesta y devuelve un mensaje indicando si aprobó o no el coloquio y el correspondiente código  de resolución, el cual se almacena en un archivo de texto (identificado como archivo en la figura 1).  El coloquio resuelto queda marcado e inhabilitado para que el alumno pueda rendirlo nuevamente.  ColoquiosEC se distribuye en CD y cada uno es identificable a través de un número que establece la  correspondencia biunívoca entre alumno y CD. El código de resolución que genera el programa  depende del número identificatorio único del CD, del número de problema que el software elige al  azar de entre una colección y de la respuesta seleccionada por el alumno. Mediante este código, la  cátedra dispone de otra aplicación que permite identificar el alumno evaluado, el ejercicio que  resolvió y la respuesta seleccionada. Una vez resuelto el ejercicio, el alumno debe levantar el  archivo de texto a la plataforma, a través de la actividad Tarea prevista en Moodle. La tarea es una  herramienta de actividad que permite configurar un período de tiempo de habilitación para el envío  de archivos adjuntos. Es importante destacar que ColoquiosEC no necesita ser instalado, ya que se  ejecuta desde el CD; siendo ésta una característica que facilita su uso ya que le permite  independizarse de la versión de sistema operativo que posea la computadora y también libera al  alumno para que acceda desde diferentes equipos, personal, de la universidad, Cyber, etc.    3.2.5 Plataforma Moodle    Moodle significa Modular Object Oriented Dynamic Learning Environment (Entorno Modular de  Aprendizaje Dinámico Orientado a Objetos). Es un sistema de gestión de cursos (Courses  Management System, CMS), diseñado para ayudar al profesor a crear fácilmente cursos en línea de  calidad. Estos sistemas e-learning también se llaman Sistemas de Gestión de Aprendizaje (Learning  Management System, LMS) o Ambientes Virtuales de Aprendizaje (AVA).      En la página principal del curso (vista parcial en la figura 2) se pueden presentar los cambios  ocurridos desde la última vez que el usuario entró en el curso, lo que ayuda a crear una sensación de  comunidad. Todas las calificaciones para los foros, cuestionarios y tareas pueden verse en una única  página y descargarse como un archivo con formato de hoja de cálculo.          Figura 2: Página principal del curso EC    A continuación comentaremos brevemente las actividades adoptadas para las evaluaciones y los  mecanismos de comunicación y organización.     Cuestionarios: a través de esta herramienta se organizaron las evaluaciones globales de coloquios.  De todas las configuraciones posibles, adoptamos las preguntas de opción múltiple, las cuales  pueden definirse con única o múltiples respuestas correctas. Se optó por la segunda alternativa ya  que de idéntica manera se evalúa a los alumnos en la instancia presencial.   Tareas: se especificaron las fechas finales de entrega de cada tarea asociada a los coloquios, que se  visualizaban en el calendario del curso. Los estudiantes subieron sus tareas (archivos) al servidor.  Se registró la fecha en que se han subido. Cada coloquio se habilitó en la plataforma por espacio de  72 horas (ver figura 3).        Figura 3: Bloque de Coloquios    Calendario: este elemento permitió informar  a los estudiantes las actividades programadas para el  periodo lectivo, de modo que conozcan el cronograma propuesto por la cátedra con suficiente  tiempo de anticipación.    4. RESULTADOS    Al momento de finalizar el cursado, se aplicó una encuesta a una muestra de 59 estudiantes que  fueron evaluados en el último parcial, cuyo contenido es Resolución de Problemas  Computacionales utilizando diagramas N-S1. Los alumnos aprobados en este parcial alcanzaron la  condición de regulares en la asignatura.    4.1 Utilidad del sitio y rendimiento académico    Respecto a la valoración de la utilidad del sitio Web de EC, dentro de la escala Mucha, Media, Poca  o Ninguna, discriminados por condición de regularidad al final del cursado, resultó lo consignado  en tabla 1.                                                    1  Diagramas Nassi Sheneiderman o diagramas de bloques estructurado.  Tabla 1: Valoración de utilidad del sitio Web de EC    Condición del alumno  Regular Libre    Grado de utilidad   del sitio Web    Recuento % col. Recuento % col.  Mucha 13 44,8% 12 42,9%  Media 14 48,3% 13 46,4%  Poca 2 6,9% 3 10,7%    4.2 Herramientas de mayor utilidad    En el siguiente gráfico se representan los servicios del ambiente virtual que obtuvieron el mayor  grado de preferencia en los alumnos, por encima de otros tales como el Glosario, el foro de  Asistencia Técnica, la Biblioteca o la Mensajería Interna. Puede observarse que, dentro de los  recursos de comunicación, el Panel de Anuncios resultó de máxima preferencia. Le siguen los  bloques de cada unidad temática, cada uno de ellos con actividades y espacios de comunicación  propios.        Figura 4: Herramientas del sitio más adoptadas      4.3 Indicadores del compromiso con la estrategia de evaluación    Como ya se dijo, los coloquios son instrumentos de evaluación que se aplican semanalmente,  siguiendo un calendario preestablecido, de tal forma que el estudiante debe acompasar el abordaje  de los contenidos a través de clases teóricas y prácticas presenciales, con sus respectivas  evaluaciones obligatorias e implementadas bajo modalidad no presencial. La tabla 2 señala, en  términos absolutos, el cumplimiento de las tareas de evaluación que se combinan entre  ColoquiosEC y la plataforma virtual.            Tabla 2: Cantidad de alumnos evaluados en el entorno virtual    Contenidos  Sistemas de Numeración Algebra de Boole  Cantidad de  alumnos  evaluados Coloquio 1 Coloquio 2 Coloquio 3 Coloquio 4 Coloquio 5 Coloquio 6  Dentro del plazo 225 277 253 229 237 225  Fuera del plazo  79       Total 304 277 253 229 237 225  Recuperación I 60 71  Recuperación II 19 26    Se identificaron inconvenientes para que los alumnos cumplieran, dentro del plazo, con la tarea  correspondiente al coloquio Nº 1. Podríamos distinguirlos en dos grandes grupos, a) los  provenientes de problemas técnicos y b) los acaecidos por falta de información y baja adecuación al  sistema. Como respuesta a este 26% de estudiantes “rezagados” se implementó una segunda  instancia de evaluación del coloquio. No fue necesario aplicar este recurso en ningún otro caso.    Respecto a los coloquios 7 a 10, todos correspondientes al tema Resolución de Problemas  Computacionales, se habilitaron actividades de auto evaluación a través de la herramienta  Cuestionario. Paralelamente se habilitó un foro denominado “Taller de Resolución de Problemas”,   cuyo objetivo fue intercambiar opiniones sobre la solución computacional de los mismos,  favoreciendo el trabajo conjunto y colaborativo, sobre todo en la fase de análisis.     4.4 Utilidad del foro como mecanismo de comunicación para el debate de problemas    Actualmente se avanza en el desarrollo de una aplicación correspondiente al tema Resolución de  Problemas Computacionales, que permita llegar a la correspondiente especificación algorítmica  usando diagramas N-S; por lo que resultó de sumo interés estudiar el intercambio entre docentes,  estudiantes y pares a través del foro. El 36% de la muestra manifestó que le resultó de utilidad. A  continuación se consigna textualmente la opinión de un alumno sobre el foro y un debate  (parcialmente ilustrado) iniciado por alumnos con aportes de docentes:     “Este foro creo que es el que me resultó más útil porque en este podía ver o comparar los  diferentes modos de interpretación que tenían demás compañeros y las apreciaciones que  hacían los profesores, creo que fue lo más importante.”             Figura 5: Ejemplo de participación en el foro    5. CONCLUSIONES Y PERSPECTIVAS    En este apartado expondremos las primeras anticipaciones que surgen de la experiencia, en la forma  de fortalezas y debilidades de la estrategia didáctica implementada, involucrándonos en el espiral  del proceso de investigación. Las primeras nos permiten documentar y sostener las decisiones de  diseño que resultaron provechosas, mientras que las segundas constituyen el punto de partida para  la revisión de las prácticas, la cual es, en sí misma, el mejor residuo de la metodología  investigación-acción en la que se apoya el PICTO.     5.1 La mejora de la práctica docente    El alumno universitario inicial transita con dificultad sus primeros momentos de cursado. Creemos  que este fenómeno se debe, en gran parte, a la baja y deficiente comunicación que puede entablarse  con un alumnado inicial y masivo en instancias presenciales. El estudiante de primer año  universitario desarrolla lentamente la adecuación a este sistema un tanto hostil que brinda trato  distante a numerosos sujetos anónimos para el docente y sus pares. Las comunicaciones impresas y  publicadas en pizarras no son suficientes para evacuar dudas puntuales. En esta primera etapa de  desconcierto, la plataforma brinda un ambiente de encuentro virtual que se constituye en canal de  comunicación efectivo y eficaz. Creemos que el proceso de evaluación, debe ejecutarse una vez que  dichos canales están habilitados. La experiencia nos señaló que el coloquio Nº 1, habilitado la  primera semana de clases, resultó prematuro y evidenció problemas organizativos (ver tabla 2),  muchos de los cuales se  tramitaron a través de la plataforma. No obstante, observamos con  satisfacción que los propios alumnos solicitaron inmediatamente ser evaluados en el coloquio Nº 1,  sin esperar a la recuperación global prevista antes del parcial, lo que se interpretó como un esfuerzo  por adecuarse al sistema y un reconocimiento de la acción formativa de la auto evaluación.  Corroboramos esta conjetura al advertir que todo el alumnado cumplió en tiempo y forma con las  evaluaciones sucesivas y los comentarios favorables manifestados en la encuesta acerca de dos  recursos de comunicación: Panel de anuncios y Calendario.    Las recuperaciones globales I y II se habilitaron antes del primer parcial y antes de la recuperación  del primer parcial, respectivamente. Como ya se señaló, los coloquios deben ser rendidos, no  necesariamente aprobados para acceder a la instancia de evaluación presencial (parcial y  correspondiente recuperación). Las recuperaciones de coloquios tienen entonces, la finalidad de  aplicar las mismas reglas a todos los alumnos, integrando a aquellos que por razones personales, de  salud u otras puedan haber quedado al margen del sistema. Los estudiantes usaron la instancia de  recuperación global para alcanzar la condición de alumno “con permiso para rendir el parcial”. Sin  embargo, la mayoría de los consignados en la tabla 2 accedieron a las recuperaciones con fines de  ejercitación y auto evaluación, aún cuando ya habían cumplido las condiciones exigidas. Este  comportamiento se evidenció también al señalar como positiva la utilidad de los Cuestionarios  aplicados a través de la plataforma.    Respecto al sitio virtual, advertimos una serie de aspectos que podemos resumir de la siguiente  manera:     No se observa una relación significativa entre el desempeño académico y la valoración de  utilidad que los estudiantes tienen del sitio Web. Se han registrado alumnos con muy alto  desempeño académico y baja actividad en el entorno virtual y viceversa; lo que conduce a  pensar que, en términos generales, la plataforma como recurso no tiene una incidencia  directa sobre la calidad del aprendizaje.   La participación de los alumnos en los foros es bipolar; por una parte hay un grupo reducido  que parece estar “siempre conectado”, promoviendo y resolviendo debates. Incluso han  llegado a adoptar un rol de moderador frente a temas de escaso interés académico. Creemos  que es muy auspicioso que haya alumnos que se comunican con sus pares para evacuar  dudas, tanto técnicas como temáticas, con la seguridad mediada por la plataforma, que el  docente monitorea todo el proceso. En el otro extremo, hay alumnos con baja comunicación,  que usa el recurso sólo para leer anuncios y descargar archivos, es decir, con una actitud más  pasiva y anónima.   La participación de los docentes en los foros también es bipolar; algunos adoptaron roles  activos moderando debates, montando actividades extras, haciendo anuncios oportunos; en  definitiva, manteniendo el sitio y dando la ilusión de una cátedra “24 horas”. Por el  contrario, algunos ni siquiera hicieron visible su imagen personal a través de su foto, tal  como se sugirió desde la administración del sitio. Resultó natural observar que, entre los  primeros, están los investigadores del PICTO; mientras que, entre los segundos, están los  profesores menos vinculados con la labor investigativa de la práctica docente. Es difícil  aventurar conclusiones respecto a su grado de compromiso con esta experiencia, ya que se  trata de profesionales informáticos que no pueden aducir razones vinculadas con una ruptura  tecnológica; a la vez, se trata de personal altamente capacitado con probadas aptitudes  didácticas en la modalidad presencial.    La comunicación virtual a través del sitio abrió nuevos y mejores canales de comunicación  presencial. Facilitó el reconocimiento fisonómico de todos, sobre todo para los profesores de  teoría que albergan en los anfiteatros alrededor de doscientos alumnos, antes anónimos y  para los propios alumnos que usualmente cursaban todo el cuatrimestre sin conocer los  nombres de sus docentes.   Se perfeccionaron los mecanismos organizativos, aprovechando la potencialidad del panel  de anuncios que se consulta virtualmente desde cualquier lugar físico, máxime teniendo en  cuenta que el campus de la universidad está a considerable distancia de varios puntos de la  ciudad. Este aspecto fue vital para cumplir con la presentación de los coloquios desde  localidades del interior de la provincia.  5.2 Futuras líneas de acción    El foro Taller de Resolución de Problemas se constituyó en espacio de encuentro que nutre la  correspondiente tarea de desarrollo del software. Los investigadores esperamos usarlo no sólo como  canal de comunicación para el debate académico, sino como insumo para la selección de problemas  apropiados, el análisis de los procesos cognitivos que el alumno pone en juego al momento de  resolver los problemas, las dificultades frecuentes y las estrategias didácticas puestas en práctica por  los docentes. De esta manera creemos que la nueva aplicación del grupo PI, será, como las  anteriores, un verdadero software a medida de las necesidades de nuestros alumnos. Sin embargo,  no menos importante, es cumplir con el objetivo del PICTO, esto es, estudiar el comportamiento de  los docentes en este nuevo escenario que los compromete a situarse como verdaderos tutores a la  distancia. En este sentido, están previstas dos acciones inmediatas:    • Un taller interno de capacitación a las cátedras del primer año universitario de la Lic. en  Análisis de Sistemas, en estrategias didácticas vinculadas con la acción formativa de la  evaluación.  • Análisis de aptitudes y actitudes docentes frente a la experiencia desarrollada.    Desde una perspectiva didáctica, creemos que la incorporación de las NTICs en instancias de  evaluación obliga a una acción formativa sobre el alumnado, que incluya estrategias de  fortalecimiento del aprendizaje autónomo y sistemático, más propio de los estudios superiores que  los presentes en el alumno universitario inicial. Por otra parte, la modalidad extended learning  implica no sólo crear y consolidar la infraestructura tecnológica sino incentivar un cambio de  actitudes que favorezcan la incorporación de tecnología a las aulas y a la práctica cotidiana de los  docentes. En este sentido estamos trabajando, completamente imbuidos de la metodología  investigación-acción adoptada por el PICTO para la consecución de una verdadera mejora de la  práctica educativa."	﻿software educativo , ntics , nivel superior , entornos virtuales , resolución de problemas	es	22009
23	Evolución diferencial con factor de mutación dinámico	﻿El algoritmo de Evolución Diferencial (DE) es un método de optimización para problemas complejos. Como todo método de optimización posee parámetros que deben ser debidamente ajustados para proveer soluciones de buena calidad. Entre estos parámetros se encuentra F ∈ [0,∞), el factor de escala de la mutación, que afecta la velocidad con la cual evoluciona la población. Dado que dicho factor juega un papel importante en la obtención del óptimo global, en el presente trabajo se realiza un estudio de algoritmos de Evolución Diferencial que implementan factor F constante y otros que lo hacen considerando una variación dinámica del parámetro F en función del tiempo. El estudio se realiza sobre un conjunto de funciones escalables ampliamente difundidas y estudiadas por la comunidad de computación evolutiva. Keywords: Evolución Diferencial, factor de mutación estático y dinámico, problemas de optimización de alta dimensionalidad, escalabilidad. 1. Introduction El algoritmo de Evolución Diferencial [9](DE por sus siglas en inglés) es una herramienta simple pero poderosa para resolver problemas de optimización global. Sin embargo, los parámetros de control involucrados en el DE están ampliamente relacionados con el problema bajo consideración y por lo tanto influyen en el rendimiento del mismo. Una de las dificultades que se puede encontrar en el DE es la temprana convergencia a óptimos locales, sobretodo en funciones complejas de alta dimensionalidad. El parámetro relacionado a la velocidad de convergencia es el factor de mutación F . La buena elección del parámetro F incrementa la precisión de la solución y estimula la capacidad de escapar de óptimos locales. En general, la habilidad de realizar búsqueda local (explotación) se logra a través de valores de F pequeños y, por el contrario, realizar búsqueda global (exploración) se consigue con mayores valores de F [12]. Hasta la actualidad se han realizado numerosas aproximaciones para determinar el valor del factor F . Una de ellas es elegir un valor fijo para F (a través de la experiencia) que logre el balance entre explotación y exploración [9]. Otros 2 Evolución Diferencial con Factor de Mutación Dinámico estudios han propuesto variar el valor de F de manera aleatoria respondiendo a alguna distribución de probabilidades (generalmente una distribución uniforme) [2,11,1,7]. Por otro lado, también se ha experimentado con un factor de mutación variante en el tiempo [2]. En este caso, cada generación produce un nuevo valor para el factor F siguiendo una función lineal decreciente. Esto significa decrementar el valor de F linealmente en relación al tiempo (generaciones o iteraciones) ya que en la mayoría de los métodos de optimización, es bueno que en las primeras etapas de búsqueda se realice una exploración del espacio de soluciones y durante las últimas una explotación de la zona donde podría encontrarse el óptimo global. Encontrar el valor adecuado del factor de mutación F , para una tarea específica, puede insumir una gran cantidad de tiempo y de cómputo. Por lo tanto, en este trabajo se propone utilizar una función para generar valores de F dinámicamente. Para lograr un buen desempeño del DE en problemas de dimensionalidad alta (D ≥ 100), se decidió experimentar con el parámetro F variable en el rango [0, 1] en función del tiempo transcurrido (es decir, el valor del factor de escala en el tiempo t está dado por F (t)). Se optaron por cinco funciones F (t) que siguen distintos comportamientos. El resto del presente trabajo se organiza de la siguiente manera: en la sección 2 se da una breve descripción del algoritmo DE. Posteriormente, en la sección 3 se brinda una descripción detallada de la propuesta realizada sobre el factor de escala F . La sección 4 presenta los estudios experimentales realizados a las diferentes variantes del DE para obtener las conclusiones expuestas en la sección 5. 2. Evolución Diferencial El algoritmo DE es un algoritmo de optimización estocástico introducido por Storn y Price en 1996 [9]. Sea S ⊆ RD el espacio de búsqueda del problema en consideración, el DE involucra una población de NP vectores (soluciones candidatas) xi,g = {x1i,g, x2i,g, ..., xDi,g} ∈ S, i = 1, 2, ..., NP . Cada xji,g se corresponde con una variable de decisión del problema y g indica la generación a la cual pertenece el vector. Dichos vectores, luego de ser inicializados, son sometidos a operaciones de mutación, recombinación y selección en cada generación g. Inicialización Se definen previamente los límites inferiores y superiores para cada variable de decisión: xij ≤ xji,1 ≤ xsj . Posteriormente se seleccionan, aleatoria y uniformemente, los valores iniciales de las variables de decisión sobre los intervalos [xij , xsj ]. Mutación Por cada vector xi,g (vector objetivo) en la generación g, se crea un vector mutado vi,g = {v1i,g, v2i,g, ..., vDi,g} utilizando, algunas de varias estrategias. Para clasificar dichas variantes se utiliza la notación DE/x/y/z, donde x indica el vector a ser mutado (“rand” o “best”), y la cantidad de restas de Evolución Diferencial con Factor de Mutación Dinámico 3 vectores realizadas (1 o 2), y z denota el esquema de recombinación utilizado (“bin”: binomial o “exp”:exponencial). Las estrategias mas comúnmente utilizadas para generar vi,g son: 1. DE/rand/1/bin: vi,g = xr1,g + F (xr2,g − xr3,g) 2. DE/best/1/bin: vi,g = xbest,g + F (xr1,g − xr2,g) donde los índices r1, r2, r3 son enteros aleatorios y mutuamente diferentes generados en el rango [1, NP ]. F es un factor entre [0,∞) para escalar la diferencia de vectores (mutación) y xbest,g es el vector con mejor valor de fitness en la población en la generación g. Recombinación La operación de recombinación (crossover, en inglés) es aplicada a cada parte del vector mutado generado vi,g y su correspondiente vector objetivo xi,g para generar un vector de prueba ui,g = {u1i,g, u2i,g, ..., uDi,g}. uji,g = { vji,g si (randj [0, 1] ≤ CR) ó (j = jrand), xji,g en otro caso; (1) donde j = 1, 2, ..., D, CR es una constante que indica la probabilidad de recombinación en el rango [0, 1) y jrand es un entero aleatorio elegido en el rango [1, NP ] para asegurar que el vector de prueba sea diferente del vector objetivo correspondiente. El operador dado en la ecuación 1 corresponde al crossover binomial. Selección Se compara el valor de fitness de cada vector de prueba f(ui,g) con su correspondiente vector objetivo f(xi,g) en la población actual. El vector con mejor valor de fitness es el que entrará en la población de la siguiente generación. xi,g+1 = { ui,g si f(ui,g) < f(xi,g), xi,g en otro caso. (2) Las últimas tres operaciones son repetidas de generación en generación hasta que sea satisfecho un criterio de detención específico. 3. Propuesta para el Factor de Escala de Mutación F Teniendo en cuenta la idea de variar el factor de escala en el tiempo, explorar el espacio de soluciones en las primeras iteraciones y explotarlo en las últimas; el presente trabajo introduce otras funciones (además de la lineal) para el factor de mutación F en el rango [0, 1]. Las funciones consideradas se presentan en la figura 1. Las funciones F (t) pueden ser agrupadas en tres clases. Las dos primeras funciones (F1(t) y F2(t)) descienden rápidamente y toman valores cercanos a 0 en las últimas iteraciones. F3(t) obtiene valores proporcionales al paso del tiempo. Las últimas dos funciones se corresponden a funciones Gaussianas con altura = 1, media = 0 y una alta varianza (2000000 y 5000000 respectivamente), y se mantienen valores de F altos durante una mayor cantidad de tiempo. 4 Evolución Diferencial con Factor de Mutación Dinámico F1(t) = 0, 9987t F2(t) = 30,01t+3 F3(t) = − 15000 t + 1 F4(t) = e t2 4000000 F5(t) = e t2 10000000 Figura 1. Funciones alternativas para F variables 4. Estudio Experimental Para determinar las diferentes variantes del factor F propuestos en la sección anterior, se realizaron estudios experimentales para compararlos, incluyendo un algoritmo estándar DE con factor fijo F = 0, 75. Se utilizó un máximo de iteraciones MaxIt = 5000, parámetro de recombinación CR = 0, 7 y tamaño de población NP = 200. Se probaron dos de las estrategias de mutación: DE/best/1/bin (estrategia B por Best) y DE/rand/1/bin (estrategia R por Random) debido a que son las más comúnmente utilizadas. Para cada combinación de parámetros se realizaron 30 ejecuciones con distintas semillas. Los algoritmos bajo observación tienen la siguiente nomenclatura: E/F (t), donde E es la estrategia del algoritmo (B o R), y F (t) es la estrategia de F elegida (1,2,3,4,5, para Fi(t) o Cte para F = 0, 75). Las variantes de DE bajo estudio fueron aplicadas a seis funciones, un subconjunto de las funciones desplazadas y escalables descritas en el benchmark del CEC 2008 [10]: f1 (Sphere), f2 (Problema 2.21 de Schwefel), f3 (Rosenbrock), f4 (Rastrigin), f5 (Griewank) y f6 (Ackley). La finalidad del presente trabajo es obtener conclusiones relevantes en cuanto al desempeño del DE con las variantes antes mencionadas. Para ello, se realizaron estudios iniciales con dimensión D = 100 y luego con D = 200 y D = 500 para analizar sus respectivas capacidades de escalabilidad. 4.1. Estudios preliminares Inicialmente se estudió el comportamiento del DE con todas las variantes del factor de mutación sobre el benchmark antes mencionado para D = 100 (ver Cuadro 1). Para poder establecer las diferencias entre dichas variantes, se realizaron diversos tests estadísticos según se explica a continuación. Dado que las muestras obtenidas no corresponden a una distribución normal, se aplicó para la comparación de los diferentes algoritmos, el test no paramétrico Kruskal-Wallis. Dado que esta prueba no identifica en dónde se producen las diferencias, es necesario utilizar algún test post-hoc para ello. En este trabajo se Evolución Diferencial con Factor de Mutación Dinámico 5 utilizó el test Tukey-Kramer para encontrar cuáles medianas son significativamente diferentes entre sí y a partir de allí concluir si hay algún algoritmo mejor que otro. f1 f2 f3 f4 f5 f6 B1 4,75e+01 7,09e+01 2,71e+06 3,43e+02 1,48 2,05e+01 B2 5,75e+02 7,32e+01 4,95e+07 3,66e+02 5,95 1,71e+01 B3 3,18e+04 1,12e+02 1, 40e+10 4,85e+02 2,78e+02 2,01e+01 B4 6e−02 5,96e+01 3,31e+01 2,65e+02 4e−02 2,05e+01 B5 4, 05e−05 5,35e+01 5,25e+02 2,31e+02 5, 40e−05 2,11e+01 BCte 0 4,79e+02 1,47e+02 7,26e+02 0 3, 20e−05 R1 0 2,34e+01 2, 10e+02 7,24 0 2,06e+02 R2 0 2,17e+01 1,49e+02 6,02 0 2,11e+01 R3 4,93e+05 1,54e+02 4, 05e+11 2,25e+03 4,18e+03 2,11e+01 R4 0 2,06e+01 1, 69e+02 9,64 0 2,11e+01 R5 0 1,55e+01 9,68e+01 1,49e+02 0 2,12e+01 RCte 3,30e+05 1,50e+02 2, 82e+11 1,84e+03 2,97e+03 2,12e+02 Cuadro 1. Error medio de todos los algoritmos para D = 100. Las muestras comparadas son los resultados obtenidos al aplicar cada variante del DE a cada función fi. Primero se compararon sólo aquellas variantes con estrategia B, luego todas aquellas con estrategia R, y finalmente todas las variantes (tanto con estrategia B como R). Para realizar un estudio sobre muestras no apareadas se aplicaron los tests a cada función fi por separado 1. Las conclusiones obtenidas a partir de los tests antes mencionados se muestran en el cuadro 2. Para todos los test, de aqui en adelante, se ha considerado tener un 95% de confianza en las conclusiones obtenidas. En base al cuadro 2 se puede decir que, teniendo en cuenta los algoritmos con estrategia B, el algoritmo BCte es mejor para todas las funciones, excepto para f4 donde es el peor. Pero sólo se puede decir esto sin considerar a B5, con el cual no se encontraron diferencias significativas en la mayoría de las funciones. Considerando los algoritmos con estrategia R, los algoritmos R1, R2, R4 y R5 muestran muy buen desempeño y el o los mejores de cada función se encuentra/n entre ellos. R3 y/o RCte demuestran ser los de peor desempeño. Sólo se pudo demostrar diferencias significativas entre el primer grupo (el de los mejores) y el segundo (el de los peores). Comparando todas las variantes del DE podemos decir que BCte, R1, R2, R4 y R5 tienen un mejor desempeño que B1, B2, B3, B4, R3 y RCte. Por otro lado R3 y/o RCte son los peores para casi todas las funciones. Dado que es necesario comparar las variantes del algoritmo DE sobre el conjunto total de las 6 funciones fi, se realizaron los tests de Friedman [3,4], Friedman Alineado [6] (F.Alineado) y Quade [8] para comparaciones múltiples apareadas, no paramétricas. Estos tests detectan diferencias significativas entre el conjunto total de algoritmos pero sin detectar entre cuáles de ellos se producen. Para ello se aplicaron algunos de los tests post-hoc sugeridos en [5] los cuales toman como algoritmo 1 Para los tests Kruskal-Wallis y Tukey-Kramer se utilizó la herramienta MATLAB 6 Evolución Diferencial con Factor de Mutación Dinámico de control al primero en el ordenamiento de los tests antes mencionados para poder concluir que efectivamente es mejor que el resto si se hubieran detectado diferencias significativas. fi Est p-value Resultados f1 B 6, 75620e−36 BCte es mejor que B1, B2, B3, B4./ B3 es peor que B1, B4, B5, B6 R 6, 66797e−36 R1, R2, R4 y R5 son los mejores./ R3 y RCte son los peores. BR 9, 88082e−70 BCte, R1, R2, R4 y R5 son los mejores. f2 B 1, 45500e−31 BCte es el mejor. R 8, 29583e−26 R5 es el mejor. BR 4, 13644e−66 R1, R2, R4 y R5 son los mejores. f3 B 4, 25961e−35 BCte es mejor que B1, B2, B3 y B4. R 7, 21483e−29 R5 es mejor que R1, R3 y RCte./ R3 y RCte son los peores. BR 1, 03206e−64 BCte, R2 y R5 son mejores que B1, B2, B3, B4, R3 y RCte. f4 B 2, 03994e−28 B5 es mejor que B1, B2, B3 y BCte./ BCte es peor que B1, B2, B4 y B5 R 3, 92147e−33 R1 y R2 son mejores que R3, R5 y RCte./ R3 es peor que R1, R2, R4 y R5. BR 3, 75781e−67 R1, R2 y R4 son los mejores./ R3 es el peor. f5 B 1, 47982e−35 BCte es mejor que B1, B2, B3 y B4./ B3 es peor que B1, B4, B5 y BCte R 2, 78199e−36 R3 y RCte son peores que R1, R2, R4 y R5. BR 2, 28013e−69 R1, R2, R4 y R5 son los mejores./ R3 es el peor. f6 B 1, 46614e−24 BCte es mejor que B1, B3, B4 y B5./ B5 es el peor. R 3, 98513e−21 R1 es el mejor. BR 1, 53811e−52 BCte es mejor que B1, B4, B5, R1, R2, R3, R4, R5 y RCte. Cuadro 2. Resultados de los tests Kruskal-Wallis y Tukey-Kramer Al igual que el estudio previo realizado a los algoritmos, se consideraron primeros los algoritmos con estrategia B, luego aquellos con estrategia R, y finalmente se consideraron todas las variantes, de ambas estrategias. Test p-value Friedman 0, 013892 F. Alineado 0, 474194 Quade 0, 0002845262 Cuadro 3. Estrategia B Test p-value Friedman 0, 005608 F. Alineado 0, 427357 Quade 0, 0001152417 Cuadro 4. Estrategia R Test p-value Friedman 0, 0000380084 F. Alineado 0, 936611 Quade 0, 0000000015243 Cuadro 5. Estrat. B y R Algoritmos con estrategia DE/best/1/bin Considerando los algoritmos y todas las funciones, los test Friedman y Quade encontraron diferencias significativas. El cuadro 3 muestra los p-values obtenidos con los tres tests. En base a los ranking obtenidos con los tests (Cuadro 6) se encontró que BCte es el mejor algoritmo según los tests de Friedman y Quade. Se encontraron además diferencias significativas de éste con B3, y con menos fuerza con B2 (según test post-hoc de Holm). Evolución Diferencial con Factor de Mutación Dinámico 7 Algoritmos Friedman Friedman Alineado Quade B1 4.0 16.166 3.857 B2 4.333 17.166 4.666 B3 5.333 32.166 5.666 B4 2.999 15.0 2.857 B5 2.5 14.5 2.0 BCte 1.833 16.0 1.952 Cuadro 6. Ranking de los algoritmos con estrategia B. Algoritmos con estrategia DE/rand/1/bin Considerando los algoritmos con estrategia R, los test Friedman y Quade encontraron diferencias significativas entre los algoritmos (Cuadro 4). Algoritmos Friedman Friedman Alineado Quade R1 2.666 12.666 2.928 R2 2.166 12.166 2.166 R3 5.5 30.5 5.857 R4 2.833 12.833 2.738 R5 2.833 12.833 2.309 RCte 5.0 30.0 5.0 Cuadro 7. Ranking de los algoritmos con estrategia R. Se observó que R2 es el mejor algoritmo (Cuadro 7) y presenta diferencias significativas con R3 para varios tests post-hoc (incluyendo el de Holm). Otros tests post-hoc mostraron diferencias también con respecto a RCte y R4. Todos los algoritmos Comparando todas las variantes del DE propuestas, los tests Friedman y Quade encontraron diferencias significativas entre los algoritmos (Cuadro 5). R2 demostró ser el de mejor rendimiento para los tests de Friedman y Friedman alineado (Cuadro 8). Este algoritmo presenta fuertes diferencias con relación a R3, RCte, y con menos fuerza con B3. Considerando la dificultad de los problemas, R5 resultó ser el mejor algoritmo según el test Quade, presentando diferencias con respecto a R3. De todos los estudios previos, se puede concluir que un F variable es mejor cuando la estrategia adoptada es R. Sin embargo, para la estrategia B lo mejor es optar por un F constante, en este caso F = 0,75 (buen desempeño para todo el benchmark excepto para f4). Considerando el conjunto completo de algoritmos, R2 demostró ser mejor que los algoritmos R3, RCte y B3, por lo que se la considera una buena opción para tratar diversos problemas en general. En el algoritmo R2 los valores de F descienden muy rápido y se mantienen en valores cercanos a 0 a partir de la mitad de la ejecución. Otro algoritmo con buen desempeño es el R5 que usa la variación F5(t) que se comporta como una función gaussiana manteniendo 8 Evolución Diferencial con Factor de Mutación Dinámico valores altos de F por una buena cantidad de tiempo, y decreciendo de manera progresiva después de la mitad de la ejecución sin llegar a F = 0. Tanto para R y B, F3(t) mostró el peor de los desempeños, descartando la idea de una variación de F proporcional al tiempo. Algoritmos Friedman Friedman Alineado Quade B1 7.333 33.5 7.714 B2 7.666 33.833 8.523 B3 8.666 35.5 9.523 B4 6.333 30.666 6.714 B5 6 30.333 5.904 BCte 4.0 31.833 3.809 R1 3.833 27.833 3.666 R2 3.5 27.5 2.952 R3 11.5 65.666 11.857 R4 4.166 28.166 3.524 R5 4.0 28.0 2.809 RCte 11.0 65.166 11.0 Cuadro 8. Ranking de todos los algoritmos. 4.2. Escalabilidad Como se ha mencionado, las dificultades para converger al óptimo global se dan, por lo general, en problemas de alta dimensionalidad. Por este motivo se realizó un estudio adicional para observar la escalabilidad de los algoritmos. Se ejecutaron todas las variantes sobre el mismo benchmark pero para las dimensiones D = 200 y D = 500. Las comparaciones realizadas entre los mejores algoritmos se encuentran sintetizados en el Cuadro 10. Se puede observar que para D = 200, en f1, f5 y f6 los errores se mantienen bajos, lo que muestra una buena escalabilidad. Sin embargo, para D = 500 las soluciones para fi con 1 ≤ i ≤ 4, el error crece significativamente. Aunque sucede algo completamente diferente con f6 donde los errores disminuyen o se mantienen cercanos a medida que se aumenta la dimensionalidad. Cuando se comparan los mejores algoritmos entre sí, vemos que BCte alcanza errores muy por encima de los otros algoritmos, a medida que se aumenta la dimensionalidad. Otra observación respecto de BCte, considerando solo los algoritmos con estrategia B, es que conforme aumenta la dimensión en f4 (función para la cual BCte era el peor algoritmo) comienza a comportarse mejor que otros algoritmos dejando de ser el peor y acercándose a los valores de B5 (el mejor algoritmo para f4 en todas las dimensiones probadas). En síntesis, se puede decir que R5 logra el mejor comportamiento en casi todas las funciones, logrando una buena escalabilidad con D = 200 en f1, f5 y f6. Además escala bien para D = 500 en las funciones f5 y f6. En el resto de las funciones, excepto para f4, R5 es el que obtiene el mínimo error en relación a los otros algoritmos. Evolución Diferencial con Factor de Mutación Dinámico 9 D=100 D=200 D=500 f1 Bcte 0 7,98e−02 8,43e+03 R2 0 2e−06 5,44e+02 R5 0 0 3,88e+02 f2 Bcte 4,79e+01 8,77e+01 1,09e+02 R2 2,17e+01 7,06e+01 1,07e+02 R5 1,56e+01 6,41e+01 1,06e+02 f3 Bcte 1,47e+02 7,78e+03 1,63e+09 R2 1,49e+02 1,84e+03 3,36e+08 R5 9,68e+02 8,51e+02 2,53e+08 f4 Bcte 7,26e+02 1,55e+03 4e+03 R2 6,02 3,94e+01 3,35e+02 R5 1,49e+02 5,67e+02 1,76e+03 f5 Bcte 0 2,19e−02 6,98e+01 R2 0 0 6,99 R5 0 0 3,25 f6 Bcte 3, 20e−05 1,04 9,61 R2 2,11e+01 1,36e−03 2,19 R5 2,12e+01 2,14e+01 1,94 Cuadro 9. Escalabilidad de BCte, R2 Y R5. Se muestra el error promedio. 5. Conclusiones Del estudio realizado, se concluye que DE con F constante es mejor que uno variable bajo la estrategia B. Por el contrario, usando la estrategia R, un F variable obtiene mejores resultados. Más precisamente, esto ocurre cuando F (t) corresponde a una función que promueve la exploración durante bastante tiempo para luego disminuir progresivamente (promoviendo explotación) en etapas avanzadas de la búsqueda. Examinando los algoritmos con estrategia B, se concluye que BCte logró un mejor comportamiento. Para el caso de la estrategia R, los mejores algoritmos fueron R2 y R5. Comparando entre sí a los mejores algoritmos de cada estrategia del DE, se observó que BCte es el algoritmo que peor escala. Por otra parte, R5 mostró mejores capacidades en cuanto a obtener soluciones cercanas al óptimo global. Por lo tanto, se puede decir, que un algoritmo de Evolución Diferencial con estrategia R (DE/rand/1/bin) y factor de mutación variable F5(t) = e t2 10000000 mejora notablemente su desempeño global. Así, la elección de un factor F variable cuyo comportamiento está dado por una función de Gauss con una desviación estándar en relación al máximo de iteraciones (σ = MaxIte ∗ 1000), mejora la convergencia para dimensiones altas, y provee un mejor rendimiento que un F constante y uno con decremento lineal. Futuros trabajos, involucran el diseño de DE con incorporación de buscadores locales y el estudio de benchmarks extendidos junto con la comparación con algoritmos del estado del arte. 10 Evolución Diferencial con Factor de Mutación Dinámico	﻿factor de mutación estático y dinámico , Evolución Diferencial , problemas de optimización de alta dimensionalidad , escalabilidad	es	22522
24	Sistema de transiciones para la dinámica de interacción argumentativa entre agentes	﻿En los sistemas multi-agentes la comunicación es imprescindible para que los agentes puedan lograr algún objetivo o realizar una tarea. En este contexto, definir la forma de interacción es fundamental para llevar un díalogo coherente. Sin embargo, no es sencillo formalizar dicha interacción en entornos complejos donde la información que los agentes manejan es incompleta o incluso sus elecciones dependen de otros agentes, principalmente al momento de establecer la información a intercambiar. En este trabajo se presentará un modelo de interacción para diálogos argumentativos. El modelo se focalizará en la dinámica intrínseca a la interacción del intercambio de argumentos, el cual será formalizado mediante un sistema de transiciones. 1. Introducción En los últimos años, el interés de las Ciencias de la Computación por los sistemas multi-agente se ha incrementado notoriamente(ver por ejemplo [5]). Resultan de particular interés aquellos sistemas donde cada agente posee información incompleta sobre un entorno dinámico y cambiante, resultando predominante la interactividad entre estos agentes como característica distintiva. Este interés se debe fundamentalmente al crecimiento de las redes de comunicación y la demanda cada vez mayor de sistemas donde la interactividad es un elemento clave para que los mismos logren sus objetivos. En la actualidad esta característica se puede observar tanto en las redes sociales, los simuladores computacionales[9], en los sistemas de tutorías inteligentes, así como también en los sistemas de soporte a las decisiones[2,1], entre otros. Según [12] unas de las propiedades básicas de los agentes inteligentes es la habilidad social, esto es interactuar con otros agentes, y posiblemente con humanos, por medio de algún tipo de lenguaje de comunicación. En este trabajo, esta interacción se estructurará a través de diálogos entre agentes. Un diálogo esta definido como una secuencia de actos del habla [10] intercambiados entre Financiado parcialmente por CONICET PIP-112-200801-02798 y Universidad Nacional del Sur PGI 24/ZN18 y PGI 24/N030. dos interlocutores siguiendo un esquema de turnos. En este intercambio el tipo de interacción puede cambiar dependiendo del tipo de diálogo: Persuasión, Negociación, Indagación, Deliberación o Consulta. Es decir, el tipo de diálogo influye significativamente en los mecanismos de interacción utilizados en una conversación. Es por esto que la idea en esta publicación es analizar la forma en que se lleva a cabo la interacción para luego definir un formalismo para modelarla. El marco conceptual donde se desarrolla la argumentación rebatible[7,8], el cual permite tanto representar conocimiento como razonar en base al mismo, otorga un escenario ideal para que múltiples entidades interactúen entre sí. De hecho, el razonamiento argumentativo, a través del intercambio de razones, denominadas argumentos, se asemeja a un diálogo. Una de las motivaciones del presente trabajo se originó al evaluar que, en un entorno realista de multi-agentes, no es una buena idea que los agentes que intervienen unan sus conocimientos y luego razonen a partir de estos, o que en una interacción un agente le envie a otro todos los argumentos posibles para una consulta dada. Esto se debe principalmente a que, por un lado, los agentes deberían solamente utilizar el conocimiento relevante (cada agente podría tener una gran base de conocimiento por lo que no sería eficiente unir toda esa información si se va usar una parte de ella). Por otro lado, cada agente podría tener información privada que no quiere compartir con los demás. Para estudiar el problema de formalizar una interacción tal que el intercambio sea únicamente de ciertos argumentos; consideremos el siguiente ejemplo: Ejemplo 1 Supongamos que Juan desea viajar de Buenos Aires a Montevideo a una entrevista de trabajo. Se dirige a una agencia de viajes la cual le sugiere dos alternativas. La primera, que viaje en colectivo de noche. Esta opción es descartada por Juan ya que si viaja de noche no puede dormir bien. La segunda opción es viajar en avión. Sin embargo, para Juan el costo es muy alto. Por lo que la agencia le ofrece un descuento si paga con tarjeta. Más allá del beneficio la opción es descartada por que el avión llega a Montevideo muy sobre la hora de la entrevista. Finalmente la agencia le propone otra alternativa, la de viajar en barco. El viajante decide optar por esta última opción. El objetivo general en este trabajo es presentar un modelo de interacción para diálogos argumentativos. El modelo se focalizará en la dinámica intrínseca a la interacción del intercambio de argumentos, como el que se presentará en el ejemplo 1, el cual será formalizado mediante un sistema de transiciones. El artículo está organizado de la siguiente manera. En la sección 2 se describen los conceptos básicos de argumentación. En la siguiente sección se presentan las principales contribuciones de este trabajo. Se proponen los conceptos necesarios para poder definir el sistema de transición. Al final de dicha sección se mostrará con un ejemplo integrador el sistema propuesto, de manera tal que se aplicarán todos los conceptos desarrollados a lo largo del artículo. Finalmente, la sección 4 incluye las conclusiones del trabajo y las líneas futuras de investigación. 2. Preliminares En esta sección se presentan los conceptos básicos de argumentación usados a lo largo del trabajo. Estos se basan en algunas de las nociones definidas en [4]. En este trabajo se modelarán a los argumentos como entidades abstractas que tienen una conclusión. A los argumentos se los notará con letras mayúsculas y para obtener la conclusión de dicho argumento se usará la función conc(A), donde A es un argumento. En argumentación los argumentos podrían estar en conflicto. Cuando dos argumentos se encuentran en conflicto, para determinar qué argumento será aceptado, es necesario observar cuál de ellos será derrotado. En la actualidad existen en la literatura numerosos criterio de comparación. Por lo tanto este artículo se abstrae de dicho proceso y se definirá la función derrot(A1, A2), tal que devuelve > si A1 es un derrotador de A2 y ⊥ en caso contrario. Un argumento que sustenta una conclusión podrá ser derrotado por otro argumento que lo contradiga. Siguiendo el ejemplo 1, el argumento Juan no puede dormir bien si viaja de noche será un derrotador para el argumento Viajar en colectivo de noche, el cual sustenta la conclusión Viajar de Buenos Aires a Montevideo. Dicho derrotador podrá a su vez ser derrotado, y así sucesivamente, generando una secuencia de argumentos llamada línea de argumentación. Definición 1 (Línea de Argumentación) [11] Sea A0 un argumento. Una línea de argumentación es una secuencia [A0, A1, A2, ...], donde cada argumento Ai+1 es un derrotador de Ai. Si se considera el proceso argumentativo completo, cada argumento puede tener múltiples derrotadores. Para determinar si un argumento es aceptado es necesario analizar todas las líneas argumentativas que lo tienen como primer elemento. Este análisis lleva a una estructura de árbol conocida como árbol de dialéctica. Definición 2 (Árbol de Dialéctica) [11] Para cada argumento puede existir más de un derrotador. La presencia de múltiples derrotadores para un argumento produce una ramificación de líneas de argumentación, dando origen a un árbol de derrotadores que se denominará árbol de dialéctica. En este árbol, cada camino desde la raíz hasta una hoja corresponde a una línea de argumentación. Para determinar el estado del argumento raíz su árbol de dialéctica es marcado. El marcado es un proceso que va desde las hojas hasta la raíz. Los nodos hoja del árbol de dialéctica corresponden a argumentos no derrotados, que serán marcados como U. Un nodo interno que tiene al menos un nodo hijo marcado como U, será marcado como D. Si el nodo interno tiene todos sus hijos marcados como D, entonces será marcado como U. 3. Comunicación entre agentes En el trabajo se propone que la interacción dada en una conversación, siempre en un contexto argumentativo, se inicia con una consulta. Esta consulta es el punto de inicio para la secuencia de hilos de conversación generados a partir de los argumentos y contraargumentos que en dicho proceso intercambian los agentes interlocutores. A continuación, se presentarán los aportes del presente trabajo. A fin de mejorar la presentación, se ha decidido dividirlos en dos secciones. En la sección 3.1 se desarrollará un modelo de comunicación en donde el eje central son los hilos de conversación que se generan a medida que el diálogo avanza. En la sección 3.2 se formalizará la dinámica intrínsica a la interacción a través de un sistema de transición de estados. 3.1. Modelo de Comunicación En un contexto basado en argumentación rebatible, los agentes establecen una conversación intercambiando argumentos; argumento como pieza de razonamiento a favor o en contra de cierta conclusión. Como se ha señalado, abordar el modelo de interacción planteado constituye un problema. Sin embargo, la estrategia sugerida brinda una guía conceptual que aborda este tópico en principio a través de lo que se denominará hilos de conversación. En particular este aporte es esencial para lo que posteriormente se definirá como conversación. En el dialogo los agentes intercambian cierta información. Los hilos de conversación estarán constituidos por esta información encapsulada en lo se denominará, mensaje argumentativo. Definición 3 (Mensaje argumentativo) Un mensaje argumentativo es una tupla < A, q,AG > donde AG es el agente que envía el mensaje, q es la consulta y A puede ser o bien un argumento para q o bien la constante end. Supongamos que el argumento A de un mensaje argumentativo que sustenta una conclusión q puede ser atacado por otros argumentos derrotadores. Dichos derrotadores pueden a su vez ser derrotados, y así sucesivamente, generando una secuencia de mensajes argumentativos a la que llamaremos hilo de conversación. Este concepto es similar al definido en [4] como línea de argumentación. Definición 4 (Hilo de conversación) Sea el mensaje argumentativo < A0, q0, AG >. Un hilo de conversación es una secuencia denotada T = [< A0, q0, AG >,< A1, q1, AG >,< A2, q2, AG >, ..., < An, qn, AG >] donde cada argumento Ai+1 es un derrotador de Ai si se cumple que derrot(Ai+1, Ai) devuelve >. Ejemplo 2 Siguiendo el ejemplo 1. Supongamos que la consulta y los argumentos estarán definidos de la siguiente forma: ∗ La consulta realizada por el viajante Viajar de Buenos Aires a Montevideo. Llegar antes de las 10:30am, se la notará con BM. ∗ Los argumentos: • El argumento Viajar en colectivo de noche. Llegar una hora antes de las 10am. Viaje de 8 horas. Opción más barata para la consulta BM, se lo notará con BN. • El argumento Si viajo de noche no puedo dormir tal que es un derrotador para BN, se lo notará con ND. • El argumento Viajar en colectivo de día. Llegar a la noche del día anterior. Viaje de 8 horas para la consulta BM, se lo notará con BD. • El argumento Viajar en barco. Llegar a las 09:30am. Viaje de 4 horas. Segunda opción más barata después del colectivo para la consulta BM, se lo notará con B. • El argumento Viajar en avión. Viaje de 1 hora. Llegar 10:30am. Opción más cara para la consulta BM, se lo notará con AV. • El argumento Llego tarde tal que es un derrotador para AV, se lo notará con T. • El argumento El costo del viaje es muy caro tal que es un derrotador para AV, se lo notará con C. • El argumento Si paga con tarjeta se le hace un descuento en el precio del viaje tal que es un derrotador para C, se lo notará con D. Considerando los argumentos que se presentaron, en la figura 1 se pueden observar dos posibles hilos de conversación, H1 y H2: Figura 1. Para determinar si un argumento A no pertenece a un hilo de conversación T , se usará la función noPart(A, T ), tal que retorna > cuando el argumento A no es parte de la conversación T , y retorna ⊥ en caso que A sea parte de T . Por otra parte, el accionar de los agentes puede llevarse a cabo en distintos escenarios. Por lo tanto, el tipo de interacción puede variar dependiendo del escenario en donde se desarrolle la misma. Teniendo en cuenta esto es necesario de algún mecanismo que regule o controle determinadas situaciones a fin de que la interacción, en ese determinado contexto, se desenvuelva coherentemente. Por ejemplo, una situación general que no es deseable es que un agente ingrese a un hilo de conversación un argumento que sea contradictorio al argumento que defiende. En [3] se definen como Restricciones de Dialéctica a los mecanismos para evitar estas situaciones de argumentación falaz. A estas restricciones en este trabajo se las agruparán en lo que se denominarán las restricciones de sensatez, aplicables sobre los hilos de conversación. Definición 5 (Restricciones de sensatez) Sean RS Restricciones de sensatez, denotada como la secuencia RS = [R1, R2, ..., Rn], donde cada elemento Ri es una restricciones de dialéctica. Se dice que un hilo de conversación es sensato si cumple todas las condiciones que conforman a las restricciones de sensatez. Definición 6 (Sensatez) Sea H un hilo de conversación y RS las restricciones de sensatez. Se dice que H es sensato con respecto a RS, si y solo si H satisface cada Ri en RS. Analizar los hilos individualmente y de forma aislada, ayudará a caracterizar ciertas propiedades que se dan en el intercambio de argumentos importante en un juego dialógico. Desarrollados los conceptos de hilo de conversación y restricciones de sensatez, se esta en condiciones de formalizar el modelo de comunicación. El mismo estará representado en lo que se denominará conversación. Definición 7 (Conversación) Sea RS el conjunto de restricciones de sensatez. Una conversación CO es un conjunto de hilos de conversación tal que ∀T ∈ CO, T cumple RS. Ejemplo 3 Como se pudo observar en el ejemplo 1, la agencia de viajes le ofrece a Juan distintas alternativas de viaje para un destino en particular. En la siguiente figura se puede observar una conversación con sus distintos hilos de conversación. Figura 2. 3.2. Dinámica de Comunicación Habiendo definido el modelo de comunicación en base a un conjunto de hilos, producto mismo de un escenario caracterizado por el intercambio de argumentos entre agentes. Es posible presentar la semántica de ejecución de una interacción entre agentes propiamente dicha. Esta semántica se definirá formalmente tomando la idea de los sistemas de transición[6]. Básicamente una interacción estará caracterizada por una secuencia de transición de estados. Una transición determinará como la interacción pasa de un estado a otro en un momento dado de la conversación. La transición estará compuesta por tres elementos: un conjunto de condiciones que deben cumplirse para poder aplicar la transición y dos estados (origen y destino), los cuales describen como avanza la comunicación. La transición Tr se notará como: Tr = CondicEstadoorigen−→Estadodestino En esta sección se utilizarán las funciones derrot, conc y noPart, para determinar si un argumento derrota a otro, la conclusión de un argumento y si un argumento pertenece a un hilo, respectivamente, las cuales fueron presentadas en secciones anteriores. Es importante que quede claro que la cantidad de condiciones dependerán de la implementación en cuestión. Un estado de transición, encapsula el estado en que se encuentra una conversación en un momento dado. Por lo tanto estará caracterizado por una conversación, la consulta que se busca analizar, el último agente que agrego un argumento a la conversación y las restricciones de sensatez. Definición 8 (Estado de la transición) Un estado de transición es una tupla < q,CO,AG,RS > donde q es una consulta, CO una conversación, AG el último agente que agregó un argumento a la conversación y RS las restricciones de sensatez. En particular se denotará al estado de transición inicial de una conversación como ∅. Es importante mencionar que la forma en que se realizará la transición de estados estará definida por el Protocolo de interacción. Este protocolo se asume público entre los agentes y será utilizado en la interacción de toda la comunicación. En este trabajo se propone un modelo de protocolo de interacción particular. Definición 9 (Protocolo de interacción) El Protocolo de interacción es un sistema de transiciones que contiene la Transición inicial, Transición de inicio de hilo, Transición General, Transición Final. Este protocolo denotará distintos momentos por los que pasa una transición de estados. Cuando una conversación es iniciada, esto es hay una consulta que responder, la siguiente transición puede usarse para generar un nuevo hilo dentro de la conversación, dando un argumento para la consulta. Definición 10 (Transición inicial) Sea el agente AGi que realiza una consulta inicial q. La transición, denotada T0, se define como: > ∅−→<q,∅,AGi,RS> Un hilo de conversación se genera recién cuando alguno de los agentes argumente alguna conclusión. En el modelo presentado esto sucede por primera vez cuando el agente que recibe la consulta, contesta con un argumento que tiene como conclusión la consulta dada. Esto se puede observar en lo que se definirá como transición de inicio de hilo. Definición 11 (Transición de inicio de hilo) Sea la consulta inicial q realizada por el agente AGi. Consideremos el estado de transición < q, ∅, AGi, RS > el cual es modificado por el agente AGj a través del mensaje argumentativo mj = < Aj , qj , AGj >. Sea la transición, denotada T1, se define como: conc(Aj)=q <q,∅,AGi,RS>−→<q,([<Aj ,qj ,AGj>]),AGj ,RS> Cuando la conversación ya esta establecida, los agentes intercambian argumentos hasta llegar a un acuerdo. Este intercambio se definirá como transición general. Definición 12 (Transición general) Sea la consulta inicial q realizada por el agente AGi. Sea el estado de transición < q, (CO ∪ [m1,m2, ..., < An, qn, AGn >]), AGi, RS > el cual es modificado por el agente AGj a través del mensaje argumentativo mj = < Aj , qj , AGj >. La transición general, denotada T2, se define como: AGi 6=AGj∧AGj 6=AGn∧derrot(Aj ,An)∧conc(Aj)=qj∧noPart(Aj ,[m1,m2,...,<An,qn,AGn>]) <q,(CO∪[m1,m2,...,<An,qn,AGn>]),AGi,RS>−→<q,(CO∪[m1,m2,...,<Aj ,qj ,AGj>]),AGj ,RS> Una vez que un agente decide finalizar la conversación, el mismo debe indicar de alguna forma, que no tiene más argumentos para la consulta inicial. En este sentido se representará dicha situación a través de la transición final. Definición 13 (Transición final) Sea el agente AGj, el cual no tiene argumentos para la consulta qj por lo que decide finalizar la conversación con el mensaje argumentativo m = < end, qj , AGj >. La transición, denota T3, se define como: AGi 6=AGj∧AGj 6=AGn <q,(CO∪[m1,m2,...,<An,qn,AGn>]),AGi,RS>−→<q,(CO∪[m1,m2,...,<end,qj ,AGj>],final,RS> Finalizada la conversación es necesario determinar la respuesta a la consulta inicial. Para esto se tendrá en cuenta el enfoque propuesto por [4], el cual a través del marcado de un árbol de dialéctica como se menciono en los preliminares, se puede determinar si un literal está garantizado o no. A diferencia de los árboles de dialéctica que se presentaron en los preliminares; en lugar de tener líneas de argumentación se tendrán hilos de conversación. En este trabajo el resultado del marcado devolverá YES si el argumento raíz del árbol esta marcado como U (no derrotado) y NO si esta marcado como D (derrotado). Se considerará que el procedimiento utilizado es abstracto, es decir, se definirá una función Eval(< q, (CO∪ [m1,m2, ..., < end, qj , AGj >]), final, RS >) que se encargará de construir el árbol de dialéctica y llevar a cabo el marcado de cada nodo, en base a los hilos que recibe como entrada. Como se menciono previamente la respuesta puede ser YES o NO dependiendo del resultado del marcado del árbol. Definición 14 (Evaluación) Sea el estado final en una conversación < q, (CO ∪ [m1,m2, ..., < end, qj , AGj >]), final, RS > . La función Eval(< q, (CO ∪ [m1,m2, ..., < end, qj , AGj >], final, RS >) es una función tal que su rango es {Y ES,NO} los cuales corresponden a la respuesta a la consulta inicial q. Ejemplo 4 Siguiendo el ejemplo 1 y 2 del viajante y la agencia de viajes. Consideremos la conversación entre el viajante AGp y la agencia AGo, modelada a través de la siguiente transición de estados: 1. El agente AGp inicia la conversación con la consulta BM representada con la transición T0. Estado de transición origen: ∅ Condición: > Estado de transición destino: < BM, ∅, AGp, RS >. 2. El agente AGo responde a la consulta BM con el argumento AV. La transición queda representada con T1. Estado de transición origen: < BM, ∅, AGp, RS > Condición: conc(AV ) = BM Estado de transición destino: < BM, ([< AV,BM,AGo >]), AGo, RS > 3. Supongamos que AGp envía el argumento C que es un derrotador del argumento AV. La transición quedaría definida por T2, de la siguiente forma: Estado de transición origen: < BM, ([< AV,BM,AGo >]), AGo, RS > Condición: AGo 6= AGp ∧AGp 6= AGo ∧ derrot(C,AV ) ∧ conc(C) = AV ∧ noPart(C, [< AV,BM,AGo >]) Estado de transición destino: < BM, ([< AV,BM,AGo >,< C,AV,AGp >]), AGp, RS > 4. Sea el agente AGo, que intenta convencer a AGp para que viaje en avión por lo que envía un contraargumento, D. La transición quedaría definida por T2: Estado de transición origen: < BM, ([< AV,BM,AGo >,< C,AV,AGp >]), AGp, RS > Condición: AGp 6= AGo ∧AGp 6= AGo ∧ derrot(D,C) ∧ conc(D) = C ∧ noPart(D, [< AV,BM,AGo >,< C,AV,AGp >]) Estado de transición destino: < BM, ([< AV,BM,AGo >,< C,AV,AGp >,< D,C,AGo >]), AGo, RS > 5. Como resultado imaginemos que AGp decide viajar en avión. Sea < end,D,AGp > el mensaje usado por AGp para finalizar la conversación. La transición quedará definida por T3: Estado de transición origen: < BM, ([m1,m2, < D,C,AGo >]), AGo, RS > Condición: AGo 6= AGp ∧AGp 6= AGo Estado de transición destino: < BM, ([m1,m2,m3, < end,D,AGp >]), final, RS > Finaliza la conversación y generados los hilos, se puede verificar que la función Eval(< BM, ([m1,m2,m3, < end,D,AGp >]), AGp, RS >) devuelve YES luego de que el proceso de marcado del árbol de dialéctica determina que el argumento raíz es U. En la figura se puede observar el marcado en el árbol de dialéctica construido a partir de los hilos. Figura 3. 4. Conclusiones En este trabajo se propuso un sistema formal de transición basado en argumentación para la interacción entre agentes. Para llegar a definir la semántica del mismo, se presentaron varios conceptos nuevos y necesarios como fueron los mensajes argumentativos, hilos de conversación, conversación y estado de transición hasta llegar a la noción de protocolo de interacción. A partir de los conceptos presentados, en el ejemplo 4 se mostró como llevar a cabo una simple conversación a través del modelo propuesto. El resultado o respuesta del sistema, una vez finalizada la interacción, se obtuvo a través de una función, definida como Eval, cuya tarea fue generar el árbol de dialéctica y a partir de este, realizar el proceso de marcado de nodos. Al modelar el sistema propuesto como la transición de un estado a otro, permitió demostrar que para avanzar en una conversación no es necesario que los agentes que intervienen tengan que reunir todos sus conocimientos para poder razonar. Es decir, la interacción puede ir evolucionando a través del intercambio de ciertos argumentos seleccionados en base a un determinado criterio de preferencia. De esta manera, se puede trasladar el modelo presentado a escenarios multi agentes más realistas. En trabajos futuros se analizará como implementar el modelo propuesto en un lenguaje de representación de conocimiento como es DeLP. Por otra parte se buscarán formas de construir los argumentos en base a los argumentos recibidos, así como también se analizarán estrategias de interacción acorde al tipo de diálogo establecido (Persuasión, Negociación, Indagación, Deliberación o Consulta). Además se estudiarán aspectos relacionados a los criterios de preferencias para la selección de argumentos.	﻿dinámica de interacción argumentativa entre agentes , Sistema de transiciones	es	22524
25	Una herramienta para la gestión de encuestas en el contexto de la colaboración	﻿de la colaboración  Claudia  Rozas, Laura Sánchez y Jorge Rodríguez, Universidad Nacional del Comahue, Facultad de Informática,  Buenos Aires 1400, Neuquén, Argentina claurozas@yahoo.com.ar  ,  lsanchez@uncoma.edu.ar  , jrodrig@uncoma.edu.ar   Resumen. Un problema inherente al planeamiento educativo es la producción  de información explicativa de la realidad contextual en la que se desarrolla la  experiencia educativa con intención de aportar elementos que apoyen la toma  de decisiones en la formulación, ajuste y evaluación de la propuesta de trabajo.  Una de las estrategias que se utiliza en el abordaje de esta problemática es la  recolección de datos mediante encuestas, siguiendo las fases operativas de la  investigación estadística.  En este trabajo se presenta una herramienta informática que soporta el proceso  de  realización  de  encuestas  desde  el  diseño  del  instrumento  de  indagación,  recolección de datos y presentación de información. La aplicación implementa  los conceptos especificados para el  desarrollo de PECSCs en lo referente  al  esquema de colaboración y al planteo metodológico. En  segunda  instancia  se  describe  el  flujo  de  interacciones  que  se  observa  durante  el  proceso  de  indagación  utilizando  la  aplicación  como  elemento  computacional de soporte. Keywords: . 1   Introducción En la Facultad de Informática de la Universidad Nacional del Comahue se desarrolla  el  Proyecto  de  Investigación  E088  –  Software  para  Aprendizaje  y  Trabajo  Colaborativos  orientado  a  la  construcción  de  conclusiones  teóricas  y  soluciones  tecnológicas para la inserción de TICs en el proceso educativo desde la perspectiva  del trabajo y aprendizaje colaborativos. En el contexto del proyecto se desarrollan líneas de investigación que buscan en el  campo  de  la  didáctica  de  la  informática  definir  enfoques  pedagógicos  para  la  disciplina en el contexto del aprendizaje colaborativo y en el campo de la informática   educativa identificar cuáles son las posibilidades de la informática y del aprendizaje  colaborativo para potenciar la construcción de conocimientos. Además  es  de  interés  en  el  campo  del  planeamiento  educacional  definir  y  desarrollar  de  los  elementos  de  la  informática  que  logren  constituirse  en  soporte  tecnológico  para  la  gestión  de  procesos  educativos  en  el  marco  del  trabajo  colaborativo.  Un problema inherente al planeamiento educativo es la producción de información,  explicativa de la realidad contextual en la que se realiza la experiencia educativa, que  apoye la toma de decisiones en la formulación, ajuste y evaluación de las propuestas  de trabajo. Una de las estrategias que se utiliza en el abordaje de esta problemática es  la  recolección  de  datos  mediante  encuestas,  siguiendo  las  fases  operativas  de  la  investigación estadística. Este proceso es, en general, de carácter colectivo por lo que el tratamiento de la  problemática  es  susceptible  a  ser  abordado  desde  la  perspectiva  del  trabajo  colaborativo en el contexto del enfoque propuesto para el desarrollo de PECSCs -  Proyectos Educativos Colaborativos Soportados por Computadoras[1].  En este trabajo se presenta una herramienta informática que soporta el proceso de  realización de encuestas desde el diseño de instrumentos de indagación, la recolección  de datos a la presentación de información. La aplicación implementa los conceptos  especificados  para  el  desarrollo  de  PECSCs  en  lo  referente  al  esquema  de  colaboración y al planteo metodológico. En segunda instancia se describe el flujo de interacciones que se observa durante el  proceso  de  indagación  utilizando  la  aplicación  como elemento  computacional  de  soporte. En la  próxima sección se realiza una descripción más precisa del  problema,  se  expresan las opciones metodológicas que se ubican como referencia conceptual para  el desarrollo de la aplicación y se exponen las elecciones adoptadas en relación a la   tecnología para la implementación. En la sección “Propuesta” se describe la herramienta informática que soporta el   proceso de realización de encuestas y se describe el flujo de interacciones observado  durante el proceso de obtención de datos. En la sección “Conclusiones” se expresan las conclusiones obtenidas en el campo  teórico y en el campo de la praxis. 2   Marco de referencia “La  pedagogía  es  sensible  al  contexto.  Cada  situación  pedagógica  cotidiana  es  necesaria  interpretarla  de  acuerdo  al  contexto  para  hacerlas  significativas  y  comprensibles  desde  un  punto  de  vista  pedagógico.  La  pedagogía  es  sensible  al  contexto de las historias personales. Las historias personales inducen a la reflexión  pedagógica.”[2]. Un problema inherente al planeamiento educativo es la producción de información  explicativa de la realidad contextual en la que se realiza la experiencia educativa. Si   bien ésta es necesaria para la  toma de decisiones que se efectúan al  momento de  formular, desarrollar, ajustar y evaluar una propuesta de trabajo, frecuentemente es  obviada por las dificultades que se presentan al procurar obtenerla. Siendo  éste  un  punto  neurálgico  del  proceso,  la  propuesta  operativa  consiste  desarrollar  una solución tecnológica que posibilite  la indagación de manera ágil y  productiva, sobre la multiplicidad de aspectos que es preciso conocer muchos de los  cuales permanecen, por lo general, en una dimensión no manifiesta. El aporte práctico de esta alternativa superadora radica en la utilidad que brinda el  sistema de gestión de encuestas, como constructor de referencias para el diseño de  elementos  generales  y  particulares  que  permiten  el  desarrollo  de  acciones  pedagógicas. Se debe tener en claro que la tarea de indagación no se asume como un fin en si   misma,  sino que se la  ejecuta por el  valor  de proporcionar indicios  que permiten  repensar, orientar y hasta modificar aquello que se considera superable. El mecanismo  previsto para de captar información relevante, acompaña todo el proceso educativo, es  componente ineludible del diagnóstico, del proceso y del término. Se trata de una construcción colectiva, que requiere de la intervención de diversos  actores: los responsables de la experiencia, los protagonistas de la realidad analizada  y de especialistas externos que contribuyen al fortalecimiento de la formulación de  instrumentos y el análisis de resultados. La caracterización expresada para la comprensión del problema hace susceptible su  tratamiento desde las perspectivas planteadas por éste grupo de investigación para el  desarrollo de PECSCs en relación al modelado de la colaboración y a las opciones  metodológicas.  La  aplicación  que  se  presenta  en  este  texto  estructura  la  organización  de  los  espacios de colaboración de acuerdo al esquema propuesto para PECSCs que actúa  como organizador de las interacciones entre los sujetos que participan en el desarrollo  de un proceso de indagación particular. El  esquema  de  referencia  se  compone  de  tres  espacios:  En  el  Espacio  de  Construcción los sujetos responsables de la experiencia establecen interacciones con  intención de avanzar en la construcción colectiva del cuestionario y conclusiones a  partir de análisis de información recolectada.  El  espacio  de  Apoyo  se  integra  por  un  grupo  de  colaboradores,  en  general  especialistas en el campo de la estadística y posiblemente algunos sujetos del grupo  objeto  de  observación,  que  a  partir  de  sus  aportes  buscan  dotar  de  solidez  metodológica a las producciones y aproximarlas al grupo.  El  espacio  de  Socialización  se  constituye  en  el  ámbito  social  en  el  que  las  producciones  son  expuestas,  en  este  contexto  el  cuestionario  es  puesto  en  juego  haciendo efectiva la recolección de la información. Más adelante en el espacio en el   que el resultado del proceso es expuesto. En  relación  al  abordaje  metodológico,  la  aplicación  presentada,  aporta  las  herramientas que posibilitan el abordaje del proceso de construcción de encuestas de  acuerdo a los planteos sugeridos para el desarrollo de PECSCs. En este contexto se   plantea  estructurar  metodológicamente  la  experiencia a  partir  de los  conceptos  de  trabajo colaborativo, abordaje por proyecto y construcción colectiva. Desde  la  perspectiva  tecnológica  la  propuesta  se  sustenta  en  los  conceptos  de  software libre y código abierto,  por lo que se optó para la implementación por el   lenguaje de programación PHP y el sistema de gestión de base de datos relacional  MySQL por sus características distintivas y su amplia difusión en este ámbito. 3   Presentación de la herramienta Con intención de proponer una solución a la problemática expresada se desarrolló una  aplicación web que permite la gestión del proceso de recolección de datos mediante  encuestas siguiendo las fases operativas de la investigación estadística. Esta  aplicación  implementa  los  tres  espacios  del  esquema  de  colaboración  propuestos, posibilitando a un grupo participar en la construcción de encuestas y la  constitución de un grupo de  apoyo integrado por  especialistas  en el  campo de la  estadística que aporta solidez metodológica y disciplinar a las producciones.  Por  tratarse  de  una  aplicación  web  este  elemento  de  software  cuenta  con  característica significativa para con la problemática planteada: dinamiza el proceso de  obtención de información permitiendo contar con los resultados en forma oportuna. 3.2   Descripción de la aplicación El  sistema  implementa  los  tres  espacios  del  esquema de  colaboración  propuesto,  asignando a cada un usuario diferentes roles de acuerdo al espacio en el que se ubica  su actividad para un proceso de indagación en particular. Es posible que a un usuario  se le asigne un rol en una instancia y uno diferente para otra, para una determinada  encuesta puede participar como autor, en otra como colaborador externo y en otra ser  sujeto a indagar. En el contexto de la aplicación se asignan los siguientes roles: administrador de   encuestas para el espacio de construcción, colaborador para el espacio de apoyo y  encuestador y encuestado para el espacio de socialización. 3.2.1   Espacio de construcción – administración de encuestas En el espacio de construcción realizan actividades propias a la gestión de encuestas  los  autores.  A estos  usuarios  se les  asigna rol  Administrador  de  Encuestas  y son  responsables de gestión de la totalidad del ciclo de vida de la encuesta. Un usuario del  sistema puede tener  asignado este rol  para  una cantidad ilimitada de procesos de  indagación Son atribuciones de los autores son la creación y edición de una encuesta. Para esto  cuentan con la posibilidad de crear una encuesta totalmente nueva o partir  de una  existente.  En relación a  la  edición los  autores  pueden agregar,  modificar  y  quitar  elementos constitutivos de un cuestionario sin restricción mientras que la encuesta  está en estado de diseño y análisis. Otra  actividad  inherente  a  administración  de  encuestas  es  la  selección  de  los  integrantes  del  grupo de colaboradores  externos,  el  sistema permite seleccionar la  cantidad que considere necesaria de entre un grupo de usuarios a los que previamente  se les asignó el rol de Colaborador.  En relación a la población objeto de la indagación los autores pueden seleccionar  uno o varios grupos sobre los que se realizará la consulta y en caso de que el proceso  de indagación sea asistido se selecciona adicionalmente el grupo de encuestadores. Una vez finalizado el proceso de recolección de datos se cierra la encuesta y los  autores  tienen acceso  la  información  aportada  a  efectos  avanzar  en  el  análisis  de  resultados.  El  informe  compuesto  por  tablas  de  frecuencia  y  gráficos  se  genera  automáticamente, lo cual asegura un acceso inmediato a los mismos y garantiza la  confiabilidad de los resultados.  A los  efectos  de  que los  autores  puedan continuar  y  profundizar  el  análisis  el  sistema contempla la posibilidad de descargarlos en formato CSV. Es posible contar  con  informes parciales  en forma on-line  una vez  que al  menos un encuestado ha  aportado respuestas. Ciclo de vida de una encuesta. Los autores gestionan el ciclo de vida de la encuesta,  este se compone de tres fases: Fase de diseño en la que se diseña el instrumento de  indagación en colaboración con los colaboradores externos; Fase de recolección en la  que  se  obtienen  las  respuestas  y  Fase  de  obtención  de  resultados  en  la  que  las   respuestas aportadas son sistematizadas en gráficos y tablas de frecuencia.  El  sistema  implementa  estas  tres  fases  por  medio  de  cuatro  estados  que  a  continuación se describen: En edición, En análisis, Abierta y Cerrada. La imagen a  siguiente (Fig. 1) describe el ciclo de vida. Fig. 1. La figura describe el ciclo de vida de una encuesta durante el proceso de producción de  información. - Fase de diseño - En edición: Este es el estado inicial. Solo los autores tienen   acceso a la edición y visualización de la encuesta.  - Fase de diseño - En análisis: Una vez definida la encuesta, este estado permite a   los colaboradores externos la visualización y  abrir puntos de discusión sobre cada  elemento del cuestionario. -  Fase de recolección -Abierta:  Este estado permite el  acceso al  cuestionario a   sujetos pertenecientes a la población a indagar. Cada encuestado accede por medio de  un navegador y aporta respuestas a las preguntas formuladas. - Fase de obtención de resultados -Cerrada: Una vez obtenidos los datos, se cierra  la encuesta a los efectos de poder analizar los resultados obtenidos. Y avanzar en la  producción de informes. Descripción de una encuesta. En las encuestas implementadas por el sistema hay  dos características que el autor debe especificar inicialmente: la modalidad y el nivel  de privacidad.  En  cuanto  la  modalidad  de  la  encuesta,  se  consideran  dos  posibilidades:  Autoasistida,  en  la  que  el  encuestado  contesta  sin  asistencia  del  encuestador  y  Asistida, en la cual la indagación es acompañada por presencia del Encuestador que lo  guiará en el proceso de respuesta a la misma.  En lo referido al nivel de privacidad, se implementan dos niveles: Total y Simple.  Este  nivel  es  relevante  en  la  visualización  de  los  resultados  obtenidos  del  grupo  encuestado. En el caso en que el nivel de privacidad sea Total, se obtienen todas las   respuestas  del  grupo  encuestado,  pero  no  es  posible  identificar  las  respuestas  aportadas por usuario determinado. Si el nivel  de privacidad es Simple es posible  asociar una respuesta a un usuario específico. La  estructura  de  una  encuesta  en  el  sistema  se  compone  de  un:  Título  y  Descripción orientados a explicar los objetivos y toda información adicional que los  autores consideren relevante. A continuación, la encuesta se organiza en secciones,  las cuales se componen por grupos de preguntas, donde cada grupo mantiene un nivel  de cohesión en lo relacionado con la información que se pretende obtener. Cada sección cuenta con un título y una breve descripción. Las secciones están  numeradas y ordenadas, el sistema permite efectuar modificaciones en dicho orden. En lo relacionado con las preguntas que pueden especificarse en una sección, el  sistema ofrece varios tipos, los mismos son descriptos en la Tabla 1. Tabla 1.  Tipo de pregunta La tabla describe los tipos de preguntas implementados.  Tipo de pregunta Descripción Una respuesta Se ofrecen varias opciones de respuesta, el encuestado  puede seleccionar una de ellas. Varias respuestas Se  proponen  varias  opciones  de  respuesta,  el  encuestado puede seleccionar una o más de ellas. Respuesta libre N o se proponen opciones de respuesta, el encuestado  responde escribiendo un texto. Tabla – numérica Se presenta una tabla las filas contienen los aspectos a  indagar y las columnas contienen la escala numérica. Tabla – respuesta  única Los aspectos a indagar se presentan en las filas  de  la  tabla,  en  las  columnas  contienen  las  opciones de respuesta  para  dichos aspectos.  El  encuestado  puede  seleccionar  una  opción  por  fila. Tabla  –  respuesta  múltiple  Los aspectos sobre los que se indaga se presentan en  las  filas  de  la  tabla,  en  las  columnas  contienen  las  opciones  de  respuesta  para  dichos  aspectos.  El  encuestado puede seleccionar una o más opciones por  fila. Una  respuesta  con  imágenes Se  muestran  varias  imágenes  como  opciones  de  respuesta,  el  encuestado  puede  seleccionar  una  de  ellas. Varias respuestas con  imágenes: Se  muestran  varias  imágenes  como  opciones  de  respuesta, el encuestado puede seleccionar una o más  de ellas. Al igual que las secciones,  las preguntas están numeradas y el  sistema permite  moverlas dentro de la misma sección o a otra sección. Además, las preguntas pueden  ser con respuesta obligatoria o no obligatoria. Una encuesta puede tener tantas secciones como sea necesario. A su vez en una   sección no hay límite en la cantidad de preguntas que la misma puede tener.  3.2.2   Espacio de apoyo – análisis  de encuestas En el  espacio de apoyo el  grupo de colaboradores  externos realiza actividades de  revisión y formulan aportes tendientes a fortalecer su diseño. A estos usuarios se les   asigna en el sistema rol de Colaborador.  Un usuario del sistema puede tener asignado este rol para una cantidad ilimitada de  encuestas. Un cuestionario puede ser asignado a varios colaboradores constituyendo  un grupo de apoyo. Son funciones atribuidas a los colaboradores externos abrir puntos de discusión  para  cualquiera  de  los  elementos  que  componen  el  cuestionario,  participar  de  discusiones abiertas por otros colaboradores y establecer dialogo con los autores. Para  esto puede formular observaciones, emitir opinión acerca de alguna pregunta, sugerir  modificaciones o responder inquietudes de los autores. Los colaboradores no tienen  facultades para participar de la edición. 3.3   Flujo de interacciones  A continuación se describe el flujo de interacciones que se observa durante el proceso  de indagación utilizando la aplicación como elemento computacional de soporte. La  figura próxima (Fig. 2) muestra las interacciones que se producen desde la fase de  diseño del instrumento de indagación hasta la publicación de resultados.  Fig.  2. La  figura  describe  el  flujo  de  interacciones  que  observa  durante  el  proceso  de  producción de información. 1. En el espacio de construcción el grupo responsable de la experiencia comparte la  formulación de un cuestionario. Cada integrante accede a la aplicación mediante un  navegador web por lo que el grupo está liberado de coincidir en tiempo y espacio. En  el  momento en que el  grupo considere oportuno exponen la producción parcial  al  grupo de apoyo. 2. Desde  el  espacio  de  apoyo  se  observa  la  encuesta  en  desarrollo  y  realizan  aportes tendientes a fortalecer el diseño del instrumento. Es posible establecer puntos  de  discusión  independientes  para  componente  del  cuestionario.  Las observaciones  planteadas por cada colaborador externo es visible a los demás y a los autores. 3. El grupo de autores retoma la edición de la encuesta tomando como referencia   las  discusiones  abiertas  y  la  propia  valoración  de  cada observación.  Realizan  los  ajustes  que  consideran  pertinentes,  esta  dinámica  se  repite  hasta  que  el  grupo de  autores  considere  que  la  producción  alcanzó  una  calidad  aceptable.  Entonces  se  procede a la publicación. 4. En el espacio de socialización, desde cualquier localización y plataforma que  permita  el  acceso  mediante  un  navegador  web,  los  sujetos  indagados  acceden  al  cuestionario y emiten respuestas a cada pregunta. Esta acción no requiere descargas  ni  instalaciones, es suficiente con disponer de acceso a contenido web por medio de  algún navegador. 5. El  sistema  de  gestión  de  encuestas  procesa  automáticamente  las  respuestas  generando  diferentes  resultados.  Nóminas  de  respuesta,  tablas  de  frecuencia  y  gráficos.  Opcionalmente  el  grupo  de  responsables  de  la  experiencia  publica  un  resumen de resultados compuesto por una selección de tablas y gráficos. 4   Conclusiones  Los conceptos desarrollados en el contexto del proyecto de investigación en relación  la estructuración del la colaboración y el abordaje metodológico aportan el marco de  referencia para avanzar en el diseño de soluciones tecnológicas para el tratamiento de  la problemática planteada. La articulación de estas concepciones con los avances alcanzados en el campo de  herramientas para el desarrollo de aplicaciones web, en particular en el ambiente del   software  libre  y  el  código  abierto,  posibilitaron  la  construcción  de  una  solución  tecnológica al  problema de la recolección de datos en el  campo del planeamiento  educativo. La solución propuesta logró posicionarse como soporte tecnológico para la gestión  de procesos educativos en el marco del trabajo colaborativo.	﻿contexto de la colaboración , gestión de encuestas	es	22621
26	Estereotipos UML para aplicar en un ambiente de simulación de procesos mineros	﻿ El diseño de estructuras de un sistema y la descripción de  su  conducta se puede llevar a cabo eficientemente a través del Lenguaje de  Modelado Unificado (UML), el cual permite, a su vez, introducir mecanismos  de extensión (estereotipos y perfiles) inherentes al mismo lenguaje, para crear  dominios específicos. En este trabajo, se muestra cómo esos mecanismos  propuestos por UML para definir un metamodelo se aplican a un dominio  concreto de simulación, referido al área de procesos mineros, específicamente a  situaciones de carga y transporte de mineral. A partir de allí se proponen un  conjunto de estereotipos, cuyas equivalencias con  las clases de UML, se  obtuvieron por ser éstas las que más se le aproximan semánticamente. Esto  permite utilizar a UML como una técnica de pre-simulación para definir el  dominio, garantizando un modelo bien formado, que responde a los requisitos  sostenidos por la OMG.  Keywords: UML, simulación, estereotipos, diagramas de actividad    1 Introducción  Enfocar un sistema cualquiera desde distintos puntos de vista permite captar claramente  la visión de lo que se quiere realizar. Esto sólo es posible  a través de una notación  sólida  que sea comprendida de igual forma tanto por los analistas, desarrolladores y  clientes. Tal función la desempeña eficazmente el lenguaje gráfico UML (en inglés,  Unified Modeling Languaje) [1], siendo una herramienta que surge a mediados de 1990  como el estándar a seguir que cambió el mundo del software [6]. UML es un lenguaje  que conlleva reglas específicas, poseedor de semánticas y sintaxis propias.      Si bien UML  ha logrado establecerse como una  herramienta de modelado capaz de  capturar el aspecto dinámico de un sistema, el lenguaje no puede, en forma precisa,  especificar conceptos de determinados dominios. Es por ello que UML, incorpora el  concepto de perfiles UML (UML Profiles), definido por la OMG (Object Management  Group) en su especificación [2],  como un conjunto predefinido  de  estereotipos,  valores etiquetados, restricciones e iconos de notación que colectivamente especializan    y  adaptan UML a un  dominio   específico o proceso. Un conjunto   de  perfiles que  amplían  la semántica y la sintaxis de UML  han sido  estandarizados por la OMG [7]   [8].  Acerca del dominio sobre el cual se va a realizar el metamodelo,  puede decirse que,  por ser  una herramienta orientada al proceso, el desarrollo de modelos a través del  software de simulación Arena [3], [4], [5] se estructura sobre una base gráfica asociada  a la construcción de diagramas de flujo, que describe la serie de pasos que debe seguir  una entidad conforme avanza en el sistema.   Como punto de partida, en este trabajo se define el metamodelo que representa el  dominio de aplicación, lo cual se muestra en la Sección 2. Una vez definido el  metamodelo se definen los estereotipos en la Sección 3. Por último, en la Sección 4 se  incluyen las conclusiones.     2 Descripción del Dominio de Aplicación  En general,  un software de simulación posibilita la construcción de los modelos a  través de una serie de  formas o bloques gráficos  que permiten desarrollar las  descripciones de los procesos que están involucrados en el sistema, también permiten,  en algunos casos,  acceder al código en el cual el modelo de simulación está  construido. La estructura jerárquica se muestra en la Figura 1 donde puede observarse  que el alto nivel de modelamiento implica menor flexibilidad porque incluye mayor  grado de  asistencia gráfica, en cambio, al descender el nivel de modelamiento se  aumenta la flexibilidad porque disminuye la asistencia gráfica por la implementación  de un código específico  en este tema.      Desde esta perspectiva, el modelo se desarrolla utilizando las formas o bloques que  forman parte de los procesos básicos, porque son los elementos básicos  de construcción  de modelos.  La dinámica asociada a los procesos  se puede describir como nodos de  una red donde circulan las entidades causando un cambio en el estado del sistema hasta  que finalmente salen del mismo. Las entidades, que tienen atributos y variables que  permiten diferenciarlas entre ellas, compiten por los servicios que les brindan los  recursos [4].     Fig. 1. Estructura jerárquica de un software de simulación       Siempre que una entidad ocupa un recurso lo debe liberar en algún momento en el  modelo. En la Figura 2 se muestran algunos de los bloques imprescindibles para  representar cualquier situación de simulación y sobre los cuales se van a establecer las  equivalencias con las clases de UML.       Los bloques requieren cierto tipo de datos de entrada [3], que son imprescindibles  para el buen funcionamiento de la simulación. En el caso que se trate de un punto de  partida (Create) de dónde una entidad  ingresa al sistema, requiere que se especifique el  nombre y tipo de entidad, el tiempo entre llegadas, las unidades (minutos, segundos,  horas, días), las unidades por llegada y el número máximo  de arribo. Este bloque, a su  vez tiene asociado un bloque  de datos  (Entity), que requiere un nombre, una  imagen,  datos de costo, entre otros.  El bloque que se refiere al uso de un servidor (Process),  requiere que se le indique el nombre del proceso, la acción, el tipo y cantidad de  recursos, el tipo de demora y las unidades. Este bloque tiene asociado dos bloques de  datos, un bloque referido a las colas de espera (Queue)  y otro bloque referido al  recurso empleado para llevar acabo un proceso determinado (Resource), los cuales  tienen sus propios atributos. Una vez liberado el servidor, la entidad puede salir del  sistema o regresar a él. El bloque de salida (Dipose) sólo requiere  un nombre, sin  necesidad de atributos adicionales.  La Figura 3 muestra un ejemplo de cómo se  relacionan los bloques para formar un modelo de simulación.    Fig. 2. Bloques básicos        Fig. 3. Ejemplo de modelo de simulación        3 Definición de Estereotipos para el Dominio de Simulación de    Procesos Mineros      A menudo sucede que es necesario adaptar UML a un dominio en particular.  Para  ello, UML se  vale de un mecanismo denominado perfil, el cual incluye tres elementos:  estereotipos, valores etiquetados y restricciones.  Los estereotipos extienden el  vocabulario de UML y es posible asociarle valores etiquetados (atributos asociados a  los elementos extendidos) y restricciones [9], [13]. El principal constructor del perfil es  el estereotipo, se indica como <<estereotipo>>, que le corresponde la misma estructura  (atributos, asociaciones, operaciones) del metamodelo de UML, por lo cual  no está  permitido modificar la semántica, la estructura y los conceptos originales  [10] de UML  estándar.       Los estereotipos propuestos para modelar el dominio de procesos básicos  del  software de simulación se corresponden de manera directa con los conceptos descriptos  en el metamodelo. Los bloques a los cuales se aplica un estereotipo son aquellos  bloques que resultan imprescindibles y necesarios en cualquier situación de simulación  que vaya a crearse a posteriori y que requieren ser referenciados para incorporarlos en  el perfil de UML.       Los estereotipos propuestos están relacionados con los componentes del Diagrama  de Actividad [11], porque por su naturaleza se adapta sin dificultad para caracterizar el  modelo que se quiere representar [12]. Este diagrama ya tiene establecido las  notaciones y conceptos adecuados que muestra los pasos (actividades), puntos de  decisión y bifurcaciones que ocurren en una operación, de tal forma que hace más  sencilla su visualización. En la Figura 4 puede verse un ejemplo del Diagrama de  Actividad donde se pueden identificar los nodos de contención ( nivel donde ocurre las   actividades fundamentales), las secuencias de control y flujo de datos entre acciones  (nivel donde ocurren las  actividades básicas) y la concurrencia de flujos y decisiones (  nivel donde ocurren las actividades intermedias).  A simple vista pueden apreciarse las  similitudes generales con la Figura 3 y la razón por la cual se extienden las clases  pertenecientes a Diagrama de Actividad en particular.      Fig. 4. Ejemplo Diagrama de Actividad de UML          El ejemplo de la Figura 3 se genera a partir de un diagrama de actividad, Figura 5,   en un ambiente UML, donde se pone en evidencia la mayor expresividad de este  diagrama, aportando un nivel más elevado de claridad en lo que se quiere describir. Por  ello se considera que utilizarlo genera mayor nivel de certeza al momento de modelar  un sistema de transporte minero en un ambiente de simulación, donde se establece la  entrada y salida de camiones de la zona de carga.         Fig. 5. Diagrama de Actividad Base    3.1 Estereotipos propuestos para las metaclases de UML      Una vez analizada la semántica para realizar las equivalencias correspondientes, se   identifican las clases inherentes al Diagrama de Actividad que son necesario extender.  Los softwares de simulación poseen un mayor número de bloques, además de  los  mencionados aquí, algunos de los cuales se identifican con otras clases de UML. En  este trabajo solo se consideran algunas de las clases, las más importantes,  que se  identifican como parte del Diagrama de Actividad.        El bloque Create se corresponde con la clase InitialNode de UML por su  comportamiento similar. Un InitialNode es un punto de inicio para la ejecución de una  actividad. En la especificación de UML, Superstructure v2.1.2 [11],  se lo define como  una generalización de un nodo de control desde donde un objeto comienza a fluir por  el sistema cuando una actividad es invocada. En la Figura 6, perteneciente al paquete  de Actividades Básicas definido en la sintaxis abstracta del Metamodelo del Kernel de  UML de la OMG [11], se muestra esta relación. No tiene atributos adicionales. El  bloque Create tiene una serie de atributos que definen los parámetros de entrada de la  entidad al momento de iniciar la simulación. Esto hace que sea necesario extender la  clase InitialNode a través de un estereotipo denominado <<Create>>.      Fig. 6. Nodos: sintaxis abstracta definida en la especificación de UML      En el punto de inicio de un modelo de simulación se considera el tipo de flujo de  llegada que se genera (si es al azar, programado, constante) así como también el  número de unidades que arriban al sistema en la unidad de tiempo indicada. En el caso  concreto de un proceso minero de carga y transporte, el punto de inicio estaría dado  por cada camión que ingresa al sistema.    En un ambiente de simulación,  los bloques se relacionan unos con otros a través de  conectores que indica hacia dónde se dirige la entidad. Este conector aplica a la  metaclase ControlFlow del Diagrama de Actividad, la cual no es necesario extender  ya que la semántica se corresponde bilateralmente.       El bloque Process se corresponde con la clase Activity [11] de UML. Una actividad  representa un tipo de  proceso que está compuesta por elementos individuales que son  acciones. Estas acciones pueden ser iniciadas porque otros procesos en esos modelos  terminaron la ejecución, o porque los objetos y datos están disponibles o porque  ocurren eventos externos al flujo. Si bien el bloque Process se comporta de manera  similar, el bloque Process tiene algunos atributos propios que son necesarios definir.  Razón por la cual  se extiende la clase Activity, a través de un estereotipo denominado  <<Process>> que contempla los atributos imprescindibles para el buen desempeño del  modelo.      En el área donde se lleva acabo un proceso en un modelo de simulación se debe  especificar la lógica dentro del modelo (si posee una solo modulo donde se lleva la  acción o existe una jerarquía de modulos), el tipo de procesamiento que ocurrirá (si  habrá demora, si se genera una cola, etc.) y la unidad de  tiempo correspondiente. En  el caso de procesos mineros referidos a carga y transporte, el bloque Process  representaría la operación de  cargar, a través de la pala, el camión que ingresa a la  zona de carga del mineral.      El bloque Batch se corresponde con la clase JoinNode [11] de UML por su  comportamiento similar. Un JoinNode es un nodo de control  que sincroniza múltiples  flujos, teniendo varias entradas y una única salida. Es necesario extender esta clase ya  que el bloque Batch posee atributos adicionales que le son propios y que no están  comprendidos en la semántica de la clase JoinNode de UML. Se extiende a través de  un estereotipo denominado <<Batch>>. En un modelo de simulación se debe  explicitar el número de entidades a ser agrupadas como así también cómo serán  agrupadas esas entidades (se refiere a si las agrupa al azar o, por el contrario, las  entidades deben agruparse según un atributo específico).       En el esquema de la Figura 7 se muestra, en forma resumida, cómo las instancias de  los estereotipos deben siempre estar linkeadas a las instancias de las metaclases  extendidas. Las metaclases corresponden al Diagrama de Actividad de UML. El  esquema está basado en la jerarquía de capas de UML establecida en su  Infraestructura [1].      Para comprender su aplicación en un sistema de transporte minero, si los conceptos  del modelo del nivel M1 se trasladan a un caso en particular, entonces el estereotipo  <<Create>> ya no sería Create 1, sino que se denominaría Camión A. En el caso del  estereotipo <<Process>> se denominaría, en lugar de Process 1, Cargar Mineral. Esto  puede aplicarse a cualquier  yacimiento. En un caso concreto de una yacimiento con  una flota de camiones específica, Camión A sería remplazado por la marca del  camión, por ejemplo CAT D25D. Si el yacimiento en explotación fuera caliza, la  operación Cargar Mineral, sería Cargar Caliza.    Fig. 7. Esquema representando las equivalencias entre metaclases y estereotipos basado en la  jerarquía de UML establecida en la especificación de la OMG    4 Conclusiones        En este trabajo se presentaron una serie de estereotipos que forman parte de un  modelo que se desarrolla en un ambiente de simulación. El hecho de utilizar un  lenguaje estándar como UML para aplicarlo a este dominio en particular, implica que  se obtiene un modelo con estereotipos que responden a reglas bien formadas. Es una  ventaja contar con un modelo que no permita tener conceptos ambiguos,  es decir,   que no presente un sesgo en alguna dirección. Asimismo, permite generar código en  distintos lenguajes de simulación como también permite basarse en una especificación  precisa utilizando la estructura de la OMG para mostrar la semántica de la extensión  del lenguaje.  El hecho de integrar la visión de estos dos lenguajes (UML y  simulación) contribuye a construir un modelo consistente al proceso a ser simulado;  por ello se utilizó la estructura de los diagramas de actividad, que son los que permiten  modelar aspectos dinámicos y ser una abstracción de  los programas de simulación.	﻿simulación , UML , estereotipos , diagramas de actividad	es	22630
27	Visión de roles de arquitectos de software desde teorías de roles de equipo	﻿ La arquitectura de Software representa durante el desarrollo de un  sistema una visión abstracta del mismo, documentando  requerimientos que van  más allá de los requerimientos técnicos, considerando mas bien la estructura  total del sistema, y previendo la manera en que se llevará a cabo la interacción  entre cada una de sus partes. Generalmente, estas arquitecturas son creadas por  equipos de trabajo, donde cada uno de los integrantes posee sus propios  intereses, ideas y experiencias anteriores. En estos equipos cada una de esas  personas manifiesta un rol determinado. En este trabajo se plantea una visión  integradora de los roles de equipo desempeñados por los arquitectos de software  con los roles considerados por dos conocidas Teorías de roles de equipo. Esta  visión integradora permitirá elegir de manera adecuada a los integrantes de un  equipo de desarrollo de software, y en consecuencia, obtener un producto final  de mejor calidad.    Palabras clave: Arquitectura de Software, Teoría de roles, Roles de equipo,  Arquitectos de Software.  1 Introducción  Con el pasar de los años y ante la necesidad de desarrollar aplicaciones en plazos  cortos, que además sean susceptibles de adaptarse a los cambios dinámicos que exige  la realidad de las organizaciones, surgió el concepto de arquitectura de software.  Una  arquitectura de software es una visión abstracta del sistema que permite documentar   requerimientos que exceden a los aspectos técnicos, considerando la estructura total  del sistema y previendo la manera en que interactuarán todas las partes que lo  conforman [4]. Dado que generalmente, las arquitecturas de los sistemas son creadas  por equipos de trabajo y no por una sola persona, resulta esencial su integridad  conceptual. Esta integridad evita caer en errores causados por malas interpretaciones  de sus integrantes, producto de experiencias personales anteriores, creencias y  conocimientos en determinadas áreas. Cuando esta integridad no se produce, el diseño  no resulta adecuado, y por ende se desarrolla un sistema que no está a la altura de lo  necesitado. Para poder determinar si un sistema resuelve correctamente la  problemática por la cual fue concebido, se deben analizar además de su funcionalidad,  otras propiedades como la modificabilidad, la  interoperabilidad y la portabilidad.  Estos atributos se conocen como atributos de calidad, y están determinados  principalmente por la estructura del software, es decir, la arquitectura de software.  Los equipos de desarrollo de software deben coexistir e interactuar con otros  equipos diferentes, por lo que cada uno de los arquitectos de software que integra esos  equipos debe ser capaz de manifestar determinados roles para alcanzar los objetivos  de la mejor manera.   Un rol puede ser visto como la tendencia de un miembro de un equipo a  comportarse, contribuir e interrelacionarse con otros integrantes de cierta forma [2].  Existen numerosos comportamientos o roles que puede asumir un individuo en un  equipo, pero sólo un número finito de esos comportamientos representa una  contribución real para el equipo y su correcto funcionamiento.   En este trabajo se consideran dos conocidas Teorías de roles de equipo, propuestas  por Belbin [1] y por Mumma [3] se establecen sus correspondencias con los roles  planteados para los arquitectos de software en la Metodología de Desarrollo Centrada  en la Arquitectura (ACDM) [5], y se plantea una visión integradora de  los mismos  que redefine los comportamientos ideales para los arquitectos de software. La  consideración de la visión integradora de roles propuesta en este trabajo, permitirá  seleccionar de mejor manera a las personas que conformen los equipos de desarrollo,  y en consecuencia, obtener un producto software de mejor calidad.   La próxima sección está dedicada a la arquitectura de software, tomando como  referencia la metodología de desarrollo centrado en la arquitectura (ACDM). En la  sección 3 se desarrollan las teorías de roles de equipo, para poder abordar en la  siguiente sección a ACDM desde esas teorías de roles. Finalmente la sección 5  contiene algunas conclusiones.   2. Arquitectura de Software  La arquitectura de software se define como la estructura del sistema, que comprende  elementos de software, las propiedades visibles externamente de esos elementos, y las  relaciones entre ellos. [4] El diseño de la arquitectura resulta importante ya que  permite un grado de abstracción tal, que facilita a la mayoría de las partes interesadas  en la construcción del sistema, a discutir, negociar, entender y consensuar sobre ella.  La metodología ACDM pone a la arquitectura en el centro del desarrollo de software,  con el propósito de dirigir aspectos que van más allá de lo meramente técnico, y  prever situaciones de conflicto durante el desarrollo del sistema. ACDM establece la  existencia de seis determinados  roles dentro del equipo de desarrollo, a saber: [5]   Ingeniero de Requerimientos: Actúa como líder en las reuniones, documentando  requerimientos funcionales. Coordina el descubrimiento de atributos de calidad y  su documentación, la creación de sentencias de trabajo, también sirve como  enlace con los clientes, y coordina los planes y la ejecución de testeo.   Jefe de Arquitectos: Coordina la creación de la arquitectura nocional y su  refinamiento, y también las revisiones arquitectónicas, captura y documenta los  riesgos de arquitectura, las ventajas y las desventajas.   Jefe Científico: Coordina y documenta la creación de experimentos y estudios de  investigaciones, el planeamiento de testeo, su documentación y su ejecución.    Ingeniero de Dirección y Manejo: Coordina todo el esfuerzo sobre el desarrollo,  la creación y documentación de planes preliminares y de producción, y sus  agendas.    Ingeniero de Soporte: Configura y mantiene las herramientas de soporte para el  desarrollo, establece y mantiene presencia web si fuese necesario, asegura que se  sigan los pasos de ACDM, y documenta cambios en caso de alguna desviación.   Ingeniero de Software: Presta especial atención en el detalle del diseño y  codificación de los elementos de arquitectura del sistema. En equipos pequeños,  todos los miembros ofician de ingenieros de software.    ACDM se estructura en base a las fases que se describen a continuación. Para cada  una de dichas fases existen roles y actividades asociadas [5].     FASE 1: Descubrir artefactos de arquitectura  Actividades: Presentar los objetivos de negocios perseguidos, y construir la tabla de  caracterización de atributos de calidad  Roles: El Ingeniero de Requerimientos planea, coordina y facilita el descubrimiento  de artefactos de arquitectura durante la fase 1, y confecciona la documentación inicial  de esos artefactos de arquitectura. El Ingeniero de Dirección y Manejo se asegura de  la ejecución y finalización de las actividades de la fase 1, asiste a los Ingenieros en  requerimientos en la captura de artefactos de la arquitectura, coordinando la logística,  y entrevistando a las partes interesadas en la construcción del sistema, para conocer  sus necesidades y expectativas.   FASE 2: Establecer alcance del proyecto  Actividades: Refinar, clarificar y consolidar la información de la arquitectura inicial.   Roles: El Ingeniero de Requerimientos coordina los esfuerzos del equipo para  clarificar y refinar los artefactos de arquitectura, coordina también la creación y  configuración de la documentación de estos artefactos, y genera  las sentencias de  trabajo. El Ingeniero de Dirección y Manejo supervisa esta fase, se asegura de que los  artefactos de arquitectura sean completados, asiste a los arquitectos en la  representación de la arquitectura, actualiza, refina y publica la información de  planeamiento necesaria, y coordina la creación y la configuración del plan preliminar  del proyecto.   FASE 3: Creación de la Arquitectura Inicial  Actividades: Crear la arquitectura nocional o inicial.  Roles: El Jefe de Arquitectos lidera al equipo en la creación de la arquitectura y en la  creación de su representación. El Ingeniero de Dirección y Manejo provee  seguimiento y supervisa las actividades de esta fase, se asegura que la documentación  de la arquitectura sea completada, coordina la logística, actualiza, refina y comunica  información de planeamiento cuando sea necesario. El Ingeniero de Requerimientos,  el Jefe científico, el Ingeniero de Soporte, y el Ingeniero de Software asisten en la  creación de la arquitectura y de su representación.   FASE 4: Revisión de la arquitectura  Actividades: Revisar artefactos de arquitectura y objetivos de negocios  Roles: El Jefe de Arquitectos presenta la arquitectura, y responde a las preguntas  sobre la misma. El Ingeniero de Dirección y Manejo facilita la reunión de revisión, y  presenta material introductorio. El Ingeniero de Requerimientos presenta una revisión  sobre los artefactos de arquitectura    FASE 5: Decisión de entrar o no en producción  Actividades: Evaluar revisión de riesgos e información sobre ventajas y desventajas  Roles: El Ingeniero de Dirección y Manejo facilita la reunión para la revisión y  presenta el material introductorio.    FASE 6: Planeamiento y ejecución de experimentos, Refinamiento de la  arquitectura  Actividades: Evalúa revisión de riesgos, información sobre ventajas y desventajas.  Planear y ejecutar experimentos. Refinar la arquitectura.  Roles: El Ingeniero de Dirección y Manejo coordina la agenda y los planes de  experimentos, y actualiza la planificación del proyecto. El Jefe Científico coordina el  desarrollo y planeamiento de los planes de experimento, y realiza el seguimiento de la  ejecución de los experimentos, asiste a los ingenieros responsables de los  experimentos en su planeamiento y ejecución. El Jefe de Arquitectos crea y ejecuta  los planes de experimentos, actualiza la documentación de la arquitectura en función a  los resultados de los experimentos. El Ingeniero de Requerimientos actualiza la  especificación de los artefactos de arquitectura basándose en los resultados de los  experimentos, crea y ejecuta los planes de experimentos. El Ingeniero de Soporte crea  los planes de experimentos y los lleva a cabo, se asegura de que estén disponibles las  herramientas y entornos necesarios para la ejecución de experimentos. El Ingeniero de  Software crea y ejecuta los planes de experimentos.    FASE 7: Planeamiento de producción  Actividades: Crear el plan de puesta en producción.  Roles: El Ingeniero de Dirección y Manejo coordina la agenda del plan de puesta en  producción. El Jefe Científico asiste creando y documentando los planes de puesta en  producción enfocándose en las tareas sobre los elementos de diseño. El Jefe de  Arquitectos asiste creando y documentando los planes de puesta en producción   enfocándose en las tareas sobre los elementos de diseño. El Ingeniero de  Requerimientos asiste creando y documentando los planes de puesta en producción  enfocándose en las tareas de testeo de elementos y de integración. El Ingeniero de  Soporte asiste creando y documentando los planes de puesta en producción  enfocándose en las tareas soporte, tales como herramientas de mantenimiento,  tiempos de configuración, backups, entre otros. El Ingeniero de Software asiste  creando y documentando los planes de puesta en producción enfocándose en las  tareas de desarrollo de elementos.   FASE 8: Producción  Actividades: Realizar el diseño detallado de los elementos, revisar diseño, construir   elementos y testearlos, integrar el sistema, y probar dicha integridad.  Roles: El Ingeniero de Dirección y Manejo coordina la agenda de los recursos, y  supervisa el trabajo del equipo sobre el plan de puesta en producción. El Jefe  Científico coordina el diseño y la revisión de los elementos. El Jefe de Arquitectos se  asegura de que los elementos sean diseñados y construidos de acuerdo con las  especificaciones de la arquitectura. El Ingeniero de Requerimientos se asegura de que  los planes de testeo sean ejecutados, y que los elementos sean producidos de acuerdo  con la especificación de los artefactos de arquitectura. El Ingeniero de Soporte se  asegura de que todas las herramientas de desarrollo, configuración, manejo y   seguimiento, se encuentren instaladas y disponibles para el equipo de desarrollo. El  Ingeniero de Software es el responsable de diseñar, desarrollar, revisar y testear todos  los elementos que componen al sistema.  3 Teoría de roles en equipos  Un rol puede ser visto como la tendencia de un miembro de un equipo a comportarse,  contribuir e interrelacionarse con otros de cierta forma [1]. Existen numerosos  comportamientos o roles que podría asumir un individuo en un equipo, pero solo un  número finito de esos comportamientos representan una contribución real para el  equipo y su correcto funcionamiento. La adecuada aparición de los roles tendrá como  resultado que se maximice la productividad del grupo cuando realicen las tareas que  les fueron asignadas. [1]  Por otra parte, resulta evidente que las personas que realizan tareas que son de su  agrado, lo hacen de mejor manera que aquellas que trabajan en algo en lo que no se  sienten a gusto. Permitir a cada integrante del equipo realizar su trabajo, en la etapa o  situación en la que se sienta más a gusto, realizando las tareas que naturalmente  resultan más sencillas de llevar a cabo por cada individuo, tendrá por resultado que se  realicen mayores esfuerzos para alcanzar los objetivos del equipo. Es sabido que los  miembros de un equipo pueden contribuir de dos maneras, desenvolviéndose en un rol  funcional, es decir, basándose meramente en sus conocimientos técnicos y  profesionales según la situación lo demande, o bien,  desempeñando un rol especifico  dentro de un grupo aportando sus comportamientos característicos, lo cual facilita el  progreso del grupo [2].  Existen varias Teorías sobre roles de equipos, siendo las de Belbin [1] y Mumma  [3], dos de las más reconocidas y utilizadas. La teoría de roles de Belbin establece la  existencia de nueve roles necesarios para lograr la efectividad del trabajo en equipo:     Cerebro: Son innovadores e inventores, con una gran creatividad, proveen las  ideas para los desarrollos de las tareas. Generalmente prefieren el trabajo aislado,  lejos del resto de los integrantes del grupo. Tienden a ser introvertidos y a  reaccionar fuertemente ante las críticas y elogios. A veces sus ideas pueden ser  poco prácticas.   Investigador de Recursos: Suelen ser personas entusiastas y extrovertidas. Son  innovadores y personas curiosas, tienen una buena comunicación tanto con las  personas de adentro de la compañía como con las de afuera. Son negociadores  naturales y expertos en buscar nuevas oportunidades y generar contactos. Suelen  perder rápidamente el entusiasmo.   Monitor - Evaluador: Son personas serias, prudentes y poco entusiastas.  Prefieren demorar en la toma de decisiones para poder analizar varias veces la  situación planteada. Tienen una gran capacidad de autocrítica y raras veces se  equivocan.   Coordinador: Son quienes encausan a los demás en el trabajo para alcanzar un  objetivo común. Suelen ser personas confiables, maduras y confidentes, que  delegan trabajo. Encuentran fácilmente los talentos de los individuos para alcanzar  los objetivos del grupo, y suelen tomar los problemas con calma, aunque también  pueden tener conflictos con los compartidores, debido a sus diferentes estilos.   Impulsor: Son personas dinámicas y extrovertidas, con mucha energía, y una gran  necesidad de realización. Les encanta desafiar a los demás y su preocupación es  ganar, prefieren liderar y poner a los demás en acción. Ante situaciones de  frustración o decepciones se suelen mostrar muy emocionales. Son muy  competitivos, y suelen tener discusiones que llegan a ofender los sentimientos de  otras personas.   Implementador: Tienen demasiado sentido común, muy buen autocontrol y  disciplina. Favorecen el trabajo duro, y toman los problemas de una manera  sistémica. Su lealtad e intereses están situados en el bienestar general, y no en su  propio bienestar. Carecen de espontaneidad y muestran signos de rigidez.   Cohesionador: Son miembros sociables y preocupados por los demás, fácilmente  dan ayuda y soporte. Tienen una gran capacidad de adaptación a diferentes  situaciones e inclusive a diferentes personas. Son perceptivos y diplomáticos, con  la habilidad de escuchar a las personas. Son generalmente miembros populares del  grupo, y operan con sensibilidad en el trabajo, pero suelen ser indecisos en  situaciones determinantes.   Finalizador: Tienen la capacidad de observar bien cada uno de los detalles. No les  gusta comenzar tareas que no puedan ser capaces de finalizar. Aunque aparentan  ser personas tranquilas, en realidad son ansiosos, introvertidos y requieren de  estímulos externos o incentivos. No son buenos delegando, prefieren resolver los  problemas por si mismos, y pueden ser intolerantes con los informales.   Especialista: Son individuos con mucha dedicación, que priorizan su aprendizaje  de técnicas y habilidades en conocimiento especifico, manteniendo sus propios  intereses y sin preocuparse por las demás personas. Pocas personas tienen las  aptitudes necesarias para ser un especialista de primera clase.    Por su lado, Mumma [3] considera ocho roles específicos: Líder, Moderador, Creador,  Innovador, Manager, Organizador, Evaluador y Finalizador; pero además, los vincula  con un ciclo de  trabajo en equipo compuestos por cuatro fases. Cada una de estas  fases posee características definidas, con la participación de determinados roles, como  se describe a continuación.    Fase 1: Iniciación. Es donde se define la tarea a realizar. Esa tarea puede ser una  necesidad que tiene que ser satisfecha, una pregunta que se tenga que responder, una  meta que alcanzar, una decisión que tomar o un proyecto que completar. Se plantean  además cuales son los alcances y el rango de trabajo, indicando que resultados se  esperan lograr cuando la tarea este completada. En esta fase no deben considerarse  demasiados detalles porque podría resultar contraproducente. Es importante además,  tener en cuenta los recursos físicos y financieros disponibles para llevar a cabo la  tarea. Los roles involucrados en esta fase son el de Líder y el de Moderador.  Fase 2: Ideación. Es donde se generan diferentes alternativas para alcanzar la tarea a  realizar. Se analizan potenciales soluciones, tratando de responder a preguntas  específicas que surgen con las mismas, y también se analizan los tipos de proyectos  que podrían enmarcarlo. En esta etapa se requiere de mucha creatividad y  originalidad, siempre dentro de las posibilidades de aplicación y tendiendo a   soluciones al alcance de los recursos disponibles. Cada una de las ideas propuestas  debe someterse a crítica, analizando lo peor de cada una de ellas, de manera tal que el  número de posibilidades vaya siendo reducido mediante el descarte de las opciones  menos convenientes. Los roles involucrados en esta fase son el de Creador y el de  Innovador.  Fase 3: Elaboración. Se estudian las ideas surgidas de la etapa anterior en detalle,  profundizando su esqueleto o estructura inicial ya construida. Se definen las personas  necesarias para llevar la idea a cabo, y se analizan las habilidades que deberían poseer  las mismas, el equipamiento necesario teniendo en cuenta su costo, las  disponibilidades económicas, los plazos de trabajo, y posibles conflictos de agenda  entre los integrantes de los equipos asignados. Los roles involucrados en esta fase son  el de Manager y el de Organizador.  Fase 4: Terminación. Se elige una alternativa y se realiza el trabajo. Se determina de  qué manera se la pondrá en marcha, estableciendo las diferentes fases y los  parámetros de éxito o fracaso de cada una de ellas. Durante esta etapa se necesita  mucha percepción de la realidad, usando criterios apropiados y métricas correctas de  los resultados obtenidos, con una gran habilidad de autocritica. Se debe cumplir con  los objetivos superando obstáculos, sin perder el foco esencial de la idea de trabajo.  Los roles involucrados en esta fase son el de Evaluador y el de Finalizador.  4 ACDM desde las Teorías de roles de equipo  Este apartado presenta una visión integradora en la conformación de los equipos de  arquitectos para el desarrollo de un sistema de información. Esta visión integradora se  logró por un lado, analizando la correspondencia entre los roles planteados por Belbin  y los contemplados por la metodología ACDM, y por otro lado, considerando la  similitud y correspondencia entre las fases y roles establecidos por Mumma con las  fases y roles fijados en ACDM. Estas vinculaciones se destallan a continuación.    a) Vinculación de los roles de ACDM con los roles de equipo de Belbin   Ing. de Requerimientos: Por su rol de líder y coordinador de planes y testeo  resulta directamente relacionable con el rol de Investigador de Recursos. Al  interactuar con los clientes en el descubrimiento de requerimiento y atributos de  calidad indudablemente demuestra su capacidad para la búsqueda de nuevas  oportunidades, negociando y formando un vínculo con las partes interesadas en la  construcción del sistema.  Además generan ideas para el desarrollo de la solución  a estos requerimientos, funcionando así como Cerebro en este proceso.   Jefe de Arquitectos: Su actitud poco entusiasta y prudente tanto para diseñar la  arquitectura nocional, como para detectar y evaluar riesgos existentes en el  desarrollo del sistema, hacen del jefe de arquitectos una persona vinculable con el  rol de Monitor – Evaluador. Son asignados a estas tareas críticas, ya que por sus  características, su prudencia y su forma de actuar, pocas veces se equivocan.    Jefe Científico: Coordina y documenta la creación de experimentos y estudios de  investigaciones. Para ello requiere de una gran capacidad de observar detalles  hasta lograr finalizar con cada una de las problemáticas estudiadas en el sistema.  Su función es la de Finalizador dentro del equipo.   Ingeniero de Dirección y Manejo: Tiene un vínculo directo con el rol de  Coordinador. Coordina todo el esfuerzo sobre el desarrollo, y también la  creación y documentación de planes preliminares y de producción, incluyendo  sus respectivas agendas.   Ingeniero de Soporte: Su preocupación por el bienestar general, y su capacidad  de adaptación a diferentes situaciones y personas que pueden resultar de la  desviación de los pasos planificados, determina que el Ingeniero de soporte tenga  una fuerte tendencia a comportarse como Cohesionador del grupo de trabajo.   Ingeniero de Software: Su predisposición con los demás miembros del equipo  para asistirlos en caso de que fuese necesario, y su especial atención a los detalles  del diseño y codificación de los elementos de la arquitectura del sistema, mucho  sentido común, autocontrol y disciplina, marcan a estos ingenieros como los  Implementadores del sistema. En equipos grandes, puede surgir la figura de un  ingeniero Impulsor cuya tarea sea la de motivar y poner en acción a otros  miembros de su equipo para alcanzar el objetivo deseado.    b) Vinculación de las fases y roles de Mumma con las fases y roles de ACDM    Iniciación: Es donde se define la tarea a realizar. En esta fase, al igual que en la  FASE 1 de ACDM, los arquitectos se reúnen con clientes y demás partes  interesadas en la construcción del sistema para determinar cuales son los  requerimientos funcionales de alto nivel, amenazas, o atributos de calidad que  deben ser alcanzados. Se plantean además, al igual que en la FASE 2 de ACDM,  cuales son los alcances y el rango de trabajo, indicando que resultados se esperan  lograr cuando la tarea esté completada. Se construye un plan preliminar de  trabajo. Según la teoría de roles de Mumma, en esta fase, uno de los roles con  mayor participación es el del Líder, conjuntamente con el Moderador.  El líder  en su tarea de identificar la tarea a realizar y motivar a los demás, es comparable  con el Ingeniero de Requerimientos planteado en ACDM. El Ing. de  requerimientos según lo descripto anteriormente, realiza en la fase 1 la  documentación de los primeros artefactos de arquitectura, y en la fase 2 asegura  que la documentación sea completada y genera las sentencias de trabajo. Siempre  con la asistencia y colaboración del Ing. de Dirección y Manejo.    Ideación: Es donde se generan diferentes alternativas para alcanzar la tarea a  realizar. Se analizan potenciales soluciones, de igual manera que ACDM en su  FASE 3 crea una arquitectura nocional, con las primeras vistas de código, físicas  y de ejecución del sistema. Se intenta responder a preguntas específicas que  surgen con las mismas. Los roles de Mumma con mayor participación en esta  fase son el del Creador y el Innovador. El creador es quien debe generar  aportes  creativos para alcanzar la tarea del grupo, de igual manera que el Jefe de  Arquitectos lidera al equipo en la creación de la arquitectura y en la creación de  su representación. Estas tareas siempre realizadas con la ayuda del Ingeniero de  manejo y soporte en la coordinación de la logística, y el apoyo del Ingeniero de  requerimientos.   Elaboración: Se estudian las ideas surgidas de la etapa anterior en detalle,  profundizando su esqueleto o estructura inicial ya construida. Se analizan costos   y disponibilidades, y al igual que en la FASE 4 de ACDM, se descubren y se  documentan los posibles riesgos y amenazas. En esta fase los roles de Manager y  Organizador son los predominantes según la teoría de Mumma. La tarea del  manager es mantener los miembros del equipo trabajando sin conflictos entre  ellos. Por otro lado, el organizador garantiza la existencia y el uso adecuado de  los recursos necesarios. Por parte de ACDM, El Ingeniero de Dirección y  Manejo Facilita la reunión de revisión, y presenta material introductorio, junto  con el Ingeniero de Requerimientos que presenta una revisión sobre los  artefactos de arquitectura. La FASE 5 de ACDM esta destinada a determinar si se  necesitan refinamientos y nuevos experimentos sobre lo que ya se ha diseñado,  antes de entrar en la producción del sistema. Si es necesaria esta revisión, se  ingresa a la FASE 6 de ACDM que es donde se diseñan y ejecutan los  experimentos e investigaciones. Esta iteración tiene analogía directa con la fase  Elaboración en donde se estudian las diferentes alternativas y se las profundiza  teniendo en cuenta las ventajas y desventajas de cada alternativa escogida.    Terminación: Se elige una alternativa y se realiza el trabajo. Se establece como  en la FASE 7 de ACDM, una planificación de la construcción del sistema. Se  establece la agenda, y los objetivos a lograr, determinando que parámetros  significaran el éxito o fracaso de cada una de ellas. Al igual que en la FASE 8 de  ACDM, con mucha percepción de la realidad, usando criterios apropiados y  métricas correctas para evaluar los resultados obtenidos, el equipo ejecuta el plan  de producción, y se propone a construir el sistema, actuando con una gran  habilidad de autocritica. Esta etapa incluye la construcción de todos los  elementos del sistema, su integración y su correspondiente testeo. En la ultima  fase de la teoría de Mumma, aparecen los roles de Evaluador y Finalizador.  El  evaluador analiza detenidamente diferentes alternativas, evalúa planificación y  los resultados previstos para cada etapa. El finalizador, sigue el desarrollo de la  planificación cuidadosamente, atendiendo con especial cuidado cada uno de los  detalles. En ACDM, durante la FASE 7 el Ingeniero de Dirección y Manejo  Coordina la agenda del plan de puesta en producción. Mientras que el Jefe  Científico, Jefe de Arquitectos, Ingeniero de Requerimientos e Ingeniero de  Soporte, asisten con tareas especificas y documentando lo realizado durante esta  etapa. Por otro lado durante la etapa final, el Jefe Científico coordina el diseño y  la revisión de los elementos. El Ingeniero de Soporte se asegura de que se  encuentren disponibles todas las herramientas necesarias, y el Ingeniero de  Software es responsable de diseñar, desarrollar, revisar y testear todos los  elementos que componen al sistema.  5 Conclusiones  Al momento de armar un equipo de arquitectos, se deben tener en cuenta las  habilidades técnicas o profesionales de cada integrante, y también otros factores que  pueden afectar o influir en el funcionamiento del equipo, como preferencias,  experiencias anteriores, y demás características que determinen que cada individuo se  sienta cómodo realizando su trabajo, disfrute del mismo, y por ende lo haga de la  mejor manera.   En particular, el trabajo realizado permitió definir una visión integradora de los  comportamientos ideales en un equipo de arquitectos de software, integrando  diferentes clasificaciones de roles de equipo (Tabla 1). Contar con arquitectos que  naturalmente manifiesten estos roles durante la dinámica de trabajo del equipo de  desarrollo resultará beneficioso y asegurará resolver la problemática de la mejor  manera. Por lo tanto, resulta una buena opción tener en cuenta el rol que desempeña  mejor cada uno de los miembros al momento de asignar las tareas. Si bien se cree que  esto garantizará el correcto funcionamiento del equipo de arquitectos, y seguramente  con ello, se obtenga un mejor resultado en el desarrollo e implementación del sistema  en cuestión, también se considera necesario efectuar en el futuro la experimentación  necesaria para validar la efectividad de la integración de teorías planteada en este  trabajo en relación con la conformación de los equipos de arquitectos de software.      Roles ACDM Roles de equipo   (Belbin)  Roles de equipo (Mumma)  Ingeniero de Requerimiento Investigador de Recursos Creador   Cerebro Innovador   Especialista   Ingeniero de Dirección y Manejo Coordinador Líder  Jefe de Arquitectos Monitor-Evaluador Evaluador  Ingeniero de Manejo y Soporte Cohesionador Moderador  Jefe Científico Finalizador Finalizador  Ingeniero de Software Implementador  Impulsor  Organizador   Tabla 1. Vinculación entre roles de equipo	﻿Roles de equipo , Arquitectura de Software , Teoría de roles , Arquitectos de Software	es	22641
28	Detección de plagio intrínseco basad en histogramas	﻿La detección de plagio intrínseco obtiene las secciones de un texto que se sospecha no fueron escritas por el autor del mismo, en base a las variaciones estilográficas observadas en el mismo. En este trabajo, se analiza la factibilidad del uso de histogramas globales para modelar el estilo de escritura de un autor y la detección de outliers, una subtarea de este tipo de detección, que identifica los cambios de estilos en el histograma. El algoritmo de detección fue testeado en el corpus de la Competencia de Detección de Plagio PAN-PC-2011, obteniendo un desempeño aceptable en comparación con los otros detectores participantes de la competencia. Palabras Claves: detección de plagio intrínseco, histogramas. 1. Introducción Todo trabajo tiene un derecho de autor intelectual y si se utiliza el mismo debe ser reconocida su autoría. Toda utilización del mismo sin consentimiento del autor se denomina plagio [11]. El plagio de texto se lleva a cabo cuando un texto está compuesto de fragmentos, cuya propiedad intelectual no le corresponde al autor que se atribuye su autoría. Por ejemplo, un caso de plagio muy frecuente es aquel en que se utilizan fragmentos que son copias exactas de otro texto, siendo éste el más simple de detectar. Es ampliamente reconocida la importancia de suministrar herramientas informáticas que permitan a los usuarios la detección automática de plagio [1]. Esta detección se dificulta cuando, a diferencia de la copia exacta del fragmento original, se modifica el mismo cambiando el orden de las palabras, reemplazando las palabras por sinónimos, sustituyendo oraciones largas por cortas etc. En este contexto, la detección de plagio se puede clasificar en términos generales como extrínseca o intrínseca [11]. En la primera, se debe proporcionar una colección de documentos de referencia además del texto a analizar y se provee como resultado las secciones en donde se produjo el plagio y la sección que se utilizó en el archivo de referencia. La detección intrínseca en cambio, no utiliza un conjunto de archivos de referencia y realiza un análisis de estilo en el texto para detectar 2 Dario G. Funez y Marcelo L. Errecalde las variaciones estilográficas. La salida del detector en este caso, son los fragmentos de texto que estilísticamente se desvían del modelo observado a lo largo del texto. La detección de plagio intrínseco es una tarea relativamente nueva e importante que ha captado la atención de distintos grupos de investigación, como ha quedado demostrado en el último Congreso Internacional de Plagio PAN-PC2011. En esta competencia, la propuesta presentada en [9] ganó la competencia del 2011 con un detector que utiliza desviación de parámetros de segmentos de texto con respecto al estilo de escritura del documento completo. La comparación entre los segmentos y el texto completo se realiza utilizando vectores de frecuencias de palabras y se calcula un valor de referencia del estilo del autor que se obtiene con el promedio de todas las diferencias de los segmentos y el texto completo. Los segmentos se clasifican como plagiados si distan de este valor. Kestemont por su parte [6], obtuvo el 2o puesto en la misma competencia con un modelo estilográfico que comprende la obtención de frecuencias de tri-gramas predefinidos más frecuentes utilizado por los autores en general. La detección de outliers se consigue con una matriz de distancia que almacena la distancia entre cada ventana con todas las demás. Los autores de [4] por su parte, en base a los buenos resultados obtenidos con el uso de histogramas en atribución de autoría, sugieren su utilización para modelar estilos de escritura en detección de plagio. Para obtener los histogramas, utilizan el framework lowbow [7], el cual ha sido empleado en diferentes aplicaciones como la visualización de documentos, la segmentación de películas en capítulos y escenas y la segmentación discursiva entre otros. En este trabajo, analizamos la factibilidad de la representación con histogramas utilizada en [4], en la detección de plagio intrínseco. De acuerdo a nuestro conocimiento, esta técnica no ha sido utilizada previamente para la modelización de estilos de escritura en tareas de detección de plagio intrínseco. El modelo estilográfico en este caso, difiere significativamente de otros modelos utilizados previamente, ya que cuenta con mucha más información, no solamente respecto a puntos aislados, sino respecto a la gráfica completa que representa el histograma. En los métodos de detección de outliers más conocidos, sólo se suele disponer de un conjunto de puntos aislados y se eligen los puntos distinguibles del resto. Un detector intrínseco, en general, debe definir las siguientes tres etapas: la descomposición del texto, la construcción del modelo estilográfico y la detección de outliers [11]. En nuestro caso, las dos primeras etapas las realiza el framework lowbow, al que se le suministra la cantidad de muestras determinadas por un algoritmo de segmentación de texto; luego, la detección de outliers elige los puntos extremos de la gráfica del histograma. Como se verá en la Sección 6, los resultados obtenidos en el corpus PAN-PC-2011 ofrecen evidencia que el uso de histogramas es una buena opción para el modelado estilográfico. El resto del trabajo se compone de las siguientes secciones. En la Sección 2 se describen aspectos generales de la detección intrínseca. La Sección 3 explica el algoritmo de segmentación de texto empleado en este trabajo. En la Sección 4 se describe el framework lowbow. En la Sección 5 se presentan los detalles de Detección de plagio intrínseco basada en histogramas 3 implementación del detector propuesto. La Sección 6 describe los experimentos realizados y los resultados obtenidos. Finalmente, en la Sección 7 se ofrecen las conclusiones y el trabajo futuro. 2. Detección de Plagio Intrínseco El problema de la detección de plagio intrínseco puede enunciarse de la siguiente manera: dado un texto t de un único autor, identificar las secciones de t que han sido redactadas por otros autores [11]. Se asume que el texto fuente fue escrito por un único autor, ya que si se considerara que varios autores colaborativamente escribieron el texto, la complejidad de la tarea de detección se incrementaría. La detección de plagio intrínseco ha ganado una relevancia creciente, ya que no necesita disponer del conjunto de archivos de donde posiblemente se extrajo la información. Este conjunto, a veces es imposible de obtener dado que existe información que no está disponible en la Web o en otros medios digitales y por lo tanto no se puede llevar a cabo la comparación con dichos archivos. La desventaja que tiene este tipo de detección, es que no puede asegurar con precisión la prueba de plagio, al no tener el documento fuente. El análisis intrínseco se fundamenta en el hecho de que cada autor tiene un estilo de escritura propio que se mantiene a lo largo de todo el texto [2]. Este estilo de escritura del autor necesita estar representado por un modelo que suele estar compuesto de medidas estilográficas que caracterizan el estilo de escritura individual del autor [11]. La detección intrínseca utiliza las variaciones significativas del modelo para obtener las secciones plagiadas. En este problema, la única clase que se conoce es la asociada con el estilo de escritura del autor, por lo que se lo considera como un problema de una única clase (one class clasification) [12] y los restantes estilos de escritura son denominados outliers. En [11] por su parte, se divide la tarea de detección intrínseca en las siguientes 3 subtareas: a) estrategia de descomposición, b) construcción del modelo de estilo y c) identificación de outliers, que se describen a continuación. 2.1. Estrategia de descomposición El documento completo necesita ser dividido para obtener información del estilo de escritura en diferentes puntos del texto. La estrategia elegida en este caso se debe seleccionar con mucho cuidado, ya que el desempeño del detector dependerá fuertemente de esta etapa. Las estrategias más simples pueden ser más sencillas de implementar y más rápidas en ejecución, pero no siempre obtienen una buena performance, como es el caso de dividir el texto en bloques del mismo tamaño. La división del texto en límites estructurales (como por ejemplo párrafos o sentencias) puede ser una mejor opción, debido a que las secciones plagiadas suelen ser párrafos completos. Una alternativa que ha logrado resultados interesantes consiste en dividir el texto en segmentos coersivos utilizando un algoritmo de segmentación de texto, como hemos propuesto previamente en [5]. 4 Dario G. Funez y Marcelo L. Errecalde 2.2. Construcción del modelo de estilo Para detectar variaciones en el estilo de escritura se necesita un modelo con información estilográfica del autor del texto. Un escritor, inconcientemente mantiene en todo sus escritos un mismo modelo, por lo que una variación significativa en el mismo hace sospechar que se esta usando otro estilo de escritura y que posiblemente un autor no referenciado sea quien lo escribió. El modelo estilográfico más común, se construye con un conjunto de medidas estilográficas, como por ejemplo índices de legibilidad, frecuencias de clases de palabras como adjetivos y sustantivos, la riqueza de vocabulario, etc [11]. Sin embargo, una opción interesante, y la propuesta en este trabajo, consiste en representar un estilo de escritura mediante histogramas globales de palabras, la cual ha demostrado ser efectiva en tareas de atribución de autoría [4]. 2.3. Identificación de outliers Con la información de la etapa anterior, se caracteriza el estilo de escritura del redactor en diferentes puntos del texto. En un texto escrito por un único autor, el estilo de escritura debe permanecer con alteraciones mínimas en todo el texto completo. Su modelo es la única información con que se cuenta y de esta manera se define un problema de clasificación de una única clase [12], ya que no es posible caracterizar el estilo de escritura de todos los posibles autores. A este tipo de problemas también se los denomina como detección de outliers ya que se deben elegir aquellos puntos en el texto en donde el estilo de escritura es significativamente diferente a las demás muestras extraídas del texto [11]. Los métodos de detección de outliers se pueden clasificar en tres categorías: Métodos de densidad : aproximan la función de densidad de probabilidad de la clase objetivo. Se considera a los outliers uniformente distribuidos y la regla de Bayes se puede utilizar para diferenciar objetos outliers de los objetivos. Estos métodos proveen buenos resultados cuando el tamaño de la muestra es grande. Métodos de límite: tratan de delimitar una región utilizando distancias entre los elementos objetivos. Los outliers son aquellos objetos que no están comprendidos en esa región. Métodos de reconstrucción: necesitan conocimiento previo sobre la generación de los elementos objetivos. Los outliers son aquellos objetos que son difíciles de reconstruir. En la siguiente subsección, se definen las medidas de evaluación generalmente empleadas para cuantificar el desempeño de un detector de plagio. 2.4. Medidas de evaluación Para evaluar el comportamiento de un algoritmo de detección de plagio, se deben computar la precisión (en inglés precision), la cobertura (recall) y la Detección de plagio intrínseco basada en histogramas 5 granularidad de las detecciones realizadas [8]. En la definición de estas medidas seguiremos las siguientes convenciones de notación: 1) s representa una sección plagiada del conjunto S de todas las secciones plagiadas, 2) r denota una sección detectada del conjunto R de detecciones, 3) SR son las secciones plagiadas que han sido detectadas, 4) | si | y | ri | denotan el tamaño (en cantidad de caracteres) de la sección correspondiente y | S | y | R | denotan la cardinalidad de los conjuntos respectivos. Finalmente, α(si) es la cantidad de caracteres detectados de si, β(ri) es la cantidad de caracteres plagiados de ri y γ(si) es la cantidad de caracteres plagiados detectados de si. En base a estos valores, la precisión, cobertura, granularidad y evaluación global (overall), se definen de la siguiente manera1: recall = 1/ | S | |S| ∑ i=1 α(si)/ | si | (1) precision = 1/ | R | |R| ∑ i=1 β(ri)/ | ri | (2) granularidad = 1/ | SR | |SR| ∑ i=1 γ(si) (3) overall = F/(log2(1 + granularidad)) (4) Estas medidas se interpretan de la siguiente manera. La precisión cuantifica el porcentaje de detecciones correctas, el recall el porcentaje de plagio detectado, una granularidad cercana a 1 significa que el algoritmo detectará cada plagio a lo sumo una vez. En todos los casos, valores cercanos a 1 indican que el algoritmo de detección tiene un buen desempeño. 3. Segmentación de texto La segmentación de texto divide un texto en unidades con el mismo tópico [3]. La implementación de Freddy Choi es realizada en dos fases sobre el texto completo. En la primer etapa las stops words (artículos, preposiciones, conectores etc.) son removidas ya que no aportan información relevante del texto. La raíz de cada palabra se obtiene mediante un algoritmo de stemming y se almacena su frecuencia en el texto en un vector. Cada oración tiene asociada un vector y la frecuencia de la palabra j en la oración i se denota como fi,j . La matriz S resultante de aplicar la similitud coseno a cada par de vectores es llamada matriz de similitud [3]. Dado que no es sencillo determinar los límites de los segmentos directamente sobre S, esta matriz es sometida a un proceso de ranking que obtiene una nueva matriz S ′ a partir de S, denominada matriz de rango. Cada elemento (valor) de la matriz S ′ resulta de desplazar una máscara (matriz cuadrada) sobre S. Cada valor r en S ′ se determina en base al conjunto 1 En la evaluación global, F refiere a la tradicional medida F , la media harmónica de precisión y recall: F = 2 × (precision × recall)/(precision+ recall) 6 Dario G. Funez y Marcelo L. Errecalde de valores que cubre la máscara en S (N) y al valor central de la máscara en S (c). La fórmula para obtener r es: r = lvalue/|N |, donde lvalue es el número de elementos en N con menor similitud que c. La última etapa del algoritmo de segmentación, utiliza los valores obtenidos en S′ y aplica un método de clustering divisivo, basado en el algoritmo de maximización de Reynar [10] para detectar los límites de los segmentos. Este algoritmo se basa en el concepto de densidad interna donde, dado un segmento delimitado por las sentencias i y j (inclusive); si si,j es la suma de los valores rango de las sentencias en el segmento y ai,j es el área interna que abarca el segmento dada por la fórmula: ai,j = (j − i+ 1)2. Si B = b1 . . . bm es una lista de m segmentos coherentes y sk y ak denotan la suma de valores rango y área respectivamente, correspondiente al segmento k en B, la densidad interna de B se define como: D = m ∑ i=1 sk/ m ∑ i=1 ak (5) El proceso comienza inicializando B con un único segmento que representa todo el documento. Cada paso del algoritmo separa uno de los segmentos en B y el punto de corte se elige de tal manera que maximiza D. La cantidad de segmentos m se determina de forma automática y queda establecida cuando el gradiente tiene variaciones inusuales. Si D(n), es la densidad interna de n segmentos, el gradiente se define como: δD(n) = D(n) −D(n−1). Para un documento con b límites potenciales, si u,v denotan la media y varianza de δD(n) con n ∈ 2, . . . , b+ 1, el m queda definido al aplicar el threshold u+ l√v a δd. A menudo, un valor de l = 1, 5 es utilizado en la práctica. 4. Lowbow Lowbow es un framework en Matlab que provee una representación secuencial, diferenciable y contínua de un documento [7]. Ha sido utilizado con éxito en problemas de atribución de autoría, donde se emplearon histogramas locales de n-gramas [4]. Lowbow requiere como entrada: a) el archivo a analizar, b) la cantidad de fragmentos o muestras en que lowbow va a dividir el texto y c) un conjunto de kernels cuyos valores oscilan entre 0 y 1.2 El framework permite obtener, dado un archivo con el texto, histogramas con diferentes kernels. Un kernel con valor cercano a 0 preserva la información secuencial y ésta se va perdiendo a medida que el valor se incrementa. También se le puede proporcionar el vocabulario, pero éste es un parámetro opcional, calculándolo el framework en forma automática a partir del texto, en caso de no ser especificado. Lowbow devuelve como resultado, archivos con distinto tipo de información relacionada a los histogramas, como por ejemplo: Cantidad de muestras y los kernels (archivo lowbow.info). Se utiliza para visualizar los histogramas. 2 Razones de espacio impiden una explicación detallada de los kernels, pero el lector interesado puede consultar [7] para mayores detalles. Detección de plagio intrínseco basada en histogramas 7 Información sobre el análisis de componentes principales (archivos *.proj ). Información sobre velocidad, curvatura, vector tangente, etc (archivos *.tan). La representación con histogramas provee de información secuencial, que es importante para modelar el estilo de escritura de un autor, y que será utilizada en el detector que se describe en la próxima sección. 5. Descripción del detector En la Figura 1 se muestran todos los pasos involucrados en nuestra propuesta de detección de plagio intrínseca. Figura 1. Arquitectura del detector. Para utilizar lowbow, se le debe proporcionar la cantidad de muestras en el texto. Este valor se obtiene al aplicar al documento completo que se desea chequear, el algoritmo de segmentación de texto explicado en la Sección 3, ya que éste permite determinar la cantidad de segmentos presentes en el mismo. Se realizaron pruebas con diferentes técnicas de descomposición de texto, como dividir el texto en segmentos de tamaño uniforme y en párrafos pero se obtuvo un desempeño inferior. Por lo tanto, y al igual que en [5] donde ya habíamos obtenido buenos resultados con la segmentación de texto, se aplicó el algoritmo de segmentación de Freddy-Choi que forma parte de la librería Morphadorner.3 A continuación, se ejecuta lowbow con los siguientes parámetros: a) la cantidad de muestras (determinada mediante el procedimiento antes explicado) y b) el kernel, el cual fué determinado experimentalmente con un valor de 0.03. El lowbow convencional produce con los parámetros anteriores distintos archivos de salida, que corresponden a distintos histogramas. De todos los posibles histogramas se seleccionó aquel que mejor representó el modelo estilográfico, en un conjunto de archivos de referencia. 3 Morphadorner es una librería java de acceso libre para PLN suministrada por la Universidad de Northwestern. 8 Dario G. Funez y Marcelo L. Errecalde La Figura 2 muestra un histograma obtenido por lowbow el cual, como se puede observar, se presenta como una función matemática contínua. Figura 2. Histograma de palabras. La detección de outliers identifica aquellas secciones del texto en el cual se producen variaciones significativas en la gráfica del histograma. Para tal fin, se empleó el script Peakdetect en Matlab, el cual detecta los picos que se encuentren en una gráfica, es decir, los mínimos y máximos locales, como se muestran en la Figura 2. Todos los fragmentos del texto que muestran estos comportamientos anómalos son sospechosos de plagio. Una vez que se cuenta con las secciones provistas por la detección de outliers, se debe verificar si existen secciones adyacentes para unirlas en una única sección, y así obtener una mejor granularidad. La salida del detector es un archivo .xml con la información de las secciones del documento que se sospecha de plagio. Cada sección consta de la información donde se posiciona en el texto, es decir, el desplazamiento desde el comienzo del texto en cantidad de caracteres y su tamaño. A continuación se exponen las diferencias del detector propuesto con el presentado en [5]: La segmentación de texto se utiliza para obtener la cantidad de segmentos en el texto requerida por lowbow, pero no se utilizan las secciones provistas por el algoritmo. El modelo estilográfico del texto completo se representa con un único histograma, mientras que en [5] se calcula un conjunto de medidas estilográficas cuidadosamente seleccionadas en cada segmento del documento. La detección de outliers en este trabajo es muy innovadora, ya que detecta los puntos outliers en el texto cuando se producen alteraciones en la gráfica del histograma. En el anterior, la detección de outliers se realiza utilizando el método de detección basado en la Meda. Detección de plagio intrínseco basada en histogramas 9 6. Experimentos Para evaluar el comportamiento del detector propuesto, se utilizó el corpus PAN-PC-2011 suministrado para la competencia [8], utilizándose el script Python perfmeasure.py, para computar la precisión, el recall, la granularidad y el Plagdet score u overall. El corpus esta compuesto por una colección de 4.753 archivos de diferentes tamaños y con distinta cantidad de secciones plagiadas. En la siguiente tabla, se muestran los resultados totales de aplicar el detector a los archivos del corpus de la competencia. plag-det Recall Precisión Granularidad 0.088486 0.167477 0.063944 1.064707 Tabla 1. Resultados totales obtenidos con el corpus PAN-PC-2011. A continuación, se muestra en la Tabla 2 los resultados comparativos de todos los competidores del PAN2011 en la detección de plagio intrínseca. Puesto plag-det Recall Precisión Granularidad 1 0.3254817 0.3397965 0.3123243 1.0000000 2 0.1679779 0.4279112 0.1075817 1.0329386 3 0.0841286 0.1277831 0.0664302 1.0549085 4 0.0693820 0.1080543 0.0783903 1.4787234 Tabla 2. Resultados de todos los participantes de la competencia PAN-PC-2011 Como se puede observar, nuestro detector se posicionaría en el tercer puesto, con un mejor recall que el obtenido por el tercer lugar. Sin embargo, una de las deficiencias observadas en el detector propuesto, es que en varios casos devuelve secciones plagiadas cuando el documento no tiene plagio (falsos positivos). Esta falencia disminuye el desempeño global del detector, ya que el corpus está compuesto por un gran porcentaje de archivos sin plagio. 7. Conclusiones y Trabajo Futuro A partir de la experimentación realizada, se ha obtenido evidencia firme de la factibilidad de la modelización del estilo de escritura mediante histogramas en la detección de plagio intrínseco. Si bien los resultados presentados son aún preliminares y pueden ser mejorados en el futuro, el enfoque propuesto en este trabajo ha demostrado ser en este momento, competitivo con respecto a otros algoritmos representativos del estado del arte en la detección de plagio intrínseco. El algoritmo propuesto tiene la ventaja que es muy rápido, ya que la obtención del modelo se reduce a recuperar un histograma de todo el documento. La detección de outliers también es muy simple y eficiente y contribuye satisfactoriamente al desempeño global del detector. Como trabajos a futuro, una posibilidad es utilizar como vocabulario en lowbow, los tri-gramas en lugar de las palabras completas, una variante que ya 10 Dario G. Funez y Marcelo L. Errecalde ha producido buenos resultados en atribución de autoría [4]. También se planea, modificar el código de lowbow para que utilice información sobre las secciones de texto provistas por el algoritmo de segmentación de texto. Además, y como objetivo inmediato, el trabajo estará dirigido a mejorar el desempeño del detector en el tratamiento de los documentos que no tengan plagio, mediante un análisis más detallado de las variaciones de los picos de los histogramas.	﻿histogramas , detección de plagio intrínseco	es	22665
29	Herramientas para instrumentación de programas paralelos en ambientes distribuidos	﻿  This paper presents a methodology and tool for instrumenting parallel programs in distributed computing  platforms with the highest possible resolution (microseconds, if possible) and the minimum interference/overhead  in programs. In the context of time instrumentation in distributed environments, it is essential to synchronize the  involved clocks. In order to design the basic synchronizing algorithm, the classical strategies used in distributed  environments were followed and adapted to the environment of a cluster or, at least, of an interconnection  network to which there is exclusive (or controlled) access for the all communications carried out among the  synchronized computers. This environment is specifically represented by those of parallel computing in clusters.   Keywords: Parallel Performance and Instrumentation, Process Synchronization, Distributed Clocks, Parallel and  Distributed Systems, Parallelism in Clusters and Interclusters, Internal and External Synchronization.   Resumen   En este artículo se presenta una metodología y una herramienta para la instrumentación de programas paralelos en  plataformas de cómputo distribuidas con la mayor resolución posible (microsegundos, si fuera posible) y la  mínima interferencia/sobrecarga en los programas. En el contexto de instrumentación de tiempo en ambientes  distribuidos es fundamental la sincronización de los relojes que intervienen. Para el diseño del algoritmo básico de  sincronización se siguieron las estrategias clásicas que se utilizan en ambientes distribuidos, adaptadas al entorno  de un cluster o, al menos, de una red de interconexión sobre la que se tiene acceso exclusivo (o controlado) para  todas las comunicaciones entre las computadoras que se sincronizan. Este ambiente es específicamente el de los  entornos de cómputo paralelo en clusters.  Palabras claves: Rendimiento e Instrumentación Paralela, Sincronización de Procesos, Relojes Distribuidos,  Sistemas Paralelos y Distribuidos, Paralelismo en Clusters e Intercluster, Sincronización Interna y Externa.    1 INTRODUCCION   Desde hace tiempo se dispone de un sistema capaz de proporcionar una referencia de tiempo en los  sistemas de cómputo [20] [22]. Dicha referencia es esencial para resolver problemas tales como el  ordenamiento de eventos (ej: envío y recepción de correo electrónico, eventos dentro de las  transacciones, inicio de procesos en tiempo real, etc.).                                                     ∗ Investigador Asistente CICPBA  1414 También cobra importancia la medición de tiempos en la optimización del rendimiento tanto en  sistemas de cómputo monoprocesador como en sistemas paralelos y distribuidos [2] [4] [7] [13]  [14]. En todos los casos, las aplicaciones con fuertes requerimientos de cómputo o procesamiento  son las que también requieren la optimización para el máximo aprovechamiento del hardware  disponible. La relación es bastante directa: a partir de la monitorización de los tiempos de ejecución  se pueden analizar los problemas de rendimiento e intentar solucionar tales problemas [10].  Aún en el caso de las mediciones en una misma computadora (usualmente en el contexto de un  sistema con un único procesador), es deseable que el registro de tiempos no influya en el tiempo de  ejecución de la misma. En todos los casos, se necesitan resoluciones de reloj acordes a los tiempos  que se deben medir en las aplicaciones. Usualmente, los métodos provistos por el sistema operativo  no son apropiados [21]. Por otro lado, los métodos y/o herramientas provistas por los lenguajes  dependen del sistema operativo y, por lo tanto, resultan inadecuados en muchos casos también.   En el caso de procesamiento en una arquitectura distribuida, con un programa que ejecuta procesos  en diferentes computadoras o en los que el tiempo de las comunicaciones es importante, la tarea de  medir intervalos de tiempo conlleva la necesidad de sincronizar los relojes de las diferentes  computadoras que se utilizan [5] [17] [18]. Sería deseable que esta tarea de sincronización se lleve a  cabo fuera del tiempo en que se ejecute el programa que se está monitorizando, y conociendo el tipo  (o al menos magnitud) de error con que se sincroniza.  Por otro lado, es deseable que la sincronización se lleve a cabo sin la necesidad de incluir hardware  adicional al del sistema. Esto implica que todo lo referente a las comunicaciones deberá utilizar la  red de interconexión entre computadoras y el sistema de medición existentes en cada sistema de  cómputo.  Básicamente, se desea contar con una herramienta de instrumentación para programas paralelos  que:   • Pueda ser usada inicialmente en un cluster de PCs, con la posibilidad de ser extendido a  clusters en general y luego en plataformas distribuidas aún más generales.   • Sea de alta resolución, es decir que se pueda utilizar para medir tiempos cortos, del orden de  microsegundos.   • Que no altere el funcionamiento de la aplicación bajo prueba, o que la alteración sea mínima  y conocida por la aplicación.   • Utilice en forma predecible la red de interconexión. Más específicamente, se puedan  determinar, desde la aplicación, los intervalos de tiempo en los cuales se utilizará la red. De  esta forma, se puede desacoplar el uso de la red de interconexión, ya que habrá intervalos de  tiempo usados para la sincronización e intervalos de tiempo utilizados para la ejecución de  programas paralelos.     2 DESCRIPCIÓN DEL PROBLEMA Y EXPERIMENTOS ASOCIADOS  Se han estudiado tanto los algoritmos básicos como las implementaciones existentes. En este  sentido, se cuenta con una amplia cantidad de información tanto de los algoritmos como de las  implementaciones. Como requisito previo, normalmente se establece que cada computadora cuente  1415 con un oscilador físico de frecuencia constante (al menos en términos ideales). A partir de este  oscilador físico se derivan los relojes lógicos que son los que se sincronizan [8] [11] [16]. En todos  los casos, lo que se tiende a resolver son las diferencias de [1] [3] [12]:  1. Referencia fija en el tiempo a partir de la cual se contabiliza el tiempo en cada computadora.  Aunque normalmente es constante, es relativamente difícil establecer con precisión su valor.  2. Frecuencia entre los relojes de las computadoras que se sincronizan. En algunos casos, se  suelen incluir en este punto las diferencias de las variaciones de los relojes, dado que los  relojes no necesariamente tienen frecuencia constante a lo largo del tiempo.  La Fig. 1 muestra dos computadoras, cada una de ellas con su oscilador, que a su vez tiene sus  propias características.        Figura 1: Relojes de Dos Computadoras.    En la Fig. 1, el instante de tiempo T01 es la referencia al tiempo real en una computadora y T02 es  la referencia de tiempo equivalente en la otra computadora. La pendiente θ1 corresponde al  oscilador de una computadora y θ2 es la correspondiente pendiente del reloj de la otra computadora.  Una de las formas más sencillas e intuitivas de sincronizar dos computadoras consiste en determinar  el reloj de una de ellas en función del reloj de la otra teniendo en cuenta algún tipo de interconexión  entre ellas. La Fig. 2 muestra esquemáticamente esta forma de sincronización, donde el reloj de una  computadora se puede establecer a partir del reloj de la otra teniendo en cuenta el tiempo de las  comunicaciones de sincronización entre las dos: t2-t1 (el tiempo de ida y vuelta o roundtrip de un  mensaje).                                             t = t2-t1  Tiempo  Clk1  θ1  T01  Tiempo  Clk2  θ2 T02     t1     t2    t1 + d    t2 + d                            d = t/2    Figura 2: Sincronización Básica.    De acuerdo con la Fig. 2, el tiempo en la computadora que recibe el mensaje enviado en t1  1416 solamente tiene que definir su propio reloj lógico como t1+d. De la misma manera, el tiempo lógico  al recibir el mensaje enviado en el instante t2 debería ser t2+d. En todos los casos, d es la mitad del  tiempo de roundtrip (o ida y vuelta) de un mensaje y hace referencia a un único reloj físico o  lógico, evitando de esta manera los errores por las diferencias entre los relojes de las diferentes  computadoras. Implícitamente, se está asumiendo que el tiempo de ida de un mensaje de una  computadora a otra es igual al tiempo de vuelta del mismo mensaje entre el mismo par de  máquinas. Los mensajes involucrados normalmente se denominan mensajes de sincronización y son  de longitud mínima, dado que no es necesario transportar muchos datos de una computadora a otra  para llevar a cabo la sincronización.     2.1 Acceso al Reloj Local de cada Computadora   La forma de hacer referencias al reloj físico y/o a los relojes lógicos con la mínima sobrecarga  normalmente se basa en evitar llamadas al sistema operativo para, al menos, evitar el consiguiente  cambio de contexto. Para ello se utilizaron instrucciones de ensamblador para consultar el registro  correspondiente de la CPU utilizando la instrucción RDTSC (ReaD Time Stamp Counter). En este  contexto, se han llevado a cabo mediciones de los ciclos de CPU invertidos en una llamada al  sistema como gettimeofday y la utilizada por la alternativa presentada (con RDTSC) en diferentes  computadoras. A modo de ejemplo, en la Tabla 1 se muestran los valores obtenidos en ciclos de  CPU en dos computadoras, cuyas características se dan en la Tabla 2.  Tabla 1: Ciclos de CPU de gettimeofday y RDTSC   PC gettimeofday RDTSC RDTSC + 1 división PF  PII266 402 25 236  XP1600 615 66 346    La primera columna de la Tabla 1 identifica la computadora (de la que se dan todos los detalles en  la Tabla 2), la segunda columna muestra los ciclos de CPU necesarios para una llamada a  gettimeofday, la tercera columna muestra los ciclos de CPU necesarios para una operación RDTSC,  y la última columna muestra los ciclos de CPU necesarios para convertir los ciclos de CPU en  microsegundos (usando la frecuencia del oscilador en la división, se podría disminuir haciéndolo en  lenguaje de ensamblador). La primera columna de la Tabla 2 identifica la computadora, la segunda  columna su CPU, la tercera columna la frecuencia del reloj del sistema y la última columna el  sistema operativo utilizado en cada una de las computadoras.   Tabla 2: Detalles de las PC de la Tabla 1  PC CPU  Frecuencia (MHz) Sistema Operativo  PII266 Intel Pentium II 266 Linux 2.4.20-8  XP1600 AMD Athlon XP 1600 Linux 2.4.18-14    Nótese que, por un lado las cantidades de ciclos de CPU de la llamada al sistema gettimeofday y de  RDTSC que se muestran en la Tabla 1 son significativamente diferentes (con su consiguiente  sobrecarga solamente para medición/registro de tiempo).  2.2 Tiempos de Demora de las Comunicaciones de Sincronización   En cuanto a la sincronización, el tiempo d observado en la Fig. 2 es variable debido a diversos  motivos, presentando una curva de distribución de frecuencias del tipo mostrada en la Fig. 3 [3].   1417                            % de  mensajes  Demora Mínimo Moda Figura 3: Distribución Típica de los Tiempos de Mensajes/Comunicaciones.    De acuerdo con la Fig. 3, la mayoría de los mensajes arribarán a destino con un tiempo moda de  retraso. Se podrían considerar válidos los valores que están entre el tiempo mínimo y alrededor del  tiempo moda. Uno de los primeros problemas detectados es que existe una asimetría entre el tiempo  de ida y el de vuelta, con lo que la demora (d en la Fig. 2) será distinta del tiempo de roundtrip/2.  Esto implica que para iguales tiempos de roundtrip pueden haber tiempos de ida diferentes.  Estas variaciones llevan a tener un margen de error en la sincronización. En la búsqueda de una  cota, se analizó este tiempo descomponiéndolo en las etapas que se muestran en la Fig. 4, que se  verifican tanto en la ida como en la vuelta de una comunicación [9]:   o Tiempo de envío: es el necesario para la construcción del paquete en las distintas capas  hasta alcanzar la capa de acceso al medio. Es variable en función del estado del sistema  operativo (context switch, scheduling, etc.). Podría reducirse en un sistema operativo de  tiempo real.  o Tiempo de acceso al medio: depende del hardware de interconexión. En el caso de Ethernet,  dependerá del tráfico en la red y en general es aleatorio. Si se usan switches para  interconectar las máquinas se vuelve más determinístico y acotado.  o Tiempo de transmisión: depende de la velocidad de la placa de red y de la longitud del  mensaje. En general es determinístico.  o Tiempo de propagación: en una red local será despreciable ya que es el tiempo necesario  para que los datos recorran el cable entre las placas de red y el switch de interconexión (que  en general existe, aunque no necesariamente siempre es parte del cableado). En general es  determinístico.  o Tiempo de recepción M. (placa de la Máquina destino): es el tiempo necesario para llegar a  los buffers o la memoria de la placa de interconexión. En general es determinístico.  o Tiempo de recepción P. (Proceso destino): es el necesario para desarmar el paquete en las  distintas capas hasta ser entregado a la aplicación. Depende del sistema operativo como el  tiempo de envío.             Envío Acceso al medio Transmisión Propagación Recepción M. Recepción P.    Figura 4: Tiempos Involucrados en el Envío y Recepción de un Mensaje.  1418   Para reducir la varianza en estos tiempos, se ha hallado la conveniencia de:  o Realizar la sincronización fuera de todo proceso (es decir sin competir con otros procesos),  para evitar variaciones en tiempos de envío.  o Tomar el tiempo con código sin llamadas al sistema operativo, evitando la sobrecarga del  mismo (básicamente de los context switches involucrados).   o La utilización de redes con switches de interconexión de las máquinas, evitando las  variaciones en los tiempos de acceso al medio.    Una vez tomados los recaudos necesarios para evitar la varianza, previamente a la sincronización se  lleva a cabo una ráfaga de intercambio de mensajes, de los cuales se mide el promedio, el mínimo y  el máximo, de tal manera de obtener una caracterización de los posibles errores cometidos al  emplear la herramienta. Para el cálculo del promedio se eliminan los valores alejados en más de 2  veces del tiempo mínimo (que, además, son muy poco probables). Para las computadoras detalladas  en la Tabla 2 interconectadas por una red local Ethernet de 100 Mb/s con switch de interconexión y  sin uso exclusivo de la red, los valores totales (d, de la Fig. 2) obtenidos son los que se muestran en  la Tabla 3.    Tabla 3: Tiempo de Roundtrip/2 sin Uso Exclusivo de Red 100 Mb/s  Promedio Mínimo Máximo  84µs 76 µs 91 µs    Los valores menores al promedio de la Tabla 3 son, también en promedio, de 8µs menos y la  diferencia promedio de los valores mayores es de 15µs. Con lo que las sincronizaciones tienen  involucrados los errores relacionados con estas diferencias también. Por otro lado, con  computadoras de mayor capacidad interconectadas por una red local Ethernet de 100 Mb/s con uso  exclusivo y un switch de interconexión, los valores obtenidos son los que se muestran en la Tabla 4.    Tabla 4: Tiempo de Roundtrip/2 con Uso Exclusivo de Red 100 Mb/s  Promedio Mínimo Máximo  54µs 53 µs 56 µs    En el caso de los valores de la Tabla 4, las diferencias promedio son de 1µs y 3µs mínimo y  máximo respectivamente. La Tabla 5 muestra los detalles de las computadoras con las que se  obtuvieron los valores de la Tabla 3.     Tabla 5: Detalles de las PC de la Tabla 4  PC CPU  Frecuencia (MHz) Sistema Operativo  P42400 Intel Pentium 4 2400 Linux 2.4.18-14    2.3 Determinación de la Pendiente de los Osciladores  El valor de la frecuencia de reloj surge de obtener el valor del registro del oscilador de la CPU en  dos instantes de tiempo t1 y t2, con lo que:    MHz = (tsc2 - tsc1) / (t2 - t1) (1)   1419 donde tsc1 y tsc2 son los valores del registro del oscilador de la CPU en los instantes de tiempo t1 y  t2 respectivamente y, además, t1 y t2 son obtenidos a partir de una referencia de tiempo de una de  las máquinas intervinientes en el proceso a partir de su reloj local con una llamada al sistema del  tipo gettimeofday. Este método aunque es sencillo y claro no está libre de errores cuando se trata de  medición de tiempo. Además de la sobrecarga de la llamada al sistema operativo que se menciona  antes, los tiempos proporcionados por gettimeofday pueden tener un error y además habría un error  en la variación de los tiempos de los mensajes involucrados para transportar las referencias de  tiempo de una máquina a otra.    En todas las computadoras en las que se han hecho experimentos, que incluyen a las que se  describen en la Tabla 2 y en la Tabla 5, todas las llamadas al sistema operativo (gettimeofday)  tienen una resolución y sobrecarga (sumadas) del orden de 1µs en las computadoras con relojes del  orden de GHz. Por lo tanto, el tiempo real tr1 involucrado, es decir teniendo en cuenta este error,  tget < 2µs (porque son dos mediciones de tiempo, t1 y t2), sería    tr1 = t2 - t1 + tget (2)   Por otro lado, habría que incluir el error determinado por la variación de tiempos de mensajes, td  que sería la diferencia entre la moda y el mínimo. Con lo cual se tendría    tr2 = t2 - t1 + tget + td (3)   Y, por lo tanto, para que el error total sea despreciable, t2 - t1 >> tget + td. Si, por ejemplo se tiene  un td entre 0 y 8µs (valores similares a los derivados de la Tabla 3 y la Tabla 4) y se consideran  datos útiles/aceptables con un error de 1/106, se tendría que     t2 - t1 = 106 x (2 µs + 8 µs) (4)   Por lo tanto, dado que t2 - t1 = 106 x (2 µs + 8 µs) = 10 segundos, se necesitaría un intervalo de  tiempo de 10 segundos o más para que el error sea de 1/106 (una millonésima) o menor. De esta  manera, el error se puede hacer tan pequeño como sea necesario/útil haciendo mayor el intervalo de  tiempo durante el cual se toman las referencias t1 y t2. Se debe notar que este método es usado para  determinar los valores de MHz de cada CPU con error acotado/conocido.     3 RESULTADOS OBTENIDOS DE SINCRONIZACION  Se elaboró una biblioteca mínima en cuanto a la medición de tiempos de cómputo en un mismo  sistema (usualmente en una máquina monoprocesador), para el sistema operativo Linux en PCs  (procesadores Intel y compatibles) en las cuales se puede hacer referencia al contador de ciclos del  oscilador. En lo referente a la sobrecarga, los datos preliminares muestran resultados altamente  satisfactorios, dado que la sobrecarga no excede las decenas de ciclos de reloj. En este sentido, ya  es posible la instrumentación de código y el análisis de las aplicaciones que se resuelven en un  mismo sistema o máquina. Por otro lado, la precisión también es satisfactoria: del orden de los  microsegundos. A modo de ejemplo, la Tabla 6 muestra la sobrecarga y la precisión de la rutina  usada para registrar tiempos en una computadora de las mostradas en la Tabla 5, comparada con la  llamada a gettimeofday. Se debe notar que la sobrecarga es igual a la precisión dado que no hay  cambios de contexto al incluir la biblioteca en el binario de la aplicación. También a partir de la  Tabla 6 se puede observar que la sobrecarga del registro de tiempos con la lectura de ciclos de CPU  1420 es de poco más de 1/3 de la que se obtiene con la llamada al sistema operativo gettimeofday. Cabe  aclarar que para la prueba , se provocó el desalojo de gettimeofday y rdtsc de memoria caché tanto  de instrucciones como de datos, que es la condición real en la que serán usados en instrumentación.     Tabla 6: Sobrecarga y Precisión de Tiempo Local   PC gettimeofday (ciclos) RDTSC (ciclos) Precisión RDTSC (µs)  P42400 7876 2828 ≅1.18    Se está desarrollando software (básicamente una biblioteca de una cantidad reducida de funciones)  para instrumentación de programas paralelos que se ejecuten en clusters de PCs. Esta biblioteca  tiene los lineamientos dados antes, con énfasis en la resolución del orden de los microsegundos, la  mínima sobrecarga de procesamiento y el desacople de la red de interconexión. La Tabla 7 muestra  los errores mínimo y máximo de sincronización con las computadoras detalladas en la Tabla 5  (P42400) interconectadas con una red Ethernet de 100 Mb/s de uso exclusivo.    Tabla 7: Errores de Sincronización Medidos  PC Red Mínimo Máximo  P42400 100 Mb/s, uso exclusivo 0 µs 5 µs    Debe recordarse que el error de sincronización se define como la diferencia en los tiempos locales  de las computadoras y estos tiempos, a su vez, tienen la resolución dada en la Tabla 6:  aproximadamente 1.18 µs.    4 CONCLUSIONES Y TRABAJOS FUTUROS  Como mínimo, se ha llegado a tener una biblioteca muy sencilla y de muy baja sobrecarga para la  instrumentación de programas secuenciales. En el contexto de instrumentación de aplicaciones  distribuidas, también se ha llegado a una biblioteca sencilla que no solamente permite registrar  tiempos del orden de los microsegundos sino que, además, mantiene los relojes de las máquinas  sincronizados de manera tal que se tenga una única referencia de tiempo.     Además, se están llevando a cabo pruebas sobre los sistemas de relojes distribuidos empleados en  este momento tales como NTP (Network Time Protocol) y DTS (Distributed Time Service), con el  fin de analizar sus ventajas y desventajas. También un análisis de los sistemas que utilizan hardware  especializado, como para tener un marco de referencia más amplio [6]. En todos estos casos, la idea  final es contar con un conjunto de programas del estilo de los benchmarks para que provean  automáticamente la caracterización del sistema de sincronización elegido/utilizado. Como mínimo,  el objetivo es contar con una metodología de caracterización de las herramientas, bibliotecas, y/o  protocolos de sincronización que incluso pueda ser aplicada a la nueva biblioteca que se desarrolle,  mencionada antes.     Por otro lado, también es importante analizar y, más específicamente, cuantificar las características  de NTP (Network Time Protocol) para ser comparado con la biblioteca que se propone en este  artículo. Esta cuantificación se refiere principalmente a:  • Resolución posible del reloj sincronizado. Aunque NTP es paramétrico esto no significa a priori  que se puede obtener una resolución arbitraria cualquiera.  1421 • Relación entre la resolución definida y la sobrecarga en la red de comunicaciones.   • Relación entre la resolución definida y la sobrecarga en la pila de protocolos TCP/ UDP/IP.   • Relación entre la resolución definida y la sobrecarga en el uso de CPU y la jerarquía de  memoria.   Todo este análisis es básicamente experimental [15] [18] [19], dado que el funcionamiento de NTP  se define por un lado paramétricamente y por el otro con el comportamiento o rendimiento de la red  de interconexión.      Todavía está pendiente investigar el comportamiento en términos de escalabilidad de la  sincronización implementada. Usualmente la sincronización se da entre dos máquinas y en el caso  particular de NTP se lleva a cabo con el modelo cliente/servidor. Es claro que cualquier tipo de  centralización (en el servidor, por ejemplo) tiene sus inconvenientes de escalabilidad y al menos  debería ser posible su cuantificación. En este contexto específico es muy interesante la posibilidad  de sincronización utilizando mensajes broadcasts con su consiguiente ahorro de comunicaciones  punto a punto.      Como extensiones futuras, siempre es deseable la sincronización externa de los relojes [8]. Este  paso está muy ligado también a la posibilidad de utilizar más de un cluster de computadoras para  cómputo paralelo y en este contexto la sincronización de los relojes va más allá del análisis de  rendimiento con el objetivo de optimizarlo.    También es necesario investigar en particular hasta qué punto las referencias a los  relojes/osciladores físicos son portables o al menos tienen funciones u operaciones equivalentes en  diferentes plataformas de hardware.    Como paso posterior, se espera extender la biblioteca para su uso en Internet y múltiples clusters  para cómputo paralelo [23]. Quizás en este punto se deban redefinir algunas características de la  herramienta o biblioteca, tal como la capa de transporte de los mensajes con los cuales se  implementa la sincronización.	﻿sincronización interna y externa , relojes distribuidos , paralelismo en clusters e intercluster , distributed clocks , parallelism in clusters and interclusters , internal and external synchronization	es	22682_2
30	Integración segura de MANETs desplegadas en zonas de recursos limitados a redes de infraestructura	﻿ Las características de las redes móviles ad hoc (MANET – Mobile  Ad hoc NETwork) la convierten en una tecnología ideal, para ser utilizada en  zonas remotas donde la cobertura de la red celular es limitada y la electricidad  es un recurso escaso.   En este trabajo realizamos el estudio de un caso de integración de una MANET,   desplegada en una zona remota y de recursos limitados, a una red de  infraestructura. Se efectuaron pruebas de comunicación sobre canales “seguros”  y “no seguros” con la finalidad de medir el consumo de recursos (ancho de  banda y energía) en los nodos de la red ad hoc. Los resultados obtenidos  permiten determinar el consumo adicional de recursos introducido por el uso de  protocolos seguros.  Palabras Clave: MANET, Seguridad, Energía, IPSec, Bluetooth, GPRS.  1 Introducción  Una red móvil ad-hoc o MANET (del inglés Mobile Ad-hoc Networks) [4] es una  colección de nodos inalámbricos móviles que se comunican de manera espontánea y  autoorganizada constituyendo una red temporal sin la ayuda de ninguna  infraestructura preestablecida (como puntos de acceso WiFi o torres de estaciones  base celulares con antenas 2G, 3G o 4G) ni administración centralizada.   Una de las principales ventajas de una MANET es la posibilidad de integrarla a  una red de infraestructura con diferentes fines, entre otros podemos mencionar el  acceso a  Internet y a sistemas de información de una organización desde un  dispositivo móvil [11].    En este trabajo realizamos el estudio de un caso de integración de una MANET,  desplegada en una zona remota y de recursos limitados,  a la red de infraestructura de  una organización (Intranet). Dicha integración se realizo a través de la red celular,  considerando los siguientes inconvenientes y limitaciones:      La energía en la zona de despliegue es escasa, lo que dificulta la capacidad de  recarga de los dispositivos que forman parte de la MANET.      Las redes celulares en zonas remotas no brindan servicios de tercera (3G) o cuarta  generación (4G), solo se dispone de tecnología 2G (GSM/GPRS) que proporciona  un ancho de banda limitado y variable.   La mayor parte de los dispositivos móviles utilizados en zonas remotas son  equipos baratos y de características básicas, que incorporan tecnologías como  Bluetooth  y 2G en lugar de  WiFi y 3G.   Las MANETs y las redes celulares utilizan un medio compartido (aire) para  transmitir los datos y se encuentran expuestas a “ataques” o accesos no  autorizados. Se requiere entonces la implementación de canales de comunicación  “seguros” entre los nodos de la red ad hoc y los equipos de la red de  infraestructura.   La implementación de niveles de seguridad elevados implica un incremento del  consumo de ancho de banda y de la energía en los nodos móviles [17]. Ambos  recursos son limitados en zonas remotas, por lo que se hace necesario elegir un  nivel de seguridad que no comprometa los recursos disponibles para el normal  funcionamiento de la MANET.    En [10], articulo presentado en CACIC 2011, montamos un escenario de pruebas  indoor sin considerar condiciones externas (distancia, interferencias, entre otros).  Continuando con esta línea de investigación, esta vez trabajamos sobre un escenario  outdoor afectado por factores externos, que disminuyen el rendimiento e incrementan  el consumo de recursos en los nodos de la red ad hoc.  El  escenario que utilizamos para realizar el estudio, conecta a la MANET remota  con un servidor de la red de infraestructura a través de la red celular (GPRS). Sobre  este realizamos pruebas extremo a extremo (nodo móvil a servidor), por canales de  comunicación no seguros y seguros (IPSec), con la finalidad de determinar el  consumo de ancho de banda y energía en los dispositivos móviles. Los resultados  obtenidos permiten establecer el consumo adicional de recursos provocado por el uso  de protocolos seguros.  2. Tecnologías de soporte para la formación de MANETs  Existen 4 estándares que permiten realizar comunicaciones inalámbricas de corto  alcance que se pueden utilizar para la formación de redes móviles ad hoc: Bluetooth  (IEEE 802.15.1), Ultra-wideband (UWB, IEEE 802.15.3), ZigBee (IEEE 802.15.4) y  WiFi (IEEE 802.11).    Elegimos la tecnología Bluetooth para realizar el despliegue de  MANETs, en  zonas de recursos limitados, por las siguientes razones:     Bluetooth utiliza un radio de corto alcance que ha sido optimizado para el ahorro  de energía y operación adecuada de la batería [12].   El consumo de energía de Bluetooth es inferior al de UWB y WiFi. En [14] se  presenta un estudio comparativo entre diferentes tecnologías inalámbricas, entre     los resultados de este estudio se observa que el consumo de energía en mili watts  de UWB y WiFi es hasta 4 veces superior al consumo de Bluetooth.   Su bajo precio y reducido tamaño [13], posibilitan que la mayor parte de los  dispositivos móviles que se consiguen en el mercado tengan incorporada la  interfaz Bluetooth (WiFi y UWB encarecen el dispositivo).    No se requieren componentes de infraestructura. Una red  WiFi requiere  la  instalación y configuración de componentes (Ej: Puntos de acceso) que requieren  energía para funcionar.   Facilidad y rapidez de despliegue.  2.1 Bluetooth (IEEE 802.15.1)  Bluetooth es una tecnología [11] para radio enlaces diseñada para que una amplia  variedad de periféricos y dispositivos móviles como netbooks, tablets, teléfonos  celulares y PDAs (Personal Digital Assistants) puedan establecer comunicación e  intercambiar información entre sí a través de enlaces de corto alcance (10 a 100mts).  Bluetooth fue diseñada para eliminar el uso de cables entre dispositivos, y luego se  comenzó a utilizar para la creación de redes personales PAN (Personal Área  Network). Una PAN es una red formada por una gran variedad de dispositivos que se  comunican entre sí mediante cables o a través del medio inalámbrico a cortas  distancias [12].   Posteriormente el estándar IEEE 802.15.1 [4] presenta una WPAN (Wíreless  Personal Área Networks) que utiliza tecnología inalámbrica Bluetooth, y soporta dos  tipos de topologías: piconet y scatternet.  La mas sencilla  es la piconet que consiste en una WPAN formada por un  dispositivo Bluetooth que actúa como maestro comunicado con hasta 7 dispositivos  Bluetooth  que actúan como esclavos, cualquier dispositivo puede ser maestro pero  teniendo en cuenta que solo debe existir uno por piconet.  La Scatternet se forma a partir de de la superposición de varias piconet, un  dispositivo puede ser esclavo de una piconet a la vez que es maestro de otra, o puede  ser esclavo de varias piconet.  2.2 IP sobre Bluetooth  El perfil  PAN (Personal Area Networking) [2] de bluetooth, proporciona el  transporte de datagramas IPv4 mediante el protocolo BNEP (Bluetooth Network  Encapsulation Protocol) [3].  El escenario de uso más habitual del protocolo BNEP es el denominado  NAP  (Network Access Point), en el cual uno de los dispositivos actúa como puente para  conectar los nodos de una piconet a una red IP.  Para realizar transporte de datos IP el protocolo BNEP reemplaza la cabecera  Ethernet (típica de las conexiones LAN cableadas) por su propia cabecera, de forma  que la cabecera BNEP y el payload de Ethernet serán encapsulados en una PDU de  L2CAP. En [10] se describe con detalle el proceso de encapsulamiento y el overhead  introducido.     3. Tecnologías de soporte para la integración de MANETs a redes  de infraestructura.  Existen 4 tecnologías que permiten integrar una MANET remota a una red de  infraestructura, estas son: 2G (GSM), 2.5G (GPRS), 3G (UMTS, HSDPA y HSUPA)  y 4G (LTE).   Los factores más importantes a considerar a la hora de elegir una de las tecnologías  son: Cobertura en la zona de despliegue de la MANET, consumo de energía y  velocidad de transmisión de datos.  En [15] y [16] se presentan estudios relacionados con el consumo de energía en  diferentes tecnologías de celulares, los resultados muestran que GPRS consume entre  un 40% y 70% menos energía comparado con UMTS.  La figura 1 ilustra el incremento de las velocidades de transferencia en las  diferentes tecnologías, desde las redes GSM (9 kbit/s) hasta las redes 4G (1 Gbit/s).   Note que las tecnologías HSDPA, HSUPA y LTE utilizan diferentes velocidades para  el enlace descendente (DL - downlink) y para el ascendente (UL – uplink).      Figura 1. Velocidad de transferencia en las distintas tecnologías celulares.  Para integrar la  MANET desplegada en zona remota elegimos la tecnología GPRS  (en lugar de UMTS o HDPSA), fundamentamos esta elección en las siguientes  razones:      El consumo de energía es menor en los dispositivos móviles que utilizan  GPRS, en comparación con los que utilizan UMTS o HDPSA – Esto se debe  a lo siguiente:  - El numero de estaciones base compatibles con los estándares  UMTS/HDPSA es limitado en zonas alejadas, por este motivo los  dispositivos móviles 3G deben conectarse a antenas situadas a grandes  distancias y utilizan mayor potencia para transmitir los datos.  - Las velocidades de transferencia alcanzables por los estándares 3G y  4G requieren de modulaciones más complejas, las cuales necesitan de  muchos cálculos adicionales y obligan a un mayor uso de CPU a los  dispositivos y, por lo tanto, a un mayor consumo de energía.   Disponibilidad de la tecnología GSM/GPRS en zonas remotas - La  tecnología 3G(UMTS o HSDPA) generalmente se encuentra en zonas con  gran concentración de usuarios y su implementación en zonas alejadas  implica un importante recambio tecnológico por parte de las compañías de  celulares.      La mayor parte de los dispositivos utilizados en zonas remotas solo soportan  GSM/GPRS - Esto se debe a que el costo de un dispositivo 2G/3G es muy  superior al de un dispositivo 2G, y su adquisición no se justifica en zonas  alejadas donde solo se dispone de redes 2G y la tecnología 3G es muy  limitada o directamente no existe.   Velocidad de transferencia – Si bien la  velocidad máxima de transferencia  que soporta la red GPRS (hasta 171Kbit/s) es pequeña en comparación a  UMTS, es suficiente para establecer una conexión con la red de  infraestructura.  3.1 GPRS  GPRS (General Packet Radio Service) es una extensión de la tecnología GSM que  permite aprovechar la infraestructura de GSM para brindar mejores servicios de  transmisión de datos a  las aplicaciones. A diferencia de GSM que utilizaba  conmutación de circuitos, GPRS utiliza conmutación de paquetes para la transmisión  de datos (packet-oriented).     BSCBTS MSC SGSN GGSNBackbone IP Vo z Datos MS MS Red IP HLR VLR MSC Red de telefonía VLRBSS RED GSM RED GPRS   Figura 2. Arquitectura de una red GSM/GPRS [6].  En la figura 2 se ilustra la arquitectura de la red GSM/GPRS, la red GRPS es la   encargada de gestionar las comunicaciones de datos y esta formada por los nodos  SGSN y GGSN interconectados a través de un backbone IP. El  SGSN (Serving  GPRS Support Node) se encarga de la entrega de paquetes desde y hacia las  estaciones móviles que se encuentran en su área de servicio. El GGSN (Gateway  GPRS Support Node) interconecta el backbone de la red GPRS y las PDNs (Packet  Data Networks) externas, actúa como un router entre la red GPRS y una red de datos  externa (Internet, intranet, etc).  3.2 IP sobre GPRS  La red GPRS utiliza el protocolo GTP (GPRS Tunneling Protocol) para transportar  los datagramas IP del usuario entre los nodos de soporte de GPRS (GSN), por debajo  de él los protocolos estándares TCP o UDP se encargan de transportar los paquetes  por la red (Figura 3). Resumiendo, en el Backbone del GPRS tenemos una     arquitectura de transporte: IP usuario – sobre GTP – sobre UDP/TCP- sobre IP  backbone.      Figura 3. Modelo de capas de GPRS [6]  4. Escenario de pruebas  En la figura 4 se observa una representación gráfica del escenario montado para  realizar las pruebas,  en el mismo se   interconecta una MANET a una INTRANET  utilizando la red GSM/GPRS, los dispositivos móviles  de la MANET acceden a la  red GPRS a través de uno de los  nodos que actúa como  punto de acceso a la red  GPRS (Gateway Bluetooth/GPRS).      Figura 4. Escenario de pruebas  La conexión de los dispositivos móviles al  punto de acceso a la red (NAP -  Network Access Point) se realizó utilizando el perfil PAN (Personal Área Network)  [12] del estándar Bluetooth [11].  El punto de acceso a la red se configuro sobre uno de los nodos de la MANET  utilizando la aplicación “Android Wifi Tether” [19], esta aplicación utiliza el  Framework netfilter e iptables [21] para implementar  un puente entre la PAN  bluetooth y la red GSM/GPRS.  El envío de un datagrama IP desde el nodo móvil hasta el servidor de la intranet, se  realiza de la siguiente manera:       1 – El nodo móvil envía el datagrama IP, encapsulado en BNEP, al punto de acceso  a  la red (NAP).   2 – El NAP transmite el datagrama al SGSN de la red GPRS, desde donde viaja al  GGSN encapsulado en GTP.   3 - El GGSN re-envía el datagrama a Internet, por donde viaja hasta llegar al router  frontera de la red destino.   4 –El router frontera de la red destino encamina el datagrama hacia el servidor,  encapsulado en una trama ethernet.  4.1 Equipamiento utilizado  En la tabla 1 se muestra la configuración de los dispositivo móviles que se  utilizaron para realizar las pruebas.     Nodo Gateway  Nodo móvil remoto  Marca y modelo Motorola ATRIX Motorola Milestone 2  SO Android OS, v2.3 (Gingerbread) Android OS, v2.2 (Froyo)  CPU Dual-core 1 GHz Cortex-A9 1 GHz Cortex-A8  GPU ULP GeForce PowerVR SGX530  Chipset Nvidia Tegra 2 AP20H TI OMAP 3630  RAM 1 GByte 512 Mbytes  2G Network GSM 850 / 900 / 1800 / 1900 GSM 850 / 900 / 1800 / 1900  Bluetooth v 2.1 v 2.1  Batería Lí-po (lithium polymer)  1930 mAh, 3.7 v.  Lí-po (lithium polymer)  1400 mAh, 3.7 v.  Tabla 1. Configuración de los dispositivos móviles ad hoc.  Los equipos fueron especialmente preparados para minimizar el consumo de  batería, se procedió entonces a: desinstalar las aplicaciones  no indispensables para su  funcionamiento, deshabilitar el acceso a redes 3G y WiFi, activar el modo “solo 2G”  para acceso a la red, deshabilitar dispositivos de hardware no utilizados en las pruebas  y habilitar el modo de bajo consumo.   4.2 Pruebas y mediciones realizadas  Se realizaron transferencias de 1 Mbyte de datos (Carga útil o Payload) entre un  nodo móvil de la MANET y el servidor de la red de infraestructura.     El tráfico de datos se generó utilizando el protocolo ICMP, primero sobre un canal  no seguro y luego sobre un canal seguro, el aseguramiento del canal  se implemento  utilizando el protocolo IPSEC [7] en modo transporte (extremo a extremo),  combinando los siguientes parámetros:    Servicios: AH autenticación [9], ESP autenticación y encriptación [8]  Intercambio de claves: Modo Agresivo – PSK.  Autenticación: HMAC-SHA-1 y HMAC-MD5.     Cifrado: DES, 3DES, AES.    Para una descripción mas amplia de los parámetros IPSEC utilizados, se puede  consultar [10], donde realizamos pruebas similares sobre un escenario de menor  complejidad.  Las mediciones de consumo de energía en el nodo móvil remoto se realizaron con  la aplicación PowerTutor [19], esta herramienta permite estimar la energía consumida  en tiempo real y por proceso utilizando el modelo de consumo de energía descrito en  [18].   Debido a los factores aleatorios y al ancho de banda variable de la red GPRS [6],  las pruebas se ejecutaron durante varios días consecutivos y en diferentes horarios, los  resultados presentados en la siguiente sección se obtuvieron promediando los valores  obtenidos.  5. Resultados  En la figura 5 presentamos un gráfico comparativo de consumo entre las diferentes  pruebas realizadas, incluyendo un canal no seguro y un canal seguro configurado  utilizando diferentes opciones de IPSec.   0 2 4 6 8 10 12 14 16 NO  S EG UR O AH  - M D5 AH  - S HA -1 ES P  - D ES ES P  - 3 DE S ES P  - A ES  12 8 ES P  - M D5  - D ES ES P  - M D5  - 3 DE S ES P  - M D5  - A ES  12 8 ES P  - S HA -1  - D ES ES P  - S HA -1  - 3 DE S ES P  - S HA -1  - A ES C o n su m o  d e  en er g ía  (J o u le )   Fig. 5. Energía consumida para transferir 1 Mbyte de datos.    En la figura 6 se muestra la distribución de consumo energía para dos  configuraciones IPSec que garantizan autenticación y confidencialidad.      Fig. 6. Distribución del consumo de energía para  ESP-MD5- AES y ESP–SHA-1-3DES      6. Conclusiones y trabajos futuros  La seguridad implica un consumo adicional de recursos que puede variar  dependiendo de los algoritmos que se elijan para el establecimiento de un canal  seguro, en el grafico comparativo de la figura 5 se visualiza que la opción que tiene el  nivel mas elevado de seguridad (ESP – SHA-1 – 3DES) es la que mayor energía  consume, duplicando el consumo de un canal no seguro. La elección de un nivel de  seguridad en los nodos ad hoc dependerá de las posibilidades de recarga que existan  en la zona de despliegue de la MANET.  Se observa que la diferencia de consumo que existe entre los algoritmos de  autenticación es baja con respecto a la diferencia que existe entre los algoritmos de  encriptación.  Se evidencian diferencias importantes en  la distribución del consumo de energía al  utilizar diferentes algoritmos de autenticación y encriptación.  Respecto a la energía consumida para el establecimiento de sesión segura IPsec, si  bien el porcentaje en la distribución de consumo (figura 6) podría parecer importante,  se debe tener en cuenta que este consumo se realiza una sola vez antes de comenzar la  transmisión de la carga útil de datos.  Comparando los resultados obtenidos en el escenario indoor  propuesto en [10] con  los resultados del escenario outdoor, concluimos que en este último los nodos tienen  un consumo mayor (entre un 20% y 30%) de energía.  Para continuar con esta línea de investigación tenemos previsto:    − Incorporar Bluetooth 3.0 en lugar de la versión 2.1.  − Realizar pruebas sobre canales seguros (IPSec)  entre el  Gateway  Bluetooth/GPRS y el servidor de la red de infraestructura, en lugar de los  canales extremo a extremo (nodo - servidor).  − Efectuar mediciones de otros parámetros en los nodos de la red ad hoc:  Latencia, troughput, utilización de la CPU.  − Incorporar compresión al protocolo IPSEC.  − Realizar pruebas utilizando otros protocolos de seguridad (SSL, TLS).  − Utilizar herramientas de simulación para modelar el comportamiento  aleatorio de la red GPRS.	﻿GPRS , MANET , IPSec , Bluetooth	es	22684
31	Análisis de requerimientos de un sistema multigente de robots que juegan al fútbol	﻿ El objetivo del Grupo de Investigación en “Robótica Inteligente”, es promover el estudio de áreas de Inteligencia Artificial a través del fútbol con robots. En la búsqueda de una metodología de análisis y diseño de sistemas multiagentes hemos hallado a Gaia. La metodología Gaia ha sido pensada para sistemas multiagentes como organizaciones computacionales que consisten de varios roles que interactúan. El objetivo de este artículo es presentar el uso de la metodología Gaia describiendo el análisis y diseño de un equipo de fútbol de robots. Palabras claves: Sistemas Multiagente, Fútbol con Robots, Gaia. *Este trabajo está parcialmente financiado por la Universidad Nacional del Comahue, en el contexto del Proyecto de Investigación “Técnicas de Inteligencia Computacional para el diseño e implementación de Sistemas Multiagentes”(04/E062), el Grupo de Investigación en Robótica Inteligente y por el convenio de trabajo conjunto con la Universidad Politécnica de Madrid. 1587 1. INTRODUCCIÓN El fútbol con robots ha sido recientemente catalogado como un problema estándar para la investigación en Inteligencia Artificial y Robótica. Un problema estándar provoca que gran cantidad de investigadores se enfoquen sobre los mismos temas, de forma tal que, aunando esfuerzos el avance sea mucho mayor que si cada uno trabajara en temas no relacionados. El fútbol con robots fue elegido para encontrar necesidades complejas del mundo real, a través de un mundo limitado, que tiene problemas manejables en tamaño y costos de investigación. El problema es atractivo porque ofrece una integración de áreas de Inteligencia Artificial y Robótica. Tales áreas incluyen: comportamiento reactivo, adquisición de estrategias, aprendizaje (learning), planeamiento (planning) en tiempo real, sistemas multiagentes, reconocimiento del ambiente, visión, control de motores, control de robot inteligentes, entre otros [4]. Trabajar con robots que jueguen al fútbol, es interesante, por lo que este juego representa a nivel mundial. El fútbol es idioma universal, sus reglas son de conocimiento común y es el deporte más popular en la mayoría de los países. Esta característica hace al problema del fútbol con robots mucho más atractivo para incentivar estudiantes, investigadores, sponsors y a la sociedad en su conjunto. En todo proceso de desarrollo de software, se utilizan metodologías. Se han evaluado varias metodologías [7] y hemos seleccionado Gaia[9]. Esta metodología fue pensada para construir sistemas multiagentes, por esta razón es útil para el sistema en cuestión: “un equipo”. En la recopilación de artículos sobre Gaia se ha observado la falta de ejemplos de uso de la metodología. Con esta motivación, el presente trabajo describe la metodología Gaia realizando el análisis y diseño de un equipo de fútbol con robots. El trabajo está estructurado de la siguiente manera. A continuación, se describe la metodología y en la tercera sección realizamos una descripción del dominio de trabajo. En las secciones 4 y 5 se realiza el análisis y diseño del equipo de fútbol con robots. Finalmente, en la sección 6 se presentan las conclusiones y se analizan futuros trabajos. 2. DESCRIPCIÓN DE LA METODOLOGÍA UTILIZADA Los agentes se están convirtiendo en uno de los principales paradigmas de las ciencias de la computación. Esto promueve el desarrollo de técnicas y metodologías de ingeniería de software específicamente pensadas para la construcción agentes. Gaia es presentada por Michael Wooldrige[9], como una metodología para análisis de requerimientos y diseño de sistemas multiagentes, desarrollada para dominios con las siguientes características: 1. Los agentes son parte central del sistema. 2. Los agentes son heterogéneos. Diferentes agentes pueden ser implementados utilizando diferentes lenguajes de programación, arquitecturas y técnicas. La metodología no hace referencia alguna acerca de la plataforma de implementación. 3. La estructura de organización del sistema es estática, las relaciones entre los agentes no cambian en tiempo de ejecución. 4. Las habilidades de los agentes y los servicios que realizan son estáticos, no cambian en tiempo de ejecución. 5. Hay un número manejable de agentes (menos de 100). 1588 Los puntos 4 y 5 restringen la metodología a sistemas con agentes que no evolucionen en su comportamiento y en la interacción con otros agentes. Este punto no impide que sistemas con aprendizaje (learning) puedan utilizar la metodología, lo que ocurre es que en una etapa de análisis se observan características del momento en que se elicitan los requerimientos y no se observan los posibles aprendizajes o evoluciones futuras. 2.1. Framework Gaia es un proceso sistemático que, partiendo desde sentencias de requerimientos, arriba a un diseño que está lo suficientemente detallado como para ser implementado directamente. El esquema de módulos de la metodología se observa en la Figura 1. Figura 1: Módulos de la metodologia Gaia El análisis y diseño deben ser considerados como un proceso que incrementa el nivel de detalle de los modelos del sistema a ser construido. Los principales conceptos de la metodología están divididos en dos categorías: abstractos y concretos (Cuadro 1). Entidades abstractas son utilizadas durante la etapa de análisis para conceptos del sistema, pero no necesariamente tienen relación directa con el sistema en tiempo de ejecución. En contraste, entidades concretas son utilizadas durante el proceso de diseño y comúnmente tienen relación directa con el sistema en funcionamiento. Conceptos Abstractos Conceptos Concretos Roles Tipos de agentes Permisos Servicios Responsabilidades Comunicación Protocolos Actividades Propiedades livianas Propiedades seguras Cuadro 1: Comparación entre conceptos abstractos y concretos de la metodología de análisis y diseño Gaia. 1589 3. DESCRIPCIÓN DEL DOMINIO El proceso Gaia comienza por sentencias de requerimientos, las cuales conforman una descripción del sistema a construir. El proceso de definición de los requerimientos del sistema es una tarea ardua y para muchos nada agradable. Con el fin de ofrecer una buena base para la construcción del sistema, los requerimientos deben ser escritos de forma clara y no ambigua. Además, deben ser rastreables, correctos y completos. Una pobre especificación hace que el proceso de construcción tarde más de lo esperado, por no anticiparse a situaciones del sistema. El tiempo dedicado a la etapa de análisis de requerimientos es insignificante en comparación con el resto de las etapas de la construcción, por lo cual hay que darle la importancia que se merece. La descripción del dominio trata de abstraerse de la plataforma y tipo de robots, pero para cuestiones específicas, como cantidad y forma de los robots, nos hemos basado en el ambiente simulado Simurosot de la competencia FIRA[2]. El problema a solucionar es un equipo de fútbol con robots. Los robots deben funcionar de manera autónoma en partidos de fútbol contra otros equipos. En las competencias que nos interesan, los agentes robóticos son cubos de tamaños variados con dos ruedas. Las canchas sobre las que se juegan los partidos son del tamaño de una mesa de ping-pong y se juega con pelotitas de golf. Cada robot, por medio de sus dos ruedas, realiza todos los movimientos posibles. Si las dos ruedas van a la misma velocidad, hacia delante o hacia atrás, los robots se mueven linealmente; si las velocidades de las ruedas varían, los robot se mueven describiendo un arco, o girando en el lugar, si una de las ruedas está parada. Las velocidades son primitivas que están relacionadas con el ambiente y el tipo de robot. Como el objetivo es abstraer el diseño del agente lógico, se trabajará sobre acciones de mayor nivel como ir a una determinada posición. Los agentes robóticos perciben del ambiente a través de cámaras y sensores. Se asume que los agentes pueden acceder a toda la información del ambiente. Esta información incluye, normalmente, la posición y orientación de cada robot y la posición de la pelota. Los robots llevan la pelota a un objetivo determinado, posicionándose detrás de ella y dirigiéndose al objetivo. No cuentan con un dispositivo para patear la pelota, por lo tanto, la acción de patear es realizada por el impacto en la misma. Los equipos pueden contar con diferente cantidad de jugadores, dependiendo de las reglas de la competencia. Se asumirá que los partidos son jugados por equipos de cinco jugadores, como en la categoría simulada Simurosot. Se trata lógicamente de un arquero y cuatro jugadores, los cuales pueden tomar diferentes posiciones dentro del campo de juego dependiendo de la estrategia. 4. ANÁLISIS El análisis constituye la segunda etapa de la metodología Gaia. La entidad más abstracta de la jerarquía de análisis es el sistema. Esta entidad puede significar sociedad, organización o equipo dependiendo del dominio en que se esté trabajando. La idea de un sistema como una sociedad es un buen ejemplo para pasar al próximo nivel de la jerarquía: roles (Figura 2). Puede parecer extraño, si decimos que un sistema de computación ha sido definido mediante roles, pero la idea es más natural cuando hablamos de una organización o sociedad. En un equipo de fútbol pueden surgir varios roles, en relación a los puestos de los jugadores. Hay un arquero, defensores y delanteros. Los objetivos y responsabilidades de cada rol son diferentes. Por ejemplo, el delantero tiene por objetivo hacer goles y el defensor marcar a los delanteros del equipo 1590 Figura 2: Conceptos de la etapa de análisis contrario. Un jugador en un momento dado está cumpliendo un rol dentro de la cancha, el cual puede cambiar según la estrategia del equipo. Un rol está definido por cuatro atributos: responsabilidades, permisos, actividades y protocolos. Las responsabilidades determinan la funcionalidad y son el atributo clave asociado a cada rol. Las responsabilidades están divididas en dos tipos: propiedades livianas y propiedades seguras. Las propiedades livianas describen que “algo bueno ocurre”. Estas definen ciertos asuntos que el agente debe realizar en diferentes circunstancias del ambiente. Por ejemplo, una propiedad liviana para el delantero podría ser: si el arco contrario está libre, entonces patear la pelota con dirección al arco contrario. En contraste, las propiedades seguras son invariantes que describen que “nada malo ocurre”. Por ejemplo, el arquero tiene como propiedad segura que la pelota se encuentre fuera del arco propio. Para cumplir con las responsabilidades, un rol tiene un conjunto de permisos. Estos determinan los recursos disponibles, que comúnmente se trata de la información que el agente puede acceder o modificar. Las actividades de un rol están asociadas a las acciones que puede llevar a cabo el agente por sí mismo, sin la interacción con otros agentes. Finalmente, un rol tiene un número de protocolos que definen la forma en que este interactúa con otros roles. La metodología en la etapa de análisis está organizada por dos modelos: el modelo de roles y el modelo de interacción. El análisis es un proceso iterativo que está formado por 3 sub-etapas: 1. Identificación y definición de roles clave. 2. Modelo de interacción. Identificación de los protocolos asociados a cada rol. 3. Modelo de roles. Definición de responsabilidades, permisos y actividades de cada rol. 4.1. Identificación y definición de roles Los roles en un equipo de fútbol son los puestos en que se desempeña un jugador y constituyen la parte central de la estrategia de juego. Se diferencian dos roles claves: el arquero y los jugadores. Se identifican solo dos roles porque el objetivo de este análisis es obtener un rol básico sin hacer mucho hincapié en el comportamiento específico de un puesto (como delantero izquierdo, mediocampista 1591 central, líbero, enganche, etc.). El arquero es un rol individual, el cual puede ser representado por un solo agente por vez. Este es el principal encargado proteger el arco propio. El rol de jugador puede ser cumplido por más de un jugador, dependiendo de la estrategia. En nuestro caso, el rol es cumplido por el resto de los jugadores. El objetivo de este rol es mostrar un comportamiento básico de un jugador, pero con la primera premisa de hacer goles en el arco contrario. 4.2. Modelo de interacción El modelo de interacción describe las dependencias, relaciones y protocolos que hay entre los roles. La interacción entre los agentes puede realizarse a través del ambiente en el cual se encuentran o directamente a través de un lenguaje compartido. En el equipo, las interacciones entre los jugadores son a través del ambiente, por los movimientos que realizan dentro de la cancha. No hay un lenguaje compartido a través de mensajes entre los jugadores del mismo equipo, sino que interaccionan a través del juego en sí. Juegan al fútbol como una forma de comunicarse. En este caso particular, el modelo de interacción define la estrategia de juego. La estrategia es compartida por todos los jugadores del equipo [3]. El equipo consta de cinco jugadores; un arquero y cuatro jugadores. El arquero debe mantener una posición cercana al arco. Luego, los cuatro jugadores se deben distribuir en la cancha y cumplir un rol en particular. Se elige jugar con un jugador en cada cuadrante para abarcar toda la cancha con la misma cantidad de jugadores. Figura 3: Estrategia del equipo Como se observa en la Figura 3, el equipo está formado por un arquero, un defensor derecho, un defensor izquierdo, un delantero derecho y un delantero izquierdo. Cada uno de estos tiene una zona asignada, con el objetivo de no se superpongan e interfieran en el juego del otro. Uno de los principales puntos de la estrategia es lograr un equilibrio para no ocasionar conflictos entre los propios jugadores (Equilibrio de Nash [8]). 4.3. Modelo de roles El modelo de roles se construye a partir de una especificación de cada uno de los roles, con sus respectivos permisos, responsabilidades, protocolos y actividades. La salida de esta etapa es un esquema para cada rol. Gaia define una notación para las propiedades livianas y seguras, que para este trabajo no serán tenidas en cuenta. Esto se debe a que la notación no es lo suficientemente expresiva como para representar las responsabilidades del sistema en cuestión. Estas serán representadas directamente a través de lenguaje natural. Se han definido dos roles: arquero y jugador que son representados por los cuadros 2 y 3, respectivamente. A continuación, realizamos una explicación general de los esquemas. 1592 Esquema Rol Arquero Descripción Este rol es el encargado de proteger el arco propio. Atajar. Protocolos y Actividades despejarPelota, irAPosicionBase, desbloquear, esperarPelotaEnArea Permisos de lectura Jugadores, Pelota, AreaPropia, PuntoBase, ArcoPropio Permisos de escritura Primitiva Propiedades Livianas Si el Jugador se encuentra atascado entonces desbloquear jugador Si la Pelota se encuentra en el Area propia, despejarPelota. Si la Pelota se dirige al AreaPropia, esperarPelotaEnArea. Si la Pelota está fuera del AreaPropia y no va en direccion a esta, irAPosicionBase Propiedades Seguras Pelota se encuentra fuera del Arco propio Cuadro 2: Esquema del rol arquero a nivel de análisis Las actividades desbloquear, irAPosicionBase, esperarPelotaEnArea, llevarPelotaArcoContrario, acompañarAlQueLlevaPelota y despejarPelota son las acciones de alto nivel que pueden realizar los agentes. En cada ciclo de acción los agentes pueden optar por realizar alguna de estas acciones, las cuales devuelven la correspondiente acción primitiva a seguir para llevar a cabo la acción. Los permisos de lectura conforman toda la información a la que tiene acceso el agente. Esta consiste de: Jugadores: posición y orientación de cada uno de los jugadores. Pelota: posición de la pelota. DireccionPelota: dirección de la pelota. AreaRol: define el área en donde se desenvuelve el rol. De esta forma, diferentes agentes pueden cumplir el mismo rol pero en diferentes áreas. PuntoBase: punto al cual se dirigirá el rol ante una determinada circunstancia. ArcoPropio: define la ubicación del arco propio. ArcoContrario: define la ubicación del arco contrario. AreaPropia: define la ubicación del área del arco propio, que debería ser la misma que el área del arquero. Los permisos de escritura constituyen toda la información que puede ser modificada por los agentes. Esta consiste de la primitiva que es definida por la correspondiente acción a seguir. Las propiedades livianas están descriptas claramente en lenguaje natural y definen bajo qué circunstancias se ejecuta cada acción. Por último, las propiedades seguras definen que nada malo está pasando. Esto, en el contexto de un partido de fútbol, es mantener en cero el arco propio. 1593 Esquema Rol Jugador Descripción Este rol es el encargado de hacer los goles en el arco contrario, presenta el comportamiento básico de un jugador. Protocolos y Actividades irAPosicionBase, esperarPelotaEnArea, desbloquear, llevarPelotaArcoContrario, acompañarAlQueLlevaPelota Permisos de lectura Jugadores, Pelota, DireccionPelota, AreaRol, PuntoBase, ArcoContrario, ArcoPropio, AreaPropia Permisos de escritura Primitiva Propiedades Livianas Si el Jugador se encuentra atascado entonces desbloquear Si el Jugador se encuentra en AreaPropia entonces irAPosicionBase Si es el jugador más cercano a la Pelota, llevarPelotaArcoContrario. Si la Pelota se encuentra en el AreaRol y no es el jugador más cercano a la Pelota entonces acompañarAlQueLlevaPelota Si la DireccionPelota apunta al AreaRol, esperarPelotaEnArea. Si la Pelota se encuentra fuera del AreaRol, si la DireccionPelota no apunta al AreaRol y no es el jugador más cercano a la Pelota, entonces irAPosicionBase Propiedades Seguras La Pelota se encuentra fuera del ArcoPropio. Cuadro 3: Esquema del rol jugador a nivel de análisis 5. DISEÑO El principal objetivo del proceso de diseño clásico, es transformar el modelo abstracto logrado por el análisis, en un modelo con el suficiente nivel de abstracción como para que pueda ser implementado directamente. Este no es el caso de Gaia, porque el diseño debe tener un nivel de abstracción para aplicar las técnicas tradicionales de diseño de agentes. El diseño está formado por tres modelos: el modelo de agente que identifica los diferentes tipos de agentes del sistema y la instanciación de los agentes en alguno de los tipos, el modelo de servicio que identifica los servicios que son requeridos para realizar los roles de agentes y el modelo de comunicación que documenta las líneas de comunicación entre diferentes agentes. 5.1. Modelo de agente El modelo de agente identifica los tipos de agentes. Estos comúnmente tienen correspondencia uno a uno con los roles definidos en la etapa de análisis. Este modelo también especifica la cantidad de agentes que instanciarán en cada rol. Los tipos de agentes en este sistema son representados por los roles de la etapa de análisis. Los tipos de agentes son arquero y jugador. 1594 La cantidad de agentes que instanciarán en cada rol define la estrategia de juego. La cantidad de agentes está limitada a cinco, por lo cual no puede haber más de cinco agentes que cumplan con un mismo rol. La instanciación de los jugadores a los roles está representada en la Figura 3. arquero 1−→ agente arquero = jugador número 1 (keñi) jugador 1−→ agente defensor derecho = jugador número 2 (epu) jugador 1−→ agente defensor izquierdo = jugador número 3 (küla) jugador 1−→ agente delantero derecho = jugador número 4 (meli) jugador 1−→ agente delantero izquierdo = jugador número 5 (kechu) Para darle un tono autóctono al trabajo, para los nombres de los jugadores se ha utilizado la traducción de los números a la lengua mapuche. El rol de arquero es instanciado solo por el agente keñi. El rol de jugador es instanciado por cuatro agentes: epu el defensor derecho, küla el defensor izquierdo, meli el delantero derecho y kechu el delantero izquierdo. Los cuatro agentes tienen el mismo comportamiento pero en diferentes zonas. Estas zonas dividen el campo de juego en cuatro. 5.2. Modelo de servicio Este modelo define los servicios asociados a cada rol. Estos tienen comúnmente una correspondencia con las acciones de la etapa de análisis. La salida de esta etapa es una descripción de las propiedades de cada servicio. Estas son: las entradas, las salidas, las pre-condiciones y las poscondiciones. Se describen seis servicios: desbloquear, despejar pelota, esperar pelota en área, llevar pelota a arco contrario, acompañar al que lleva la pelota e ir a posición base. Cada uno de estos tiene una relación directa con las acciones definidas en el modelo de interacción. El modelo está descripto en el Cuadro 4. 5.3. Modelo de comunicación Aquí se establecen las relaciones entre los roles definidas por los protocolos. La salida de este modelo es un grafo dirigido formado por nodos que representan a los roles y arcos de la forma a −→ b, que representan que el rol a envía mensajes al rol b. En el sistema actual, los agentes no interaccionan a través de mensajes y por ello este modelo no tiene sentido. Las relaciones entre los roles están definidas en el modelo de interacción del apartado 4.2, que describe la estrategia del equipo lo suficiente, como para ser implementada. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este trabajo hemos presentado el uso de la metodología Gaia, describiendo el análisis y diseño de un equipo de fútbol con robots. La obtención del documento de análisis, sirvió de base para la construcción del equipo de fútbol Rakiduam que participó en la categoría Simurosot del CAFR 2006 [1, 6]. El documento de análisis inicial fue modificado incluso durante la competencia para realizar cambios en las estrategias de juego. Una de las principales modificaciones que surgieron fue la asignación dinámica de roles, que no había sido tenida en cuenta inicialmente. La posibilidad de división de roles y el modelo de servicios, permite dividir el proceso implementación en subsistemas, que pueden ser desarrollados en forma paralela. De esta forma, varios analistas pueden trabajar al mismo tiempo, reduciendo el tiempo de implementación. 1595 Servicio desbloquear Descripción Este servicio devuelve la próxima Primitiva a ejecutar para desbloquear al jugador Entrada Jugador Salida Primitiva Pre-condiciones true Pos-condiciones true Servicio despejar pelota Descripción Este servicio devuelve la próxima Primitiva a ejecutar para que el agente lleve la Pelota lo mas rápido fuera del AreaRol Entrada Jugadores, Pelota, AreaRol Salida Primitiva Pre-condiciones La Pelota se encuentra en el AreaRol Pos-condiciones true Servicio esperar pelota en área Descripción Este servicio devuelve la próxima Primitiva a ejecutar para que el agente espera la Pelota en los límites del AreaRol Entrada Jugadores, Pelota, AreaRol, DirPelota Salida Primitiva Pre-condiciones La Pelota no se encuentra en el AreaRol y la DireccionPelota apunta al AreaRol Pos-condiciones true Servicio llevar pelota a arco contrario Descripción Este servicio devuelve la próxima Primitiva a ejecutar para que el agente lleve la Pelota al ArcoContrario Entrada Jugadores, Pelota, ArcoContrario Salida Primitiva Pre-condiciones true Pos-condiciones true Servicio acompañar al que lleva la pelota Descripción Este servicio devuelve la próxima Primitiva a ejecutar para que el agente acompañe al jugador que lleva la Pelota Entrada Jugadores, Pelota, ArcoContrario Salida Primitiva Pre-condiciones true Pos-condiciones true Servicio ir a posición base Descripción Este servicio devuelve la próxima Primitiva a ejecutar para que el agente lleve se mueva al PuntoBase Entrada Jugadores, Pelota, PuntoBase Salida Primitiva Pre-condiciones true Pos-condiciones true Cuadro 4: Modelo de servicios 1596 Uno de los objetivos propuestos para el futuro, es trabajar en conjunto con la cátedra de Inteligencia Artificial, utilizando la interfaz entre Prolog y el simulador de fútbol con robots [5], en la práctica del próximo cursado de la materia. Concretamente, los alumnos aplicarán las técnicas aprendidas durante el cursado, en un práctico que consiste en el desarrollo de un equipo de fútbol.	﻿Futbol con Robots , Sistemas Multiagente , Gaia	es	22717_2
32	Procesamiento de imágenes en tiempo real utilizando tecnología embebida	"﻿ Se presenta un dispositivo de rehabilitación visual, capaz de mejorar  la calidad de vida de pacientes con disfunciones visuales severas (que padecen  visión subnormal o baja visión). El mismo permite adquirir y procesar imágenes  en tiempo real, efectuar un realce selectivo de la información visual y traducir  dicha información a un patrón de estimulación apropiado para cada paciente. El  software fue enteramente desarrollado empleando librerías open-source  (OpenCV y Qt) y está lo suficientemente modularizado como para permitir una  rápida adaptación a nuevos dispositivos embebidos, y un efectivo rediseño  ajustando la plataforma a la evolución de la patología del paciente. El sistema  está diseñado para correr sobre plataformas embebidas, como la placa de  desarrollo Beagleboard, lo cual lo hace fácilmente configurable, portátil y de  bajo costo. Se describen las herramientas utilizadas para implementar los  diferentes procesamientos y se presentan los resultados obtenidos al aplicar los  mismos sobre señales reales.   Keywords: Baja visión, rehabilitación visual, sistemas embebidos, OpenCV.  1   Introducción  El sentido de la visión es fundamental para llevar a cabo las distintas actividades que  cotidianamente realizamos los seres humanos. Tareas tan habituales como cruzar una  calle, leer un libro, ver televisión, entre otras tantas, se ven seriamente afectadas por  traumatismos y patologías oculares que producen ceguera parcial o completa en  millones de seres humanos. A nivel mundial, un gran número de personas padecen  disfunciones visuales severas sin llegar a ser completamente ciegos (condición  conocida con el nombre de baja visión o visión subnormal). La OMS define baja  visión como: ""pérdida de agudeza visual (AV) y/o campo visual (CV), que incapacita  para la realización de tareas de la vida diaria, incluso tras un tratamiento y/o  corrección refractiva convencional”. Pacientes con reducción severa del CV,  presentan una movilidad disminuida (no logran evitar obstáculos o identificar  defectos del terreno de manera efectiva). Si la visión periférica está intacta pero la AV  se encuentra comprometida, la realización de actividades que necesitan alta agudeza  visual (como leer, escribir, coser, etc.) se ven seriamente afectadas. Mediante el  empleo de elementos ópticos y sistemas electrónicos (magnificadores de imágenes,  circuito cerrado de TV, etc.), es posible brindar un cierto grado rehabilitación visual a  estos pacientes. El acceso a dichos dispositivos es generalmente restrictivo debido a  su alto costo, y no siempre brindan un beneficio sustancial al paciente. En general,  sólo compensan una de las deficiencias y no se adaptan a la evolución temporal de la  patología.   En este trabajo se presenta el desarrollo de un dispositivo de rehabilitación visual  diseñado para mejorar la calidad de vida de personas con disfunciones visuales  severas. El sistema se caracteriza por ser reconfigurable, portátil y de bajo costo. El  mismo permite adquirir y procesar imágenes en tiempo real, efectuar un realce  selectivo de la información visual y mapear dicha información en un patrón de  estimulación apropiado a la situación de cada paciente. El software se basa en  plataformas libres (open-source) y está lo suficientemente modularizado como para  permitir una rápida adaptación a nuevos dispositivos embebidos o librerías, y un  efectivo rediseño ajustando la plataforma a la evolución de la patología del paciente.  2   Materiales y Métodos  El dispositivo desarrollado para asistir visualmente a personas con problemas  visuales, consiste en un sistema reconfigurable basado en tecnología embebida y  algoritmos de procesamiento de imágenes digitales. La plataforma consta de tres  componentes: un módulo de adquisición de señales de video, un módulo de  procesamiento del tipo ARM (Advanced RISC Machines) [1] y un módulo de  visualización. La adquisición se efectúa mediante una mini-cámara (Logitech Pro9000) de 1600x1200 pixeles de resolución espacial y foco ajustable, que se conecta a  la unidad de control mediante un puerto USB 2.0. El dispositivo de salida consiste en  unos videolentes (video eyewear EVG920V) que poseen dos displays LCD TFT  gráficos de 640x480 pixeles (VGA), mediante los cuales se generan imágenes  virtuales de aproximadamente 80” a 2 m de distancia. Ellos aceptan señales de video  compuesto (NTSC/PAL/SECAM), consumen menos de 1.1 W y su batería dura unas  4 horas de uso. El corazón del sistema es una placa de desarrollo Beagleboard [2] que  posee un núcleo de procesamiento ARM® Cortex™ A8 con frecuencia de trabajo de  600 MHz, 256 Mb de RAM, un DSP C64x+ y un acelerador gráfico. La placa tiene  conectividad con periféricos (teclado y mouse USB), webcam, LCD, memorias SD,  etc. Las pruebas preliminares del sistema demuestran que es posible realizar  adquisición, procesamiento y visualización en tiempo real, dando resultados muy  satisfactorios [3].  Para el diseño del dispositivo de rehabilitación visual se tuvo en  cuenta una serie de requisitos, entre los cuales mencionaremos:     Tamaño físico: fue necesario reducir al mínimo el tamaño del equipo para mejorar su  portabilidad y adaptarlo a un abanico más amplio de situaciones cotidianas. Para ello  se optó por un system on chip que reúne múltiples funciones en una sola plataforma.  En la Fig. 1 se pueden observar las dimensiones de la unidad de control (localizada en  el interior de una caja de acrílico especialmente diseñada) en relación con el tamaño  de un mouse y un celular.       Fig. 1. Dimensiones de la unidad de control y procesamiento. La placa Beagleboard se  encuentra localizada en el interior de una caja de acrílico diseñada a medida.  Consumo de energía: las dimensiones del dispositivo influyen a su vez en el tamaño  de la batería, y por consiguiente, en el tiempo de uso sin necesidad de recarga. Los  procesadores de tipo ARM son una buena opción debido a su gran capacidad de  cálculo y su bajo consumo de energía.    Calidad de la imagen de entrada: las webcam estándares poseen sensores de baja  resolución y alto blurring (difuminado, borroneado) entre cuadros de la señal de video  (frames). Para que el sistema opere de manera autónoma se requiere el diseño de un  mecanismo de autoenfoque y una mini-cámara digital de resolución espacial  aceptable (por lo menos 640x480).     Capacidad de procesamiento: las operaciones del pipeline gráfico, especialmente las  de dewarping (para remover la distorsión esférica) y reconocimiento óptico de  caracteres (OCR, Optical Character Recognition), requieren gran capacidad de  cálculo. La placa de desarrollo posee una gran potencia de cálculo gracias a su DSP  (Digital Signal Processing), su microprocesador de 32 bits y su aceleradora gráfica.    Latencia y usabilidad de interfaz gráfica: uno de los requerimientos del sistema es  que su respuesta sea en tiempo real, por lo cual fue necesario minimizar la latencia,  incluyendo en el diseño de las clases mecanismos de reducción de bloqueos de  interfaz de usuario, implementando threads para las operaciones concurrentes.    Dentro de las principales características del software podemos mencionar:  reconfigurabilidad (se adapta al paciente y al estadio de su enfermedad), simplicidad,  versatilidad (multifunción), amplia conectividad (a video-lentes, monitor de PC,  televisor LCD), zoom digital ajustable y autofoco, visión aumentada, OCR+TTS  (reconocimiento de caracteres y conversión de texto a speech), etc. La adquisición, el  procesamiento y la visualización de las imágenes, involucra elementos del stack  completo de software: un sistema operativo, drivers de kernel, librerías gráficas de  bajo nivel, toolkits gráficos y herramientas del user space. Se optó por el sistema  operativo embebido Ängstrom v2.6.32 [4] y la adquisición de imágenes se realizó a  través de los drivers de V4L2, que son llamados desde la librería highgui de OpenCV.  La librería OpenCV [5,6] se encarga de proveer las estructuras de datos y los  elementos algorítmicos tanto iniciales como avanzados. En la Fig. 2 se presenta un  esquema con los bloques que conforman la solución de software.      Fig. 2. Arquitectura del software desarrollado.  3   Resultados  Se implementaron una serie de algoritmos, algunos de los cuales describiremos a  continuación. Se implementó un algoritmo para realizar autofoco, el cual consiste en  determinar el operador F que da una idea del grado de enfoque actual de la cámara,  debido a que el driver de Linux no proporciona uno. Para calcular F se emplea la  ecuación 1:                                                               ( )S I F = n        ∑                                               (1)    donde S es un filtro Sobel de dimensión 3x3 que se aplica sobre cada cuadro de la  señal de video (matriz I). Se calcula el valor promedio de la imagen filtrada, siendo n  la cantidad de píxeles. El valor de F se relaciona con la presencia de altas frecuencias  espaciales, cuanto más enfocada está la imagen filtrada, mayor es el valor de F. La  webcam seleccionada permite al usuario administrar su enfoque a través del envío de  números entre 0 y 255. Con el fin de generar un autofoco, se ideó un algoritmo para  encontrar el mejor enfoque en una cantidad iteraciones mínimas. A continuación  describimos el algoritmo de cálculo secuencial del foco óptimo:      Setear variable max=0.0  Setear maxFoco=0  para (i=MinFocusValue;i<= MaxFocusValue;i=i+Step)       {   SetearFoco(i)           capturar imagen           derivada =  Sobel (imagen)           resultado = mean ( abs ( derivada ) )             if (resultado > max) {               max=current;              maxFoco = i ; };    Pseudocódigo de método de cálculo de imagen mejor enfocada.      Una mejora al algoritmo de autofoco sería emplear una técnica de búsqueda en zigzag, con el fin de evitar volver al punto inicial en cada iteración, pudiendo cambiar de  esta manera el sentido de testeo del enfoque. En la Fig. 3 se puede apreciar el  resultado de aplicar el algoritmo autofoco.      Fig. 3.  Efecto de la aplicación del algoritmo de autofoco sobre la nitidez de texto.  Con el fin de mejorar la distinción de los rasgos sobresalientes (features) de las  imágenes capturadas, se buscó una combinación de algoritmos que permita resaltar  los bordes de los objetos presentes en la escena visual. El procesamiento se efectúa en  tres etapas: primero se realiza un difuminado (blurring) para reducir los efectos de la  compresión jpg que genera artefactos de blocking móviles, luego se aplica un  umbralizado (thresholding) adaptativo y finalmente un operador morfológico  (erosión), con el fin de acentuar las divisiones entre las características resaltadas (Fig.  4). Como operación extra, los valores extremos de luminancia (que superan el  umbral) se igualan a un valor intermedio preservando la información específica de  color, y de esta manera evitar la generación de anillos (ringing) que se pueden esperar  para zonas con excesiva diferencia de luminancia.                                (a)                                                                 (b)      (c)  Fig. 4.  a) Imagen original, b) Imagen con máxima luminancia y aplicación del operador  morfológico erosión de dimensión 5x5, c) Imagen con luminancia media y erosión de 3x3.  Para resaltar los bordes de los objetos en las escenas visuales se implentó un filtro  Canny. En la Fig. 5 se puede obsevar la interfaz del prototipo en modo de adquisición  con webcam y el resultado de aplicar a dicha señal de video, en tiempo real, un filtro  Canny para detectar bordes (otros filtros que se pueden aplicar son el filtro  Laplaciano, el filtro Sobel).    Fig. 5. Operación detección de bordes aplicada sobre la señal de video capturada por la  webcam: (izq.) Imagen original y (der.) Imagen filtrada  Otra de las funciones desarrolladas es el reconocimiento de textos OCR (Optic  Character Recognition) y su traducción a un archivo que será leído por medio de un  engine de Text to Speech, TTS (Fig. 6). Para que el software de reconocimiento pueda  cumplir su papel, es necesario que reciba las features gráficas sustancialmente  diferenciadas del fondo y una imagen con un mínimo de transformaciones. Entre de  los algoritmos de preprocesamiento destinados a la realización de OCR, el operación  de binarizado cumple un papel fundamental debido a que es el encargado de separar  del fondo del documento las rasgos característicos correspondientes a los caracteres.         Fig. 6. Algoritmos de preprocesamiento de imágenes para OCR    Dentro de los esfuerzos realizados para la binarización de imágenes documentales, el  algoritmo de Sauvola [7] tiene un rol preponderante por su sencillez y efectividad.  Para ésta técnica, el umbral de binarización t(x,y) es calculado utilizando la media  μ(x,y) y la desviación standard σ(x,y) de las intensidades de los pixeles en una  ventana Wxy  centrada alrededor del pixel con coordenadas x,y (ecuación 2):    ( ) ( ) ( )1 1 σ x, y t x, y = μ x, y +k R    −         (2)    donde R es el valor máximo de la desviación standard (R=128 para un documento de  escala de grises) y k es un parámetro que toma valores positivos. La implementación  computacional actual de dicha binarización es una variación eficiente de la misma,  implementada en el paquete OCRopus (http://code.google.com/p/ocropus/). La Fig. 7  muestra una comparación entre los algoritmos de umbral adaptativo - utilizado en la  etapa 2 de la binarización tradicional - y el de Sauvola, sobre una imagen de un texto  comprimida con el formato jpg (lo cual es común en frames de webcam). Como se  puede apreciar, el algoritmo de Sauvola es muy resistente a las perturbaciones locales,  como los efectos de ringing y blocking propios de los archivos jpg.                        (a)                                        (b)                                          (c)  Fig. 7. Comparación de los algoritmos de binarización: (a) imagen Original, (b) umbral  adaptativo y (c) algoritmo de Sauvola.  Además de los procesamientos mencionados previamente, la interfaz desarrollada  permite manipular documentos del tipo pdf (formato de documento portátil). En la  Fig. 8 se puede observar una partitura en formato pdf, sobre la cual se pueden aplicar  diferentes procesamientos, como por ejemplo, inversión de colores, detección de  bordes, etc. Cabe destacar que el usuario puede desplazarse por las diferentes páginas  del documento, empleando un simple control localizado en la barra lateral  desplegable.     Fig. 8. Visualización de un archivo en formato PDF (partitura musical)  Un importante algoritmo desarrollado es el de visión aumentada, el cual  proporciona  a los pacientes con visión túnel, información necesaria acerca de objetos ubicados en  su periferia (fuera del campo visual del paciente, el cual varía según el estadio de la  enfermedad), permitiendo mejorar la movilidad de los mismos, sin comprometer la  visión central residual. Para conformar las imágenes a ser visualizadas en los display  portátiles, los cuadros adquiridos de la señal de video son filtrados utilizando un  detector de bordes. En un mismo frame se presenta la imagen original ampliada  (zoom) y los bordes de los objetos (en color blanco, con posibilidad de seleccionar  otra tonalidad, según gusto del usuario) de la escena visual original (que presenta el  mayor campo de visión posible) (Fig. 9). El objetivo de esta herramienta  computacional es la de brindarle al paciente información sobre los objetos dentro de  un campo visual amplio (con menor resolución o detalle, pero suficiente para la  navegación), y a la vez, seguir disfrutando de la alta resolución de su visión central  residual.       Fig. 9. Modo de visualización configurado para visión aumentada.  La captura de imágenes desde una cámara sin fijar, posee comúnmente deformaciones  debidas a múltiples fuentes: perspectiva desde la cual se tome, curvatura de las lentes  de webcams (diseñadas para tomar rostros desde una distancia de más de 30-40 cm), y  los efectos de deformación debido a la encuadernación. Con el fin de resolver dichas  perturbaciones, se están desarrollando dos procesamientos: una corrección de la  relación de aspecto por medio de la estimación de un trapecio contenedor y sus 4  puntos de control asociados, y un algoritmo orientado a la corrección de curls (como  el basado en coupled snakelets de Bukhari [8]).  Conclusiones  Se presentó el desarrollo de un dispositivo de rehabilitación visual destinado a  mejorar la calidad de vida de pacientes con disfunciones visuales severas. El  dispositivo está diseñado para poder adquirir y procesar imágenes en tiempo real,  efectuar un realce selectivo de la información visual y traducir dicha información a un  patrón de estimulación apropiado para cada paciente. Se mostraron los detalles de  implementación del hardware y los algoritmos empleados. El desarrollo utiliza  únicamente bibliotecas open source, y está diseñado para permitir una rápida  adaptación a nuevos dispositivos embebidos y a diferentes patologías. Se presentaron  los resultados preliminares obtenidos en situaciones reales, los cuales muestran que el  desarrollo es efectivamente aplicable como dispositivo de rehabilitación visual, y  actualmente se encuentra en etapa de testeo con pacientes voluntarios. Un trabajo  futuro que se está considerando es la utilización de implantes neuroestimuladores para  poder utilizar este dispositivo en casos de pérdida completa de visión."	﻿Baja visión , rehabilitación visual , sistemas embebidos , OpenCV	es	22734
33	Trabajo preliminar para la obtención de tiempos sincronizados en clusters con nodos de múltiples núcleos	"﻿ En este artículo se presenta una extensión de la metodología y  herramienta para instrumentación de programas paralelos en plataformas de  cómputo distribuidas que se venía elaborando en trabajos anteriores.  Específicamente, la extensión contempla el hardware de múltipes núcleos que  actualmente se utiliza en los nodos individuales de los clusters. En el contexto  de instrumentación de tiempo en ambientes distribuidos es fundamental la  sincronización de los relojes que intervienen. Se mantiene el algoritmo básico  de sincronización, usando las estrategias clásicas que se utilizan en ambientes  distribuidos para entornos de cluster, con una red de interconexión sobre la que  se tiene acceso exclusivo (o controlado) para todas las comunicaciones entre las  computadoras que se sincronizan. Este ambiente es específicamente el de los  entornos de cómputo paralelo en clusters.   Palabras claves: Rendimiento e Instrumentación Paralela, Sincronización de  Procesos, Relojes Distribuidos, Sistemas Paralelos y Distribuidos, Paralelismo  en Clusters e Intercluster, Sincronización Interna y Externa.  1   Introducción  En este trabajo se continúa la línea de investigación sobre sincronización de relojes de  computadoras, con fines de realizar instrumentación [5] [6] [7] [8] [9] [10] [11] [12].  Se trata de extender la herramienta presentada oportunamente a fin de incluir los  sistemas con múltiples núcleos. En estos sistemas, las estrategias utilizadas en los  sistemas monoprocesador presentan inconvenientes. La referencia de tiempo  utilizada, proporcionada originalmente por el dispositivo de hardware “TimeStamp  Counter” (TSC), tiene dos limitaciones:   1) Cada núcleo posee su propio TSC, independiente y a priori sin sincronización  respecto a los demás.  2) Es posible que los sistemas de ahorro de energía alteren la frecuencia con la  que se actualiza la referencia en el TSC. Es decir que, a priori, no se puede  asumir que la frecuencia para la actualización del TSC sea constante (más  allá de las características físicas del propio TSC y del ambiente).                                                             1  Profesional Principal CICPBA  2  Investigador CICPBA    Estas limitaciones han llevado a buscar una alternativa de referencia de tiempo  que, sin dejar de cumplir con los requerimientos en los que se basaba el diseño de la  herramienta de instrumentación [8], no presente estos inconvenientes. Los  requerimientos originales son:  • Una herramienta que pueda ser usada inicialmente en un cluster de PCs, con la  posibilidad de ser extendido a clusters en general y luego en plataformas  distribuidas aún más generales.  • Sea de alta resolución, es decir que se pueda utilizar para medir tiempos cortos,  del orden de microsegundos.  • Que no altere el funcionamiento de la aplicación bajo prueba, o que la alteración  sea mínima y conocida para realizar los análisis de rendimiento que sean  necesarios.  • Utilice en forma predecible la red de interconexión. Más específicamente, se  puedan determinar, desde la aplicación, los intervalos de tiempo en los cuales se  utilizará la red. De esta forma, se puede desacoplar el uso de la red de  interconexión, ya que habrá intervalos de tiempo usados para la sincronización e  intervalos de tiempo utilizados para la ejecución de programas paralelos.    Analizando los últimos cambios en los dispositivos periféricos, se contempla la  utilización de los registros temporizadores “High Precision Event Timers” (HPET) [4]  como referencia de tiempo. En primera instancia se cumple con los requerimientos  anteriores y provee una referencia única para todos los núcleos de un mismo nodo de  cada cluster. Evidentemente, nodos diferentes son independientes y deben ser  sincronizados, para lo cual en principio se seguirá con la metodología/algoritmo  estándar.  2. Hardware y HPET   Las especificaciones industriales en los manuales dedicados a los usuarios OEM y  proveedores de BIOS que usen series de chipset Intel para basar sus productos [3]  describen la “Platform Controller Hub” (PCH) como término genérico que engloba  las funciones y capacidades para soporte de E/S. En esos manuales se detallan las  características de las señales (temporales y eléctricas), empaquetado, mapeo de  memoria y registros así como las distintas interfaces y sus registros de configuración.  Entre las funciones y capacidades tradicionalmente utilizadas encontramos las  especificaciones para PCI, USB, SATA, PCI Express y LAN. Se agregan además   nuevas capacidades entre las que se encuentra hardware adicional de temporizadores  para complementar y eventualmente reemplazar las funciones de generación de  intervalos de tiempo y de interrupciones periódicas provistas por el 8254  Programmable Interval Timer y el Real Time Clock en las computadoras personales  basadas en arquitecturas Intel (IA-PC). Los temporizadores de eventos de alta  precisión (en inglés High Performance Event Timers – HPET) son definidos [4] como  un conjunto de registros de temporización separados en bloques que el sistema  operativo puede utilizar e inclusive en el futuro podrá asignar específicamente para  ser utilizados en forma directa por una aplicación. Se propone su utilización en  sincronización de audio y video, en programación (scheduling) de tareas, hilos o  procesos por generación de interrupciones y operaciones de marcado de tiempo en  plataformas multiprocesador.  2.1 Registros Temporizadores  Cada temporizador o timer puede ser configurado para generar una interrupción  separada. La arquitectura permite bloques de 32 timers, pudiendo soportarse hasta 8  bloques, dando un total de 256 timers. Se establece el soporte de 3 timers como  mínimo. Los timers son implementados con un único contador ascendente (main  counter) y un conjunto de comparadores y registros de comparación (match register).  El contador ascendente se incrementa de forma monotónica, es decir cuando se  realizan dos lecturas consecutivas del contador, la segunda lectura siempre tiene  valores mayores que la anterior salvo el caso límite de desborde (roll over). Cada  timer incluye un comparador y un registro de comparación. Cada timer puede generar  una interrupción cuando el valor en su registro de comparación es igual al valor del  contador ascendente. La interrupción generada por cada timer poseerá características  definidas en un registro asociado para configuración y ruteo en el que se puede por  ejemplo, habilitar la generación de esa interrupción en forma  periódica. Los registros  asociados con los timers se asignan al espacio de memoria de modo similar que el  controlador avanzado de interrupciones de entrada/salida (I/O APIC, Advanced  Programmable Interrupt Controller). Sin embargo, no se implementan como una  función estándar PCI. El BIOS reporta al sistema operativo la ubicación asignada en  memoria para el bloque de los timers. No se espera que el sistema operativo cambie la  ubicación de estos timers una vez que se establece por/en el BIOS.  2.2 Mapeo en Memoria  La CPU puede acceder directamente a cada registro timer porque éstos están  mapeados en memoria. El espacio de registros timer es de 1024 bytes. Son alineados  en límites de 64 bits para simplificar su implementación con procesadores de 64 bits.  El modelo de registros permite que cada bloque timer contenga hasta 32 timers, donde  cada timer consiste de un comparador más un registro de comparación. La Figura 1  muestra una visión genérica de los registros timer y su mapeo en memoria. En ella  debe observarse que el funcionamiento del bloque de HPET, se define en tres  registros generales (de capacidades e identificación, de configuración y de generación  de interrupciones) y un único contador ascendente. Estos están ubicados a partir de la  dirección base del bloque, en los desplazamientos 0H, 10H, 20H y F0H  respectivamente. En el primero de los mencionados registros (registro de capacidades  generales e identificación) se encuentran definidos el período con el que trabaja el  contador ascendente y la cantidad de timers implementados en el hardware. En las  posiciones de memoria a continuación de las anteriores se encuentran en oren  sucesivo los registros asociados a cada timer comenzando con el timer 0. Por cada  timer se tienen 3 registros de 64 bits (de configuración, de comparación y de ruteo de  interrupciones) que se ubican cada 32 bytes. La ubicación relativa de los registros de  cada timer puede determinarse siguiendo la fórmula (20*n+100H) donde n es el  número de timer.   Figura 1. Modelo de Registros y su Ubicación Relativa en Memoria  3   El Problema de la Sincronización de Relojes  Como en la gran mayoría de los sistemas distribuidos, en un cluster no existe la  posibilidad de acceso de todos los nodos a un único reloj. Por ello, se debe  implementar la posibilidad de acceder al propio reloj local de cada nodo, y mantener  los relojes locales sincronizados con respecto al reloj de un servidor (sincronización  interna) [2]. La sincronización de relojes de computadoras en un ambiente distribuido  se basa en acotar las diferencias de tiempo en un instante dado. Para ello se debe  comunicar una referencia inicial a todas las máquinas. A partir de dicha referencia, se  debe estimar la deriva de los relojes individuales, a fin de corregirla. Para ello se hace  necesario el intercambio de más referencias de tiempo entre servidor y cliente. La  comunicación de estas referencias, a través de la red de comunicaciones subyacente,  encuentra un problema en la variabilidad de los tiempos de comunicaciones. Esto se  acota utilizando un algoritmo basado en los métodos estadísticos de los tiempos de  comunicaciones [1] [2]. En este algoritmo se realiza una estadística de los tiempos de  comunicaciones de mensajes entre los diferentes nodos. Establecido el tiempo mas  frecuente (tiempo  moda), sólo se consideran válidos los mensajes que lleguen en este  tiempo. La validación del tiempo de referencia se realiza en el nodo que envía el  mensaje al arribar el aviso de reconocimiento y calcular el tiempo RTT (Round Trip  Time). En la Figura 2 vemos que la referencia llegará con una demora D = RTT/2, si  estimamos que los tiempos de ida y de vuelta son iguales (ó simétricos), lo cual es  altamente probable en los ambientes de cluster y redes locales. La variabilidad en la  simetría de los tiempos de comunicaciones es parte de los errores de sincronización  que efectivamente se tendrán en la implementación final.   Interfase E/S  Mapeada en  memoria Configuración General  Estado General IRQ  Capac. Generales e ID  Contador ascendente  Timer0 Comparador  Timer0 Ruteo IRQ  Timer0 Conf.&Capac.  Timer31 Comparador  Timer31 Ruteo IRQ  Timer31 Conf.&Capac.    Registros  Timer  (1K)  Dirección base de bloque  63 0 Offset  000-007H    010-017H    020-027H      0F0-0F7H      100-107H    108-10FH    110-117H    120-137H  140-157H  -  -  -    Figura 2: Tiempos de comunicación de una referencia entre nodos  4   Experimentos para Determinar Sobrecarga y Resolución  En la instrumentación de aplicaciones distribuidas se pretende mejorar su rendimiento  a partir del análisis de la sucesión de eventos en tiempo de ejecución.  Para el análisis  correcto de la sucesión de eventos de una aplicación paralela es necesario contar con  una referencia horaria única, o al menos con diferencias acotadas, en los nodos del  cluster. Se debería tener en cuenta que la utilización de dicha referencia no cause  diferencias de rendimiento (o funcionamiento, en general) significativas de la que  tendría la aplicación sin instrumentar (intrusión). Se realizaron experimentos a fin de  determinar sobrecarga y resolución de las diferentes fuentes de hora, a fin de  comprobar que se cumplan los requerimientos al respecto. Todos los experimentos se  llevaron a cabo en un cluster compuesto por nodos homogéneos, formados por  procesadores Intel XEON E5405 2GHz con 2GB RAM y Sistema Operativo Linux  2.5.32-5.  En la Tabla 1 se muestran los resultados estadísticos de los tiempos de sobrecarga  de diferentes fuentes de suministro de hora local, en microsegundos.    Tabla 1.  Sobrecarga de Diferentes Fuentes de Tiempo.  Fuente Máx. Mín. Prom.  gettimeofday 0,980918 0,674944 0,79087915  HPET 0,989917 0,506958 0,5185626  TSC 0,614949 0,059995 0,07464648    La Figura 3 muestra los histogramas de frecuencia de sobrecarga de cada uno de  los métodos que pueden proporcionar una referencia de hora local. Se puede apreciar  que tanto en los valores de tiempos de lectura del HPET como en los del TSC  los  RTT = T2 – T1  D = RTT/2  tiempo sincronizado = (Tref  + D)    Línea de tiempo Cliente  Línea de tiempo Servidor  Round Trip Time (tiempo de ida y vuelta)  Demora ""D"" a adicionar a referencia   T1    T2  Tref  valores se concentran cerca del mínimo, mostrándose los de gettimeofday más  dispersos, probablemente debido a que se trata de una llamada al sistema, con lo que  intervienen varios factores que determinan su tiempo de ejecución. Si bien el menor  tiempo de sobrecarga se obtiene con TSC, se recuerda que se utiliza HPET para  disponer de una referencia única para todos los núcleos de todos los procesadores de  cada nodo.  Figura 3. Distribución de Tiempos de Sobrecarga de gettimeofday, HPET y TSC.    En cuanto a la resolución, en la mayoría de los sistemas el contador del HPET se  actualiza a una frecuencia del orden de los 14Mhz, pero en el caso de hacer  mediciones sucesivas a máxima velocidad, se obtiene un tiempo entre mediciones de  0,55 microsegundos, lo que pudiera limitar a este valor la resolución. Aún en este  valor, la resolución cumple con el requerimiento de ser menor o igual a 1  microsegundo.  5   Conclusiones y Trabajos Futuros  Los sistemas multiprocesadores continúan evolucionando, con lo que se deben seguir  investigando formas de disponer de una referencia de tiempo en las máquinas de este  tipo, que cumpla con los requerimientos enunciados anteriormente. En este trabajo se  ha demostrado la posibilidad de utilización del HPET y de acuerdo con los resultados,  la sobrecarga se puede considerar dentro de los límites de lo aceptable. Todo indica  que teniendo una referencia de tiempo única para todos los núcleos de los posibles  múltiples procesadores de un nodo se podrían sincronizar todos los nodos de un  cluster. Como es usual, el trabajo inmediato para continuar con esta línea de  investigación será la implementación de la sincronización en todo un cluster  completo. Esta implementación deberá contemplar los límites (errores) de la  sincronización.   Una vez implementada la sincronización, se podrá efectivizar su uso en la  evaluación de rendimiento de aplicaciones paralelas en clusters. En todos los casos se  deberá tener en cuenta la red de interconexión que no solamente se utilizará para  sincronización sino también para la propia aplicación paralela. Como hasta ahora, se  mantendrá un uso controlado y conocido a priori de la red de interconexión. Siempre  que la red mejore en rendimiento se podría mejorar también en términos de reducción  de las cotas de error de sincronización."	﻿Sincronización de Procesos , Sincronización Interna y Externa , Rendimiento e Instrumentación Paralela , Relojes Distribuidos , Sistemas Paralelos y Distribuidos , Paralelismo en Clusters e Intercluster	es	22736
34	Patrones de seguridad aplicados a la función autorización	﻿ La característica de “software seguro” reside en la naturaleza de los  procesos y las prácticas utilizadas para especificar, diseñar, desarrollar y  desplegar el software. Un proceso mejorado para la seguridad incorpora  prácticas para reducir el número de errores y debilidades explotables. Los  patrones de seguridad constituyen un aporte para salvar el vacío entre teoría y  práctica, y pueden emplearse en organizaciones con distinto grado de madurez.  Este trabajo se centra en patrones de seguridad asociados al diseño y considera  la inclusión de funciones relativas al control de acceso. En particular trata la  autorización de los privilegios de usuarios: proceso en que se determina qué  acciones puede ejecutar un usuario autenticado, tanto sobre las funciones  propias del software como sobre los datos que éstas manipulan. Se analiza un  prototipado del patrón ROLE-BASED ACCESS CONTROL (RBAC), aplicado  a sistemas de información de la gestión universitaria.  Palabras claves: Software seguro. RBAC. Autorización  1 Introducción  La principal característica de un software seguro reside en la naturaleza de los procesos  y las prácticas utilizadas para especificar, diseñar, desarrollar y desplegar el software  [1]. Un proyecto que adopta un proceso de desarrollo de software mejorado para la  seguridad incorpora un conjunto de prácticas que permiten reducir el número de errores  y debilidades explotables. A lo largo del tiempo, estas prácticas se vuelven más  sistemáticas, por lo que debería disminuir la probabilidad que tales vulnerabilidades  estén presentes en el software en el momento en que se lo libera. Los resultados en el  campo de la investigación y las experiencias en la industria indican la importancia de   reducir tales vulnerabilidades potenciales tan temprano como sea posible dentro del  ciclo de vida del desarrollo del software. La adopción de procesos y prácticas mejoradas  para la seguridad resulta muchísimo más rentable que la solución tan difundida en la  actualidad de desarrollar y liberar parches para el software operativo [2].  Esta atención temprana de la seguridad tiene que ver con la adopción de un conjunto  de actividades que hacen posible la integración de la misma en el ciclo de vida de  desarrollo de software [3], las que incluyen: i) identificar objetivos de seguridad, ii)  aplicar guías de diseño de seguridad, iii) crear modelos de amenazas, iv) conducir  2  revisiones seguridad de la arquitectura y el diseño, v) completar revisiones de seguridad  de la implementación, y vi) ejecutar revisiones de seguridad del despliegue [4].  El proceso de desarrollo de software seguro encuentra en los patrones de seguridad  una vía para salvar el vacío existente entre teoría y práctica. Si bien existen abordajes  teóricos, éstos se encuentran limitados a sistemas de relativa complejidad, además de  requerir de un nivel de experiencia y conocimiento que no está disponible en el nivel  necesario. A esto se agrega que el requerimiento de seguridad es uno más de los  muchos que se deben atender durante el desarrollo de software, pudiendo observarse  un abordaje ad-hoc.  2 ¿Qué hace seguro al software?  Antes de poder determinar las características de un software para hacer de éste un  software seguro, se debe establecer cuáles son los problemas de seguridad que  deberán ser atendidos durante su proceso de desarrollo, y seleccionar qué patrones de  seguridad aplicar en las diferentes fases de dicho proceso. Es necesario definir las  propiedades mediante las que puedan ser descriptas estas características. Estas  propiedades comprenden:  1. un conjunto de propiedades fundamentales cuya presencia (o ausencia) son el  terreno firme que hacen seguro al software (o no),  2. un conjunto de propiedades conducentes que no hacen seguro al software en forma  directa, pero que permiten caracterizar cuán seguro es un software.  2.1 Taxonomía de las propiedades de seguridad  Centrando el análisis en las propiedades fundamentales (que se pueden tratar también  como atributos de seguridad), se considera que los efectos de vulnerar la seguridad  del software se pueden describir en términos de los efectos sobre estas propiedades  fundamentales. Las mismas se enumeran a continuación. [5]   Confidencialidad. El software debe asegurar que cualquiera de sus características  (incluidas sus relaciones con su ambiente de ejecución y sus usuarios), los activos que  administra, y/o su contenido se encuentran enmascarados u ocultos de las entidades  no autorizadas.  Integridad. El software y los activos que administra son resistentes y flexibles a la  subversión, la que se logra mediante modificaciones no autorizadas (del código, los  activos administrados, la configuración o el comportamiento del software) por parte  de entidades autorizadas, o cualquier modificación por entidades no autorizadas; se  debe preservar tanto durante el desarrollo del software como durante su ejecución.  Disponibilidad. El software debe estar operativo y accesible a sus usuarios  autorizados (humanos o procesos) siempre que se lo necesite; simultáneamente, su  funcionalidad y sus privilegios deben ser inaccesibles a usuarios no autorizados  (humanos y procesos) en todo momento. Para las entidades que actúan como usuarios  se requieren dos propiedades adicionales, generalmente asociadas con los usuarios  finales, que se indican a continuación.  3  Responsabilización. (en idioma inglés, accountability). Todas las acciones relevantes  relacionadas con la seguridad del software que actúa como usuario se deben registrar  y rastrear con atribución de responsabilidad; el rastreo debe ser posible tanto durante  como a posteriori de la ocurrencia de las acciones registradas.   No repudiación. La habilidad de prevenir que el software que actúa como usuario  desmienta o niegue la responsabilidad relativa a acciones que han sido ejecutadas;  asegura que no se puede subvertir o eludir la propiedad ‘responsabilización’.  3 Atributos requeridos en un software seguro  Con el propósito de proveer las propiedades fundamentales indicadas, es preciso  incluir en el software seguro funciones relativas al control de acceso. Estos  requerimientos están presentes tradicionalmente tanto en los componentes de la  infraestructura como en las aplicaciones que conforman los sistemas de información  basados en tecnologías de la información. Estas funciones hacen posible forzar el  cumplimiento de políticas en base a las que los usuarios verificados ejecutan las  diversas funciones provistas por el software conforme a su rol, y se evita que lleven a  cabo aquéllas que no le competen. El control de acceso incluye: [6]  ─ Autenticación de usuarios  ─ Autorización de sus privilegios  ─ Auditoría para controlar y registrar las acciones de los usuarios  El presente trabajo centra el análisis en la segunda de las funciones; en particular, la  autorización de los privilegios de usuarios comprende el proceso en que se determina  qué acciones puede ejecutar un usuario autenticado, tanto en los que se refiere sobre  las funciones propias del software como sobre los datos que éstas manipulan. A los  fines de diseñar dicho proceso, se hace uso del conocimiento documentado vía  patrones de seguridad.  4 Patrones de seguridad  El concepto de patrón es conocido por la comunidad como una solución a problemas  comunes en el desarrollo de software, cuya efectividad se ha comprobado resolviendo  problemas similares en ocasiones anteriores y tal que sea reutilizable (aplicable a  diferentes problemas de diseño en distintas circunstancias). En el caso de los aspectos  de seguridad del software, que se encuentran a lo largo de todas las fases del  desarrollo, su definición original [7] establece que: “Cada patrón es una regla de tres  partes, expresada como una relación entre un determinado contexto, un determinado  sistema de fuerzas que ocurren repetidamente en este contexto, y una determinada  configuración de software que permite que estas fuerzas se resuelvan a sí mismas.”  Extendiendo el concepto a la seguridad del software, los patrones de seguridad  documentan soluciones bien conocidas a problemas recurrentes de seguridad de la  información, permitiendo una transferencia eficiente de experiencia y de  conocimientos. Los mismos aplican el concepto de patrón al dominio de la seguridad,  describiendo un problema particular de seguridad recurrente que ocurre en un  contexto específico y presentando una solución genérica bien probada y aceptada por  4  la comunidad de expertos [8]. Al explicitar los supuestos bajo los que resultan  aplicables sus soluciones, reducen el riesgo de su empleo inadecuado.   La solución propuesta por un patrón de seguridad consiste en un conjunto de roles  interactuantes que pueden ser organizados en múltiples estructuras (aplicables a la  fase de Requerimientos, Diseño o de Implementación, según corresponda el patrón)  concretas, así como también un proceso para crear una estructura particular en éstas  [9]. De acuerdo a lo expresado en [10] “Un patrón define un proceso y una cosa: la  ‘cosa’ es creada por el ‘proceso’.” Los patrones de seguridad se pueden categorizar  de acuerdo a un punto de vista asociado a un ciclo de vida de desarrollo de software  [11]. De esta forma existen patrones para la fase de requerimiento, patrones para la  fase diseño, y patrones para la fase de implementación. Este trabajo se centra en el  empleo de patrones de seguridad para el diseño.  5 Empleo de patrones de seguridad para el diseño de la función  Autorización  En las organizaciones suele haber muchos sistemas, los que son utilizados por múltiples  usuarios, los que a su vez tienen diferentes privilegios para usar los sistemas y las  diferentes funciones y datos que estos gestionan. Los privilegios forman diferentes  subconjuntos los que pueden ser asignados a diferentes usuarios (personas realizando el  mismo trabajo o similar). El problema que se plantea entonces es resolver la asignación  de privilegios de usos (autorización) de una manera ágil y flexible.   Una primera solución a este problema se conoció como el patrón ROLE [12]. Esto  evolucionó, incorporando el concepto de ROLE-BASED ACCESS CONTROL (RBAC)  [13], donde el problema se enfoca en las funciones de trabajo que las personas tienen  que realizar en su actividad diaria. RBAC está basado en el principio de acceso con el  menor privilegio [14]: un rol sólo concede los mínimos privilegios requeridos por un  individuo para realizar su trabajo.  El principio básico de RBAC (tal como se muestra en la Figura 1 [15]) es que un  sujeto puede ejecutar una transacción sólo si tiene asignado un rol con los permisos  requeridos para hacerlo. Los roles pueden ser asignados estática o dinámicamente,  mientras que una jerarquía de roles simplifica el modelado de roles habilitando roles  superiores para heredar los privilegios a roles inferiores. Se pueden aplicar  restricciones a las asignaciones y jerarquías de roles para prevenir que se asignen  roles conflictivos a los usuarios, o que se asigne demasiada autoridad a un usuario.  5.1 Proceso de implementación  Si bien la teoría de base de RBAC es relativamente simple, introducirlo en una  organización madura es un proyecto de trabajo intensivo. Por esta razón, se presentará  un proceso sugerido para incorporarlo a una organización, y luego se lo redefinirá para  poder comenzar con el prototipado de implementación del mismo a un caso concreto.  En [16] los autores sugieren el siguiente proceso para una planificación de Roles:  1. Establecer el Caso de Negocio. Se debe seleccionar un caso de negocio bien  comprendido para obtener patrocinio ejecutivo y fondos; se definen métricas que  permiten evaluar los resultados.  5    Fig. 1. Estándar RBAC.  2. Evaluar las capacidad para RBAC. Determinar cuál es la experiencia y  herramientas disponibles en procesos de asignación de autorización.  3. Seleccionar herramientas y colaboradores para la implementación. De acuerdo  a los niveles y características a implementar de RBAC, seleccionar la herramienta.  Es importante incorporar al menos un colaborador con experiencia.  4. Establecer línea base de Roles. Se establece una base de roles, a partir de la cual  se realizará la definición de roles de granularidad fina.  5. Identificar participantes e interesados del proyecto.  6. Articular metas y objetivos para el proyecto. Los interesados técnicos y de  negocio deben acordar las metas y objetivos del proyecto: enfoques y estrategias de  implementación, definición de métricas, planificación del proyecto, etc.  7. Implementación iterativa. La estrategia de implementación debe ser realizada en  fases para que la misma sea exitosa.  Este proceso es válido para llevar a cabo una implementación de Roles teniendo  cierta experiencia previa (actividades 2 y 4, especialmente), pero en nuestro caso, se  plantea hacer una experiencia piloto (actividad 7) por lo que se adapta el mismo para  seguir con nuestro proyecto. El proceso a seguir es el siguiente:  1. Establecer un Caso de Negocio  2. Definir un esquema de roles para el caso de negocio.  3. Establecer la línea base de Roles  5.1.1. Caso de Negocio  Como caso para realizar el prototipado señalado se seleccionó trabajar con los  sistemas de información que se aplican a la gestión de una entidad universitaria (en  particular, una facultad). Para establecer la definición de roles se eligió una de las  secretarías que conforman la estructura organizativa de una Institución Universitaria, a la  se denomina simplemente “Secretaría”; la misma tiene una estructura jerárquica clásica  6  (ver Figura 2), conformada de la siguiente manera: Secretario, Direcciones (Académica,  Departamentos, Posgrado), Áreas (Educación a Distancia, Acceso a la Universidad). La  Dirección de Departamentos, a su vez, se compone de un Departamento por cada carrera;  y la Dirección Académica se conforma por cuatro Departamentos (Alumnos, Biblioteca,  Legajos, y Títulos y Egresados). Además, cada unidad organizacional está compuesta por  ‘Administrativos’, que colaboran en la realización de las funciones de la unidad.     Fig. 2. Estructura organizativa del caso de estudio.  Los principales Objetos protegidos (OP) considerados para este caso son (no es la  lista exhaustiva, y sólo se considera a nivel de ‘sistema’): a) Sistema de Gestión  Académica, b) Sistema de Autogestión Docentes y Alumnos, c) Sistemas de  Información Gerencial, d) Información de Ingresantes, y e) Sistema de Gestión de  Biblioteca.  5.1.2 Esquemas de roles.  La estructura y objetos definidos en 5.1.1 permiten definir los siguientes Roles de  Negocio (RN): a) Secretario, b) Director Académico, c) Director de Departamento  Carrera, d) Director de Posgrado, e) Responsable Área Educación a Distancia, f)  Coordinador Acceso a la Universidad, g) Director Dpto. Alumnos, h) Director Dpto.  Biblioteca, i) Director Dpto. Legajos, j) Director Dpto. Títulos y Egresados, k)  Administrativo Departamento Carrera, l) Administrativo Posgrado, m) Administrativo  Educación a Distancia, n) Administrativo Acceso a la Universidad, o) Administrativo  Alumnos, p) Administrativo Biblioteca, q) Administrativo Legajos, r) Administrativo  Títulos y Egresados, s) Docentes, y t) Alumnos.  Una característica destacable de la Institución es la existencia de usuarios que,  debido a que participan en ella con diferentes roles (por ejemplo: Secretario, Docente  7  y Alumno) simultáneamente, deben, en consecuencia, contar con distintos privilegios  para acceder a los sistemas de información. Esto conlleva un importante grado de  complejidad en definición del modelo a adoptar para el patrón de seguridad RBAC.  Para explicitar los privilegios que se pueden otorgar sobre los OP, sin asociarlos  directamente a Usuarios o a RN, se definen los Roles Técnicos (RT), planteados en el  alto nivel. En la Tabla 1 se muestra sólo un subconjunto representativo, vinculado a los  Privilegios que estos tienen sobre los OP. Finalmente, y según las definiciones  anteriores, se hace la primera asignación de RT a RN, como se muestra en Tabla 2.  Rol Técnico Privilegio Objeto Protegido  Secretario Académico Análisis de la información de  carreras, cursos y alumnos  Sistema Información  Gerencial  Administrador Sistema  Académico  Configuración del sistema Sistema Académico  Administrativo Sistema  Académico  Carga de operaciones diarias Sistema Académico  Docente Consultas de cursos y  actualización de datos de cursos  Sistema Autogestión  Estudiante Consultas de información propia  e inscripciones  Sistema Autogestión  Director Dpto. Carrera Análisis de Información de  carrera, cursos y alumnos  Sistema Información  Gerencial  Administrativo Dpto.  Carrera  Consultas evolución alumnos  carrera  Información Alumnos  (Historia Académica)  Administrativo  Ingresantes  Consultas y actualización Información Ingresantes  Administrador Biblioteca Configuración Sistema Gestión Bibliotecas  Administrativo Biblioteca Consultas y actualización Sistema Gestión Bibliotecas  Socio Biblioteca Consultas y préstamos Sistema Gestión Bibliotecas  Nota: Esta definición relaciona un privilegio directamente con un OP (relación 1 a 1).  Tabla 1. Definición del Rol Técnico - Privilegio - Objeto Protegido.  5.1.3 Línea base de roles.  Si bien los RN sirven para asignar un rol a algún Usuario que integra una estructura  organizativa, son los RT los que en definitiva permitirán hacer la asignación de  Privilegios sobre los OP. Por lo tanto, se utilizan los RT para realizar la definición de  la Línea Base de Roles. La Tabla 2 es una vista parcial de la Línea Base resultante  para el caso (sólo se consideran algunos ítem como ejemplos).  El primer nivel de asignación de roles a Usuarios que se establece emplea ‘RBAC  Flat’ (correspondiente al Level 1 de los cuatro niveles definidos por los estándares  RBAC [15]). En la Tabla 3, a cada Usuario se le asigna un RN, el que a su vez le  direcciona a un RT. Los privilegios asociados son los que se corresponden a los  definidos en la Tabla 1, y el modelo RBAC para este caso se muestra en la Figura 3.  Analizando la Tabla 3, se observa que es posible establecer niveles de herencia y/o  composición entre algunos RT específicos. Por ejemplo, un Administrador Sistema  8  Rol de Negocio Rol Técnico  Secretario Secretario Académico  Director Académico Administrador Sistema Académico  Administrativo Alumnos Administrativo Sistema Académico  Docente Docente   Estudiante Estudiante   Director de Dpto. Carrera Director Dpto. Carreras   Administrativo Dpto. Carrera Administrativo Dpto. Carrera  Administrativo Acceso a la Universidad Administrativo Ingresantes  Director Dpto. Biblioteca Administrador Biblioteca  Administrativo Dpto. Biblioteca Administrativo Biblioteca  Docente Socio Biblioteca  Estudiante Socio Biblioteca  Tabla 2. Asignación de Rol Técnico a un Rol de Negocio.    Fig. 3. RBAC Flat adaptado.  Usuario Rol de Negocio Rol Técnico  Juan P. Secretario Secretario Académico   Docente Docente    Docente Socio Biblioteca  María V. Director Académico Administrador Sistema Académico  Patricia Z. Administrativo Alumnos Administrativo Sistema Académico  Horacio L. Director de Departamento Carrera Director Dpto. Carreras    Docente Docente    Docente Socio Biblioteca  Nora R. Administrativo Departamento Carrera Administrativo Dpto. Carrera  Lucía M. Director Dpto. Biblioteca Administrador Biblioteca  Alejandra A. Administrativo Dpto. Biblioteca Administrativo Biblioteca  Luis J. Administrativo Acceso a la Univ. Administrativo Ingresantes  Susana R. Docente Docente   Docente Socio Biblioteca  Juan R. Estudiante Estudiante    Estudiante Socio Biblioteca  Tabla 3. Asignación de Rol Técnico a Usuarios conforme su Rol de Negocio.   Académico puede heredar los privilegios de un Administrativo Sistema Académico, y  de esta manera aumentar sus privilegios; un Secretario Académico puede agregar a  sus privilegios los de un Docente; un Administrador Biblioteca puede heredar los  9  privilegios de un Administrativo Biblioteca; un Docente puede agregar los privilegios  de un Socio Biblioteca; etc. De esta manera, se presentan relaciones de herencia y  composición entre roles. Para modelar esta relación de herencia y composición se  aplica un patrón de diseño COMPOSITE [16], resultando el modelo representado en  la Figura 4 a) Herencia de RN, y b) Herencia y Composición de RT, en el que se  muestra hasta un nivel 3 de herencia).      (a) (b)  Fig. 4. RBAC con herencia y composición.  Para una mayor comprensión, se instancian algunas de las clases propuestas  (solamente a nivel de roles). Entonces, por ejemplo: “AdministradorSistemaAcadémico:RolNivel2” es un “AdministrativoSistemaAcadémico:RolTécnico”; “AdministradorBiblioteca:RolNivel2” es un “AdministrativoBiblioteca:RolTécnico”; “Docente:  RolCompuesto” agrega “SocioBiblioteca:RolTécnico”.  De esta manera, con el modelo anterior, es posible representar hasta tres niveles de  herencia de roles, y la estructura compuesta es aplicable a cualquier nivel. Es  necesario incorporar instancias de roles no asignables cuando se requiere representar  roles simplemente compuestos por otros roles (por la relación de herencia entre Rol  Técnico y Rol Compuesto, Figura 4 b). El modelo definido constituye una primera  fase del proceso de implementación de  RBAC descripto en el punto 5.1, el cual  puede ser extendido en fases posteriores, refinando las relaciones encontradas y/o  incorporando nuevas relaciones.  6 Conclusiones  El ‘Caso de Negocio’ se seleccionó con el propósito de atender a requerimientos  relativos al mejoramiento del proceso de diseño de los componentes de software de  los sistemas de información que prestan servicios a las diferentes unidades  funcionales de la Institución. Estos sistemas, cuya criticidad, número y universo de  usuarios ha venido evolucionando con un constante aumento, plantean nuevos  desafíos en cuanto a la atención de los requerimientos funcionales relacionados con el  control de acceso a los recursos que proveen los diferentes componentes de software  y, en particular, con la autorización de los privilegios de usuarios. El objetivo último  es lograr que dichos sistemas empleen componentes de software que posean mejores  propiedades de seguridad.  El relevamiento preliminar realizado ha permitido observar que los diferentes  sistemas de información atienden a estos requerimientos aplicando criterios ad-hoc,  10  fuertemente determinados por la plataforma tecnológica que los soporta. Así, uno de  los requerimientos planteados es la necesidad de contar con guías de diseño para la  implementación de funciones relacionadas con la autorización, las que se aplicarán en  el desarrollo de nuevos sistemas e integración de nuevos componentes. Además, en el  caso que el nivel de impacto sea aceptable, se utilizarán para introducir mejoras en los  sistemas existentes que lo permitan.  Este trabajo ha sido precedido de otras actividades conducentes a la atención de la  seguridad de la información de manera integral, tal como la creación de un Comité de  Seguridad a nivel institucional, así como la definición y difusión de Políticas de  Seguridad. Actualmente se está completando el relevamiento de OP y refinando el  modelo que se propone en este trabajo. También se están diseñando instrumentos que  permiten gestionar (procesos de Alta/Baja/Modificaciones) de autorizaciones de acceso.  Este prototipo que aplica  RBAC ha permitido difundir los Patrones de Seguridad  como una herramienta disponible y que se puede emplear como uno de los primeros  pasos para comenzar a “atender a la Seguridad” en organizaciones que emplean  sistemas centrados en software.	﻿Software seguro , RBAC , Autorización	es	22761
35	Desarrollo de Webs interactivas con filosofía AJAX El TRIVIAL GZ	﻿ We present in this paper the architecture and some implementation details of a web-based version of a Trivial game. Our implementation achieves such a high degree of interactivity between the players that they perceive the game as being played real-time. More importantly, no plug-in or applet is used in the architecture of the system. These properties are achieved by means of a carefully designed architecture that uses AJAX (Asynchronous JavaScript and XML) for data exchange. Using this approach, it is possible to develop any type of web-based collaborative software with few load on the web server. In the paper, we analyze traditional architectures for web-based applications and we show how our approach overcomes their limitations. Furthermore, we proof the efficiency of our approach by means of an empirical comparison. Keywords: Collaborative software, Web, AJAX Resumen En este artículo se presenta la arquitectura y algunos detalles de la implementación de una aplicación Web (un juego virtual de tipo Trivial) que, sin el uso de ningún plug-in o applet, permite una gran interactividad entre los usuarios, hasta el punto de que estos tienen la percepción de comunicación entre ellos en tiempo real. La percepción de interactividad en tiempo real se consigue mediante una arquitectura especialmente diseñada que se apoya, además, en la filosofía de AJAX (Asynchronous JavaScript and XML) para el intercambio de datos. Utilizando esta aproximación es posible desarrollar cualquier tipo de software colaborativo en Web con muy poca carga de trabajo para el servidor. En el articulo se analizan las arquitecturas tradicionales para el desarrollo de aplicaciones Web y se muestra cómo nuestro enfoque supera sus limitaciones. Además, se avala la eficacia de esta aproximación con los resultados de la valoración empírica realizada. Palabras claves: Software colaborativo, Web, AJAX 1671 1. INTRODUCCIÓN El nivel de madurez de los usuarios en Internet y la calidad de las conexiones y los servicios disponibles, están produciendo una demanda creciente de interactividad en las aplicaciones Web, no sólo entre el usuario y el sistema, sino también entre los propios usuarios. Sin embargo, las características tradicionales de las aplicaciones Web dificultan el desarrollo de aplicaciones colaborativas o de juegos que requieran interacción entre los usuarios en tiempo real debido a dos factores fundamentales: No permiten que los diferentes clientes intercambien información entre sí. Es decir, en toda aplicación Web la comunicación se establece entre un cliente y el servidor y nunca entre dos clientes, por tanto, el intercambio de datos entre clientes debe hacerse a través del servidor. Un servidor Web sólo responde a peticiones de clientes y no puede nunca tomar la iniciativa de enviar información nueva a los clientes conectados. Esto significa que el servidor no puede comunicar a los demás usuarios la información que le llega de uno de ellos mientras que estos no la soliciten, dificultando así las posibilidades de interacción entre usuarios. Como consecuencia de estas dos características propias de las aplicaciones Web, cuando se desea crear una aplicación en la que los usuarios colaboren e interactúen entre sí en tiempo real en el desarrollo de una tarea o juego, es preciso que cada uno de ellos descargue e instale en su propio ordenador un software de tipo plug-in para el navegador web que permita mantener la conexión y gestione el intercambio de mensajes entre ellos. Una alternativa que se podría considerar, en casos en los que el intercambio de datos entre los usuarios no sea muy denso, es que cada cliente le envíe al servidor los nuevos datos generados y, en la siguiente petición de página por parte de los demás clientes, el servidor les envíe la nueva página con la información actualizada. En este caso los clientes pueden estar programados para hacer peticiones periódicas (y frecuentes) al servidor mediante scripts incluidos en la página. El problema de esta alternativa es que si el intercambio de datos es muy frecuente o, en el peor de los casos, se desea que se perciba como en tiempo real, el servidor tendrá una carga de trabajo muy elevada, ya que deberá crear y enviar nuevas páginas constantemente, lo que en la práctica se traduce en una limitación del número de usuarios que pueden interactuar. Sin embargo, esta aproximación presenta con respecto a la anterior la ventaja de liberar a los usuarios de tener que descargar, instalar y configurar un plugin, lo que, en algunos dominios de aplicación (sistemas dirigidos a entornos en los que puede haber usuarios poco expertos), es una restricción insalvable. En el Laboratorio de Bases de Datos de la Universidad de A Coruña [3] hemos desarrollado una arquitectura específicamente concebida para simular la interactividad entre usuarios, que, además, utiliza la filosofía AJAX para el intercambio de información entre el servidor y los clientes. Con esta aproximación se produce en los usuarios la percepción de que interactúan entre sí en tiempo real como si estuviesen usando una aplicación que mantuviese una conexión múltiple entre ellos. Esa arquitectura, que puede ser usada para el desarrollo de cualquier aplicación Web colaborativa, la hemos usado para implementar una versión virtual del Trivial, el clásico juego de mesa de preguntas y respuestas. Esta estrategia nos permitió crear un juego que, a diferencia de otras aplicaciones de este tipo, no requiere que los jugadores descarguen e instalen un plug-in, permitiendo al mismo tiempo un número muy grande de partidas simultáneas con múltiples jugadores/as en cada una de ellas. Nuestro trivial virtual, al que llamamos Trivial.gz tiene, con respecto a la versión original del juego de mesa, algunas variaciones orientadas a sacarle partido al entorno virtual, minimizando el problema que 1672 supone que los jugadores no compartan el mismo espacio físico durante las partidas. El Trivial.gz fue promovido por la Asociación Socio-Pedagóxica Galega (AS-PG) [6] para potenciar el uso de la lengua gallega en Internet y fue subvencionado por el gobierno de Galicia. El Trivial.gz fue inaugurado durante las jornadas Xuventude Galiza Net [2], celebradas en Santiago de Compostela durante los días 7, 8 y 9 de abril de 2006, y está actualmente disponible en la siguiente URL http://www.as-pg.com/trivial.gz/. Este desarrollo nos ha permitido evaluar y comparar nuestra propuesta con las aproximaciones tradicionales de desarrollo de aplicaciones Web. En este artículo presentamos la arquitectura utilizada, una descripción básica de la filosofía AJAX y los resultados de nuestra experiencia. Como se podrá observar en el apartado de datos empíricos, el software desarrollado permite que un servidor normal (Pc monoprocesador con 1 Gb de RAM) haya sido, en nuestras pruebas, capaz de atender un número muy importante de partidas simultáneas (del orden de 1000) con numerosos jugadores (hasta 6) en cada partida. El resto del artículo se estructura como sigue. En la sección 2 se describe el funcionamiento y las reglas del Trivial.gz con el objetivo de transmitir el nivel de interactividad que es posible implementar con este nuevo paradigma de programación Web. A continuación, en la sección 3, se presentan las diferencias entre la arquitectura tradicional de las aplicaciones web y la arquitectura de nuestra aplicación. En la sección 4 se describe con más detalle AJAX, citando las limitaciones que supera con respecto a otras tecnologías y las ventajas que ello supone para el desarrollo sistemático de aplicaciones web interactivas. La arquitectura de la aplicación se describe con detalle en la sección 5. En la sección 6 se describe el entorno en el que se ha inaugurado el Trivial.gz así construido y se presentan algunas cifras obtenidas de los accesos recibidos. Finalmente, en la sección 6 se presentan nuestras conclusiones y algunas ideas para trabajos futuros. 2. EL TRIVIAL.GZ El Trivial.gz fue promovido por la Asociación Socio-Pedagóxica Galega [6] para fomentar el uso del gallego entre la gente joven y su creación fue financiada por el gobierno de Galicia. Se trataba de crear un juego para la Web basado en el clásico trivial de mesa, que fuese multipartida y multijugador y que se pudiese ejecutar directamente sobre cualquier navegador Web sin ningún tipo de plug-in. Se modificaron ciertos aspectos del juego de mesa para adaptarlo al entorno Web, por ejemplo, el hecho de que sólo quien posee el turno puede realmente jugar en cada momento, produciría aburrimiento en la versión virtual. Fue necesario además simular en el espacio virtual la interacción entre jugadores, las acciones de tirar el dado, mover la ficha, observar las posiciones y movimientos de los demás jugadores y jugadoras, etc. Por otro lado, se adaptó el juego para que también se pudiese jugar en solitario acumulando puntos y comparándose con un ranking general de jugadores/as. Para fidelizar jugadoras/es se incentiva que se registren de modo que sólo se mantiene la acumulación de puntos en la base de datos de quienes están registrados. Además sólo las personas registradas pueden crear partidas y parametrizarlas, abriéndolas a todo el mundo o cerrándolas para que sólo puedan jugar sus amistades por invitación. Se incorporó además la posibilidad de configurar listas de amistades para facilitar la invitación a partidas concretas. En líneas generales, el Trivial.gz consiste en contestar correctamente preguntas, previamente clasificadas en tres niveles de dificultad y en seis temas diferentes: Cultura y espectáculos, Geografía, Historia, Lengua y Literatura, Ciencia y Mundo). Todas las preguntas tienen tres respuestas de las cuales sólo una es correcta. El elemento principal del juego es un tablero en forma de hexágono dividido en casillas de diferentes colores (ver figura 1), en el que cada color va asociado a un tema concreto. 1673 Figura 1: Pantalla durante el juego Inicialmente, todos los jugadores/as parten con sus fichas de la casilla central. Quien tiene el turno tira el dado (pulsando sobre una animación que representa el dado girando) y se mueve por el tablero, en cualquier dirección, saltando tantas casillas como indique el dado (el sistema controla que sólo se pueda mover a casillas válidas para el valor del dado). Cuando se elige una casilla, se presenta a todos los jugadores una pregunta del tema asociado al color de la casilla. Siempre que se acierta una pregunta (se tenga o no el turno) se acumulan los puntos correspondientes. Si no se acierta se pierde el turno que pasa al siguiente jugador/a. Para ganar la partida hay que conseguir reunir los seis pentágonos (los quesitos del juego de sobremesa) y, después de haberlos conseguido, acertar una pregunta sobre el hexágono central del tablero. Así las casillas que se buscan, al moverse durante el juego, son las de los vértices del hexágono ya que cada vez que se acierta la pregunta asociada a un vértice se consigue el pentágono del color correspondiente. Evidentemente, hay un vértice de cada color. Consideramos que los elementos que se le han incorporado al Trivial de mesa en el Trivial.gz son imprescindibles para que cualquier juego de mesa tenga éxito en la Web, donde quienes juegan una misma partida no comparten el mismo espacio físico. Sin embargo, estos cambios aumentan notablemente el nivel de interacción entre usuarios que debe soportar la arquitectura de la aplicación. Así, durante las partidas, se informa a todos los jugadores/as de lo que marca el dado en cada tirada, de la casilla a la que mueve quien tiene el turno y de que pregunta salió. Además, se mantienen actualizadas las posiciones de las fichas de cada jugador/a en el tablero (indicando quien tiene el turno), las puntuaciones y los pentágonos conseguidos por cada uno, de manera que se perciba que se sigue la evolución de la partida en tiempo real. Además, se ha incluido un chat para que quienes juegan puedan hablar entre sí. En resumen, las características del juego virtual Trivial.gz, que en ocasiones no coinciden con el juego de mesa, son: Se ve el tablero con las fichas, el valor del dado y se indica el turno. Evidentemente en el juego de mesa también se ve todo esto que aquí hay que reproducir virtualmente para que haya sensación de compartir el espacio virtual de la partida. Se puede ”hablar”: Durante la partida, los jugadores pueden comunicarse entre sí a través de un chat. De nuevo ese chat trata de simular la interacción verbal de los jugadores/as del juego de mesa. 1674 Los pentágonos también se pierden: Para hacer el juego más dinámico y divertido hemos introducido la modificación de que los pentágonos también se puedan perder si se falla la pregunta de la casilla del vértice al que corresponda. Se facilita caer en los vértices: Si se cae en la casilla en forma de pentágono que hay en medio de cada lateral del hexágono se salta automáticamente al vértice del color correspondiente facilitándose así la obtención o la perdida de pentágonos. Todos juegan: Todos los jugadores/as pueden intentar contestar la pregunta que le salga a quien tiene el turno y aumentarán su puntuación si eligen la respuesta correcta, aunque sólo podrá conseguir/perder pentágonos quien tenga el turno. Además, mientras se está pensando la respuesta a una pregunta, se puede ver como otros jugadores/as aumentan sus puntos por haberla contestado bien. La respuesta correcta no la verá cada quien hasta que agota el tiempo o responde erróneamente. Tiempo límite: Quien tiene el turno debe contestar la pregunta en un tiempo máximo de 30 segundos. Los restantes jugadores tienen solo el tiempo que tarde en contestar quien tiene el turno. Es decir, quien tiene el turno, en cuanto acierta una pregunta, puede volver a tirar el dado y mover su ficha generndo así una nueva pregunta que es enviada de nuevo a todos los jugadores/as de la partida, cortándoles el tiempo disponible para contestar la pregunta anterior, que en todo caso será como máximo de 30 segundos también. Así, quien tiene el turno puede dificultar que los demás jugadores consigan acumular puntos respondiendo rápidamente. Historial de juego: Se anota la actuación de cada jugador/a registrado en cada partida, de manera que la aplicación ofrece datos sobre el número de partidas jugadas, ganadas y perdidas, puntos por temas y clasificaciones globales. Cualquier persona registrada puede comenzar una partida y, al hacerlo, se convierte en el director/a de la misma. Los parámetros que debe establecer para configurar la partida antes de comenzar a jugar son: Número de jugadores: es posible jugar al Trivial.gz en solitario o se puede competir con otras personas. En una partida puede haber hasta un máximo de seis jugadores/as. Quien crea la partida puede restringirla a personas concretas o dejarla abierta a cualquier persona (registrada o invitada) que acceda al juego. Puede además tener una lista de amistades para facilitar el restringir la partida a esas personas concretas Número de temas: es posible jugar con los seis temas o restringir su número a tres que podrá elegir. Dificultad: se puede elegir el nivel de dificultad de una partida: fácil, intermedia o difícil. La elección que se haga implicará que aproximadamente el 60 por ciento de las preguntas serán de ese nivel de dificultad. En la figura 1 puede verse una captura de pantalla durante el desarrollo de una partida. En ella pueden distinguirse una serie de elementos estáticos (por ejemplo, el tablero de juego) y otros que cambian según los datos que se reciben (por ejemplo, la posición de las fichas de los participantes, el valor del dado, las puntuaciones, las preguntas, etc.). En la actualidad el Trivial.gz está instalado en el servidor web de la Asociación Socio-Pedagóxica Galega (http://www.as-pg.com/trivial.gz), y tiene más de 500 usuarios registrados y de 2.500 preguntas. 1675 3. DIFERENCIAS CON LAS APLICACIONES WEB TRADICIONALES Hasta el momento la arquitectura de la mayor parte de las aplicaciones Web sigue una de las dos filosofías habituales: Aplicaciones ejecutadas del lado del servidor: Son las clásicas aplicaciones Web. Todo el procesamiento recae en el servidor. Así, cada petición de un usuario le supone al servidor un tiempo de procesamiento y el envío de una página Web completa al cliente. Debido a que el número de páginas que el servidor tiene que procesar y enviar a los clientes crece cuanto mayor sea la interactividad (usuario-aplicación) permitida por la aplicación y según aumenta el número de clientes simultáneos, este tipo de arquitecturas son poco escalables ya que se hacen insostenibles en aplicaciones con mucho intercambio de datos y/o que requieran atender a muchos usuarios simultáneos. Aplicaciones del lado del cliente: En este tipo de arquitectura, todo el proceso recae sobre el equipo del cliente, minimizando el intercambio de información con el servidor que, de este modo, se libera de procesar tantas peticiones de usuario y de generar tantas nuevas páginas Web para enviar cada respuesta. Este tipo de aplicaciones puede implementarse mediante programas plug-in que es preciso descargar, instalar y configurar, o, en el mejor de los casos mediante Applets incrustados en la página Web que requieren, también, que esté instalada y activada la Java Virtual Machine (JVM). En cualquier caso, esta filosofía hace que las aplicaciones sean incómodas de configurar para los usuarios y requieren cierto grado de destreza informática para la descarga e instalación del plug-in o de la JVM y su configuración, que limitan su uso generalizado. Una alternativa intermedia es el uso de simples scripts en las páginas Web de modo que la aplicación cliente pueda tener cierta capacidad de proceso sin necesidad de instalar y configurar la JVM u otro plug-in. AJAX (Asynchronous JavaScript and XML), es una nueva filosofía para crear aplicaciones Web que se basa precisamente en las posibilidades de JavaScript. Con AJAX todo el proceso que pueda ser realizado en el cliente se programa mediante JavaScript, que, como es sabido, no precisa ningún tipo de configuración especial ni una Java Virtual Machine (JVM) para poder ser ejecutado. La comunicación con el servidor se implementa mediante el intercambio de mensajes cortos formateados con XML [7]. Esos mensajes son interpretados en el servidor que formatea y envía una respuesta, también en XML, en vez de una página Web completa. El mensaje XML recibido por el cliente es interpretado mediante código JavaScript y utilizado para hacer las modificaciones oportunas en la página Web actual. Google [5] ha sido pionero en el uso de AJAX [4]. Así, podemos verlo en Google Suggest, Google Maps o Gmail. El uso de AJAX facilita el trabajo de los servidores de aplicaciones Web que tienen que atender cada vez a más usuarios potenciales por lo que el coste de procesamiento del servidor para la generación de nuevas páginas se convierte en un factor crítico en el desarrollo de dichas aplicaciones. Pero el uso de AJAX por sí mismo no resuelve la creación de aplicaciones Web que permitan la interactividad entre usuarios en tiempo real, ya que, aunque agiliza el intercambio de datos servidorclientes, no cambia la arquitectura de las aplicaciones Web. Es decir, el intercambio de datos sigue realizándose entre servidor y clientes (y no entre clientes) y el servidor sigue sin poder tomar la iniciativa de enviar a los demás clientes los datos que acaba de recibir de uno en concreto. El Trivial.gz está soportado por una arquitectura diseñada para permitir/simular interactividad entre usuarios, que, además, utiliza la filosofía AJAX para el intercambio de información entre el servidor y los clientes. Básicamente, la arquitectura implementada consiste en que cada cliente cursa peticiones al servidor cada 4 segundos de modo que cada 4 segundos recibe del servidor toda la 1676 información relevante de la actividad de los demás jugadores de la misma partida (el servidor puede atender un número indefinido de partidas simultáneas). Además, el servidor actúa de modo diferente según el estado en el que se encuentra la partida y cambia de estados según las acciones que le comunica el jugador que tiene el turno en esa partida. Evidentemente, esta filosofía produce un elevado intercambio de datos entre el servidor y los clientes que sería inmanejable si no fuera por el uso de AJAX. Recuérdese que AJAX permite la utilización de un protocolo ligero (XML) de intercambio de datos entre el servidor y los clientes, que libera al servidor de tener que crear las nuevas páginas Web cada vez que le llegan peticiones desde los clientes. La idea fundamental es no centralizar toda la lógica en el servidor, sino delegar parte del proceso en los clientes, aunque codificado en Javascript de modo que no es necesario descargar ningún software ni configurar el navegador de modo especial ni tener instalada la Java virtual machine. Resumiendo, el cliente sigue realizando peticiones al servidor pero no de páginas completas, sino sólamente de aquellos datos que necesita. Los clientes, una vez reciben esos datos formateados en XML, los decodifican e interpretan y modifican la página actual que esté viendo el usuario. 4. LA FILOSOFÍA AJAX AJAX (Asynchronous JavaScript and XML) es el nombre que recibe una nueva técnica de reciente aparición en el ámbito de desarrollo de aplicaciones web. En realidad, AJAX no es una nueva tecnología sino que es el resultado de la combinación de distintas tecnologías ya existentes. El elemento central es la utilización de forma asíncrona del API XMLHttpRequest presente en los navegadores web de última generación. Esto permite que, utilizando un lenguaje de tipo script, la página web que se está visualizando en el navegador del cliente haga una petición de información a un servicio web sin bloquear la actividad del usuario. Cuando el servicio web devuelve la información, el navegador invoca una función específica del lenguaje script que puede procesar la respuesta y modificar la página en consecuencia. (a) Aplicación web tradicional (b) Aplicación web AJAX Figura 2: Interacción cliente-servidor en distintos modelos de aplicaciones En la Figura 2 se muestra un diagrama de secuencia representado mediante el lenguaje UML que describe este funcionamiento. En ambas figuras el usuario invoca tres acciones sobre el interfaz de 1677 usuario de la aplicación web. En la figura 2(a) se muestra el funcionamiento en el caso de aplicaciones tradicionales. En este caso el usuario tiene que esperar a que una acción termine antes de poder invocar otra. Además, la cantidad de procesamiento requerido en el servidor web y la cantidad de información transferida es bastante elevada. En la figura 2(b) se muestra la interacción en el caso de una aplicación que utiliza la tecnología AJAX. En este caso el usuario aprecia una mayor velocidad de respuesta en la aplicación porque no tiene que esperar a que termine la anterior petición. Además, la cantidad de procesamiento e información que se transfiere entre el servidor web y el cliente es menor. Este modo de funcionamiento es mucho más ágil para el usuario que las tecnologías tradicionales de desarrollo de aplicaciones web en las que el peso de la interacción recaía en el servidor web. Con estas tecnologías, cualquier actualización del contenido de la página requiere una recarga completa de una parte de la misma y que el usuario espere a que esa recarga se complete. Utilizando AJAX ya no es necesario recargar la página sino que se sólo se altera su contenido, y el usuario no tiene que esperar a que este cambio se produzca. Por otra parte, el hecho de reducir la cantidad de información que se intercambia entre el cliente y el servidor permite que las aplicaciones web desarrolladas con AJAX tengan mucha más interactividad que las tradicionales ya que la capacidad de procesamiento del servidor web se puede utilizar para atender un mayor número de peticiones simultáneas. En resumen, AJAX es una técnica que consiste en el uso de estas tecnologías: HTML dinámico para presentar la información al usuario. Cambiar el modo de funcionamiento de las aplicaciones web para que hagan uso de servicios web que contestan a peticiones específicas en lugar de servidores web que componen páginas completas. XML para representar las peticiones a los servicios web y las respuestas de los mismos. Lenguajes de tipo script, como JavaScript, para implementar la lógica de las aplicaciones web. En torno a AJAX surgen un conjunto de herramientas de desarrollo que permiten hacer más cómodo su empleo. Una de estas herramientas es Direct Web Remoting (DWR) [1]. Esta librería de código abierto proporciona dos grandes ventajas sobre el uso directo de AJAX. En primer lugar, permite al código de una página web utilizar clases programadas en Java en el servidor de forma transparente. Esto se consigue mediante la generación dinámica de código JavaScript que encapsula la utilización de AJAX para efectuar desde el cliente la llamada a código del servidor. Por otra parte, proporciona utilidades para facilitar la actualización de los contenidos de las páginas web en el cliente. Estas dos tecnologías son el centro de la arquitectura de nuestra aplicación. Sin embargo, por sí solas no son la solución a los requerimientos planteados, sino que es necesario realizar un importante trabajo de diseño de la arquitectura de la aplicación que describimos en la siguiente sección. 5. ARQUITECTURA DETALLADA DEL SISTEMA En la figura 3 se muestra la arquitectura de la aplicación de modo genérico. Como toda aplicación web, en la arquitectura del sistema se encuentran dos módulos diferenciados: el módulo de la aplicación que se ejecuta en el servidor web utilizando Java Server Pages (JSP), y el módulo que se ejecuta en el navegador web del cliente utilizando JavaScript y HTML. Además, parte de la aplicación no presenta ninguna novedad (registro de usuarios, configuración de listas de amistades, consulta de estadísticas, etc.), por lo que no es objeto de este artículo, en el que sí nos centraremos en explicar cómo se realiza el control de las partidas para simular interactividad. 1678 Figura 3: Arquitectura de la aplicación 5.1. Funcionalidad del cliente y del servidor El servidor controla las partidas a través de una máquina de estados, que funciona de modo independiente para cada una de ellas, y llevando un registro de los datos relevantes de cada partida que se está ejecutando. Durante el desarrollo de una partida, el servidor va pasando cíclicamente por una serie de estados marcados por las acciones del jugador que tiene el turno. Debido al funcionamiento del juego, hay dos tipos de jugadores diferenciados: el que tiene el turno, y los demás. El primero tiene el control de la partida y genera eventos que producen la actualización del estado de la partida (por ejemplo, tirar el dado, elegir la casilla, o contestar la pregunta). Los jugadores que no tienen el turno sólo generan un evento al contestar a la pregunta, el resto del tiempo consultan periódicamente al servicio el estado actual de la partida para actualizar el interfaz de usuario. Estos estados son los siguientes: Estado inicial. Antes de que el jugador en posesión del turno tire el dado. No hay información que enviar a los demás usuarios. Dado tirado. A los jugadores que no tienen el turno se les enviará el valor del dado cuando soliciten información de la partida. Casilla movida. A los jugadores que no tienen el turno se les enviará la casilla del jugador que sí tiene el turno junto con la pregunta actual (generada al llegarle al servidor el movimiento del jugador con el turno). Pregunta contestada. A los jugadores que no tienen el turno se les enviarán los puntos acumulados por cada jugador. También es posible que haya que enviar información relativa al cambio de turno, en caso de que el jugador que lo poseía hubiese fallado la pregunta. Internamente, este módulo mantiene una lista de las partidas que están en funcionamiento en un instante dado. Cada una de estas partidas contiene a su vez una lista con los jugadores que participan en ella y una lista de preguntas en memoria preparadas para ser enviadas (conceptualmente, un taco 1679 de preguntas). Esta lista de preguntas funciona a modo de caché en memoria que permite que los accesos a la base de datos sean menos frecuentes. Así, en lugar de acceder a la base de datos cada vez que se necesita una pregunta, se accede una única vez recuperando una gran cantidad de ellas que se utilizarán durante la partida. Por otra parte, para cada jugador que participa en la partida se mantiene en memoria el número de puntos acumulados, los pentágonos ganados, y la estadística de preguntas intentadas y acertadas por tema. En el lado del cliente consiste en una página HTML con código JavaScript incrustado en ella. Su funcionamiento se basa en un temporizador implementado con código JavaScript ejecutándose en segundo plano que hace que cada 4 segundos se compruebe si se ha producido alguna modificación en el estado de la partida y se informa al servidor de los cambios que se hayan producido en el estado de cada jugador en particular. Para interactuar con la parte servidor del sistema se utiliza DWR, es decir, el código JavaScript del cliente no tiene que utilizar directamente el API XMLHttpRequest, sino que invoca direcamente los métodos y los objetos del servidor de una forma muy similar a como se hace en el caso de servicios web o de CORBA. 5.2. Funcionamiento del sistema Figura 4: Protocolo de comunicación En la figura 4 se puede ver el funcionamiento de un turno de la partida. Los números que hay en la figura se corresponden a una ordenación temporal de la secuencia de eventos y se van a emplear para organizar la explicación: 1. En primer lugar, el temporizador marca el momento de pedir información al servidor. 2. A continuación, se invoca la función del cliente que informa al servidor del estado del mismo, y solicita la información del estado de la partida. 3. Al servidor le llega la petición y se encarga de delegarla en su modelo de negocio. 4. Como ya se ha expuesto, en este contexto en concreto el servidor puede verse como una máquina de estados y como tal, en función del estado en el que se encuentre, obtendrá la 1680 información necesaria del sistema de almacenamiento de la aplicación para generar la respuesta adecuada. 5. La respuesta se envía encapsulada al cliente. Con los datos obtenidos de la capa modelo simplemente se crea y se devuelve un objeto que representa el estado de la partida. 6. En el cliente se recibe la respuesta y se desencapsula la información de estado. 7. Finalmente, se actualiza el contenido de la página con la información de la respuesta y se vuelve a iniciar el turno. 6. VALORACIÓN EXPERIMENTAL El Trivia.gz fue inaugurado en las jornadas Xuventude Galiza Net [2] de 2006. Durante estas jornadas, más de 1.500 usuarios de informática en general se dieron cita en el Palacio de Congresos de Galicia para participar en diversos concursos y juegos informáticos. El Trivial.gz se instaló en un Pentium IV a 3.2 Ghz y con 1 GB de memoria RAM. Durante el evento, el servidor web registró casi un millón de hits(peticiones realizadas al servidor) y más de 600 visitas (conjunto de peticiones consecutivas desde una única IP). La mayor parte de las peticiones estuvieron concentradas en el transcurso de una hora durante la que se celebró un concurso en el que más de 200 usuarios compitieron por lograr más puntos que los demás. Durante esa hora se jugaron más de 800 partidas (776 de ellas completas, de las incompletas no se guarda registro) con una media de 3 jugadores por partida. La tabla 1 presenta algunos datos sobre la cantidad de información que envía el servidor del Trivial.gz y su comparación con el mismo servidor si la aplicación se hubiese construído con una arquitectura web tradicional. El código de la página web de la partida ocupa 35 Kb, sin tener en cuenta las imágenes ni el código Javascript responsable del envío de mensajes encapsulados en DWR. Por otra parte, el objeto DWR que el servidor envía a cada ordenador cliente, suponiendo un único jugador, ocupa 313 bytes, es decir, 0,31 Kb. Como explicamos en el apartado dedicado a la arquitectura del sistema, es necesario actualizar la información de la partida en la pantalla de cada jugador de manera frecuente porque se trata de un juego en tiempo real y porque es necesario simular una interactividad entre los jugadores. Si tomamos 4 segundos como el tiempo óptimo de recarga (tal y como se está haciendo en la aplicación), eso supondría que en una aplicación web tradicional, el servidor tendría que enviar esos 35 Kb que ocupa la página 15 veces durante un minuto. Un total de 525 Kb por minuto y por jugador, cuando en el Trivial.gz suponen únicamente 4,7 Kb por minuto y jugador. Considerando que durante 10 minutos de las jornadas en las que se inauguró el Trivial.gz, se jugaron más de 100 partidas simultáneamente (de las más de 800 que se jugaron durante la hora de competición), con una media de tres jugadores en cada una de ellas, los cálculos dicen que en una aplicación web tradicional se enviarían desde el servidor un total de 1,5 Gigabytes de información frente a los 20 Megabytes del Trivial.gz, un 75 % más de información. Como consecuencia de este tráfico de información en la red, para una aplicación tradicional se necesitaría, en el mejor de los casos (considerando que el tráfico es uniforme), un ancho de banda de 2,56 MBytes/sg. Sin embargo, en el Trivial.gz el servidor sólo necesita un ancho de banda de 34,20 KBytes/sg. 1681 Aplicación tradicional Trivial.gz 1 mensaje, 1 partida, 1 jugador 35 KBytes 3,1 KBytes 1 partida, 1 minuto, 3 jugadores 1.575 KBytes 20,52 KBytes 100 partidas, 10 minutos, 3 jugadores 1.538,09 MBytes 20,04 MBytes Tráfico en la red en estos 10 min. 2,56 MBytes/sg 34,20 KBytes/sg Tabla 1: Datos empíricos. 7. CONCLUSIONES En este artículo se ha presentado la arquitectura de una aplicación web que implementa un juego de tipo trivial en el que los jugadores pueden seguir la partida en tiempo real. Esto se ha conseguido sin el uso de ningún tipo de plug-in o applet, lo que permite que el juego pueda ser utilizado en cualquier equipo sin presentar los problemas típicos asociados a la instalación de estos componentes. Además, en la implementación se usa la filosofía AJAX para el intercambio de datos, lo que unido al protocolo de intercambio de mensajes diseñado para el Trivial.gz, junto con la arquitectura general para aplicaciones web colaborativas que proponemos, hace que el tráfico de información entre el servidor y los clientes sea mínimo. Utilizando esta aproximación es posible desarrollar cualquier tipo de software colaborativo en Web con muy poca carga de trabajo para el servidor. Finalmente, en el artículo se ha realizado una comparación empírica entre la aplicación basada en la arquitectura que presentamos y una aplicación desarrollada utilizando la arquitectura tradicional de aplicaciones web. En esta comparación se aprecian claramente las ventajas de nuestra arquitectura en cuanto a necesidades de ancho de banda y capacidad de procesamiento del servidor web. Como trabajo futuro podemos mencionar la mejora del Trivial.gz para permitir partidas con más jugadores, o desarrollar una versión del juego totalmente configurable por el usuario en cuanto a temas disponibles y preguntas en cada tema. Otra posible línea de trabajo es el estudio de las necesidades de adaptación de la arquitectura para otros tipos de aplicaciones colaborativas, como por ejemplo enseñanza a distancia o trabajo colaborativo.	﻿software colaborativo , Web , AJAX , collaborative software	es	22770_2
36	Hacia la prueba de corrección de clases	﻿ La actividad de desarrollar programas orientados a objetos, los cuales involucran referencias a memoria, pueden introducir errores difíciles de identificar con el uso de un razonamiento operacional. Esto da lugar a la necesidad de contar con un marco teórico para la prueba de corrección de clases. A partir de esta motivación y basandonos en la idea propuesta por B. Meyer en [Meyer 03a], trabajamos en el desarrollo de una semántica formal para probar, matemáticamente, que clases equipadas con contratos satisfacen los mismos. Keywords: Métodos Formales, Orientación a Objetos, Prueba de Corrección, Contratos, Funciones de Abstracción. Abstract The activity of object oriented program development, which may involve references variables, is prone to errors which are hard to find by operational reasoning. Hence the need for a theoretical framework for proving class correctness. Given this motivation and based on ideas proposed by B. Meyer in [Meyer 03a] we develop a formal semantics for mathematically proving that contract equipped classes satisfy their specifications. Keywords: Formal Methods, Object Orientation, Correctness Proofs, Contracts, Abstraction Functions. 1. Introducción Tal como se lo muestra en varios trabajos ([EWD 1036], [EWD 76]), el razonamiento operacional sobre programas es una actividad no sólo mentalmente desgastante, sino también proclive a la introducción de errores. Si además se consideran aquellos programas que involucran variables conteniendo referencias a memoria, la situación es aún peor ya que aparecen casos de aliasing. Para ilustrar sobre 1717 los problemas que acaecen cuando se trabaja con referencias a memoria, damos a continuación un ejemplo. Supongamos que se define una clase A de la siguiente manera: class A feature{NONE} f:A g:A feature{A} get f is do Result := f end Supongamos además que tenemos dentro de A un método cuya implementación se compone de las siguientes sentencias: S; f := g; a := f.get f donde f y g son los atributos que arriba declaramos. Ahora, dada esta otra secuencia de instrucciones ( siendo S, f y g los mismos que en la implementación anterior ): S; a := g.get f Se plantea el interrogante si en ambas implementaciones el valor de a al finalizar la ejecución de las sentencias respectivas será el mismo. En este punto quizá el lector este receloso de responder afirmativamente. Tal vez resulte más interesante preguntar por que la proposición anterior no es verdadera para el caso general. Con las herramientas desarrolladas en este trabajo se puede dar una respuesta (formal) a este interrogante. La necesidad de una semántica para la prueba de corrección de programas se pone de manifiesto además cuando queremos probar que clases equipadas con contratos satisfacen los mismos. Pero esto no es posible si no se cuenta con un marco teórico adecuado. Consideremos por ejemplo el método make de la clase LIST SET la cual implementa el álgebra de los conjuntos usando listas, y make crea un objeto representando el conjunto vacío. Ahora preguntemonos cual es la postcondición de este método. Con los constructores sintácticos de Eiffel (o de cualquier otro lenguaje de programación imperativo) no es posible formalizarla, es necesario contar con un mayor poder expresivo en los contratos. Tal requerimiento es satisfecho en este trabajo, pero todavía esto no es suficiente. Suponiendo que en las aserciones se permite la utilización de funciones matemáticas abstractas y que tenemos un atributo Ds: LIST[G] el cual esta destinado a contener los elementos del conjunto, es probable que se quiera expresar la postcondición de make del siguiente modo: Ds = ∅ . Sin embargo la expresión anterior sufre dos inconvenientes. El mas leve es que es dependiente 1718 de la implementación, un conjunto podría quizá estar representado mediante un atributo con diferente nombre, o peor aún podrá representarse simultáneamente usando un árbol y una pila, por ejemplo, por lo que mas allá del nombre la postcondición anterior no nos sirve. Entonces resulta un tanto contradictorio pensar en dos post condiciones diferentes para dos métodos cuyo comportamiento esperado es exactamente el mismo. Pero mas grave aún, la expresión anterior se encuentra incorrectamente tipada, estamos igualando un objeto del tipo LIST con un conjunto. Es importante tener clases probadas, en especial aquellas que implementan álgebras como enteros, árboles, grafos, etc, porque son utilizadas exhaustivamentes por el programador de aplicación, quien construye su software basándose en estas. Uno puede optar por omitir estas pruebas de corrección, pero trayendo consigo el riesgo de errores graves para la consistencia de los programas. El lector puede intentar analizar el siguiente método (implementado en el lenguaje Eiffel) que invierte el orden de los elementos de una lista e intentar determinar si el mismo cumple su propósito1: reverse is -- Change the list to have the same elements in reverse order localprevious, next: NODE[G] do from next := first until next = Void loop previous := first first := next next := next.right first.put right(previous) end El lector perceptivo, habrá notado que el error es difícil de encontrar. Pero lo que es más grave aún, es que el cliente del método asume (como es de esperar) que el mismo está correctamente implementado, mientras que para el caso considerado el simple hecho de invocar a reverse y luego recorrer la lista dejará a su programa en un loop infinito. Es importante notar que errores como estos pueden encontrarse en varios programas en uso. El esfuerzo que requiere la correccion matemática de componentes de software es justificado por el amplio reuso que los mismos poseen. Y si tales piezas de software han de ser usadas en sistemas críticos, esta justificación se hace necesidad. En el presente trabajo, se introducen las nociones básicas del formalismo con el objetivo de exponer luego la semántica definida. Además, se presenta la noción de modelo asociado a una clase con el propósito de servir como función de abstracción de forma tal de especificar los contratos de una clase, resolviendo los inconvenientes que presentamos anteriormente. El trabajo concluye con un ejemplo 1la clase NODE contiene un campo right que mantiene una referencia al próximo elemento de la lista. El método put right(x) actualiza dicho campo con el valor referenciado por x 1719 práctico, donde se hace uso del formalismo desarrollado para establecer la corrección de un método. 2. El formalismo básico En esta sección se definen algunos de los conceptos utilizados en el presente artículo. Los mismos se introducen en [Meyer 03a] y se encuentran definidos con mas detalles en [Blanco 05], en donde además se corrigen algunos errores del trabajo original. Habiendo introducido estos conceptos, se describirá la semántica formal asociada a un subconjunto del lenguaje de programación Eiffel. 2.1. Nociones preliminares Las abstracciones aquí presentadas sirven de base para la definición de la semántica sobre la cual se realizan las pruebas matemáticas de corrección de clases equipadas con contratos al estilo Eiffel. A continuación se brinda una definición de cada uno, y cual es su significado en este contexto de prueba. Address: Denota el conjunto de todas las posibles direcciones abstractas de memoria. Objects: Denota el conjunto de todas las direcciones que pueden albergar objetos. Cabe destacar que Objects ⊆ Address. Expanded: Denota el conjunto de todos los valores que no son referencias; por ejemplo los enteros, los booleanos, etc. Values: Denota el conjunto de todos los posibles valores de un sistema; es decir Values .= Objects ∪ Expanded. States: Denota el conjunto de todos los posibles estados de un sistema. Un elemento s ∈ States se define como una tupla cuyo primer componente es un conjunto de funciones (Objects 7→ Objects) que modela las variables involucradas en el sistema; y la segunda componente es un conjunto que contiene los objetos presentes en ese estado. Así en este sistema formal una variable representara una función con alguno de los siguientes perfiles: Objects 7→ States 7→ Objects, para aquellas que constituyan una referencia a otros objetos. Objects 7→ States 7→ Expanded, para aquellas las cuales contengan un valor subclase de EXPANDED, como por ejemplo las del tipo INTEGER. Donde Expanded es el conjunto formado por la unión disjunta de los conjuntos matemáticos B, R, etc. Éste enfoque expresa el hecho de que el objeto el cuál una variable f referencia o el valor que ésta contiene (en el caso que f sea de tipo EXPANDED) en una instancia de la ejecución de un programa Eiffel, varia según el estado y el objeto en donde el identificador f se halle. 1720 2.1.1. Interpretando cambios de estados Un programa orientado a objetos consiste de una secuencia de cambios de estados que reflejan la ejecución de determinados constructores. Así la invocación a un procedimiento r(a) del objeto x (x.r(a)), el cual produce un cambio de estado en nuestro sistema, podremos modelarlo como una función de la forma A 7→ States 7→ States donde A denotan los parámetros del procedimiento y la función States 7→ States describe el nuevo estado producido por la ejecución del procedimiento en términos del estado previo. De esta manera los posibles cambios de estados, dado que un estado está definido como un conjunto de objetos y una colección de funciones sobre esos objetos, son 1. Cambiar una de las funciones en el primer componente del estado; al nivel mas básico esto significa cambiar el valor de una de las funciones en uno de sus posibles argumentos. 2. Cambiar el conjunto de objetos presente en la segunda componente del estado. 2 2.2. Semántica de los operadores básicos A continuación presentamos la semántica formal de un subconjunto del lenguaje Eiffel. Esto permite “mapear” programas a funciones matemáticas que los modelan. De esta forma se define una semántica funcional de un lenguaje imperativo, permitiendo de esta manera introducir la noción de función de abstracción en este ultimo paradigma, como veremos mas adelante. 2.2.1. El operador de asignación La operación fundamental en el paradigma imperativo, transversal al orientado a objetos, es la asignación. En nuestro modelo, la asignación, como transformador de estados básico, estará representada por una función denominada “sustitución de funciones” y denotada como “:=”, cuyo perfil es (Objects 7→ States 7→ Objects) 7→ (Objects 7→ States 7→ V alues) 7→ Objects 7→ States 7→ States. Así, se permite una sustitución de la forma f := E De esta manera definimos la semántica del operador de asignación de Eiffel, mediante la función :=, del siguiente modo 3 Regla 1 (asignacion) f := g . = [f := g] El operador de substitución de funciones, intuitivamente, dado un objeto o y un estado s, retorna un estado s′ en donde solo cambia f para un único elemento de su dominio como se muestra en la fig. 1. Para nuestro trabajo utilizaremos las siguientes propiedades básicas de este operador, cuya demostración puede ser encontrada en [Blanco 05] junto con una definición formal de :=. 2Notar que esto no implica que se modifique el conjunto Objects definido anteriormente. 3En adelante para denotar la semántica asociada a un constructor del lenguaje c, utilizaremos la notación c, y se utilizarán además los corchetes para agrupar funciones 1721 f × // f :=g %%J J J J J J J J J J J J J Si f 6= g g // Figura 1: Ilustración de la Asignación Propiedad 1 f o ([f := g] o s) = g o s Propiedad 2 o 6= o′ ∨ f 6= h⇒ h o ([f := g] o′ s) = h o s 2.2.2. Secuenciamiento de instrucciones La semántica para el secuenciamiento de instrucciones hace uso de un nuevo operador de composición de funciones, denominado rightmost composition (se denota ), y se define como4 Definición 1 (rgmc)  :: (A 7→ B 7→ C) 7→ (A 7→ C 7→ D) 7→ A 7→ B 7→ D [f  g] a b . = [[f a] [g a]] b Dado que las instrucciones son abstraídas en funciones, el modelo para el secuenciamiento de las mismas, es la composición de sus modelos. De esta manera se define su semántica como Regla 2 (secuenciamiento) i; j . = [i  j] Es importante notar que tanto el perfil de i como el de j es de la forma Objects 7→ States 7→ States, dado que los mismos son transformadores de estados. 2.2.3. Comandos condicionales Se define la semántica de los comandos condicionales de manera tradicional utilizando expresiones condicionales del lenguaje funcional Regla 3 (if) if b then i else j end .= if b then i else j La expresión condicional if se encuentra además definida de la siguiente manera Definición 2 (if) [if b then i else j end] o s .= if b o s then i o s else j o s dado que el perfil de b es de la forma Objects 7→ States 7→ Bool y de i, j de la forma Objects 7→ States 7→ States 4El operador  denota la composición de funciones usual 1722 2.2.4. Comandos iterativos La semántica de los comandos iterativos, se define recursivamente de la siguiente manera Regla 4 (loop) . from i until b loop j end .= i  [until b loop j end] Regla 5 (until) . until b loop j end .= until b loop j Definición 3 (until) . Definimos a until como [until b loop j] o s .= if b o s then s else [j  [until b loop j]] o s 2.2.5. Llamada a rutinas La semántica de la llamada a rutinas utiliza una variante de la composición de funciones usual (denotada ) y definida como Definición 4 ()  :: (A 7→ B 7→ C) 7→ (C 7→ B 7→ D) 7→ (A 7→ B 7→ D) [f  g] a b . = g ( f a b ) b Cuando se invoca una rutina f.r(a1, ...,an), donde r tiene parámetros formales x1,..., xn, antes que r comience a ser ejecutada, ocurre que estos últimos son ligados con los parámetros efectivos( o actuales ), de modo tal que referencien a los mismos objetos. En el nivel mas básico del sistema formal, el efecto anterior puede ser modelado en términos de nuestro operador de sustitución de funciones a través de la siguiente regla Regla 6 Llamadas a rutinas Sea r con parámetros formales x1,..., xn, luego f.r(h1, ..., hn) = [f  x1 := h1]  ... [f  xn := hn]  f  r(x1, ..., xn) Luego puede demostrarse la siguiente propiedad Propiedad 3 f  set g(h)  f  g ∗ = h 3. El modelo asociado a una clase Si queremos especificar que una clase satisface su contrato, el primer paso es la elección de un modelo asociado la misma. Dado que estas clases (en particular las mas básicas) están destinadas a modelar álgebras abstractas, sus métodos deberán implementar las funciones de esta ultima, y sus instancias (objetos) se corresponderán con elementos pertenecientes al tipo de interés del álgebra abstracta. 1723 A m // L M  B L M  LAM µ // LBM Figura 2: Corrección del método concreto en base a su especificación abstracta Ahora, al momento de la verificación de un método m que implementa una función abstracta µ, deberemos probar que la aplicación de m sobre un objeto o, tiene el mismo efecto que la aplicación de µ sobre el elemento del tipo de interés que o representa. Es decir, debemos ver que el diagrama de la figura 2 conmuta Pero entonces se plantea el interrogante de como es posible obtener este último a partir del objeto o. Surge entonces el concepto de modelo. El modelo asociado a una clase lo representamos a través del atributo model, el cual es una función que tiene como rango el tipo de interés del álgebra abstracta que la clase particular representa, y cuyo propósito es definir la función de abstracción para cada una de sus instancias, es decir, aquella función que nos posibilitará obtener el elemento que estos representan. Así, model aparece como un atributo más de la clase, con el propósito de especificar los contratos de la misma y probar su corrección. Su definición estará dada, de acuerdo al formalismo que hemos presentado, en un lenguaje funcional. Notar que la semántica de model nos arrojará una función que tiene como rango un tipo abstracto, que no forma parte del sistema de tipos de Eiffel. Sin embargo, a nivel de implementación no habrá problemas puesto que model será usado solo para las aserciones (no apareciendo explícitamente en el cuerpo del programa), y por contrapartida este enfoque nos permitirá escribir post condiciones concisas y expresivas. De este modo, se tienen ahora herramientas para definir las funciones de abstracción sobre objetos. Para ilustrar sobre la forma en la cuál se puede especificar el modelo asociado a una clase damos a continuación dos ejemplos. Primero definimos a model para la clase NODE[G], la cuál va a representar una secuencia cuyos elementos serán las abstracciones de los objetos contenidos en el atributo item de cada uno de los objetos del tipo NODE[G] que se hallen conectados entre si vía su otro atributo next. Declaramos a NODE de la siguiente manera: class NODE[G] ... feature{NONE} item: G next: NODE[G] ... feature{SPECIFICATION} model :: Objects 7→ States 7→ [LGM] 1724 model = abst id abst:: Objects→ Objects 7→ States 7→ [LGM] abst f o s = if f o s = ⊥ then [ ] else [Lf  itemM ∗ : Lf  nextM] o s ... Aquí, id denota la función identidad, ∗: es el constructor de listas5 que dado un elemento x y una lista xs retorna una lista cuyo primer elemento es x y el resto de la lista es xs. Mientras que, para referirnos al rango de la función model definida para una clase A, usaremos la notación LAM. Así por ejemplo LINTEGERM = Z, al estilo de la semántica denotacional clásica. A su vez, el operador L M se sobrecarga para denotar la función de abstracción sobre objetos, la cual se define formalmente como: Definición 5 L M :: (Objects→ States→ Objects)→ Objects→ States→ A LfM = f  model En la definición anterior cabe la posibilidad que la lista sea infinita, en el caso de que un ciclo exista entre los objetos de la clase NODE. La prohibición de esta condición podría constituir el invariante de clase de LINKED LIST, la cuál puede implementarse utilizando objetos de tipo NODE. 4. Un caso de estudio Mostramos en esta sección un ejemplo de la aplicación de todas las herramientas teóricas expuestas en el presente trabajo, donde se pondrán de manifiesto todas las carencias existentes a la hora de establecer la corrección de clases en ausencia del formalismo aquí presentado, las cuáles son enumeradas en la conclusión. En adición veremos de que manera las post condiciones de los métodos son de gran ayuda a la hora de modularizar las pruebas. Dado el siguiente método de la clase LINKED LIST implementado en Eiffel y anotado con aserciones, probaremos que el cuerpo del ciclo mantiene el invariante6. in( x: G ): BOOLEAN is do from g := f; b := false invariant I1 ∗ ∧ I2 variant ∗ # LgM until ( b or g = Void ) loop b := ( x.is equal( g.get item ) ); g := g.get next 5Hay en realidad una sutil diferencia sobre la cual no conviene ahondar en el presente trabajo. Sin embargo esta omisión no presentara mayores obstáculos 6Para evitar extendernos demasiado en el trabajo, no se expone la prueba de corrección total 1725 end Result := b ensure Result ∗ ≡ LxM ∗ in LfM end donde I1 : b ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM)) I2 : LfM = (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM)) ∗ −‖ LgM Antes de comenzar con la prueba hacemos las siguientes aclaraciones con respecto a la notación y funciones usadas: Las funciones lógico-matemáticas que tienen un asterisco arriba pueden considerarse como las usuales. Si bien no son las mismas funciones, tienen idéntica semántica, difiriendo solo en los tipos de sus argumentos iniciales7. x ∗ in xs y retorna verdadero si y solo si el elemento se halla contenido en xs. La función ∗ # retorna el numero de elementos contenidos en una lista. xs ∗ ↑ n retorna una secuencia con los primeros n elementos de xs; y xs ∗. n retorna el n-ésimo elemento de la secuencia xs 8. ∗ −‖ , es el operador de concatenación de secuencias. Así, debemos demostrar, siendo Body el cuerpo del ciclo (I1 ∗ ∧ I2) ∗ ∧ ∗ ¬ B ∗ ⇒ Body  I1 (1) El uso de funciones de abstracción, nos permite hablar de propiedades asociadas a las implementaciones. Así, de acuerdo a las post condiciones de los métodos usados en la implementación de la función in, es posible inferir (prueba omitida) las siguientes propiedades9 que nos servirán en la prueba de corrección 1. g.get next ∗= LgM ∗ ↓ ∗ 1 2. g.get item ∗= ∗ hd LgM 3. x.is equal(y) ∗≡ LxM ∗= LgM 7A los efectos de brevedad no definimos el operador ∗. Sin embargo esto no dificultara la comprensión de los temas aquí tratados 8Es claro que estas funciones son parciales, no están definidas en caso que la secuencia posea menos de n elementos 9Estas, nos permiten comprender la función que realizan estos métodos de una manera simple, sin necesidad de revisar el código para esto 1726 Demostremos la propiedad 1 enunciada anteriormente Sea [b := ( x.is equal( g.get item ) )][g := g.get next] I1 ≡ { hipotesis } [b := LxM ∗= L ∗ hd LgMM][g := LgM ∗ ↓ ∗ 1] I1 ≡ { def. I1; prop. LLxMM ∗= LxM } [b := LxM ∗= ∗ hd LgM][g := LgM ∗ ↓ ∗ 1]  b ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM)) ≡ { prop. Q(x← E) ∗= [x := E]Q } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LLgM ∗ ↓ ∗ 1M)) ≡ { prop. LLxMM ∗= LxM } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM ∗ ↓ ∗ 1)) ≡ { prop. #Xs ↓ 1 = #Xs− 1; algebra } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM ∗ ↓ ∗ 1 +1)) ≡ { prop. Xs ↑ (n + 1) = (Xs ↑ n) −‖ (Xs.n); hipotesis (g ∗ 6= V oid) } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM) ∗ −‖ LfM ∗. ( ∗ # LfM ∗ − ∗ # LgM)) ≡ { prop. in } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in (LfM ∗ ↑ ( ∗ # LfM ∗ − ∗ # LgM)) ∗ ∨ LxM ∗ in (LfM ∗. ( ∗ # LfM ∗ − ∗ # LgM)) ≡ { hipotesis; logica } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in ([LfM ∗. ( ∗ # LfM ∗ − ∗ # LgM)] ≡ { prop. As −‖ Bs.(#As) = hd Bs } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗ in ([LfM ∗. ( ∗ # LfM ∗ − ∗ # LgM)] ≡ { def. in } LxM ∗= ∗ hd LgM ∗ ≡ LxM ∗= ∗ hd LgM QED Así demostramos la implicación planteada en 1. 5. Conclusión La semántica aquí introducida ha permitido detectar casos de aliasing como el del ejemplo expuesto en la introducción, satisfaciendo así un agudo requerimiento que aparece al razonar con programas que contienen referencias a memoria. Si bien aún se están sentando las bases de este marco de trabajo matemático, los resultados que se consiguieron hasta aquí nos hace mirar el futuro con optimismo. Una de las carencias a la hora de especificar e implementar tipos abstractos de datos en lenguajes imperativos es que no se dispone de una semántica apropiada para la introducción de funciones de abstracción. Por ello uno de los logros de esta semántica es permitir trabajar con este tipo de funciones dentro del paradigma imperativo. 1727 Como se puede apreciar en el caso de estudio, si no contáramos con las herramientas teóricas que presentamos, no se podría demostrar, mediante formalismos como la lógica de Hoare, que el método implementa correctamente la operación del álgebra de las secuencias. Principalmente porque no tendríamos forma de ir de clases concretas a álgebras abstractas. La función de abstracción asociada a cada clase permite expresar los contratos de una manera más simple y completa, evitando así subespeficaciones o sobre especificaciones. Por lo que a diferencia de lo que se establece en [Meyer 03b], los contratos de las clases de librerías como Eiffel Base no deberían ser extendidos, sino que deberían ser reformulados. Es importante señalar la utilización de las post condiciones de los métodos utilizados en la prueba de corrección del ejemplo que aquí presentamos, lo que permite modularizar las demostraciones, simplificando las mismas. 5.1. Trabajos futuros El primer paso hacia la prueba de corrección de clases es extender la la semántica hasta ahora definida al resto de los constructores del lenguaje Eiffel. Luego, es menester incorporar la semántica dentro de algún demostrador de teoremas de manera de generar automáticamente las obligaciones de prueba. Por otro lado es nuestra meta extender el proceso realizado en el presente trabajo a una amplia variedad de estructuras básicas y algoritmos fundamentales. Esto involucra construir y desarrollar las teorías asociadas a estas clases, extender los contratos de las mismas, realizar las pruebas y refinar las mismas en el proceso. Todos estos trabajos futuros conforman una linea importante de investigación, que se inserta dentro del creciente interés de la comunidad de orientación a objetos en los métodos formales.	﻿Metodos Formales , Orientación a Objetos , Prueba de Corrección , Contratos , Funciones de Abstraccion	es	22786_2
37	Cómputo paralelo interclusters Herramientas y evaluación de rendimiento	﻿  Se presenta en este artículo la experiencia desarrollada referente a la utilización de múltiples clusters para  cómputo paralelo. Inicialmente, se presentan algunas consideraciones importantes en cuanto la instalación y  configuración del middleware o software de soporte necesario para realizar cómputo paralelo en este tipo de  arquitectura que es relativamente nueva. Una de las premisas que se tiene en cuenta en este contexto es la de  utilizar software herramientas relativamente estándar y estable. Una vez que se establece la configuración  básica para ejecutar programas paralelos interclusters (en más de un cluster), se presentan los resultados  obtenidos en lo referente a la caracterización de las transferencias de datos entre los clusters intervinientes.  El rendimiento de las comunicaciones interclusters puede ser considerado como una de las características  propias de este tipo de plataformas de cómputo paraelo y, por lo tanto, es importante tener una metodología  y/o herramienta de caracterización de las mismas. Finalmente, se presentan algunos resultados interesantes  de paralelización de dos aplicaciones sencillas a ejecutarse en computadoras de diferentes clusters. Esta  paralelización muestra, por un lado, ejemplos específicos reales de ejecución programas paralelos  intercluster y por otro lado, la aplicación de algunas ideas preexistentes de balance de carga que son  aplicables también en este contexto.   Palabras claves: Comunicación de Procesos, Caracterización de Rendimiento, Sistemas Paralelos y  Distribuidos, Paralelismo en Clusters e Interclusters.    Abstract This paper presents the experience developed using multiple clusters for parallel computing. Initially, some  relevant considerations are presented in terms installation and configuration of middleware or supporting  software necessary to carry out parallel computing over this relatively new type of architecture. One of the  premises taken into account within this context is that of using relatively standard and stable software and  tools. Once the basic configuration is established to run interclusters (on more than one cluster) parallel  programs, the obtained results regarding the characterization of data transferences among clusters are  presented. Interclusters communication performance can be considered as one of the typical characteristics  of this type of parallel computing platforms and, thus, it is important to count with a characterization  methodology and/or tool of them. Finally, some interesting results of parallelizing two simple applications  run in machines of different clusters are presented. On the one hand, this parallelization shows actual and  specific results of running interclusters parallel programs and, on the other, the application of some preexisting ideas of load balance, which are also applicable within this context.  Keywords: Process Communication, Performance Characterization, Parallel and Distributed Systems,  Parallelism in Clusters and Interclusters.   † Investigador Principal CONICET. Profesor Titular D.E. Facultad de Informática UNLP  ‡ Investigador Asistente CICPBA  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1169 1 INTRODUCCION La utilización de clusters para cómputo paralelo está bien establecida desde hace varios años [3] y  actualmente existe una gran cantidad de clusters instalados que están siendo utilizados con software  paralelo en producción. El modelo de programación de pasaje de mensajes también ha madurado en  definiciones tales como las de PVM (Parallel Virtual Machine) [5] y MPI (Message Passing  Interface) [14]. La biblioteca MPI se convirtió rápidamente en uno de los  estándares más  importantes para el desarrollo y la ejecución de programas paralelos. Esta biblioteca rápidamente se  ha implementado en los sistemas de cómputo paralelo más distribuidos o desacoplados como lo son  los clusters. De hecho, desde hace varios años existen al menos dos implementaciones de uso libre:  MPICH [11] y LAM/MPI [4]. Aunque se puede asociar pasaje de mensajes y PVM y MPI en particular con hardware de cómputo  distribuido o MIMD (Multiple Instruction Stream, Multiple Data Stream), PVM y MPI no imponen o presuponen ninguna característica sobre/de el hardware. Esto permite una gran independencia y  portabilidad de los programas paralelos, incluyendo variantes como las de clusters de SMP  (Symmetric MultiProcessing) y computadoras paralelas de memoria compartida [15]. En general,  las bibliotecas de pasaje de mensajes resuelven básicamente dos problemas técnicos de manera  satisfactoria para ser utilizadas:  1. Identificación única de procesos. Los programas paralelos no son mucho más que un conjunto  de procesos que se pueden identificar de manera unívoca, independientemente de que se  ejecuten en un ambiente de memoria compartida o memoria distribuida, por ejemplo.   2. Transferencia de datos entre los procesos. Si bien se reconoce que las primitivas básica de  comunicación son del tipo send() y receive() punto a punto entre dos procesos, también se  suelen incluir otras variantes como las comunicaciones colectivas del tipo de broadcast(), por  ejemplo, o variantes semánticas de las comunicaciones punto a punto (haciendo alusión  explícita a buffers de memoria, por ejemplo).  De hecho, en cualquier plataforma de cómputo donde se pueden resolver estos problemas, se podría  utilizar una implementación de MPI, por ejemplo. Evidentemente los clusters han sido apropiados  desde la perspectiva de las implementaciones de bibliotecas de pasaje de mensajes para cómputo  paralelo, y estos dos problemas se resuelven de manera relativamente sencilla. Todo lo relacionado  con transferencias de comunicaciones se ha resuelto utilizando protocolos estándares como TCP/IP  y no es necesario un gran esfuerzo para asignar y mantener una identificación de procesos para  bibliotecas como la más utilizada actualmente de MPI 1.1, donde no hay creación dinámica de  procesos. De hecho, PVM también incluye la creación dinámica de procesos y tampoco en este caso  es un gran problema mantener actualizada la asociación proceso-identificador en un ambiente  distribuido. Evidentemente no es un problema en un ambiente centralizado (como la de un SMP),  donde hay un único sistema operativo y, de hecho, cualquier sistema operativo tiene identificación  única de procesos además de muchos otros datos de información de estado del sistema.  La propuesta en este artículo es la de plantear la solución al problema planteado por la ejecución de  un programa paralelo en más de un cluster o, lo que es igual: la utilización de más de un cluster para  ejecutar un programa paralelo, que se considerará sinónimo de “cómputo paralelo interclusters”. De  una manera o de otra, hay varias alternativas de solución a este problema, algunas de ellas  disponibles desde hace bastante tiempo. De hecho, se podría resolver inicialmente con las mismas  bibliotecas PVM o implementaciones de MPI si no fuera por los controles de seguridad que se  imponen actualmente al tráfico sobre Internet de la mayoría (si no todas) de las instituciones que  tienen clusters disponibles para cómputo paralelo. De hecho, la propuesta avanza un poco más en el  sentido de no solamente utilizar más de un cluster para un programa paralelo sino de hacerlo con el  mínimo costo de instalación y mantenimiento de software y, además, con lo que se supone la menor  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1170 sobrecarga (overhead) de cómputo y comunicaciones  posible. Justamente desde esta perspectiva se  analizarán las propuestas existentes (al menos las más importantes o consideradas suficientemente  representativas) en la siguiente sección. Una vez resuelto el problema básico de cómputo paralelo interclusters (con la utilización de más de  un cluster) comienza, en realidad, un problema mayor desde la perspectiva de paralelización de  aplicaciones y la optimización de rendimiento. En un ambiente de cómputo paralelo interclusters no  se puede asumir que todas las comunicaciones punto a punto tienen el mismo rendimiento, por  ejemplo. Desde la perspectiva de un proceso que envía, algunos receptores son locales (en la misma  red local, el mismo cluster) y otros no, están en otro cluster, al que se llega, en el caso más usual, por ruteo IP (normalmente). En cualquier caso, el rendimiento de las comunicaciones (el tiempo  necesario para efectuar una transferencia de datos) no será el mismo. Otra de las características  inherentes del cómputo paralelo interclusters es la de heterogeneidad de las computadoras a utilizar.  Si bien en un cluster normalmente todas las computadoras son iguales (y, en particular, con la  misma potencia de cálculo), es muy poco probable que las computadoras de dos o más clusters sean  exactamente iguales entre sí. Y esto afecta directamente el balance de carga para que el cómputo  sea equilibrado en cuanto al tiempo necesario en cada una de las computadoras que se utilizan. Los  problemas planteados en cuanto a rendimiento de comunicaciones y balance de carga  computacional (y tiempo de cómputo asociado) serán analizados en otra sección de este mismo  artículo. 2 TRABAJOS PREVIOS RELACIONADOS Aunque la terminología varía bastante, dado que no ha sido estandarizada, entre los primeros  esfuerzos de utilizar computadoras en más de un cluster se puede encontrar la idea de “clusters  geográficamente distribuidos”. Entre los primeros esfuerzos se pueden encontrar herramientas o  bibliotecas tales como MagPIe del proyecto Albatros [24]. La biblioteca MagPIe fue orientada  directamente a la optimización de las funciones de comunicaciones colectivas de MPI en redes de  área extensa o extendida (WAN: Wide Area Network) [13] [12]. En algunos otros casos, se pueden  encontrar los esfuerzos por hacer diferentes versiones de MPI interoperables, asumiendo que se  tienen varios clusters, cada uno con su implementación propia (o propietaria o específicamente  orientada a un tipo de máquina paralela o cluster). Entre estas iniciativas se pueden mencionar MPIConnect [7] con su precedente PVMPI [6] y también IMPI [10], que se propuso con mayor  generalidad para la interoperabilidad, con la definición de un protocolo apropiado.  La gran mayoría de las propuestas son de finales de la década de 1990, mayormente entre los años  1995 y 2000. Quizás por esta razón, de alguna manera o de otra estas propuestas han sido  absorbidas o incluidas es lo que hoy se conoce como en Grid Computing [9]. Por ejemplo, el  proyecto Albatross se asocia directamente con DAS [24] y el sitio web de DAS indica que ha sido  sucedido por DAS-2 y DAS-3 [DAS-3], donde este último (DAS-3) se define como la  infraestructura de Grid Computing en Holanda. Sin embargo, parece conveniente en ciertas  aplicaciones mantener la propuesta de cómputo paralelo interclusters separado de Grid Computing  por al menos dos razones:  x Grid Computing está propuesto como una solución integral o completa para compartir recursos  de cómputo y almacenamiento a gran escala de distribución y de capacidad. Obviamente esto  incluye la utilización de varios clusters, pero desde la perspectiva de compartir de manera  controlada múltiples recursos disponibles en múltiples instituciones y/o instalaciones de  cómputo. Esto incluye también la idea de mantener un gran sistema distribuido más que un gran  sistema paralelo (aunque el sistema distribuido pueda utilizarse para cómputo paralelo).  x Grid Computing tiene mucho más que lo necesario para cómputo paralelo interclusters. Por  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1171 ejemplo, se intenta proveer SSI (Single System Image) no solamente a nivel de Single Sign On (o único punto de identificación y conexión) sino a nivel de proveer y obtener recursos para o  de todo el sistema de Grid Computing. Esto implica, por ejemplo, un sistema muy elaborado y  complejo para proveer adaptación a los diferentes sistemas de seguridad que se utilicen  localmente en cada instalación/institución conectada a grid.  La propuesta de este artículo consiste en mantener la posibilidad de utilizar computadoras de más  de un cluster sin tener que instalar toda una infraestructura (o middleware) como la de Globus [8].  Un ejemplo sencillo puede ser la colaboración puntual de dos o más instituciones para resolver un  problema específico, donde esto no implique la definición e instalación de toda una infraestructura  de software específica para el desarrollo de la solución, además de lo que realmente es necesario:  desarrollar la solución. Cada una de las instituciones (o, más específicamente, cada uno de los  clusters a utilizar) no debería ser mayormente afectado en cuanto a infraestructura de software para  la colaboración en cuanto a cómputo paralelo. En cierta forma, puede considerarse que esto  restringe o limita la escalabilidad de aplicaciones, pero sin lugar a dudas también reduce la  complejidad de instalación y mantenimiento de la infraestructura de software necesaria. En el otro  extremo, se podría mencionar la colaboración explícitamente ad hoc como en [1], en el sentido de  desarrollar no solamente la aplicación sino también las comunicaciones y el control de la aplicación  paralela. Aunque sin lugar a dudas esta es la forma con menor requerimiento de infraestructura de  software a priori, también involucra un alto costo de desarrollo extra sobre el programador. Las  comunicaciones, por ejemplo, se deberían llevar a cabo usando métodos que son relativamente  rudimentarios para cómputo paralelo como el desarrollo directo sobre la biblioteca de sockets BSD. En este contexto, el objetivo es mantener un mínimo estable de desarrollo y ejecución de programas  paralelos como el de MPI sin tener que recurrir a sus implementaciones para Grid Computing, por  ejemplo, donde se requiere, además, todo el soporte que corresponde, como el que provee  específicamente Globus. Desde hace algún tiempo, se han estudiado características específicas de cómputo paralelo  interclusters tales como la del problema generado por las interconexiones no dedicadas y los  problemas de seguridad involucrados [2] [18]. A modo de resumen, en estos trabajos previos se  reportan algunos detalles técnicos importantes a tener en cuenta para cómputo paralelo interclusters:  x En los ambientes no dedicados, muchos de los problemas de disponibilidad de las  computadoras de los clusters a utilizar son propios de la falta de control sobre los mismos, no  de las comunicaciones o estabilidad de las computadoras.   x Aunque la interconexión de los clusters no es dedicada, siempre hay  conectividad entre los  clusters a utilizar, salvo algunas excepciones relativamente muy poco frecuentes. Esto se debe  básicamente a que la interconexión está involucrada con el tráfico de Internet, que es mantenido  y monitoreado independientemente de la utilización de cómputo paralelo interclusters.  x El rendimiento que se podría considerar como raw (medido a partir de tráfico ICMP, por  ejemplo) de las comunicaciones entre los clusters no es constante dado que no es dedicado,  pero en general es muy cercano al máximo absoluto, al menos para transferencias de  relativamente pocos datos (decenas de K Bytes, por ejemplo). Como es de esperar, el  rendimiento para las comunicaciones interclusters fluctúa dependiendo de días y horarios.  x Los mecanismos básicos de seguridad que se imponen en las instituciones (y que son de uso  común en casi todas las instalaciones de computadoras) normalmente impiden la utilización  directa de implementaciones de MPI como MPICH y LAM/MPI. Estas implementaciones  imponen un patrón de tráfico TCP/IP que normalmente es cancelado por los firewalls y/o  mecanismos de seguridad de las instituciones.   XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1172 3 UN SOPORTE SIMPLE PARA COMPUTO PARALELO INTERCLUSTERS Quizás el soporte más simple para el cómputo paralelo interclusters es el que proveen las propias  implementaciones de MPI que, a priori, permiten la utilización de múltiples computadoras  independientemente de su ubicación geográfica y/o en clusters. Esta posibilidad debe ser descartada  de plano por los múltiples problemas de seguridad y administración que involucra. Desde la  perspectiva de seguridad, se deberían relajar los niveles de control, al menos desde la perspectiva  de los firewalls que son de uso extendido en todas las instituciones. Como se reporta en [17], tanto  el o los protocolos (TCP y/o UDP) como la cantidad y el tipo o clase (privilegiados o no) de puertos a utilizar por las aplicaciones que utilizan implementaciones de MPI no es configurable. Esto  llevaría a dejar sin protección o sin control el tráfico entre las computadoras involucradas de los  diferentes clusters. Aún en el caso de considerar que eliminar el control de tráfico entre las  máquinas no es suficientemente peligroso, sí es un problema de administración. Al menos un  administrador por institución debe encargarse de quitar estos controles para que las aplicaciones  MPI puedan ser ejecutadas. Este problema se suele complicar debido a que normalmente los  administradores de la seguridad no tienen relación directa con cómputo paralelo y que en las  instituciones con control más distribuido se involucra a más de un administrador. Por lo tanto,  mantener el uso directo de MPI no es sustentable desde el punto de vista técnico por la seguridad ni  operativo por la necesidad de recurrir a múltiples administradores de las múltiples instituciones  involucradas. La siguiente alternativa para ejecutar aplicaciones paralelas interclusters basadas en MPI podría ser  la utilización de algunas de las herramientas enumeradas en la sección anterior: MagPIE, MPIConnect, PVMPI, o IMPI. Algunas de las alternativas podrían considerarse con el costo agregado  de instalación de la o las bibliotecas más la recompilación de los programas paralelos. Sin embargo,  como se comentó en la sección anterior, el problema más significativo es que la mayoría (sino  todas) de estas herramientas simplemente se trasladaron al ambiente de Grid Computing. Esto  significa que, como mínimo no tienen soporte ni actualizaciones para los clusters (o interclusters)  actuales. De hecho, en el contexto de Grid Computing directamente existen implementaciones de  MPI que evitarían el uso de otra biblioteca. Como también se comenta en la sección anterior, el  objetivo es evitar la instalación de la infraestructura de una implementación para Grid Computing  por el propio costo de la instalación y además para evitar el costo en tiempo de ejecución para el  acceso a recursos (CPU, memoria, etc.) vía Grid Computing.  Tal como se adelanta en [18] a nivel preliminar, la idea es recurrir a la utilización de una red  privada virtual, o VPN (Virtual Private Network) como middleware asociado en cierta forma a la  implementación de MPI que se utilice. Desde un punto de vista técnico, todo lo que necesita MPI  para ser utilizado en un cluster es que se tenga conectividad TCP/IP como en una red local. Esto es,  justamente, lo que provee una VPN en cualquiera de sus versiones o implementaciones [25] [23].  Sin lugar a dudas, se tienen costos por la utilización de una implementación de VPN para cómputo  paralelo interclusters:  x Se debe instalar y configurar el software de la implementación de VPN que se haya elegido.  Normalmente no es muy complejo ni requiere muchos conocimientos previos. Casi todas las  distribuciones de Linux tienen o incluyen alguna implementación. Este es el caso de OpenVPN,  que es el que se utilizó para el trabajo de este artículo y cuya tarea de instalación y  configuración se reporta en [19].  x Desde la perspectiva de seguridad o en realidad lo que se puede ver afectado por las controles  de  seguridad existentes vía firewalls, las implementaciones de VPN no impone mayores  complicaciones. Normalmente se tiene un esquema cliente/servidor con un puerto bien  conocido para el servidor, al cual los clientes hacen requerimientos. Esto implica que todo el  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1173 tráfico se “concentra” (de allí en parte el nombre de “túnel” o “entubamiento” o tunnel que  suele aparecer en la bibliografía de VPN) en un puerto bien conocido del servidor que se  instale. Esto de hecho simplifica lo relacionado con la seguridad, dado que la configuración de  los firewalls se orienta, justamente, al control sobre números de IP y puertos a ser utilizados en  tráfico TCP/UDP sobre IP. Al establecer un único IP (el del servidor) y un único puerto (el  puerto bien conocido del servicio de VPN, que de hecho es configurable) las tareas son  mínimas. De hecho, los controles actuales de tráfico peer to peer se pueden evitar con cierta  sencillez dado que el servidor de VPN se puede configurar para que utilice puertos  privilegiados en vez de los no privilegiados que normalmente se utilizan y se filtran en los  firewalls por el tráfico peer to peer. x Evidentemente en tiempo de ejecución existe una sobrecarga de procesamiento y en cierto  modo también de tráfico de datos por el entubamiento de las comunicaciones sobre el esquema  de cliente/servidor sobre un puerto bien conocido. En principio, en este artículo se pondrá  énfasis en la factibilidad, por lo tanto no se estudiará específicamente el problema de la  sobrecarga. Sin embargo, se puede estimar a priori que la utilización de una VPN implica  menor sobrecarga que una infraestructura como la de Grid Computing.  La Fig. 1 muestra esquemáticamente el ambiente de ejecución de programas con MPI a partir de la  utilización de una VPN. Desde la perspectiva del programador y sin tener en cuenta lo relacionado  con el rendimiento de las comunicaciones, no hay ningún cambio, ni siquiera es necesario  recompilar, dado que los binarios generados son independientes de que se utilice VPN o se ejecute  en un cluster de computadoras en una LAN. En la Fig. 1 no se muestran las conexiones reales (las  que resuelve la implementación de VPN instalada) entre las máquinas sino que con líneas de punto  se muestran las conexiones vía VPN, como se utilizan desde los programas con MPI. El tráfico  entre las computadoras de diferentes clusters se resuelve normalmente vía el que se define como  servidor de VPN, dependiendo de la implementación y la configuración utilizadas.  Figura 1: Infraestructura Sencilla para Cómputo Paralelo Interclusters con VPN.  4 EXPERIMENTACION: EVALUACION DE COMUNICACIONES  Tal como se reporta en [18], se utiliza el método sencillo de ping-pong de mensajes para evaluar el  rendimiento de las comunicaciones entre dos procesos (en este caso en dos clusters diferentes),  siguiendo el modelo de tiempo para las transferencias de datos  t(n) = D + E n donde n es la cantidad de datos, D es el tiempo de latencia (mínimo tiempo siempre se necesita para  las comunicaciones) y E es la inversa del ancho de banda, o costo en tiempo por dato a comunicar.  Uno de los primeros problemas encontrados, fue que no es posible utilizar cualquiera de las  implementaciones de MPI sobre la VPN. En particular, LAM/MPI tiene problemas de ejecución,  probablemente por las transferencias de datos entre los procesos lamd que ejecuta en cada una de  las máquinas utilizadas. Para evitar el estudio de las razones de este problema, se recurrió a una  implementación de MPI totalmente estática en el sentido de que no hay procesos de administración Cluster 1 Cluster 2  VPN Aplicaciones con MPI  Red Red XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1174 o intermedios (del tipo de los lamd para LAM/MPI) para la ejecución de aplicaciones, MPICH,  versión 1.2.4. Si bien en cierta forma se contradice la idea expresada en cuanto a que cualquier  implementación de MPI se podría usar sobre la VPN, siempre es posible:  1. Encontrar el problema por el cual no funciona una implementación en particular y resolverlo (o  hacer el requerimiento a quienes producen la implementación).  2. Encontrar una implementación de MPI que funcione sobre la VPN. En este caso, al menos para  se recurre a MPICH que es de las implementaciones de MPI más utilizadas y respetadas de  entre las de uso libre. El entorno de experimentación elegido fue el más sencillo posible en cuanto a cantidades de  computadoras: dos PCs con Linux, cada una en una red local diferente. Las dos redes locales  involucradas son en realidad dos subredes de la 163.10.xx.yy de la UNLP, una en el ámbito de la  Facultad de Informática y la otra en el ámbito de la Facultad de Ingeniería. La red de interconexión  entre las dos redes locales no es exclusiva y, por lo tanto, se comparte o compite por el ancho de  banda disponible entre estas redes con el tráfico usual de otras aplicaciones relacionado con  Internet. La cantidad de computadoras con el que se comparte esta interconexión está en el orden de  las centenas. La cantidad y el tráfico que involucran los experimentos ping-pong fueron  determinados de forma tal que:  x No utilicen más del 5% de 10Mb/s que se asume como el máximo ancho de banda disponible  entre las redes locales, con múltiples routers intermedios, algunos de los cuales utilizando  placas Ethernet de 10 Mb/s. En cualquier caso, este tráfico es muy conservador en el sentido de  evitar al máximo cualquier congestión de tráfico en la red de interconexión compartida.  x Los experimentos se distribuyen de manera uniforme durante todo el tiempo de ejecución, de  forma tal que se monitoricen tiempos de uso normal de las redes locales y de tráfico de Internet  con el que se compite con otras aplicaciones en el tramo de interconexión entre los clusters.  x El 50% de los experimentos se orienta a identificar la latencia de los mensajes entre dos  procesos de una aplicación paralela y el otro 50% se orienta a la identificación del ancho de  banda disponible o posible entre los procesos. La longitud de los mensajes de los experimentos  orientados a latencia se establece en 8 bytes y la longitud de los mensajes orientados a ancho de  banda se establece en 20000 bytes (básicamente para no provocar ráfagas de uso muy intensivo  sobre la red compartida, como se explica en el primer punto).  x Los experimentos se llevaron a cabo durante aproximadamente 10 días corridos, para tener  datos de días y horas de uso normal de las computadoras y las redes intermedias y también de  días y horas con relativamente poco de uso de las computadoras y redes involucradas.   La Fig. 2 muestra el histograma de la distribución de los tiempos de latencia de los experimentos,  donde se puede observar que la gran mayoría de los mensajes tiene una latencia de entre 1 y 5  milisegundos. Es claro que esta latencia es muy elevada para las capacidades de cómputo de las PCs  actuales (que son del orden de Gflop/s), pero en cierto modo es muy importante identificar con  experimentos tan sencillos que alrededor del 95% de las comunicaciones tienen valores de latencia  en este rango. Esta información es particularmente útil para las aplicaciones paralelas, dado que dan  una idea de la granularidad mínima: no tiene sentido comunicarse entre tiempos de cómputo  menores a los de la latencia. La Fig. 3 muestra la distribución del ancho de banda de los  experimentos realizados con 20000 bytes, donde como en el caso de la latencia se puede observar  que la mayoría está concentrado en un rango de valores relativamente pequeño. A modo de resumen  de los valores mostrados en la Fig. 3, la gran mayoría (alrededor de 95%) de los experimentos se  lleva a cabo con entre 50 y 58 KB/s. Es de destacar que durante todo el tiempo de ejecución de los  experimentos no hubo que reinstalar ni reconfigurar OpenVPN, la conectividad entre las  computadoras se mantuvo dentro de la VPN, a diferencia de lo que había sucedido en los  experimentos reportados en [20].  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1175 04000 8000 12000 16000 20000 <1 1-2 2-3 3-4 4-5 5-6 6-8 8-10 >10 Rango de Lantencia (ms) C an tid ad Figura 2: Distribución de Tiempos de Latencia (Startup Time) con MPI en VPN.  0 4000 8000 12000 16000 20000 < 48K/s 48-50K/s 50-52K/s 52-54K/s 54-56K/s 56-58K/s > 58K/s Rango de Ancho de Banda (KB/s) C an tid ad Figura 3: Distribución de Anchos de Banda con MPI en VPN.  5 EXPERIMENTACION: PARALELIZACION DE APLICACIONES  Como ya se ha comentado, el hecho de que sea factible el cómputo paralelo interclusters no  resuelve los problemas asociados a la paralelización de aplicaciones. De hecho se podría afirmar  que por un lado aporta un horizonte nuevo de paralelización y sus problemas asociados. Uno de  ellos es la asimetría o diferencia en las comunicaciones intra e inter clusters que se ha comentado.  A partir de los resultados que se muestran en la sección anterior se tienen datos más precisos al  menos del entorno sobre el que se llevó a cabo la experimentación:  x El rendimiento de las comunicaciones interclusters no es constante, aunque los rangos son  reducidos. x El rendimiento de las comunicaciones interclusters es varias veces menor que el de las  comunicaciones dentro de un mismo cluster. Solamente a modo de ejemplo, en un cluster  interconectado con Ethernet de 100 Mb/s se tienen aproximadamente 10 MB/s entre procesos,  lo cual es muy superior al rango de 50-58 KB/s que se muestra en la Fig. 3.   Las aplicaciones denominadas altamente paralelas o embarazosamente paralelas (embarrassingly  parallel) son las primeras opciones para ser resueltas en cualquier plataforma de cómputo paralelo.  Esto se debe a que en realidad paralelizar estas aplicaciones no requiere casi ningún esfuerzo más  allá de la propia codificación del programa, por ejemplo con MPI. Por otro lado, estas aplicaciones  normalmente requieren muy pocas comunicaciones durante el tiempo de cómputo o, puesto de otra  forma, se pueden paralelizar con granularidad muy gruesa. Esto las hace mejores en cuanto a la  obtención de rendimiento óptimo o cercano al óptimo en ambientes con muy bajo rendimiento de  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1176 comunicaciones, como el que se utiliza para la experimentación en este artículo.    Se han elegido dos aplicaciones muy conocidas pero con diferentes características de paralelización  con la finalidad de evaluar rendimiento: integración numérica (cálculo del número S) y cálculos  asociados con el conjunto de Mandelbrot. En ambos casos, los problemas de cálculo asociados son  muy sencillos de paralelizar dado que se pueden identificar partes de cómputo independiente de  manera natural, sin recurrir a cambios en el tipo de cómputo secuencial básico. Nuevamente el  ambiente de experimentación es el de mínima complejidad en cuanto a cantidad de computadoras:  una computadora en cada uno de los clusters. Dado que es importante la capacidad de cómputo para  estos experimentos, en la Tabla 1 se detallan las características técnicas más importantes de las dos  computadoras utilizadas, donde se puede notar que son muy diferentes en cuanto a capacidad de  procesamiento y almacenamiento.  Tipo CPU Memoria RAM Sistema Operativo  PC Pentium 4 2.4 GHz 1 GB Linux 2.4.18-14  PC Pentium II 400 MHz 128 MB Linux 2.4.18-14  Tabla 1: Características de las Computadoras Utilizadas en los Experimentos.  Un problema común en el contexto de cómputo paralelo interclusters es el de la heterogeneidad en  cuanto a la capacidad de cómputo de las computadoras que se utilizan. Desde la perspectiva de la  paralelización, esto genera un problema de balance de carga implícito en la suposición de que a  todos los procesadores se les asignará la misma cantidad de operaciones a realizar. Se debe notar  que esto no es un problema de paralelización sino de rendimiento (quizás implícito) de la  paralelización. Para balancear la carga o, más específicamente, para asignar la carga de  procesamiento acorde a la capacidad relativa de cada procesador se tienen que resolver dos  problemas:  1. Conocer las diferencias relativas de capacidades de cómputo, es decir para cada par de  computadoras cuánto mejor o peor es una respecto de la otra.  2. Asignar la cantidad de cómputo de cada computadora de acuerdo con su capacidad relativa  respecto de las demás.   El primer problema es sencillo de resolver utilizando la idea ya planteada en [21] y [16], es decir  ejecutando el mismo problema pero con tamaño reducido en cada una de las computadoras y  relacionando directamente los tiempos de cómputo. Con estos cálculos no solamente se puede  definir el balance de carga sino también la evaluación de rendimiento. El segundo de los problemas  planteado normalmente depende de la aplicación, es decir que se resuelve caso por caso.  5.1 Cálculo de S Una forma numérica de calcular el número S se lleva a cabo vía integración numérica con el cálculo   ¦    u  1 0 2 )1/(4 n i ixhS (1) donde h = 1/n, n es la cantidad de puntos a utilizar en la integración numérica y xi = h × (i + 0.5). En  general, a mayor cantidad de puntos (n mayor) es mejor o más precisa la aproximación de S que se  obtiene. Claramente, cada término de la sumatoria puede ser calculado de manera absolutamente  independiente de todos los demás y, por lo tanto, estos cálculos (y las sumas intermedias de los  mismos) pueden ser paralelizados en tantos procesadores como se decida. Si bien la paralelización  es trivial, no sucede lo mismo con el balance de carga, dado que no se logra balancear la carga en p XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1177 procesadores realizando la suma parcial de n/p términos de la Ec. (1) en cada uno de ellos. En este  caso particular, se puede aprovechar de manera directa otra de las características del cálculo  planteado en la Ec. (1): todos los términos de la sumatoria implican el mismo costo en términos de  operaciones numéricas. En este sentido, el problema no solamente es sencillo en cuanto a su  división en partes sino que también es sencillo en cuanto a balance de carga en ambientes  heterogéneos: la cantidad de términos de la sumatoria de la Ec. (1) a resolver en cada computadora  es directamente proporcional a su potencia de cómputo relativa. En el caso de las PCs de la Tabla 1,  la capacidad de cómputo relativa es tal que una es casi tres veces mejor que la otra en términos de  capacidad de cálculo para S. Los resultados de los experimentos realizados con el cálculo de S se  resumen en la Tabla 2, donde se muestran las velocidades relativas normalizadas con respecto a la  de mayor capacidad de cómputo.  Velocidades Normalizadas 1 y 0,36  Eficiencia de la Paralelización 94%  Tabla 2: Resumen de la Experimentación con el Cálculo de S Interclusters.  Aunque la cantidad de computadoras es la mínima, el hecho de obtener una eficiencia del 94% es  muy satisfactorio dado el bajo rendimiento de las interconexiones entre los clusters utilizados. En el  caso de estas aplicaciones muy sencillas de paralelizar y balancear en cuanto a carga de trabajo, es  de esperar que la eficiencia se mantenga alta aumentando la cantidad de computadoras.  5.2 Cálculos Asociados al Conjunto de Mandelbrot La Fig. 4 muestra una de las formas más comunes de cómputo utilizado para el gráfico relacionado  con el conjunto de Mandelbrot (mencionado en algunos casos como “escape time algorithm” [26])  para un punto dado como (x0, y0) [22].  x = x0; y = y0; iter = 0;  while ( x*x + y*y < (2*2)  AND  iter < max)   {     xtemp = x*x - y*y + x0; y = 2*x*y + y0;      x = xtemp; iter = iter + 1;  } color = iter Figura 4: Cómputo Relacionado con el Conjunto de Mandelbrot.  Comparándolo con el cómputo de S anterior, el que se muestra en la Fig. 4:  x Es similar en cuanto a que el valor de un punto en particular es totalmente independiente de  todos los demás valores.  x Es diferente en cuanto a que la cantidad de operaciones necesarias para el cálculo del valor de  un punto en particular no es conocida a priori, depende del punto mismo (iteración while). Normalmente, lo que se paraleliza es el cálculo de los diferentes puntos y evidentemente no  requiere mucho esfuerzo. No es tan directo el balance de carga, aún cuando se conozcan las  velocidades relativas de las computadoras a utilizar. Tampoco es demasiado complejo, dado que se  puede recurrir a un esquema similar al utilizado en cómputos de álgebra lineal, por ejemplo, donde  se distribuye el cálculo de forma tal que cada proceso debe obtener valores relativamente dispersos  en el espacio total a calcular [16]. En el caso de álgebra lineal, esto se aplica sobre la matriz a  factorizar en L y U, por ejemplo, y en este caso se aplicará al conjunto de puntos a calcular. Visto  como una matriz, el conjunto de puntos a calcular se puede dividir en múltiples bloques de  relativamente pocas filas o columnas y estos bloques se asignan a las diferentes computadoras.  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1178 Teniendo una cantidad de bloques suficientemente grandes, la cantidad de bloques asignados a cada  computadora es directamente proporcional a su velocidad relativa. En el caso específico del  conjunto de Mandelbrot, se llevó a cabo el cálculo para un espacio de 800x800 puntos que se  dividió en bloques de 100x800 puntos (bloques de 100 filas) y de cada uno de estos bloques, la  cantidad de filas asignadas a cada computadora es proporcional a la velocidad relativa de las  mismas. La Tabla 3 muestra el resumen de los experimentos realizados con este problema,  aplicando el balance de carga que se describió previamente. Una vez más, la eficiencia de la  paralelización es muy satisfactoria para la plataforma de cómputo subyacente.  Velocidades Normalizadas 1 y 0,36  Eficiencia de la Paralelización 89%  Tabla 2: Resumen de la Experimentación con el Cálculo de S Interclusters.  6 CONCLUSIONES Y TRABAJO FUTURO  En este artículo se ha mostrado la utilización de una VPN para cómputo paralelo interclusters  utilizando una implementación específica de MPI. Esto muestra que, al menos en principio, no es  necesaria una infraestructura muy compleja sino middleware estándar para cómputo paralelo  interclusters. Los experimentos han mostrado que es posible caracterizar satisfactoriamente el  rendimiento de las comunicaciones interclusters de manera sencilla, y a partir de esta información  se pueden tomar decisiones importantes de paralelización. También se ha mostrado que no  solamente es factible paralelizar aplicaciones, (al menos las del tipo embarrassingly parallel), sino  que se puede obtener rendimiento muy satisfactorio, con eficiencia de la paralelización de alrededor  del 90% o mayor. También se ha mostrado que las estrategias conocidas de balance de carga dan  resultados satisfactorios, al menos en las aplicaciones que se mostraron.  Una de las extensiones inmediatas de este trabajo consiste en la utilización de más de una  computadora en cada cluster involucrado. Sin lugar a dudas el objetivo final de cómputo paralelo  interclusters debería ser el de utilizar todos los clusters de manera completa, para aprovechar al  máximo la capacidad de cálculo disponible. En caso contrario, se debería establecer una estrategia o  política de utilización de las computadoras de cada cluster que justifique el hecho de no utilizar las  computadoras disponibles de los clusters.	﻿paralelismo en clusters e interclusters , parallelism in clusters and interclusters , caracterización de rendimiento , performance characterization , process communication	es	22916
38	Algoritmos genéticos incrementales	﻿   Este artículo presenta el estudio, diseño e implementación de un modelo particular de algoritmos  genéticos paralelos, los algoritmos genéticos incrementales. La principal ventaja del motor de  algoritmos genéticos implementado es su potencialidad de ejecución en entornos no dedicados,  detectando la carga generada por los usuarios del entorno y consumiendo  de un modo “inteligente”  los recursos computacionales disponibles. Se presenta la descripción teórica del modelo de  algoritmos genéticos incrementales, los detalles de diseño e implementación del motor desarrollado  y su evaluación al aplicarse a la resolución de un complejo problema de diseño de redes de  comunicaciones confiables.      Palabras clave: algoritmos genéticos, modelos paralelos, entornos no dedicados.                                Destinado al Workshop de Agentes y Sistemas Inteligentes  1.  Introducción  Este artículo presenta el estudio, diseño e implementación de un motor de algoritmos genéticos  paralelos que sigue la propuesta del modelo de algoritmos genéticos incrementales, presentado por  Alba y Troya [3]. El software toma ventaja de la capacidad del modelo, adecuado para ejecutar en  entornos heterogéneos, en especial sobre redes de computadores no dedicadas. Basados en el  paralelismo de islas o de subpoblaciones con migración [2], los algoritmos genéticos incrementales  mantienen las ventajas de calidad de búsqueda de los modelos centralizados, y permiten la ejecución  en paralelo con un número variable de islas distribuidas en varios procesadores. La implementación  desarrollada adecua el modelo para permitir su ejecución en redes de computadores no dedicadas,  adaptándose a condiciones de carga variable del entorno de ejecución compartido, configurando  automáticamente el número de islas activas dependiendo de la carga generada por otros usuarios.  Si bien existe una amplia variedad de bibliotecas e implementaciones de modelos de algoritmos  genéticos (AG) secuenciales y paralelos, el modelo incremental ha recibido una atención casi nula.  Las principales bibliotecas de algoritmos genéticos no ofrecen la posibilidad de manejar un número  dinámico de islas y ninguna brinda soporte para configurar el número o la estructura de las islas  dependiendo de la carga presente en cada computador. Esta característica es un requisito muy  importante para el caso de redes no dedicadas, donde se comparte el uso de un conjunto de recursos  computacionales con un número importante de usuarios que realizan variadas tareas en un día  habitual de trabajo. Para lograr el objetivo de utilizar racionalmente la mayor cantidad de recursos  computacionales sin perturbar el normal trabajo de otros usuarios, la implementación desarrollada  detecta la presencia de otros usuarios, evalúa su actividad y requerimientos de poder de cómputo y  libera los recursos de ser necesario, eliminando islas del modelo incremental. Asimismo, el  monitoreo de actividad identifica la disponibilidad de recursos computacionales libres y asigna a  ellos la ejecución de islas previamente detenidas o creadas a demanda del propio modelo.  El resto del documento se organiza del modo que se describe a continuación. La sección 2 presenta  el modelo de algoritmos genéticos incrementales, describiendo sus variantes. La sección 3 presenta  el análisis del motor, describiendo sus características y sus funcionalidades. Los detalles de diseño e  implementación se presentan en la sección 4. La sección 5 ofrece la evaluación del sistema mediante  la resolución de un problema de optimización relacionado con el diseño de redes de comunicaciones  confiables. Por último, la sección 6 ofrece las conclusiones y propuestas de trabajo actual y futuro.  2. Algoritmos genéticos incrementales  El modelo de algoritmos genéticos incrementales fue propuesto por Alba y Troya en 1995 [3] y el  único antecedente conocido de implementación corresponde a un trabajo interno no publicado [4].  Es un modelo híbrido que busca conjuntar las ventajas de los modelos centralizados (buena calidad  de búsqueda de soluciones), y las ventajas de los modelos distribuidos (mejora en la diversidad,  buenos valores de eficiencia computacional y de tiempo efectivo para hallar buenos resultados).  El modelo se compone de dos tipos de procesos: un monitor central y un conjunto de islas que  trabajan cooperativamente en paralelo, conformando un sistema homogéneo ya que todas aplican el  mismo procesamiento sobre distintas poblaciones. La única vía de comunicación es a través del  proceso monitor central, quien se comunica con las islas y viceversa: no existen migraciones entre  islas. Una de las diferencias fundamentales de los algoritmos genéticos incrementales con respecto a  los modelos migratorios tradicionales consiste en que el monitor recibe los individuos desde las  islas de manera asíncrona. Se genera de este modo una macro-población compuesta por los mejores  individuos de cada isla, de manera incremental y no determinista: la mezcla de individuos  provenientes de las islas no guarda un orden determinado respecto a los procesos que los crearon.  Las islas no trabajan siempre sobre la misma población, dado que en ciertos puntos de su evolución  cada isla recibirá del monitor una nueva población para procesar.   La Figura 1 presenta el esquema de los algoritmos genéticos incrementales.  Según la actividad genética del proceso monitor, y dependiendo de la variación dinámica de la  cantidad de islas en el sistema, se distinguen cuatro modelos de algoritmos genéticos incrementales:  · Modelo estático–pasivo: El monitor no realiza trabajo genético. El número de islas se define al  iniciar el trabajo y permanece constante hasta el final del algoritmo.  · Modelo estático–activo: El monitor realiza trabajo genético y el número de islas permanece  constante hasta la terminación del algoritmo.  · Modelo dinámico–pasivo: El monitor no realiza trabajo genético, y el número de islas puede  variar durante la ejecución del algoritmo. Las islas pueden eliminadas, y también es posible la  creación de nuevas islas que realicen tareas de búsqueda genética.  · Modelo dinámico–activo: El monitor realiza trabajo genético y el número de islas puede variar  durante la ejecución del algoritmo.    AG  MONITOR  (activo ó pasivo)  AG  AG  AG  AG  AG  AG  MONITOR  (activo ó pasivo)  AG  AG  AG  Modelo estático Modelo dinámico    Figura 1. Esquema general de los algoritmos genéticos incrementales  3. El motor de algoritmos genéticos incrementales  Esta sección presenta los requisitos del motor de algoritmos genéticos incrementales, una descripción  precisa del modelo implementado y las características generales del sistema desarrollado.  3.1. Requisitos del motor  El principal requisito planteado para el motor de algoritmos genéticos incrementales es la capacidad  de ser ejecutado en redes no dedicadas. Por este motivo, es fundamental contar con la creación y la  eliminación dinámica de islas, descartándose la utilidad de los modelos estáticos. Los modelos activo  y pasivo son semejantes algorítmicamente y la calidad de búsqueda entre un modelo dinámicoactivo con N islas y un modelo dinámico-pasivo con N+1 islas será similar para N razonablemente  grande. La eficiencia computacional y el tiempo real de ejecución para hallar resultados de calidad  razonable deben ser similares para ambos modelos.   Como consecuencia, se decidió fijar como alcance del trabajo el desarrollo de una implementación  de algoritmos genéticos incrementales modelo dinámico-pasivo. De todas formas, se ha propuesto  también una especificación del modelo dinámico-activo que intenta subsanar ciertas desventajas de  la propuesta original, y el motor de algoritmos genéticos incrementales se ha desarrollado teniendo  en cuenta su posible extensión para incorporar el modelo activo en el futuro.  3.2. Modelo dinámico-pasivo   3.2.1.  Propuesta original  En la propuesta original de modelo dinámico-pasivo [4], el proceso monitor comienza con una  población vacía y durante la evolución crea dinámicamente islas con una población de tamaño Tp.  En cada generación, las islas calculan su porcentaje de mejora, considerando las relaciones entre el  mejor valor y el promedio de fitness en dos generaciones sucesivas, y lo comunican al monitor  cuando este proceso lo requiere. Por cada individuo generado en una isla se envía una copia al  monitor, que los almacena de manera no determinista en su población. Cuando dispone de Tp  individuos toma una decisión en función del número actual de islas y de los porcentajes de mejora:   · Si el número actual de islas ejecutando en el sistema es menor que el máximo permitido,  entonces el monitor creará una nueva isla con la población incremental.  · Si se encuentra ejecutando la cantidad máxima posible de islas, entonces el monitor le envía la  población incremental a la isla de peor porcentaje de mejora.  Cuando una isla recibe una población del monitor, abandona su población y comienza a procesar la  nueva población recibida, aplicando el proceso evolutivo del AG. En general, la condición de parada  del modelo involucra una cantidad específica de entregas de la población incremental del monitor.  3.2.2. Desventajas de la propuesta original de modelo dinámico-pasivo   La propuesta original de modelo dinámico-pasivo presenta una serie de desventajas:  · La tasa de comunicación entre las islas y el monitor es altísima. Al enviar cada nuevo individuo  al monitor se centraliza demasiado la comunicación, degradando la eficiencia computacional  del modelo. Además, al comunicarse individuos que no tienen buen fitness, se ocasiona una  latencia en la difusión de buenos individuos al resto de las islas.  · Dado que en un modelo de AG generacional se crea toda la población en una generación, se  corre el riesgo de no mezclar adecuadamente poblaciones diferentes en el monitor.  · Como cada isla informa su porcentaje de mejora ante solicitudes del proceso monitor, en el  caso de utilizar comunicaciones asincrónicas el monitor debe esperar a recibir todos los  porcentajes de mejora, o enviar poblaciones basándose en información no reciente.  · Al momento de crear nuevos procesos no se tiene en cuenta que realmente existan recursos  computacionales con la capacidad adecuada de ejecutar el nuevo proceso genético.  3.2.3. Modelo dinámico-pasivo propuesto  Considerando las desventajas presentadas del modelo original, se propusieron incorporar ciertas  características al modelo dinámico-pasivo.  Se propone reducir el envío de individuos desde las islas al monitor, comunicando Tp/N individuos  seleccionados, siendo N la cantidad de islas activas en el sistema. El envío no se realizará en cada  generación: un parámetro adicional del modelo determinará la frecuencia de comunicación. Al   permitir la evolución independiente de las islas durante cierto número de generaciones, se mejora la  eficiencia computacional reduciendo el acceso al cuello de botella de las comunicaciones, y se  aumenta la diversidad de la población en el monitor, evitando que predominen los individuos  provenientes de una determinada isla. En cada generación las islas enviarán sus porcentajes de  mejora al monitor sin necesidad de solicitud previa, permitiendo al proceso monitor consultar de  forma inmediata los porcentajes de mejora actualizados de cada isla, sin esperar a recibirlos.  Para permitir su ejecución en ambientes compartidos, se considerará la carga actual de cada host del  entorno para determinar si se deberá crear una nueva isla o no, evitando que los hosts sean saturados  con procesos genéticos, afectando las tareas de otros potenciales usuarios del entorno de ejecución.  Adicionalmente, se incorporan al modelo flexibilidades para considerar casos específicos que  permitan una mejora en el desempeño computacional al ejecutar sobre una red de computadores:   · Cuando el monitor posee Tp individuos y no sea posible crear nuevas islas, enviará los Tp/N  “mejores” individuos de su población incremental a la isla con menor porcentaje de mejora,  que seleccionará para reemplazar sus “peores” Tp/N individuos.   · Se incorpora la posibilidad de comunicación de material genético entre islas sin pasar por el  monitor mediante mecanismos de migración asincrónica.  · En el momento de eliminar una determinada isla, se almacenará la información correspondiente  para que el trabajo genético realizado no se pierda. Con este objetivo se definirá un conjunto de  check-points, especificado por un número de generaciones que se incorpora como parámetro  adicional al modelo. En cada check-point se almacenará en disco la información (status) de las  islas, que podrá ser utilizada para reanudar la tarea en el futuro.  · De modo similar, el monitor almacenará su status cada cierto número de envíos de Tp (ó Tp/N)  individuos. De esta forma se dispondrá del estado del sistema en conjunto almacenado para ser  recuperado en caso de que se interrumpa la ejecución del sistema, continuando con el  procesamiento genético desde el último check-point disponible.  3.3. Características del  sistema   La principal característica del sistema es su capacidad de ejecución sobre redes de computadoras no  dedicadas, un entorno de tiempo compartido que se utiliza para múltiples fines, donde muchos  usuarios trabajan concurrentemente, generando cada uno carga de CPU y memoria sobre el sistema.  Para evitar consumir la totalidad de los recursos del entorno, perjudicando el trabajo normal de  otros usuarios de la red, la propuesta incorporará la capacidad de detectar automáticamente la carga  generada por los usuarios del entorno, permitiendo la creación y eliminación dinámica de procesos  genéticos. Específicamente, el motor deberá ser capaz de realizar las siguientes actividades:  · Detectar la presencia de usuarios utilizando un recurso computacional (host) y eliminar la  cantidad necesaria de procesos genéticos para liberar recursos, salvando su estado de ejecución.   · Detectar la disponibilidad de recursos computacionales. Ante la ausencia de usuarios utilizando  un host, se deberá crear la cantidad de procesos genéticos que el sistema considere necesario  para aprovechar al máximo los recursos disponibles del entorno de ejecución.  · Poder recuperar el estado de ejecución de todo el sistema, evitando perder el trabajo de  búsqueda realizado hasta el momento.   3.4. Formalización del Modelo Dinámico-Pasivo propuesto  Siguiendo la especificación definida por Alba y Troya, la Figura 2 presenta la formalización del  modelo y una descripción de sus parámetros.        · AG[]: Conjunto de AGs presentes en el modelo (islas).   · Tp: Tamaño de la población presente en cada isla del sistema.  · M : Cantidad de procesadores que conforman el entorno de ejecución.   · Td=alcanzar(P): Condición de finalización del algoritmo genético incremental.  · N: Número de islas activas en el sistema.  · Nmin: Número mínimo de islas que se permiten en el sistema en cualquier momento.   · Nmax: Número máximo de islas permitidas en el sistema en cualquier momento. Inicialmente N=Nmáx, o  sea, el sistema comienza a trabajar con la cantidad máxima de procesos genéticos.  · island_send_rate: Frecuencia de envío de individuos desde las islas hacia el proceso monitor,  medida en generaciones.  · island_migration_rate: Frecuencia de migración entre islas, medida en generaciones.  · island_migration_size: Tamaño de la migración entre islas.  · CARGA_MIN: Carga de un host por encima de la cual no se crearán nuevas islas.  · CARGA_MAX: Carga de un host por encima del cual se eliminaran procesos genéticos.   · check_async: Frecuencia para chequear recepción de individuos, medida en generaciones   · time_check_process: Tiempo (en segundos) que indica la frecuencia de actualización de las  lecturas de carga de cada host perteneciente al entorno de ejecución.  · save_status: Frecuencia para almacenar el estado de los procesos. Medida en generaciones para las  islas y en número de veces que envió Tp (o Tp/N) individuos a las islas para el monitor.  · proc(): Lista con la cantidad máxima de procesos genéticos que pueden ejecutar en cada host.  Figura 2: Especificación del modelo dinámico-pasivo propuesto.  AG_Inc_Din_Pas (AG[], Tp, NÎ[Nmin,Nmax], M, Td=alcanzar(P), Monitor=pasivo,  island_send_rate, island_migration_rate, island_migration_size, CARGA_MIN,  CARGA_MAX, check_asynchronous, time_check_process, save_status, proc(Maq)).  3.5. Descripción del sistema  El sistema opera según los seudocódigos que se presentan y describen a continuación.  Proceso MONITOR:  Para todo AG hacer iniciar_AG(AGi,i,false,NULL,NULL);  // Iniciar los N procesos genéticos.  nro_envios = 0  Hacer P veces:                 // P = Número de generaciones que realizará cada AG.   num_ind_Monitor = 0     Mientras num_ind_Monitor <= Tp    Recibir(msg)   // Recibir un mensaje.    msg:nuevoIndividuo(i,Individuo) è // Se recibe individuo: agregar a pob. incremental.     poblacionMonitor+=individuo     num_ind_Mon = num_ind_Mon + 1     Actualizar_Estadísticas()    msg:porcentaje_mejora(i,porc) è // Se recibe info: actualizar lista de mejoras.     Lista_mejoras(i)=Porc    msg:kill_ack(i,path): è agregar(kill_process,path)  // Eliminar isla.       kill(i);                                       Cada time_check_process: è actualizar(Lista_carga).   fin mientras;   nro_envios = nro_envios + 1   Si (nro_envios % save_status==0)       // Guardar status si corresponde.    guardar_status_disco(path_archivo_status)   receptor = destino(Lpi,Lpm,crear_nuevo_proc) // Determinar destino de la pob. incremental.   Si crear_nuevo_proc    iniciar_ag(receptor,false,Población_Monitor) // Iniciar AG con la pob. incremental.   Sino     Pob_env = Seleccionar(Población_Monitor,Tp/N) // Seleccionar individuos para envío.    nueva_poblacion(AG_receptor,Pobenv)     fin Si   fin Hacer  Para todo AGi hacer terminar_AG(i);  Reportar resultado y estadísticas.  Figura 3: Seudocódigo del proceso monitor.  El monitor almacena en una tabla, actualizada cada time_check_process segundos, los datos de carga  de cada host, evaluados mediante una combinación lineal entre los porcentajes de uso de procesador  y de memoria en el último período, de acuerdo a la expresión presentada por la Ecuación 1.   CARGA = peso_CPU * porcentaje_carga_CPU + peso_memoria * porcentaje_carga_memoria  Ecuación 1: Modelo lineal para la determinación de la carga de un host.  El consumo de CPU y de memoria son los parámetros más representativos para evaluar la carga en  entornos compartidos [5]. El modelo lineal considerado ha proporcionado precisos resultados para  determinar la relación entre carga y tiempo de ejecución efectivo de aplicaciones de calculo  científico en nuestro entorno de trabajo [9]. Los pesos fueron determinados mediante la evaluación  empírica del tiempo de ejecución de un programa determinista bajo diferentes condiciones de carga  generadas por programas estándares del entorno de trabajo compartido (compilaciones, aplicaciones  de oficina, scripts utilitarios, etc.), ajustando mediante aproximación de mínimos cuadrados los  valores correspondientes a las contribuciones de carga de CPU y memoria respectivamente.  De acuerdo a los valores de tolerancia definidos, el proceso monitor eliminará islas en aquellos hosts  cuyo valor de carga supera CARGA_MAX. Para evitar eliminar procesos cuando la carga de usuario  decrece, se realiza una extrapolación lineal a partir de los valores de carga anterior y actual: la  eliminación se hará cuando la predicción de carga futura supere al umbral CARGA_MAX. La predicción  de carga mediante una extrapolación lineal ha mostrado una capacidad aceptable sobre nuestro  entorno de ejecución, mejorando los valores de predicción ingenua (naive) usados por los algoritmos  de despacho de las bibliotecas de programación paralela, tal como se ha reportado en [9].  Si existen varias islas ejecutando en un host cuyos valores de carga superan el umbral, se eliminarán  de forma ordenada cada time_check_process segundos, considerando sus porcentajes de mejora.   El monitor almacena en una lista el camino absoluto del archivo con el status de los procesos  genéticos eliminados. La creación por parte del monitor de un nuevo proceso genético en un host H  depende de tres condiciones: que el número de islas sea menor que el valor indicado por el  parámetro Nmax, que el número de procesos genéticos en el host H sea menor que el parámetro  proc(H), y que el valor de carga del host H sea menor que el parámetro CARGA_MIN. Para evitar crear  un proceso que deberá ser eliminado inmediatamente, se predice la carga del host H en los siguientes  time_check_process segundos, extrapolando linealmente a partir de los últimos dos valores de carga.  La Figura 4 presenta las regiones delimitadas por los umbrales de carga y las acciones de creación,  eliminación de procesos y ejecución “en régimen” (sin creación ni eliminación de islas).  0 10 20 30 40 50 60 70 80 90 100 Tiempo Po rc en taj e d e c ar ga   Figura 4: Regiones delimitadas por umbrales de carga y acciones correspondientes.  Cada save_status envíos hacia las islas, el monitor almacena su estado a disco, definiendo un punto  de sincronización para reanudar la ejecución en un momento posterior.    AGi(Agi, i, leerarchivo, path, población)  Si(leer_de_archivo)   Población_Isla = leer_población_de_archivo(path)  // Retomar ejecución previa.   Si (no_leer_de_archivo) y (población == vacío)   Población_Isla = Generar_Población_AG()   // Iniciar ejecución.  Sino   Población_Isla = población;    // Sustituir por pob. incremental.  Hasta recibir el mensaje terminar_ag(i) hacer:   Mientras no se reciba mensaje(msg) hacer:    Evolucionar(Población_Isla)                 // Aplicar mecanismo evolutivo.    Si (generacion % migration_rate == 0)  // Migración          dest = destinoMigracion();                Pob_Mig = Seleccionar(Población_Isla, migration_size)      Enviar Migracion(Pob_Mig ,dest)    Si (generacion % check_async == 0) // Chequear por inmigrantes          recibir_Migracion(origen, ind_ mig, i)          reemplazar(Población_Isla , ind_mig)          Actualizar_porcentaje_de_mejora()    Enviar porcentajemejora(i,porcentaje_de_mejora).    Si (generacion % save_status == 0)  // Salvar status a disco si corresponde.     guardar_status_disco(path_archivo_status)         Si (generacion % island_send_rate == 0) // Envíar individuos al monitor si corresponde.     Inds = Seleccionar(Población_Isla, island_send_size)     Enviar_individuos(i, Inds)   Fin Mientras   msg:nueva_población(nueva_población)      // Se recibe nueva población.     remplazar(Población_Isla, nueva_población) // Reemplazar los peores individuos      msg:kill_island           guardar_status_disco(path_archivo_status); // Salvar status y envíar kill_ack    envíar_kill_ack(i,path_archivo_status)   Figura 5: Seudocódigo de los procesos isla.  CARGA_MIN  CARGA_MAX  Carga(H) < CARGA_MIN ®   Si (#islas < Nmax) y (#islas(H)) < proc(H)   crear proceso(s) genético(s).  Carga(H) > CARGA_MAX ®   eliminar proceso(s) genético(s).  CARGA_MIN < Carga(H) < CARGA_MAX ®   ejecución “en régimen”.  Las islas realizan la búsqueda poblacional de un AG tradicional, en base a operadores evolutivos. La  población inicial puede leerse desde un archivo (si se reanuda una ejecución previamente truncada),  puede recibirse desde el proceso monitor (si se crea un nuevo proceso isla con una población  incremental), o puede generarse aleatoriamente mediante el procedimiento de inicialización.  Se incorpora la posibilidad de intercambio de material genético entre islas a través de migraciones  asincrónicas: cada migration_rate generaciones, una isla selecciona island_migration_size  individuos y los envía a su vecino más cercano, considerando a los procesos genéticos conectados  de acuerdo a una topología de migración que corresponde a un anillo unidireccional. Cada isla  verifica cada check_async generaciones la recepción de individuos inmigrantes y efectuará el  reemplazo manteniendo el criterio de mantener los mejores individuos en la población.  Cuando recibe un mensaje de nueva_poblacion, la isla procede a reemplazar la totalidad de la  población actual por la población incremental que le envía el proceso monitor.  El procedimiento iterativo se realiza hasta recibir un mensaje de finalización (kill_island). Luego  se almacena el estado para evitar la perdida del trabajo realizado, estableciendo un check-point para  reanudar la evolución en caso de disponer de recursos computacionales en el futuro. A  continuación, la isla envía la confirmación de finalización (kill_ack), y finaliza su ejecución.  4. Diseño e Implementación   El sistema desarrollado se compone de dos grandes módulos. Un módulo brinda las funcionalidades  del modelo de algoritmos genéticos incrementales descrito en la sección anterior, implementando el  proceso maestro y los procesos islas. Por otra parte, un segundo módulo se encarga de monitorear la  carga de los hosts que componen el entorno de ejecución, proporcionando la información necesaria  al proceso monitor para decidir si corresponde crear o eliminar procesos en el entorno.   4.1. El módulo de algoritmos genéticos  El modelo desarrollado se basa en un monitor que no realiza trabajo genético. Su tarea consiste en  tomar decisiones sobre la creación o eliminación de procesos genéticos según la carga generada por  los usuarios en cada host y al porcentaje de mejora de cada proceso genético existente en el sistema.   El módulo de algoritmos genéticos incrementales se diseñó extendiendo MALLBA, una biblioteca  que proporciona esqueletos de software que implementan algoritmos de optimización [1]. Los  detalles de los algoritmos, la interacción con el problema y el paralelismo se implementan mediante  clases C++ que abstraen a las entidades involucradas en cada método de resolución:  · Las clases provistas implementan aspectos cada algoritmo, independizándose del problema. Las  principales clases provistas son Solver (el algoritmo) y SetUpParams (fija parámetros).  · Las clases requeridas especifican la información relevante del problema a resolver. Cada  esqueleto incluye las clases requeridas Problem y Solution que encapsulan las entidades  dependientes del problema necesarias para los métodos de resolución.  La biblioteca MALLBA está disponible públicamente en http://neo.lcc.uma.es/mallba.  La comunicación entre los procesos isla y el proceso monitor se realiza utilizando la biblioteca de  desarrollo de programación paralela y distribuida MPI [6].  4.2. El módulo medidor de carga  El módulo medidor de carga o medidor de performance realiza lecturas de uso de CPU y memoria  para cada host del entorno de ejecución. Se implementó como una aplicación cliente/servidor con  comunicación entre procesos mediante invocación de procedimientos remotos (RPC). La obtención  de datos de carga es una operación en que el monitor invoca funciones remotas o servicios sobre  cada host del entorno de ejecución. Se seleccionó RPC como mecanismo de comunicación por su  simplicidad conceptual, pero el diseño prevé utilizar cualquier otro mecanismo de comunicación, ya  que abstrae y encapsula toda la lógica relacionada con la tecnología de comunicación.  Un proceso servidor de carga ejecuta en cada host del entorno de ejecución, evaluando su carga. El  proceso cliente de carga, presente en el host que ejecuta al monitor, se comunica remotamente con  cada servidor de carga para obtener los valores de cada host del entorno de ejecución.  Los módulos del sistema se relacionan de acuerdo al esquema que se presenta en la Figura 6. El  monitor dispone de una colección de clientes de carga, uno por cada host del entorno de ejecución.  Existe una independencia total entre cada proceso isla ejecutando en un host y el servidor de carga  correspondiente, que sólo conoce el número de islas en ejecución en el host que monitorea.   Resumiendo, si bien el diseño de los módulos encapsula y abstrae el mecanismo de comunicación  utilizado para no ligarse a una tecnología en particular, el sistema utiliza dos tipos de comunicación:  · una comunicación entre procesos genéticos (monitor e islas) que se realiza mediante MPI.  · una comunicación entre el monitor (a través de su colección de clientes de carga) y los  servidores de carga, utilizando RPC.               Figura 6: Arquitectura del Sistema  5. Evaluación del sistema   Para verificar la correctitud de la implementación y evaluar su capacidad de trabajo sobre un  entorno no dedicado respetando las consideraciones de diseño, se utilizó el motor de algoritmos  genéticos incrementales para la resolución del problema de Steiner generalizado (GSP).   El GSP propone optimizar el diseño de una red de comunicaciones respetando requisitos de  conectividad entre nodos distinguidos denominados terminales. El problema ha sido abordado  previamente por nuestro grupo de trabajo utilizando el modelo de islas de MALLBA sobre un  entorno de red de computadoras dedicado [7,8]. La evaluación experimental se orientó a comparar  la calidad de resultados y la eficiencia computacional del modelo de algoritmos genéticos  incrementales con los resultados previamente obtenidos. Se trabajó sobre las tres instancias del  problema abordadas en trabajos previos (grafo_50_15, grafo_75_25 y grafo_100_10, nombres que  refieren al número de nodos y de terminales), que proporcionan diferentes niveles de complejidad  para problemas representativos del diseño de redes de comunicaciones de mediano tamaño.  5.1. Plataforma de ejecución  Los experimentos de evaluación se realizaron sobre un clúster no dedicado compuesto por 8  computadores heterogéneos, con procesadores Pentium II y Pentium III de entre 400 MHz y 1.2  GHz  y entre 256 y 1024 MB de RAM, con sistema operativo Linux Suse 8.2 y conectadas por  Ethernet común de 10 Mbits.  Referencias  Comunicación entre Monitor e Islas (MPI).  Comunicación entre islas para migración (MPI).  Comunicación entre el Monitor (módulo Cliente  del Medidor de Performance) y el Servidor de  Performance del Host (RPC).  5.2. Evaluación de resultados  La Tabla 1 resume los resultados obtenidos por el modelo de algoritmos genéticos incrementales,  presentando las mejores soluciones, valores promedio y desviación estándar sobre 10 ejecuciones  para cada instancia de prueba considerada. Se comparan los valores con los resultados del modelo  de islas ejecutando sobre un cluster dedicado de 8 computadores.   Mejor AGI Promedio AGI Desv. Est. Mejor Islas Promedio Islas  grafo 50_15 9498.65 9410.17 65.10 9535.29 9393.25  grafo 75_25 5444.69 5344.75 44.86 5447.23 5379.41  grafo 100_10 4536 4483.25 37.17 4537 4489  Tabla 1: Comparación de resultados.  La Tabla 1 muestra que el modelo de algoritmos genéticos incrementales alcanza resultados  similares al modelo tradicional de islas implementado por MALLBA para las instancias estudiadas.  5.3. Evaluación de eficiencia computacional  Para evaluar la eficiencia computacional del modelo implementado, se calculó el speedup y la  eficiencia del algoritmo paralelo. El speedup (SM), relaciona el tiempo medio de ejecución al usar  un solo procesador (T1) con el tiempo medio al utilizar M procesadores (TM) según la Ecuación 2.1.  La eficiencia (EM), normaliza el valor del speedup respecto a los recursos computacionales  utilizados, según la Ecuación 2.2.  M M T TS 1=   (2.1)                M SE MM =    (2.2)  Ecuación 2: Definición de speedup y eficiencia.   grafo 50_15 grafo 75_25 grafo 100_10   S8 E8 S8 E8 S8 E8  AG Incrementales 2.64 0.33 4.65 0.58 2.48 0.31  AG Islas [8] 5.04 0.63 5.20 0.65 5.2 0.65  Tabla 2: Resultados comparativos de eficiencia computacional  La Tabla 2 presenta los resultados comparativos de speedup y eficiencia de los modelos estudiados,  utilizando 8 procesadores para resolver cada instancia del problema. Se observa que el modelo de  islas supera notoriamente al modelo incremental en las instancias menos complejas, y que el modelo  incremental es competitivo en la instancia de mayor complejidad (el grafo con mayor cantidad de  terminales). La degradación de la eficiencia del modelo incremental en las instancias menos  complejas se debe a dos factores: la influencia del trabajo en el entorno no dedicado, que agrega  algoritmia para manejar la interacción con otros usuarios; y el incremento de las comunicaciones  debido a la transferencia de individuos entre islas y monitor para formar la población incremental.  La influencia de estos factores disminuye al aumentar la complejidad de los problemas; por este  motivo el modelo incremental logra alcanzar valores aceptables de eficiencia para el grafo_75_25.  Se estudió la escalabilidad del modelo incremental para resolver la instancia grafo_75_25, limitada a  los recursos computacionales disponibles (8 procesadores). Los resultados se ofrecen en la Tabla 3  y en la Figura 7, mostrando el comportamiento de speedup sublineal del modelo incremental. Los  valores de eficiencia disminuyen al usar 6 y 8 procesadores, degradándose la performance a medida  que se agregan recursos computacionales al incrementarse las comunicaciones entre islas y  monitor.   procesadores 1 2 4 6 8   S1 E1  S2 E2 S4 E4 S6 E6 S8 E8  AG Incrementales 1 1 2.03 1.02 3.64 0.58 4.42 0.31 4.65 0.31  Tabla 3: Resultados de escalabilidad.  0 1 2 3 4 5 1 2 4 6 8 procesadores Sp ee du p  Figura 7: Evaluación gráfica de la escalabilidad.  5.4. Interacción con otros usuarios  Las pruebas de evaluación de eficiencia computacional se realizaron en un entorno no dedicado,  interactuando con procesos de otros usuarios. La lógica del sistema detecta la presencia de usuarios  y toma acciones para no interferir con el uso habitual de los recursos computacionales. Para evaluar  la correctitud de esta funcionalidad, se monitoreó el tiempo promedio que cada ejecución del  modelo incremental trabajó por encima del umbral de carga especificado en la configuración. Sobre  las 30 ejecuciones realizadas, el modelo compartió CPU con otros usuarios en un tiempo promedio  inferior al 5% del tiempo total de ejecución, un valor muy razonable, que permite concluir que el  mecanismo que detecta usuarios y libera los recursos computacionales funciona correctamente.  5.5. Conclusiones de la evaluación del sistema  La evaluación permite concluir que el sistema cumple sus especificaciones y es capaz de obtener  resultados similares a los del modelo distribuido estándar de algoritmos genéticos. La eficiencia  computacional se degrada como consecuencia de la comunicación excesiva entre islas y monitor,  pero su efecto se hace menos influyente al aumentar la complejidad del problema a resolver.  La principal característica del modelo de algoritmos genéticos incrementales es su capacidad de  ejecutar la sobre un cluster no dedicado, sin perturbar el trabajo de otros usuarios. Esta cualidad  lo posiciona como una herramienta muy importante para el desarrollo e investigación en nuestro  entorno de trabajo, donde no se dispone de recursos computacionales dedicados y es necesario  utilizar alternativas -como la red local de trabajo y enseñanza- para resolver problemas complejos.  6. Conclusiones y trabajo futuro  Este trabajo presenta el diseño, la implementación y las pruebas de verificación y evaluación de un  modelo de algoritmos genéticos del cual no se tienen antecedentes recientes de implementación. Se  incorporó un mecanismo de detección de usuarios en la red que permite la creación y eliminación de  procesos genéticos, posibilitando la ejecución en un entorno no dedicado. El modelo se mostró  capaz de alcanzar una buena calidad de resultados, y aceptables valores de eficiencia computacional  y escalabilidad al resolver un problema de optimización que modela el diseño de redes de  comunicaciones confiables. La detección de usuarios funcionó adecuadamente, permitiendo la  ejecución del modelo sin perturbar la normal operativa de la red de recursos computacionales.  Algunos aspectos del trabajo han dejado líneas abiertas para abordar en el futuro inmediato. La  creación y eliminación de procesos puede ser mejorada cuando se disponga de una implementación  de MPI que incluya la primitiva para lanzar procesos spawn. Esta funcionalidad está especificada en  el estándar MPI2 pero las implementaciones actuales de la biblioteca no la incluyen, restringiendo  el manejo de creación de procesos. El diseño del sistema contempla la implementación del modelo  incremental activo, en la que se trabaja actualmente. Por último, se prevé utilizar el sistema  diseñado para la resolución de otros problemas de optimización NP difíciles, con el objetivo de  complementar la evaluación de la calidad de resultados y la eficiencia computacional del modelo.	﻿modelos paralelos , algoritmos genéticos , entornos no dedicados	es	22928
39	Modelo y simulación de agentes en el contexto de vida artificial aplicado al control aéreo en un ambiente de vuelo libre	﻿de Vida Artificial  aplicado al Control Aéreo en un Ambiente de Vuelo Libre    J. Ierache 1,2,Victor Battista 1, D. Rodríguez3, P. Britos3,2, R. García-Martínez 3,2  1. Instituto  de Sistemas Inteligentes y Enseñanza Experimental de la Robótica. Facultad de  Informática, Ciencias de la Comunicación y Técnicas Especiales. Universidad de Morón  2. Laboratorio de Sistemas Inteligentes. Facultad de Ingeniería. Universidad de Buenos Aires  3. Centro de Ingeniería del Software e Ingeniería del Conocimiento. Escuela de Postgrado. Instituto  Tecnológico de Buenos Aires  jierache@yahoo.com.ar, jierache@unimoron.edu.ar, victor2206@gmail.com, rgm@itba.edu.ar  Tel: +5411 56272000 (189/746)    Abstract  In this work one appears the results of a simulation in an artificial life environment and an approach of  a model multiagents under an architecture based on beliefs, desires and intentions of the agents who  implement the concept of free flight “Free Flight”, this concept proposes a solution before the  vertiginous growth of the aerial traffic, that today is a problematic one at world-wide level. The  airways with the passage of time are multiplied to the rate of the rising world-wide economic markets.  In Free Flight, the airplanes, are their own controllers, can choose their own routes of navigation,  speed, height, regimes of ascent and reduction, maneuvers. For the support, analysis and management  of one ambient of free flight, the characterization of a model of simulation is considered based on  surroundings of artificial life in order to observe the evolution of the airspace under this concept, being  considered a initials of airships with diverse intentions with respect to its destination airports.     Key words: Simulation, Artificial Life, Agents, Air Traffic Controller, Free Flight     Resumen  En este trabajo se presenta los resultados de una simulación en un ambiente de vida artificial y una  aproximación de un modelo multiagentes bajo una arquitectura basada en creencias, deseos e  intenciones de los agentes que implementan el concepto de vuelo libre “Free Flight”, este concepto  propone una solución ante el crecimiento vertiginoso del tráfico aéreo,  que hoy es una problemática a  nivel mundial. Las rutas aéreas con el paso del tiempo se multiplican al ritmo de los nacientes  mercados económicos mundiales. En Free Flight, los aviones, son sus propios controladores, pueden  elegir sus propias rutas de navegación, velocidad, altura, regimenes de ascenso y descenso, maniobras.  Para el apoyo, análisis y gestión de una ambiente de vuelo libre, se considera la caracterización de un  modelo de simulación basado en un entorno de vida artificial a fin de observar la  evolución del espacio  aéreo bajo este concepto, considerando una población inicial de aeronaves con intenciones diversas  respecto a sus aeropuertos de destino.     Palabras Claves: Simulación, Vida Artificial, Agentes, Trafico Aéreo, Vuelo Libre.    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1322 1. INTRODUCCIÓN  En el campo demográfico, 14 centros urbanos del mundo tendrán más de 15 millones de habitantes;  cuatro de ellos, más de 20 millones (Tokio, con alrededor de 30 millones, seguido por Lagos, Bombay  y Sao Paulo) [1]. En el pasado, como en el presente, Europa, Asia Oriental y Estados Unidos son  referentes económicos y generadores de tráfico aéreo. Sin embargo, en los últimos años han ganado  mucho terreno la India, China y el sur de Sudamérica. Los dos primeros apoyados por una fuerte  expansión económica y la última por el potencial turístico que ostenta. La aviación comercial desde sus  comienzos al día de hoy ha avanzado notablemente logrando un medio de transporte seguro y efectivo.  Hacia finales de febrero de 2006, la empresa Boeing anunció que había entregado 5.009 aviones de la  familia ‘737’ [2]. En la actualidad, el crecimiento del tráfico aéreo es analizado intensamente. El  congestionamiento ha alcanzado límites tales que alrededor del mundo distintos pronósticos señalan el  colapso de los sistemas de control de vuelo en los años próximos. Las rutas aéreas con el paso del  tiempo se multiplican, el tránsito aéreo es cada vez más intenso y las pérdidas económicas a causa de  los retrasos y congestionamiento son millonarias [3]. La solución clásica a este tipo de problemáticas  era el incremento del número de aeropuertos y la creación de nuevas pistas de aterrizajes. Con esta  perspectiva de la realidad, un cambio de enfoque en el control de tráfico aéreo surge como la solución  más ventajosa dada las desventajas o implicancias de las propuestas clásicas. En el presente trabajo se  describe aproximación a un modelo  al control del tráfico aéreo diferente al enfoque actual en uso. La  propuesta es un modelo multiagente considerando los roles mas significativos e incorporando el  concepto de vuelo libre [4], [5] para la elección de las rutas de vuelo.    2. CONTROL AÉREO INTELIGENTE  El modelo de sistema multiagente que se desea desarrollar se desenvuelve en un ambiente altamente  ágil y dinámico. Si bien, los componentes identificados son unos pocos, el modelo puede ser  especialmente complejo de acuerdo a la cantidad de instancia de un elemento esencial: las aeronaves.  Cada una de ellas, en el sistema, se modelizan a través de un agente “Piloto Aeronave”. El mismo, por  definición deberá mostrar un comportamiento flexible. Dicha flexibilidad le permitirá, a pesar de las  distintas evoluciones que presente el ambiente, alcanzar su objetivo: lograr el aterrizaje a tiempo en su  destino. Si pensamos en los distintos desafíos que un agente “Piloto Aeronave” puede encontrar  durante un vuelo veremos que entran a jugar un amplio rango de eventos que pueden influenciar de  forma positiva o negativa. Por ejemplo, supongamos que una aeronave parte de un aeropuerto A y se  dirige a un aeropuerto B. Al principio, el agente determinará una ruta de vuelo posible según un estudio  primario de sus creencias en relación al ambiente: condiciones meteorológicas, congestionamiento del  aeropuerto, distancias de vuelo, rutas de vuelo tradicionales, su capacidad de combustible, entre otras.  Así el agente aeronave parte con una intención/plan de vuelo para cumplir con su deseo de arribar al  aeropuerto de destino en el menor tiempo con el  menor consumo de combustible. Sin embargo, el  sistema puede variar drásticamente, en ocasiones espontáneamente apelando a las características  reactivas del Agente; otras veces, paulatinamente dando lugar a un comportamiento más pro-activo. La  formación de tormentas eléctricas, la aparición de aeronaves, las condiciones meteorológicas en  general, son sólo algunas de las distintas manifestaciones del ambiente en el cual se moviliza el agente  “Piloto Aeronave”. Finalmente, la aeronave se aproxima al espacio aéreo correspondiente al área  terminal. Es en este momento, donde quizás la capacidad de negociación del agente “Piloto Aeronave”  cobra vital importancia. La aeronave  que se traslada de un aeropuerto A a otro B debe aterrizar. El  primer problema es que la pista de aterrizaje es de por si un recurso finito y su disponibilidad sólo  ocurre en pequeñas “ventanas” de tiempo. La pista de aterrizaje es un recurso que todas las aeronaves  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1323 deberán compartir en un instante de tiempo particular, sólo una de ellas podrá hacer uso de la misma  para despegar o aterrizar. Esto genera una situación de competencia entre los distintos agente “Piloto  Aeronave”. Puede resultar que la aeronave a su llegada al aeropuerto se encuentre sólo esperando  autorización para aterrizar, de modo que no tendrá problemas en hacerlo. Sin embargo, sabemos que  esto no  será así en las mayorías de los casos; precisamente es esta la motivación que da lugar a este  trabajo de investigación. Frecuentemente, el Agente aeronave deberá llegar a un acuerdo tripartito con  otros agentes aeronaves y controladores aéreos, teniendo en cuenta la disponibilidad de la pista y rutas  de vuelo, el combustible con el que cuenta cada aeronave, su autonomía, afluencia de trafico, entre  otros aspectos para conseguir un lugar en la cola de aterrizajes. El objetivo del sistema se centra en  alcanzar la maximización del aprovechamiento de los recursos aeroportuarios en orden de aliviar la  congestión en los mismos. Además, el sistema deberá gestionar los recursos para que cada uno de los  aviones pueda realizar sus vuelos de forma segura. Ante el crecimiento de la congestión del tráfico  aéreo la autoridad de aviación civil australiana1 ha dado origen al programa de mejoramiento del  manejo de tráfico aéreo. En un desarrollo conjunto, con el Instituto de Inteligencia Artificial  Australiano, dio lugar a OASIS2[6]. Un sistema de administración de tráfico aéreo prototipo  desarrollado para el aeropuerto Kingsford Smith de Sydney. Dicho proyecto fue tomado como  referencia para este trabajo. Considerando el nuevo enfoque innovador sobre la problemática, se ha  considerado el concepto importante y de especial auge dentro del área del control de vuelo: el vuelo  libre (Free Flight). El control de vuelo clásico, utilizado, se ha dejado de lado en favor de la  implementación del vuelo libre. Los aviones, así, son sus propios controladores. Pueden elegir sus  propias rutas de navegación, velocidad, altura, regimenes de ascenso y descenso, maniobras, en este  contexto el objetivo del sistema es la planificación de los aterrizajes de las aeronaves como se detalla  en la figura 1.El sistema le brinda total libertad al piloto para que vuele según sus convicciones, sin  embargo, un sistema de control de vuelo descentralizado analiza el ambiente con el objetivo de velar  por el cumplimiento del objetivo final: un vuelo seguro y sin inconvenientes. En consecuencia se  pretende  alcanzar un alto grado de cooperación  y la capacidad de trabajar por un mismo objetivo.       Figura 1. Objetivos del sistema    3. FREE FLIGHT  Free Flight [7], propone un sistema de control que abandona el monitoreo centralizado, desde tierra, del  tráfico para adoptar una arquitectura descentralizada y altamente cooperativa. El concepto descansa  sobre dispositivos digitales altamente precisos para la ubicación de aeronaves (GPS: Global  Positioning System), un sistema de comunicación de datos digital, que brinda información a todos los  involucrados, apoyados con  la automatización de gran parte de las tareas del los controladores aéreos.                                                              1 Denominada Airservices Australia    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1324 Bajo el “vuelo libre” los controladores no acaparan responsabilidades sino que ellas son delegadas a los  pilotos. Los pilotos son sus propios controladores de vuelo, son libres de escoger trayectorias de  vuelos, altitudes, velocidades como alguna vez lo hicieron antes de que existieran los primeros sistemas  de control. El “vuelo libre” podría reducir la congestión de los aeropuertos más concurridos elevando  el promedio de aterrizajes por unidad de tiempo. El concepto de free Flight [8] se define como una  capacidad de operación de vuelo bajo condiciones de vuelo por instrumentos en el cual los operadores  tienen la libertad de elegir su trayectoria y velocidad en tiempo real. Las restricciones al tráfico aéreo  se imponen sólo para asegurar la separación, para evitar el exceso de capacidad de aeropuerto, para  prevenir vuelo no autorizado a través espacio aéreo de uso restringido/prohibido, y para garantizar la  seguridad del vuelo.    4. MODELO MULTIAGENTE PARA CONTROL AÉREO  El modelo propuesto se compone principalmente por cinco roles: Vigilancia y Control, Planificador,  Meteorólogo, Controlador, Piloto Aeronave; según se detalla en la figura 2.      Figura 2. Modelo de roles del sistema  El rol “Vigilancia y Control” es el encargado de individualizar cada aeronave que ingresa en el espacio  aéreo de su área de control. Deberá, entre otras cosas, informar la posición actual de la aeronave e  identificarla.   El rol “Planificador” sugiere un plan de vuelo eficaz con base en el análisis de la aeronave, las  condiciones climáticas y la programación de aterrizaje. Se encarga de armar un plan de vuelo eficaz  para la aeronave y lo informa como directivas similares al formato de una ‘lista de tareas’ pendientes.  El rol “Meteorólogo” actualiza la información ambiental a través de los datos que puedan recoger los  distintos aviones en el aire junto con la que él mismo posee y además brindan los sensores   meteorológicos del área.   El rol “Controlador” es el designado para hacer el seguimiento del desempeño de los aviones próximos  al aeropuerto. Además, es el encargado de detectar cualquier tipo de inconveniente, accidente o  problema que pudiera surgir, deberá corregir los planes de vuelo evaluando la situación actual de la  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1325 aeronave y, a partir de allí, componer una serie de modificaciones al plan de vuelo original.  Periódicamente verificará que la trayectoria del aeronave se encuentre dentro del margen de error  aceptable para el plan de vuelo estimado.  El rol “Piloto Aeronave” se encargará de tomar las decisiones de vuelo, por ejemplo, sobre: la  velocidad, la altitud, la dirección. Verificará que el plan de vuelo estimado esté dentro de las  posibilidades técnicas de la aeronave. Mantendrá actualizada, y disponible para el sistema, toda la  información del plan de vuelo de la aeronave. Todas las correcciones al plan de vuelo original serán  verificadas con el objetivo de constatar que sean compatibles con las capacidades de la aeronave.  El comportamiento de un agente aislado queda determinado por sus motivaciones individuales, sus   creencias acerca del mundo y sus propias habilidades. De este modo, esta caracterización resulta  insuficiente para modelar a un agente con una actitud cooperativa. La coordinación de los agentes, al  igual que en otros dominios, es un aspecto importante para lograr un comportamiento coherente. La  distribución de los datos y el control entre varios agentes autónomos a menudo deriva en situaciones de  conflictos como consecuencia de la existencia de distintos puntos de interés, los recursos escasos y las  motivaciones particulares u objetivos. El aspecto de la comunicación entre los agentes es un factor  fundamental a considerar. La tendencia actual, es utilizar lenguajes de comunicación de agentes  basados en la teoría de “speech acts” como por ejemplo KQML [9] y FIPA ACL [10]. En el caso de  JADE es un middleware que proporciona un entorno para desarrollar agentes, que explota el potencial  de la plataforma java. Otro aspecto importante es que el Framework, ofrece como servicios propios  funcionalidades básicas que requiere un agente y  permite centrarse en el desarrollo de los aspectos  lógico de los agentes, algunas de las características que presenta JADE en cuanto a su Portabilidad: La  API que proporciona JADE es independientemente de la red sobre la que va a operar, así como de la  versión de Java utilizada, teniendo la misma API para J2EE [11] , J2SE [4]  y J2ME [12] , Finalmente,  JADE simplifica la comunicación y la cooperación entre los agentes; los agentes JADE pueden  controlar su propio ciclo de vida, y pueden ser programados para que dejen de funcionar o empiecen a  hacerlo dependiendo del estado del sistema y de la función que debe realizar el agente; JADE cumple  con la especificación de FIPA [13], por lo tanto, puede comunicarse con agentes realizados en otros  entornos pero que utilicen este estándar el Framework de desarrollo de agentes provee un amplio  soporte para las comunicaciones entre agentes a partir del estándar FIPA ACL. Para ejemplificar este  esquema, consideremos el caso en que dos aviones se aproximen en una área de control a un aeropuerto  y se comunican con el controlador del área para establecer su lugar en la cola de aterrizajes. En este  caso, ambos agentes “Piloto Aeronave” ingresan en una situación de competencia en relación a un  recurso escaso, como lo es la pista de aterrizaje. Si bien éste no es un recurso agotable tiene la  particularidad que puede estar disponible sólo para un agente en un instante de tiempo puntual. Cada  uno de los agentes “Piloto Aeronave” tiene como uno de sus objetivos principales el ahorro de  combustible y la construcción de un plan de vuelo que apunte a la maximización de este recurso. Por  esta condición, al momento de ingresar al área de control, el agente “Piloto Aeronave” deberá  interactuar con el agente “Vigilancia y Control” y para conseguir un lugar en la cola de aterrizajes  deberá negociar con el “Planificador” quien tiene como objetivo lograr la mejor configuración del  espacio aéreo para todos los agentes involucrados, según se detalla en la figura 3.           XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1326 Figura 3. Diagrama de secuencias correspondiente al  ingreso de un aeronave    Cada uno de los agentes que componen el sistema ha sido modelado desde la perspectiva BDI [14].El  agente “Vigilancia y Control” tiene el deseo, objetivo de lograr individualizar e identificar cada  aeronave que se desplaza en el espacio aéreo. Sus creencias se conforman de la información que le  suministran distintos sensores de vigilancia y control del espacio aéreo. Finalmente, sus intenciones o  acciones se focalizan en la transmisión de información actualizada acerca de los aviones en su espacio  aéreo. El agente “Planificador” posee el deseo de programar los aterrizajes para lograr que el agente  “Piloto Aeronave” pueda trazar un plan de vuelo eficaz. Sus creencias se basan en la meteorología, sus  conocimientos acerca de la aeronave en cuestión, su autonomía de vuelo y la disponibilidad del recurso  de la pista de aterrizaje, entre otras. Sus intenciones son la gestión de la cola de aterrizajes en el  aeropuerto y sugerir al piloto las maniobras más convenientes para su aproximación dentro del área  terminal con destino  a su aeropuerto de  aterrizaje. El agente “Meteorólogo” tiene el objetivo de  actualizar la información ambiental a través de la información que puedan recoger los distintos aviones  en el aire (sus creencias), apoyado con los sistemas meteorológicos, su intención es brindar un modelo  de ambiente meteorológico. El agente “Controlador” debe detectar los inconveniente, potenciales  incidente o problema que pudiera surgir con las rutas de vuelo de los agentes “Pilotos aeronaves”.  Recabará información a través de mensajes con los distintos agentes “Pilotos aeronaves” en vuelo y  con el agente “Vigilancia y Control” quien le brindara la situación  del espacio aéreo. Entre sus  actividades se enumeran la evaluación de las rutas de vuelo y la configuración del espacio aéreo y la  alerta de colisiones al agente “Piloto aeronave”. El agente “Piloto Aeronave” tiene el deseo de alcanzar  un aeropuerto de destino llevando adelante un vuelo seguro y eficaz sin complicaciones. Se nutre de la  información que obtiene del amplio rango de instrumental de vuelo (TCAS [15], GPS) para generar sus  creencias. Para alcanzar su objetivo llevará adelante verificaciones periódicas del plan de vuelo y la  corrección del mismo ante una alerta de colisión.    5. MODELO DE SIMULACIÓN PARA EL CONTROL  AÉREO EN AMBIENTE  DE VUELO LIBRE  En este contexto según se representa sintéticamente en la figura 4, se propone una aproximación para el  ambiente de vuelo libre caracterizado en un entorno de vida artificial [16], seleccionado este por su  capacidad para la simulación de sistemas descentralizados donde los autómatas celulares resultan un  marco para explorar fenómenos de autoorganización, presentes en el concepto de vuelo libre, donde no  se cuenta con diseñador centralizado para el control aéreo. Observando las características de un hábitat  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1327 de vida artificial se considera como Depredadores a los agentes “Piloto aeronave” y como Presas se  presentan dos tipos constituidos por la presa principal denominada como presa tipo pista de aterrizaje,  correspondiente a su destino y la presa tipo espacio aéreo (X,Y,Z) a ocupar por un agente “Piloto  aeronave” durante un  tiempo (T), suficiente para asegurar las separaciones entre aeronaves y evitar  colisiones, si bien son un recurso renovable como alimentos para sus depredadores, se encuentran  agotados durante un margen de tiempo al ser consumidos por otro depredador representado por el  agente “Piloto aeronave”. Como Hábitat de los Depredadores y Presas se considera el espacio aéreo y  aeroportuario  a través de parcelas o celdas organizadas en una grilla de modo similar al de un  autómata celular con depredadores que deambulan por encima (aeronaves que vuelan en el espacio  aéreo), cada parcela se caracteriza por la acción de los agentes Planificador, Controlador, Meteorólogo,  Vigilancia y Control y la presencia de una Presa representada por el espacio aéreo que se corresponden  con un juego de valores limites XYZ de la celda salvo los casos de espacios prohibidos de vuelo o  restringidos temporalmente para el uso en los cuales no se consideran presas, las celdas podrán estar  ocupadas por depredadores agentes “Piloto aeronave” respetando los liftes de separación entre  aeronaves considerados para garantizar la seguridad aérea, y también por presas del tipo pista de  aterrizaje. La energía de los depredadores (agentes “Pilotos aeronaves”) se considera en función del  consumo de combustible, pudiendo agotarla o no para poder cumplir su vuelo en los márgenes de  tiempo establecidos, ocurriendo la muerte del depredador si se agota su combustible o aterriza fuera de  los márgenes de tiempo establecidos. El observador mira a lo depredadores (agentes “Pilotos  aeronaves”) y administrar las tazas de creación de nuevos depredadores y su agotamiento en función de  los márgenes de tiempo que establece para el consumo de su presa, (representado por el arribo de la  aeronave a su pista del aeropuerto de destino), además  el observador  monitorea la actividad de las  presas y de las parcelas o celdas del hábitat (espacio aéreo). La extinción de la especie se daría por el  agotamiento de la energía de  los depredadores, y el consumo de sus presas en un instante de tiempo de  acuerdo a los márgenes seleccionados por el observador para lo cual los depredadores agentes “Pilotos  aeronaves” no puedan consumir a su presa elegida (espacio aéreo X,Y,Z durante un tiempo T, pista de  aterrizaje del aeropuerto de destino).    Figura 4. Ambiente de vuelo libre modelado en un entorno de vida artificial.    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1328 Para el desarrollo del modelo se analizaron herramientas orientadas a agentes como JADE [17], la  especificación FIPA ACL [10] para mensajes interagentes, Protegè [18] para crear una ontología del  sistema, MaSE [119] como metodología., StarLogo [20] para vida artificial. En Jared et al [21] presenta  los resultados de una simulación basada en teoría de juegos para la toma de decisiones en vuelo libre  por parte de las aeronaves considerando una actitud altruista especifica para la negociación en un  ambiente multiagente a fin de evitar incidentes aéreos entre aeronaves en un entorno de vuelo libre, sin  embargo en nuestra propuesta se considera que las aeronaves cuentan con sistemas TCAS (sistema de  alerta de colisión entre aeronaves) para alertar incidentes aéreos en su espacio aéreo y realizar las  maniobras de evasión a fin de mantener una separación vertical y horizontal considerada para las  aeronaves que conforman el hábitat de simulación en un entorno de vida artificial. La Simulación  implementada en nuestra propuesta se recrea en un ambiente de vida artificial, que se caracteriza por:   Permitir la simulación en nuestro caso de diez aviones en competencia por los aeropuertos, la  simulación se puede llevar adelante con dos (2) o tres (3) aeropuertos.   Energía Máxima de los aviones. Este concepto es análogo a la cantidad de combustible que el  avión posee para realizar su vuelo. Máximo parametrizable: cuatrocientos (400)   Consumo de energía. Es la cantidad de energía que un avión consume al viajar una (1) unidad  de distancia en busca de alcanzar su destino.   Capacidad del aeropuerto. Es la cantidad de aviones que podrán permanecer en un aeropuerto,  en un instante de tiempo dado, antes de volver a despegar. Máximo diez (10).  El Hábitat Simulado evoluciono enmarcado en las siguientes reglas:   Los aviones tienen como objetivo lograr su aterrizaje en un destino conocido.    Los aviones tienen un consumo de energía ligado a la distancia recorrida en busca de un  aeropuerto. Al acabarse dicha energía si el avión no alcanza su destino principal o alternativa,  sufre el siniestro.   Los aviones no vuelan a una altura predefinida. Cada avión elige su altitud arbitrariamente.   Cuando un avión llega a destino sólo hará el aterrizaje si la capacidad del aeropuerto no está  colmada. De ser así, el avión recibe un nuevo destino de alternativa.   Cuando el avión logra el aterrizaje permanecerá en “reposo” en el aeropuerto durante un  tiempo, equivalente al que estuvo en vuelo, con el objetivo de recuperar la energía perdida.  Mientras el avión permanece en el aeropuerto ocupa una plaza de capacidad disponible del  aeropuerto.   Los aviones en maniobras de despegue o de aterrizaje demoran un determinado tiempo “X”  ocupando una plaza de capacidad disponible del aeropuerto, tiempo en el cual el recurso no  puede ser explotado por otro avión.    Los aviones en vuelo deben respetar un límite de separación, tanto vertical como horizontal.  Esto define un área de seguridad que permite que el avión vuele sin riesgos de colisión.   Se presenta en la figura 5 la pantalla principal del simulador, en la que se genera el ambiente para  iniciar la simulación utilizando los valores parametrizables detallados.     XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1329  Figura 5. Pantalla de simulación en Starlogo.  La interfase de representación de la simulación cuenta, además con un monitor ubicado en el sector  superior para medir el comportamiento de las aeronaves en  su hábitat  en tiempo real, brindando  información en primer lugar: relacionada con la disponibilidad de combustible/energía de los aviones,  según se detalla en la figura 6. El segundo se brinda información de la evolución de la separación  vertical promedio calculada a partir de la separación con respecto al avión más próximo que registra  cada avión, según se detalla en la figura 7      Figura 6. Disponibilidad de combustible. Figura 7. Evolución de la separación vertical    El tercer lugar se brinda información de la evolución de la separación horizontal promedio calculada a  partir de la separación con respecto al avión más próximo que registra cada avión. Según se detalla en   la figura 8.    Figura 8. Evolución de la separación horizontal Figura 9. Distribución de la altitud entre aviones.     El cuarto lugar se brinda información de la  distribución de la variable altitud de los diferentes aviones  en vuelo. Según se detalla en la figura 9    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1330 7. RESULTADOS  Nuestro simulador han sido puesto a prueba en ambientes de simulación similares a los que otros  investigadores han utilizado [22], [23], [24], [25]. El simulador elegido es el Starlogo dado que nos  permite trabajar con ambientes densamente poblados sin problemas. Si bien es cierto que el potencial  de esta herramienta es muy alto, también es cierto que dada la alta demanda de recursos de hardware  necesarios para la simulación, algunas de las restricciones que hemos impuesto para nuestro modelo se  deben a esta causa. Entre ellas, podemos citar la cantidad máxima de aviones o aeropuertos a simular.  El espacio aéreo, una de las “presa”, es simulado a través de las “celdas” definidas en la grilla de  simulación. A su vez, la “presa” aeropuerto, es también identificada por una “celda” pero  particularmente diferente a las anteriores. Luego, los aviones serán agentes que reemplazarán las  conocidas tortugas del simulador. Las distancias recorridas por los aviones se miden en cantidades de  celdas, y el tiempo se mide en ciclos de reloj. Por cada ciclo de reloj el ambiente artificial evoluciona  bajo las reglas que lo enmarcan. A pesar de que el ambiente de simulación está preparado para soportar  aviones volando a distintas alturas, para llevar adelante las pruebas se restringirá la altura de vuelo a un  valor único, igual para todas las instancias de agentes aviones. Así, los resultados podrán ser  comparables a los obtenidos en otras investigaciones ya mencionadas. Además, como otros autores  señalan, de esta forma, la facilidad para generar ambientes densamente poblados es mucho mayor, y  nos brinda la posibilidad de experimentar la robustez y confiabilidad del modelo planteado. Los  aviones, por cada ciclo de reloj, se trasladan hacia alguna celda contigua a la que ocupan. Cada una de  las decisiones que el avión deba tomar acerca de su orientación y dirección de vuelo, se hace efectiva  de forma inmediata y en el instante anterior a su traslación a su próxima celda. Los aviones cuentan  con una “zona de exclusión” a su alrededor. Dicha zona, idealmente, libre de aviones, marca un sector  de potencial peligro de colisión para las aeronaves. Los aviones escogen libremente su ruta de vuelo,  sin embargo, cuando su ruta los lleva a ingresar en la “zona de exclusión” de otro avión, una señal de  colisión se dispara y comienzan las acciones de evasión.    7.1 Escenario de espacio aéreo abierto  Este escenario se basa en la experiencia [22], y refleja el vuelo a cielo abierto. Los aviones no tienen  otros obstáculos más que otros aviones y todos tienen un destino que alcanzar. En la experiencia  original, como unidad de medida había sido elegida la milla. En nuestra experiencia, para poder  generar una escala de equivalencias, cada milla será simbolizada por una “celda”. La disposición  inicial de los aviones genera un círculo en cuyo interior se apostarán los distintos puntos de destinos de  los aviones. Cada avión se instancia con una zona de seguridad a su alrededor de modo que pueda  ingresar al ambiente de simulación libre de potencial peligro de colisión. La tabla 1 muestra los  resultados promedio en un esquema de pruebas similar al  estudiado por Hill. Aún en las densidades de  tráfico más, ningún incidente fue reportado. Los incidentes, y alarmas de colisiones en los escenarios  de baja densidad eran esporádicos y sólo se hizo presente una fuerte tendencia en alza a partir de que  las poblaciones alcanzaban los 70 aviones.    7.2 Punto de intersección  Este escenario, descrito en [23], propone un punto de intersección entre dos rutas de vuelo definidas  para un conjunto de aviones. Cada flujo de aviones, sigue una ruta a lo largo de uno de los ejes  cartesianos de la grilla de simulación; ya sea el eje “X” o el eje “Y”. De esta forma, en punto fijo  ambas rutas confluyen materializando una zona de riesgo de colisión para los aviones. Los mismos,  inicialmente parten de un punto fijo de la grilla y se les asigna un destino al lo otro lado de la grilla, de  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1331 modo que suponga una ruta de vuelo en línea recta. En todos los casos en que fue realizada la prueba,  no se han constatado colisiones entre los aviones. Por el contrario, se divisó un bajo nivel de alarma de  incidentes para el rango de aviones inferior al los 20 aviones y una alta eficacia al igual similar a la que  se había podido experimentar en la simulación a espacio abierto. Sin embargo, se encontró un punto de  inflexión en el cual los resultados marcan una leve tendencia al crecimiento tanto de las alarmas de  incidentes como de la pérdida de eficiencia. Este patrón de comportamiento se comprobó como  resultado de la disminución del delta de espacio de separación de las naves, a la hora de su  instanciación, para alcanzar los límites exigidos por la prueba.    Aviones Incidentes Eficiencia (%)  Aviones Incidentes Eficiencia (%)  > 20 25 – 35 < 98.5    90 0/20 97.0  < 20 < 15 99.0 – 99.5    80 0/15 97.1       70 0/9 97.6       60 0/1 97.9       50 0/0 98.1  Tabla 1. Resultados de simulación de espacio  aéreo abierto   Tabla 2. Resultados de simulación de Punto de  Intersección    7.3 Escenario “Same Point - Same Time”  Este no es un escenario real, pero bien sirve para estudiar la reacción del sistema en condiciones  delicadas que exigen un sistema robusto. Para la prueba, la población de aviones se dispone formando  un círculo y a cada avión se le asigna un punto de destino al otro lado del círculo obligándolo a trazar  una ruta de vuelo ideal a través del centro del mismo. De esta forma, si todos ellos siguieran esta ruta,  alcanzarían el punto central del círculo al mismo tiempo desencadenando la tragedia. La figura “10”  muestra la evolución del ambiente artificial. El trabajo de [25], para esta experiencia, describe que  durante la simulación se forman patrones de organización de los aviones en forma de “olas”. En  nuestro caso, dichos patrones no se generan, sin embargo, se consigue un patrón distinto. En principio,  los aviones se acercan hacia el interior del círculo como, sin advertir el conflicto. Luego, cuando el  conflicto es tangible, los aviones se empiezan a entrelazar desdibujando toda disposición geométrica.  Una vez que los aviones logran atravesar el centro del círculo, se vuelven a trasladar en formación   geométrica hacia su destino.                                  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1332      Aviones Incidentes Eficiencia (%)  32 0/29 90.4  28 0/23 91.7  24 0/17 93.4  20 0/13 95.0  16 0/10 97.0  12 0/7 97.5  Tabla 3.Resultados de simulación de Same Time”     Vuelos Incidentes Eficiencia (%)  800 8 97.0  1300 0/10 97.4  Tabla 4. Resultados de Simulación de Aeropuertos      Figura 10. Captura de pantallas para la simulación  “Same Point Same Time”    Una vez más, los resultados no arrojan siniestros registrados. Sin embargo, el índice de incidentes  conseguido fue superior a las experiencias de Hill. Esto se debe, principalmente, al método de  resolución de la problemática. Como se explicó recién, en aquella simulación, los aviones, para sortear  la posibilidad de incurrir en colisiones frontales o llegar a destino por una ruta lo más directa posible,  generan “revoluciones” u “olas” haciendo que la eficiencia del algoritmo baje notablemente. En  nuestro caso, los aviones al intentar seguir una ruta lo más parecida a la ideal, y sólo maniobrando para  evitar violar la “zona de exclusión” de otro avión, los retrasos por maniobras caen de forma  pronunciada aumentando visiblemente la eficiencia.    7.4 Escenario: Aeropuerto  En un escenario real, los aviones trabajan uniendo destinos distantes. Cada avión en un instante de  tiempo T tiene un destino que alcanzar. En su progresión el avión será responsable de elegir la ruta más  eficiente. En su camino sólo tendrá un par de restricciones: otros aviones en vuelo; y la capacidad del  aeropuerto en el que debe aterrizar. En espacio aéreo abierto, el avión deberá resolver los conflictos  evitando violar las zonas de exclusión de otros aviones. Al intentar aterrizar el avión deberá tener en  cuenta: no interrumpir otro avión en maniobras de aterrizaje o despegue; y verificar que la capacidad  del aeropuerto no esté comprometida. En caso de que el avión no pueda llevar adelante el aterrizaje,  queda en espera realizando circuitos de espera en las cercanías del aeropuerto hasta que el mismo  quede libre.    8. CONCLUSIÓN Y FUTURAS LÍNEAS DE INVESTIGACIÓN  La capacidad de coordinación y de cooperación de los agentes para cumplir con los objetivos facilita  una aproximación de un modelo multiagentes bajo una arquitectura basada en creencias, deseos e  intenciones de los agentes que implementan el concepto de Free Flight, los aviones, así, son sus  propios controladores. Pueden elegir sus propias rutas de navegación, velocidad, altura, regimenes de  ascenso y descenso, maniobras. Para el apoyo, análisis y gestión de una ambiente de vuelo libre dada  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Procesamiento Distribuido y Paralelo _________________________________________________________________________     1333 sus características propias de un sistema descentralizado sin un director central a cargo del control  aéreo, se considero la caracterización de un modelo de simulación basado en un entorno de vida  artificial a fin de observar la  evolución del espacio aéreo bajo un concepto de Free Flight, para una  población inicial de aeronaves con intenciones diversas, respecto a sus aeropuertos de destino. Dentro  de las futuras líneas de investigación se considera la elaboración de un Framework para correr el  modelo multiagente destinado al apoyo, análisis y gestión de una ambiente de vuelo libre, y la  elaboración de ontologías para el concepto de Free Flight	﻿free flight , vida artificial , trafico aéreo , vuelo libre , air traffic controller , artificial life	es	22975
40	Verificando diseños BON mediante Alloy	﻿ En este artículo presentamos una técnica para traducir diseños estructurales expresados en el lenguaje BON, al lenguaje formal Alloy. En donde, la principal ventaja de la traducción es que puede realizarse automáticamente mediante herramientas de software. Adicionalmente, esta metodología puede ser usada para validar propiedades sobre los diseños utilizando el Alloy Analyzer. Para finalizar, mostramos la aplicación a un caso de estudio de Darwin Tool, una herramienta que implementa parte de esta traducción. Palabras Clave: Métodos Formales, Ingeniería de Software, Orientación a Objetos, Diseños Arquitecturales, Alloy, BON. CACIC 2005 - II Workshop de Ingeniería de Software y Bases de Datos 1. Introducción Con la aparición del paradigma de orientación a objetos, surgieron diferentes lenguajes de modelado para soportar esta nueva tecnología. En los últimos tiempos apareció como una alternativa a UML (Unified Modeling Language [5]), uno de los más utilizados, el lenguaje BON (Bussiness Object Notation [3]). En [4] se muestra como BON permite una aplicación transparente de los métodos formales. Esto se debe principalmente a su notación simple y a que fue diseñado para soportar el método de Diseño por Contratos, incluyendo un lenguaje formal de aserciones. Otras características importantes de BON son Seamlessness (desarrollo sin transiciones), que intuitivamente significa que el paso de una etapa en el desarrollo a la siguiente es directo, y Reversibilidad, que implica que los cambios realizados en una etapa del desarrollo son reflejados en las anteriores. Estas favorecen el reuso y simplifican el mantenimiento del software. El presente trabajo se basa en la integración de dos herramientas que pueden ser utilizadas en la etapa de diseño del desarrollo de un sistema de software, la notación BON, y el lenguaje formal Alloy. Mostramos aquí una forma de realizar la traducción de un diseño en BON a una especificación Alloy, basada en [4], la cual puede efectuarse por medio de herramientas de software, de manera automática. Además mostramos un caso de estudio en el que se utilizó una herramienta implementada por los autores, llamada Darwin Tool, que permite traducir un subconjunto de BON a Alloy. La estructura de este artículo es la siguiente: en la secciones 2 y 3 se presenta una breve introducción a los lenguajes BON y Alloy respectivamente. En la sección 4 se muestra la traducción de BON a Alloy. En 5 se presenta la herramienta Darwin Tool y en 6 una aplicación de la misma a través de un caso de estudio. Finalmente, presentamos las conclusiones y los trabajos futuros. 2. Introducción a BON BON (Business Object Notation)[3, 7] es un lenguaje de modelado para especificar y describir sistemas bajo el paradigma de orientación a objetos. Una de sus características es que no sólo provee un lenguaje gráfico para la descripción de sistemas, sino que también posee un proceso recomendado para el desarrollo de software. Otra característica importante es que todo modelo gráfico BON tiene una forma textual equivalente. BON es extremadamente simple en comparación a otros lenguajes: solo posee 2 diagramas de modelado, y no permite diferentes vistas de un diseño, lo cual evita las inconsistencias que puedan ser introducidas al existir distintas descripciones de un mismo componente. BON fue diseñado para soportar tres técnicas principales: Seamlessness, Reversibilidad y Diseño por Contratos. Seamlessness: se refiere al principio de utilizar un conjunto consistente de conceptos y notaciones a través del ciclo de vida del software, desde el análisis a la implementación y mantenimiento. Por lo que el paso siguiente en cada etapa del desarrollo es directo. Los beneficios de seamlessness son numerosos. Reversibilidad: los cambios producidos en una etapa del desarrollo pueden ser automáticamente reflejados en las etapas anteriores. Por ejemplo, un cambio en una clase de Eiffel es directamente reflejada en cambios sobre la clase de diseño. Diseño por Contratos: es una metodología de desarrollo de software orientado a objetos que se basa en el concepto de contrato. Un contrato es una especificación de las responsabilidades y características de una componente de software. Esta noción está basada en la idea de prepostcondición, introducida por Hoare y Floyd en los años 70 - lo que permite producir software de alta calidad y fácilmente mantenible. Al igual que otros lenguajes orientados a objetos, el principal mecanismo de especificaciones en un modelo BON es el concepto de clase. Una clase en BON introduce un nuevo tipo y se la considera un módulo. Las clases BON estan compuestas de: un nombre, un conjunto de features, y un contrato compuesto de precondición, postcondición e invariante de clase. El invariante es una aserción que todas las instancias de la clase deben satisfacer. El contrato especifica las funcionalidades de la clase, y es expresado utilizando el lenguaje de aserciones que posee BON, el cual está basado en la lógica de primer orden tipada. Los features (o características) de una clase pueden ser queries o commands: Queries: Dentro de esta categoría entran las funciones y atributos de los lenguajes tradicionales. Es importante destacar que las queries no producen cambios en el estado del sistema. Commands: Corresponden a los procedimientos de los lenguajes Orientados a Objetos tradicionales. Se ejecutan para producir cambios de estado. Cada feature tiene asociada una restricción de visibilidad, y puede tener diferentes modos de implementación, puede ser abstracta, efectiva o redifinida. Es interesante destacar que BON sólo soporta dos tipos de relaciones dentro de un diagrama de clases: Relación Cliente-Proveedor: indica que una clase (el cliente), utiliza algún servicio brindado por otra clase (el proveedor). Hay tres tipos de relaciones cliente-proveedor en BON: asociación, agregación y asociación compartida (shared association). Relación de Herencia: una clase hereda comportamiento de una o más clases padres. La herencia introduce una relación de refinamiento sobre los contratos de las clases asociadas. Además, en BON, una clase hijo es considerada un subtipo de sus clases padres. 3. Introducción a Alloy Alloy [1, 2] es un lenguaje formal, basado en la lógica relacional de primer orden, que permite modelar formalmente sistemas de software; especificando el espacio de estados, restricciones, y propiedades de los mismos. Un modelo Alloy está compuesto por signaturas, hechos, predicados, funciones y aserciones. Una signatura introduce un conjunto de átomos, y a la vez un tipo. Además en la misma declaración pueden definirse relaciones con otros tipos, como por ejemplo: sig Persona { DNI: Int } que define el tipo Persona, con una relación binaria llamada DNI, que asocia cada persona con su número de documento. Las relaciones pueden ser de cualquier aridad y pueden imponerse restricciones de cardinalidad sobre las mismas como en: sig Persona { DNI: Int, trabaja_en: set Empresa } sig Empresa { empleados: set Persona, id: empleados -> one Int } en donde la relación trabaja_en: Empresa×Empresa relaciona a una persona con las empresas en que trabaja, e id: Empresa×Empleados × Int es una relación ternaria que a cada empleado de la empresa le asigna un único entero que es su identificador. EnAlloy, los conjuntos introducidos por diferentes signaturas son disjuntos a menos que se declare una signatura como extensión de otra. Esto se lograría de la siguiente forma: supongamos que, en el ejemplo anterior, queremos diferenciar a las personas que trabajan para alguna empresa de las que no: sig Persona { DNI: Int } sig Empleado extends Persona { trabaja_en: set Empresa } sig Empresa { empleados: set Empleado, id: empleados -> one Int } Aquí, la signatura Empleado es un subconjunto del asociado a Persona y además un nuevo tipo. Los hechos (o facts) son fórmulas que restringen los valores posibles para los conjuntos y relaciones. Por ejemplo, si queremos asegurar que dos personas diferentes no tienen el mismo DNI: fact unicoDNI { all p1, p2: Persona | p1 != p2 => p1.DNI != p2.DNI } además, un empleado está relacionado a una empresa por medio de la relación trabaja_en si y sólo si pertenece al conjunto de empleados de la misma: fact { all emp: Empleado | all e: Empresa | e in emp.trabaja_en <=> emp in e.empleados } En las fórmulas anteriores el operador (.) representa la composición de relaciones y la palabra clave in significa pertenencia a un conjunto. También se pueden definir funciones, fórmulas parametrizadas que pueden ser invocadas en diferentes contextos, reemplazando sus parámetros formales por parámetros actuales del mismo tipo, así como predicados, los cuales difieren de las funciones en que, en lugar de expresiones, son fórmulas parametrizadas que no retornan valores. Un ejemplo de predicado sería el siguiente: pred Despedir (e, e’: Empresa, emp: e.empleados, emp’: Empleado){ e’.empleados = e.empleados - emp e’.id = e.id - emp -> (emp.(e.id)) emp’.trabaja_en = emp.trabaja_en - e emp’.DNI = emp.DNI } que quita al empleado emp del conjunto de empleados de la empresa e. Aquí las variables primadas e’ y emp’ indican el estado de la empresa e y del empleado emp posterior a la ejecución de la operación. Por último, en un modelo Alloy podemos definir aserciones, es decir, fórmulas que el diseñador espera que sean válidas, con respecto a las restricciones impuestas por medio de los hechos. Por ejemplo, podemos esperar que después de despedir a un empleado, este ya no tenga ninguna relación con la empresa: assert despedido { all e1,e2: Empresa | all emp1: e.empleados | all emp2: Empleado | Despedir(e1, e2, emp1, emp2) => not (e2 in emp2.trabaja_en) && not (emp2 in e2.empleados) } Alloy posee dos tipos de comandos que se pueden ejecutar sobre modelos: Check assert for n: Busca un contraejemplo de la aserción assert, instanciando cada signatura con a lo sumo n átomos. Run f for n: Busca un modelo para la función o predicado f, n tiene el mismo significado que el comando anterior. Una particularidad de Alloy es que fue diseñado conjuntamente con una herramienta, el Alloy Analyzer, que permite simular modelos y ejecutar los comandos mencionados anteriormente, con el fin de chequear que se cumplan ciertas propiedades de los modelos. Cabe aclarar que el hecho de que el Alloy Analyzer no encuentre un modelo para una aserción, función o predicado, no significa que la aserción se verifique, ni que la función o predicado sea insatisfacible, ya que para poder realizar las verificaciones es necesario imponer una cota en la cantidad de átomos de las signaturas y relaciones. A pesar de esto, una gran cantidad de aplicaciones prácticas pueden beneficiarse de la utilización de este lenguaje. 4. La traducción BON a Alloy Como mencionamos anteriormente, se desarrolló una forma de traducir BON a Alloy. Podemos presentar esta traducción mediante los siguientes items. 4.1. Clases y estados El primer paso en la traducción es expresar los tipos de un modelo BON en la especificación Alloy. Para esto, se introduce en la especificación una signatura por cada clase en el modelo. Por ejemplo, si tenemos la clase Persona debe agregarse a la especificación: sig Persona { } La signaturas no tienen estructura interna debido a que esta se expresará en una signatura más general llamada State. Intuitivamente, State representa los posibles estados del sistema. Cabe destacar que State simula el concepto de estado en Alloy. Por ejemplo, si tenemos las clases A, B y C, la signatura State debería ser la siguiente: sig State { as: set A, bs: set B, cs: set C } intuitivamente as, bs y cs son las posibles instancias de A, B y C respectivamente. 4.2. Términos y fórmulas La traducción de las fórmulas es muy sencilla. Los operadores se traducen según la siguiente tabla: Los términos como t.query que representan una navegación se traducen indicando un estado BON Alloy and && or || implies => iff <=> not ! x : T x : T x.r x.r determinado. De esta manera, lo anterior se traduce en: t.(s.query) siendo s una variable de la signatura State. 4.3. Queries La traducción de las queries depende principalmente de su contrato y del tipo de retorno. Una query sin contrato, y cuyo tipo de retorno es no Booleano, se traduce en una relación de la signatura State, como se muestra en el siguiente ejemplo: class C feature q: T -- T /= Bool end el modelo anterior se traduce en: one sig undef { } sig C { } sig State { cs: set C, ts: set T, q: cs -> one (ts + undef) } La relación q de State asocia cada instancia de C con un valor de tipo T, el cual representa el valor del atributo q en un estado particular. Notemos que se agregó una signatura llamada undef, cuyo único átomo representa el valor void de BON. Que una instancia de la clase C este asociada al valor undef por medio de q en un cierto estado s, indica que dicha instancia no posee un valor para el atributo q en s. El caso de una query con contrato es similar al anterior, sólo que en este caso debemos restringir los conjuntos de instancias de las clases a aquellos que respeten el contrato de la query. Esto se logra introduciendo hechos a la especificación Alloy. Por ejemplo, equipemos la query q anterior con pre y postcondición: class C feature q: T require pre ensure post end end ahora su traducción es: sig C { } sig State { ts: set T, cs: set C, q: cs -> one (ts + undef) } pred Pre_q(s: State, self: s.cs) { τ(pre) } pred Post_q(s: State, self: s.cs, result: s.ts) { τ(post) } fact { all s: State | all self: s.cs | Pre_q(s, self) && Post_q(s, self, self.(s.q)) } En donde τ es una función de traducción de fórmulas Alloy a aserciones BON, que implementa las reglas introducidas en la tabla 1. El caso de las queries Booleanas es diferente a los anteriores ya que Alloy no provee el tipo Bool, por lo que para utilizarlo deberíamos introducirlo primero, junto con todas sus operaciones. El problema que surge es que si agregamos los Booleanos a cada especificación, la verificación sería más ineficiente. La solución propuesta es traducir los atributos Booleanos como predicados, para poder utilizar las operaciones Booleanas de Alloy sobre ellos. Esto se llevaría a cabo de la siguiente forma: class C feature q: Bool require pre ensure post end end se traduce en: sig C { } sig State { cs: set C } pred Pre_q(s: State, self: s.cs) { τ(pre) } pred Post_q(s: State, self: s.cs) { τ(post) } pred q(s: State, self: s.cs) { Pre_q(s, self) Post_q(s, self) } 4.4. Commandos Un comando se traduce en un predicado, que tiene como parámetros adicionales dos estados, s y s’, donde intuitivamente s representa el estado del sistema anterior a la ejecución del comando, y s’ representa al estado posterior de la ejecución del mismo. Veamos un ejemplo: class C feature q1: T1 -- T1 /= Bool q2: T2 -- T2 /= Bool cmd require pre ensure post end end suponiendo que cmd modifica sólo el atributo q1, el modelo anterior se traduce en: sig C { } sig State { cs: set C, q1: cs -> one (t1s + undef), q2: cs -> one (t2s + undef) } pred Pre_cmd(s: State, self: s.cs) { τ(pre) } pred Post_cmd(s, s’: State, self: s.cs) { τ(post) self.(s’.q2) = self.(s.q2) Delta_C(s, s’, self) } pred cmd(s, s’: State, self: s.cs) { Pre_cmd(s, self) Post_cmd(s, s’, self) } pred Delta_C(s, s’: State, self: s.cs) { s’.cs = s.cs all c: s.cs - self | c.(s’.q1) = c.(s.q1) && c.(s’.q2) = c.(s.q2) } Notar que, en la postcondición del método, se mencionan todos los atributos de self que no cambian. Esto se debe a una incompatibilidad semántica entre BON y Alloy. Mientras que en el primero los atributos que no se nombran en una postcondición se supone que no cambian, en Alloy los atributos que no se mencionan pueden tomar cualquier valor. Por esta razón se introduce el predicado Delta_C, que asegura que todos los objetos que no se nombran en la postcondición no sufren cambios. 4.5. Los invariantes Una de las ventajas más relevantes de la traducción de BON hacia Alloy es que nos permite verificar automáticamente que: Los comandos mantienen a los invariantes. Los invariantes pueden ser satisfacibles, es decir, no son inconsistentes. Para traducir un invariante se utilizan los párrafos pred y también los assert, los primeros sirven para expresar la fórmula lógica del invariante, y los segundos nos permiten verificar que los comandos mantienen a los invariantes. Un punto a destacar es que los predicados que expresan a los invariantes son dependientes de los estados, es decir, estos toman como parámetro a un estado. Ilustremos estas nociones con un ejemplo: class C feature com require pre ensure post end invariant inv end las signaturas de la clase y State se generan como se mostró anteriormente, y el invariante se traduce: pred C_Inv(s: State) { all o: s.cs | τ(inv) } Aquí es importante destacar que la cuantificación sobre s.cs se debe a que el invariante predica sobre los elementos de la clase C. Falta decir que el comando com debe mantener el invariante, esto puede expresarse por medio de un assert, de la siguiente forma: assert { all s, s’: State | all c: s.cs | C_Inv(s) && C_com(s, s’, c) => C_Inv(s’) } En el caso de que una especificación tenga diversos invariantes, supongamos: Inv1,..., InvN debe verificarse que exista un modelo que satisfaga a todos (es decir, que la especificación es consistente), esto puede hacerse por medio de un fórmula que combine a todos, es decir: pred Consistent(s:State) { Inv1(s) && Inv2(s) && ... && InvN(s) } Y luego con el comando run Consistent se puede verificar automáticamente el modelo con la herramienta Alloy Analyzer. 4.6. Verificando Herencia La herencia es una de las herramientas más importantes de los lenguajes orientados a objetos, y como tal es soportada por BON. Es interesante analizar como la herencia influye en los contratos, a saber: Los invariantes de las superclases deben ser satisfechas por las subclases. Las precondiciones de los métodos pueden ser solo debilitadas por las subclases. Las postcondiciones de los métodos pueden ser solo fortalecidas por las subclases. Los items redefinidos pueden solo tener el mismo tipo, o tipos descendientes de ellos. Teniendo en cuenta los anteriores items, puede decirse que la herencia es concebida como un refinamiento. Para tratar automáticamente con la herencia, BON agrega en los métodos de las subclases mediante una disyunción - la precondición definida en la superclase. De igual forma, en las postcondiciones se agregan - por medio de conjunciones - las postcondiciones de las superclases, cumpliendo de esta forma la idea de refinamiento. Similarmente, en las subclases se agregan por medio de conjunciones los invariantes de las superclases. Sin embargo, esta metodología puede traer algunos problemas: El invariante de una subclase puede ser contradictorio con el de su superclase, con lo cual la subclase nunca podría satisfacer su invariante. La postcondición de una rutina de una subclase puede ser incompatible con la postcondición de la superclase, lo cual significa que el proveedor de un servicio nunca podrá cumplir un contrato, disparándose una excepción siempre que este servicio sea invocado. La precondición de una rutina puede sufrir el mismo problema que las postcondiciones, provocando que ningún cliente pueda cumplir el contrato del servicio afectado. Para detectar estos problemas podemos utilizar Alloy, para lo cual se deben agregar las siguientes obligaciones de prueba (asserts) en la traducción: La conjunción de la precondición de una rutina heredada con la precondición original debe ser satisfacible. La conjunción de los invariantes debe ser satisfacible. La conjunción de la postcondición de una rutina heredada con la postcondición original debe ser satisfacible. Por otra parte, estructuralmente la relación de subtipo puede ser expresada por medio del modificador extends en Alloy 3.0. Como ejemplo veamos el siguiente modelo: class A feature q: A require pre ensure post end invariant inv end class B inherit A redefine q feature q: B require pre’ ensure post’ end invariant inv’ end la traducción de los tipos del modelo es: sig A {} sig B extends A {} sig State { as: set A, bs: set B, q: as -> one (as + undef) } { bs in as } (1) fact { all s: State | b: s.bs | b.(s.q) in s.bs } (2) En donde, la línea (1) garantiza que en los estados se va a preservar la relación de inclusión entre el conjunto de instancias de B y el conjunto de instancias de A. Mientras que en (2) se expresa la restricción de que q está redefinida en B. El feature q de A se traduce de la manera usual. A continuación mostramos como se realiza la traducción de la query q de B. pred B_Pre_q(s: State, self: s.bs) { τ(pre’) } pred B_Post_q(s: State, self: s.bs) { τ(post’) } fact { all s: State | all self: s.bs | (B_Pre_q(s, self) || A_Pre_q(s, self)) (3) (B_Post_q(s, self) && A_Post_q(s, self)) } (4) Observemos que en la línea (3) se debilita la precondición original de la query q, y en la línea (4) se fortalece la postcondición original. Que exista un modelo que satisfaga dicha especificación, equivale a que pre’ y post’ son consistentes con la pre y postcondición original de q (pre y post), esto puede chequearse automáticamente con el Alloy Analyzer. Para finalizar veamos como probar la consistencia del invariante de B con respecto al invariante de A. Para esto introducimos un predicado que realiza la conjunción de los invariantes individuales de A y B: pred A_Inv(s: State) { τ(inv) } pred New_B_Inv(s: State) { τ(inv’) } pred B_Inv(s: State) { A_Inv(s) && New_B_Inv(s) } Ejecutando el comando run B_Inv n en el Alloy Analyzer logramos nuestro objetivo. En la sección siguiente describimos la herramienta Darwin Tool que implementa parte de la traducción expuesta anteriormente. 5. Darwin Tool Darwin Tool es una herramienta que permite editar diagramas en notación BON, y realizar automáticamente la traducción de los mismos a Alloy. Por el momento, la herramienta implementa una parte de la traducción presentada, que incluye: clases, relaciones cliente-proveedor y el subconjunto proposicional del lenguaje de aserciones provisto por BON. 5.1. Un Caso de Estudio utilizando Darwin Tool Hemos testeado el funcionamiento de la herramienta con varios casos de estudio, en este artículo mostramos sólo uno de ellos por cuestiones de espacio. El siguiente gráfico presenta en detalle un diseño BON. El cual fue traducido automáticamente por Darwin y verificado por la herramienta Alloy Analyzer, obteniendo un modelo del diseño que asegura su consistencia. Persona Current /= Current.trabajaen.duenio implies result /= Current and result.trabajaen = trabajaen   enpareja:Bool divorciar jefe:Persona pareja /= void and pareja.pareja = Current  pareja = void and (old pareja).pareja = void ! ! ? pareja:Persona ! result /= Current and (result /= void  implies pareja.pareja = Current) Empresa result.jefe = void duenio:Persona ! trabajen: Empresa Figura 1: Caso de estudio Uno de los modelos encontrados por el Alloy Analyzer para este caso de estudio es el siguiente: State0 Persona1 Persona0 Empresa2 Empresa1 Empresa0 undef0 Empresas jefe[Persona0] duenio[Empresa2] pareja[Persona1] trabajaen[Persona0] Empresas Empresas pareja[Persona0] duenio[Empresa0] jefe[Persona1] Personas trabajaen[Persona1] Personas duenio[Empresa1] Figura 2: Modelo encontrado por Alloy Analyzer 6. Conclusiones Consideramos que este proyecto hace un aporte hacia la integración de métodos formales con las técnicas de diseño informales, las cuales son usadas ampliamente en la actualidad. Por una parte, la poca utilización de los métodos formales en la industria se debe a que requiere cierta clase de conocimientos matemáticos por parte del usuario, y este tipo de entrenamiento es costoso en tiempo y dinero. Por otra parte, también sabemos que, en aplicaciones críticas, los métodos formales son necesarios si se quiere asegurar el correcto funcionamiento de las mismas. Aquí es donde Darwin Tool entra en escena, ya que, permite utilizar Alloy para validar ciertas propiedades de un diseño realizado en la notación BON, esto implica que la utilización de formalismos es intuitiva para el usuario, es decir, no necesita tener conocimientos matemáticos para aplicarlos. Este enfoque es completamente diferente al de las herramientas existentes en la actualidad, como por ejemplo zeves, que permite modelar en el lenguaje Z y realizar verificaciones con la asistencia del desarrollador; Isabelle/HOL, el demostrador semi-automático de teoremas; e inclusive el mismoAlloy, ya que fueron pensadas para usuarios con una preparación adecuada. 7. Trabajos Futuros Actualmente estamos extendiendo y testeando las herramientas y técnicas explicadas, lo cual implica un conjunto de tareas que consideramos como trabajos futuros: Formalizar la traducción y demostrar su correctitud. Extender el lenguaje BON para soportar operadores de la lógica relacional. Extender Darwin Tool para que soporte la edición y traducción de todas las construcciones de BON.	﻿BON , Métodos formales , Ingeniería de software , Orientación a Objetos , Diseños arquitecturales , Alloy	es	23067
41	CIMTool una herramienta para la definición de un diagrama de clases UML	"﻿  Model Driven Architecture es un framework de desarrollo de software  cuyo concepto clave es la transformación automática de modelos. Uno de estos  modelos, el Computer Independent Model (CIM), se usa para definir el modelo del  negocio. En este trabajo se presenta CIMTool, una herramienta que implementa un  proceso de definición automática del CIM. Este proceso aplica un conjunto de  reglas de transformación a modelos de requisitos basados en lenguaje natural  derivando un diagrama de clases UML. CIMTool puede integrarse con cualquier  herramienta CASE que acepte archivos XML. Así, el CIM derivado puede ser la  base para un desarrollo basado en MDA.            Palabras clave: Derivación automática de Modelos de Requisitos, Modelos de Requisitos basados en  Lenguaje Natural, Model Driven Architecture (MDA), Diagramas de clase UML.        1. Introducción  La Model Driven Architecture [1], conocida como MDA, es un framework para el desarrollo de  software definido por OMG [2] que se basa en la definición de modelos para cada aspecto y nivel  de abstracción de un sistema de software. Uno de los aspectos claves de MDA es la transformación  automática entre modelos. El primer modelo de un  desarrollo MDA es el modelo CIM (Computer  Independent Model) el cual describe el negocio independientemente del sistema de software que se  vaya a implementar. Aunque dentro del contexto de MDA tanto este modelo como su proceso de  construcción son los menos estudiados, han aparecido recientemente algunos trabajos relacionados  [3]. También pueden mencionarse algunas herramientas que si bien no fueron concebidas en el  contexto de MDA, construyen automáticamente modelos UML, en particular [4] deriva diagramas  de secuencia  a partir de casos de uso y [5] deriva diagramas de clase a partir del modelo  organizacional i*.   En trabajos anteriores [6] se ha presentado y formalizado un proceso de derivación que,  partiendo de modelos de requisitos basados en lenguaje natural, aplica un conjunto de reglas de  transformación para definir un diagrama de clases UML. Este diagrama puede ser considerado  como un CIM, ya que representa el negocio y no al sistema de software. Los modelos de requisitos  usados [7] son un modelo léxico para representar el lenguaje del negocio y un modelo de escenarios  para representar su comportamiento.   En este artículo se presenta CIMTool, una herramienta que implementa este proceso de  derivación permitiendo la definición automática de un diagrama de clases UML que puede ser  considerado un CIM dentro de un proceso de software basado en MDA. La herramienta genera  documentos XML por lo que puede integrarse perfectamente con cualquier CASE de desarrollo  orientado a  objetos que acepte este formato.  El artículo está organizado de la siguiente manera. La Sección 2 describe brevemente los  modelos de requisitos. En la Sección 3 se presenta sintéticamente el proceso de derivación del  diagrama de clases a partir de los modelos de requisitos. En la Sección 4 se detalla la herramienta  CIMTool que automatiza el proceso. En la Sección 5 se presentan ejemplos del uso de la  herramienta. Finalmente, la Sección 6 presenta algunas conclusiones y trabajos futuros.     2. Modelos de Requisitos en Lenguaje Natural   Los modelos que se presentan brevemente en esta sección son conocidos, usados y aceptados  por la comunidad de Ingeniería de Requisitos. Una descripción completa de los mismos puede  encontrarse en [7]. Los modelos son:   Léxico Extendido del Lenguaje (LEL): Es una estructura que permite la representación de  términos significativos del Universo de Discurso. Está compuesto por un conjunto de símbolos que  tienen un nombre y un conjunto de sinónimos, una noción (describe qué es el símbolo) y un  impacto (describe cómo repercute el símbolo en el sistema). Los símbolos del LEL definen objetos,  sujetos, frases verbales y estados. En la descripción de  los símbolos deben seguirse  simultáneamente dos reglas: el “principio de circularidad” y el “principio de vocabulario mínimo”.  Modelo de Escenarios: Un escenario describe situaciones en el Universo de Discurso. Cada  escenario está vinculado con el LEL, y está compuesto por: un título que lo identifica, un objetivo  que describe su propósito, un contexto para definir ubicaciones temporales y geográficas, y  precondiciones, actores que son entidades involucradas activamente en el escenario (generalmente  personas u organizaciones), un conjunto de recursos que identifican entidades pasivas con las cuales  los actores trabajan, y un conjunto de episodios donde cada episodio representa una acción realizada  por actores usando recursos.     3. El Proceso de Transformación del Modelo de Requisitos al CIM   En esta sección se describe brevemente el proceso automático para obtener el diagrama de  clases UML que representa los aspectos estructurales de un CIM. El proceso consiste en un  conjunto de pasos que aplican reglas de transformación a los modelos de LEL y escenarios para  definir las clases y sus relaciones.   El proceso está organizado en tres etapas que se ejecutan de manera secuencial:      - Identificación de clases: las dos reglas definidas para esta etapa, identifican las clases a partir de  los símbolos del LEL clasificados como sujetos y objetos.  TRC1: Regla de Transformación Sujeto-Clase  Cada sujeto del LEL se transforma en una clase UML. Los atributos de la clase se definen  de la siguiente manera: para cada entrada en la noción del símbolo LEL analizado que no  referencia  a otro símbolo LEL, la regla identifica cada sustantivo y lo define como un  atributo de la clase.  TRC2: Regla de Transformación Objeto-Clase  Cada objeto del LEL se transforma en una clase UML. Los atributos de la clase se definen  de la siguiente manera: para cada entrada en la noción del símbolo LEL analizado que no  referencia  a otro símbolo LEL, la regla identifica cada sustantivo y lo define como un  atributo de la clase. Los métodos de la clase se definen agregando el prefijo GET a cada  nombre de atributo para definir un método de acceso y agregando el prefijo SET a cada  nombre de atributo para definir un método de modificación.    - Identificación de métodos: las dos reglas propuestas para esta etapa definen los métodos (con sus  correspondientes parámetros) para las clases provenientes de símbolos del LEL clasificados como  sujetos.  TRM1: Regla de Transformación ImpactoDeSujeto-Método  Cada entrada del impacto de un símbolo del LEL clasificado como sujeto que fue  modelado como una clase aplicando TRC1, se modela como un método de esta clase.  TRM2:  Regla de Transformación InformaciónSujeto-ParámetrosMétodo  Según el proceso de construcción de escenarios [7], cada entrada del impacto de un  símbolo del LEL clasificado como sujeto, se transforma en un escenario. TRM2 modela a  los actores y a los recursos de un escenario como parámetros del método definido por  TRM1 a partir de la correspondiente entrada del  impacto que generó dicho escenario. El  actor que referencia al símbolo del LEL que generó el escenario es excluido.    - Identificación de relaciones: el diagrama de clases se completa con la definición de relaciones de  herencia, agregación y asociación mediante la aplicación de la regla de transformación TRR que  analiza la noción de símbolos del LEL que se transformaron en clases.  TRR: Regla de Transformación RelacionesDeLEL-RelacionesDeClase  Esta regla se aplica tanto a símbolos del LEL clasificados como sujetos u objetos que han  sido definidos como clases por TRC1 o TRC2. La regla analiza cada entrada de la noción  de un símbolo del LEL L1, previamente modelado como clase, con el objetivo de detectar  otros símbolos del LEL definidos también como clases por TRC1 o TRC2. Para cada  símbolo del LEL detectado, llamado L2, se define una relación de asociación entre las  correspondientes clases. El tipo de la relación se determina sobre la base de un patrón  lingüístico sugerido por [9] que determina una clasificación de verbos.   Relación de herencia: L1 y L2 tienen la misma clasificación (objeto o sujeto). L1 aparece  en la noción de L2. Las entradas de las nociones de L1 y L2 involucradas contienen, de  manera complementaria, dos clase de verbos [9]: bottom-up (es un, es un tipo de, es una  clase de) o top-down (es, puede ser, puede ser clasificado como, clasifica como).  Relación de agregación: en la noción del símbolo considerado como Contenedor deben  aparecer verbos del tipo component-composition: consistir, contener, tener, poseer, incluir,  formar, componer, dividir (estos tres últimos en voz pasiva).  Asimismo en la noción del  símbolo correspondiente a la clase Componente deberán aparecer verbos del tipo contentcomposition: forma parte, pertenece, es un componente, esta incluido en, entre otros.  Debido a que no es posible distinguir automáticamente entre una relación de agregación o  composición, la regla define a la relación como una agregación.  Relación de Asociación: cualquier relación entre símbolos del LEL que no represente a  ninguna de las dos relaciones previas, se considera como una relación de asociación. El  verbo que aparece en la noción, clasificado por [9] como general, se toma como el nombre  de la asociación.    Estas reglas de transformación asumen decisiones fijas  de diseño, por lo que el diagrama de  clases UML resultante de su  aplicación  debe ser revisado y  mejorado por los ingenieros de  software. Una descripción completa del proceso que incluye una formalización en OCL [8] de las  reglas de transformación se encuentra en [6].     4. Herramienta CIMTool  CIMTool permite la ejecución automática del proceso presentado en la Sección 3 generando un  CIM que será el primer modelo para un desarrollo basado en MDA. La Figura 1 describe la  herramienta y su contexto. CIMTool toma como entrada dos archivos: uno con los modelos de LEL  y Escenarios y otro con un diccionario del lenguaje utilizado en la descripción de los modelos de  requisitos. La salida son dos archivos: el archivo con la especificación XMI del diagrama de clases    y un archivo de log con información del resultado del proceso. El archivo XMI respeta el  formatoXMIv1.2 [10] lo que permite integrar el resultado de esta herramienta con otras  herramientas CASE. Por ejemplo podemos citar las herramientas Poseidón Comunity Edition v2.6  [11] y Enterprise Architect v4.0 [12], que fueron utilizadas para evaluar los resultados obtenidos en  las ejecuciones de la herramienta.      Figura 1: Contexto CIMTool    4.1 Arquitectura de la herramienta  CIMTool fue desarrollada con la herramienta Óracle JDeveloper 10g en el lenguaje Java,  utilizando el Java 2 Runtime Environment Standard Edition v1.4.2. La figura 2 muestra la  arquitectura de la herramienta la cual  está organizada en los siguientes paquetes:  • uml : Implementación del meta-modelo UML v1.5 [13]  • nl2oo :   o prg : Es el paquete encargado de la interacción de los demás  paquetes para llevar a cabo  la transformación.  o dic :  Este paquete permite la clasificación de una palabra determinada.  o req : Este paquete contiene un conjunto de clases java que definen los modelos de  LEL  y Escenarios según [6].   o rules :  Este paquete implementa las reglas de transformación.    A continuación, se describen brevemente los dos paquetes principales de la herramienta: req y  rules.      Figura 2: Arquitectura de CIMTool      4.1.1 Paquete req: Modelo de Requisitos  El modelo de requisitos representado en los modelos de LEL y Escenarios se implementó en el  paquete java denominado req. La figura 3 muestras las clases java  que componen este  paquete.  Para la representación del modelo de requisitos  se creó una especialización del lenguaje XML a la  que se denominó XRD (XML Requirement Declaration). XRD integra el LEL y el Modelo de  Escenarios.  XRD incluye además una serie de características requeridas por CIMTool, como por  ejemplo referencias XML explícitas entre elementos del lenguaje, verbos destacados que darán  lugar a nombres y tipos de relaciones en el modelo UML resultante.           Figura 3: Diagrama de clases UML del LEL y del Modelo de Escenarios    4.1.2 Paquete rules: Reglas de Transformación  El paquete rules presentado en la Figura 4 implementa las reglas de transformación. Cada  regla se implementó en una clase java, que  hereda de la clase abstracta Rule, la cual brinda la  interfaz para todas las reglas  con el método constructor que  tiene como parámetros objetos  Diccionario, LEL y UML::Model, y el método apply, el cual es el punto de entrada para la  ejecución o aplicación de la regla. Como clase auxiliar se definió la clase RuleSecuence, que es un  helper que implementa una regla que permite agrupar otras, y cuyo método apply no hace más que  llamar al método apply de las reglas que contiene en el orden en que fueron agregadas. Como se  explicó en la Sección 3, es importante el orden de aplicación de las reglas ya que una regla puede  utilizar elementos UML creados por la aplicación de reglas anteriores. Por ejemplo, la regla TRR es  la encargada de encontrar relaciones entre las clases UML que las reglas aplicadas anteriormente  encontraron. Otras clases auxiliares definidas en este paquete son las clases TopDowVerbs,  BottomUpVerbs, ContentCompositionVerbs y ComponentCompositionVerbs que son utilizadas por  la clase TRR para identificar los diferentes tipos de verbos y clasificar las relaciones encontradas.     Figura 4: Implementación de las reglas de transformación    4.2 Ejecución  CIMTool es actualmente una herramienta de línea de comando que, como se muestra en la  Figura 1, toma como entrada un archivo XRD con la definición de los  modelos de LEL y  escenarios de un caso de estudio concreto y otro archivo con el diccionario en el idioma  correspondiente, y produce a partir de la aplicación de las reglas un archivo de salida XMI con la  especificación del diagrama de clases UML resultante. La figura 5 muestra un diagrama de  secuencia con la ejecución de la herramienta, en la que destacamos los siguientes pasos:   En primera instancia CIMTool utiliza el método estático load de la clase XML2Dic para  cargar el diccionario; luego utiliza, análogamente, el método estático load de la clase  XML2REQ para cargar el modelo de requisitos y luego crea un modelo UML inicialmente  vacío. Posteriormente, la implementación de las reglas utilizará los objetos Diccionario y LEL  para crear los elementos que formarán parte del modelo UML.   El paso siguiente crea una instancia de la clase RuleSecuence, a la que se le agregan  posteriormente, en orden, instancias de las clases que implementan las reglas a aplicar. En  este caso se agrega una regla TRC1, luego una TRC2, una TRM1, en cuarto lugar una TRM2  y por último una TRR.   Luego, se llama al método apply de la regla que es instancia de la clase RuleSecuence, y este  método invoca a los métodos apply de las reglas que han sido agregadas anteriormente a la  secuencia de reglas.  El llamado al método apply de cada regla crea elementos del metamodelo UML y los inserta  en la instancia del modelo UML creado al inicio de la ejecución, completándolo a medida que  se aplican las reglas.  Como último paso de la ejecución se utiliza la clase UML2XMI para crear el archivo XMI  resultante y grabar en él el contenido del modelo UML derivado.     Figura 5: Secuencia de ejecución    Durante la ejecución se genera un archivo de log con información cuantitativa (tal como detalle  de las clases, atributos, métodos y relaciones derivados) y problemas detectados al aplicar las  reglas. Los problemas que frecuentemente pueden encontrarse son dos:  - Se detecta un símbolo del LEL que referencia a otro y éste no referencia al primero. En estos  casos se agrega una relación con el verbo que figura en la referencia, y se indica con un  warning que dice ""relación unilateral"".  - Se detectan  dos símbolos que se referencian mutuamente pero los verbos involucrados no  son complementarios, en este caso se deja una de las relaciones y se elimina la otra, indicando  con un warning que dice ""Verbos no complementarios"" y se indica la relación excluida. Este  punto podría ser configurable, preguntando al usuario cuál de las dos relaciones se agrega, o si  se agregan las dos.  Este archivo puede ser usado por el ingeniero de software para mejorar el diagrama de clases  resultante.    5. APLICACIÓN DE CIMTOOL A UN CASO DE ESTUDIO  En esta sección se muestran ejemplos de la ejecución de CIMTool en un Sistema de  Producción Lechera [14]. Por cuestiones de espacio se muestra la aplicación de algunas de las  reglas. En cada una de las partes del ejemplo veremos un extracto del archivo de entrada XRD con  parte de la definición de los símbolos del LEL involucrados y parte de la especificación de los  archivos de salida en XMI   A continuación se muestra un ejemplo de la Regla TRC1. A partir de un símbolo del LEL  correspondiente a un sujeto detallado parcialmente en un archivo XRD, la herramienta encuentra la  clase “dairy_farmer” con tres atributos: “name”, “salary” y “employees”, como se ve en el archivo  de salida XMI.      XRD File:     <symbol classification=""subject"" id=""dairy_farmer"">      <name>DAIRY FARMER</name>      <notion><text>Person in charge of the activities in a dairy farm.</text>       <symbol_ref symbol_id=""dairy_farm"" verb=""in charge""/> </notion>      <notion><text>He has a name.</text></notion>      <notion> <text>He has a salary.</text>  </notion>      <notion> <text>He may have one or more employees.</text> </notion>      …     </symbol>    XMI File:  <UML:Class  xmi.id=""104c035cfe6e77dbbe7703ca62d""       name=""dairy_farmer"" isSpecification=""false""       isRoot=""false"" isLeaf=""false"" isAbstract=""false"" isActive=""false"">   <UML:Classifier.feature>               <UML:Attribute  xmi.id=""104c035cff62b5bb44488f3849b""            name=""name"" isSpecification=""false"" visibility=""public""/>               <UML:Attribute  xmi.id=""104c035cff6be34408d59fd9040""            name=""salary"" isSpecification=""false"" visibility=""public""/>                <UML:Attribute  xmi.id=""104c035cff63e5c0b056e192960""            name=""employees"" isSpecification=""false"" visibility=""public""/>    …   </UML:Classifier.feature>   …   </UML:Class>            Log: --- Aplicando TRC1 ---     Log: Clases encontrada: dairy_farmer     Log: Clases encontradas     :1     Log: Atributos encontrados  :3      En el siguiente ejemplo  se muestra como la Regla TRM1 encuentra y define en un archivo  de salida XMI el método “registers_heat” para la clase “dairy_farmer”, previamente  identificada con la Regla TRC1. Para esto se basa en los impactos del término correspondiente  cuya descripción se encuentra en el archivo XRD dado como entrada.    XRD File:     <symbol classification=""subject"" id=""dairy_farmer"">      <name>DAIRY FARMER</name>      …       <behavioral_response scenario_id=""register_heat"">         <text>He registers heat.</text>         <symbol_ref symbol_id=""heat_is_registered""/>       </behavioral_response>       …     </symbol>       XMI File:  <UML:Class  xmi.id=""104c035cfe6e77dbbe7703ca62d""       name=""dairy_farmer""   …   <UML:Classifier.feature>      <UML:Operation  xmi.id=""104c035d034b9312dbe45f9c586""         name=""registers_heat"" isSpecification=""false""         visibility=""public"" isQuery=""false"" concurrency=""sequential""         isRoot=""false"" isLeaf=""false"" isAbstract=""false"">    </UML:Operation>   </UML:Classifier.feature>   …  </ UML:Class >     Log: --- Aplicando TRM1 ---   Log: Métodos encontrados  :1      El siguiente ejemplo muestra la aplicación de la Regla TRR que a partir de un conjunto de  símbolos del LEL que se modelaron como clases por la  Regla TRC2  detecta una relación de  herencia mostrada en la Figura 6. En este ejemplo se utilizó el Enterprise Architect para  visualizar el diagrama resultante.    XRD File:       <symbol classification=""object"" id=""cow"">       <name>COW</name>      …       <notion>        <text>It may be a calf, a heifer, or a dairy cow.</text>        <symbol_ref symbol_id=""calf"" verb=""may be""/>        <symbol_ref symbol_id=""heifer"" verb=""may be""/>        <symbol_ref symbol_id=""dairy_cow"" verb=""may be""/>       </notion>       …     </symbol>     …     <symbol classification=""object"" id=""calf"">       <name>CALF</name>       <notion>         <text>It is a cow of less than 12 months age.</text>         <symbol_ref symbol_id=""cow"" verb=""is a""/>       </notion>       …     </symbol>     <symbol classification=""object"" id=""heifer"">       <name>HEIFER</name>       <notion>         <text>It is a fermale cow of 12 months age or more which has not yet had a calf.</text>         <symbol_ref symbol_id=""cow"" verb=""is a""/>  …       </notion>       …     </symbol>     <symbol classification=""object"" id=""dairy_cow"">       <name>DAIRY COW</name>       <notion>         <text>It is a female cow which has had at least one calf.</text>         <symbol_ref symbol_id=""cow"" verb=""is a""/>         …       </notion>       …     </symbol>        Figura 6: Aplicación de la regla TRR    El último  ejemplo muestra  un caso del primer tipo de warning que el sistema detecta. El  archivo XRD describe un símbolo del LEL denominado Cow y otro denominado Concentrated  food.  En una entrada de la noción del segundo símbolo existe una referencia al primero, pero Cow  no contiene referencias a Concentrated food. Esto provoca un fallo a las precondiciones de las  reglas, pero al haber encontrado una referencia se la considera como una relación del tipo  “unilateral”. El usuario verá el reporte de lo sucedido, que indica la detección de la asociación y el  warning que indica el problema.    XRD File:  <symbol classification=""object"" id=""cow"">    <name>COW</name>    <notion>     <text>It is a large animal kept in a farm to produce milk or meat.</text>      <symbol_ref symbol_id=""milk"" verb=""produce""/>   </notion>    ....  </symbol>    <symbol classification=""object"" id=""concentrated_food"">    <name>CONCENTRATED FOOD</name>    <notion>       <text>It is a mixture of grains (corn,barley,wheat) or balanced food given to cows as food.</text>      <symbol_ref symbol_id=""cow"" verb=""given to""/>      <symbol_ref symbol_id=""balanced_food""/>    </notion>    ....  </symbol>    Log: Asociación: concentrated_food given to cow  Warning: Relación unilateral: cow no referencia a concentrated_food  6. Conclusiones y Trabajos Futuros  En este trabajo se presenta CIMTool, una herramienta que permite la derivación automática de  modelos de requisitos basados en  lenguaje natural hacia modelos UML. Más concretamente, la  herramienta analiza modelos de LEL y escenarios y aplica una secuencia de reglas para derivar un  diagrama de clases UML. Esta herramienta está basada en la estrategia  definida en [6], y puede  integrarse en un desarrollo de software basado en MDA, definiendo de manera automática un  modelo CIM orientado objetos. Al automatizar las reglas de derivación, se deben tomar decisiones  fijas sobre determinados aspectos de diseño, por lo que el diagrama de clases resultante debe ser  revisado por los ingenieros de software, quienes sobre la base de su experiencia y al archivo de log  generado por CIMTool harán las modificaciones necesarias al modelo.   Como la herramienta genera documentos XML puede integrarse perfectamente con cualquier  CASE de desarrollo orientado a  objetos que acepte este formato. Dentro de los futuros trabajos se  pretende integrar a CIMTool como plugin dentro de algunas de estas herramientas, en primera  instancia  para Poseidón. También se planea incorporar herramientas intermedias, que realicen  actividades adicionales; por ejemplo, la herramienta Enterprise Architect extiende el formato XMI  con elementos para describir la disposición de los elementos UML gráficamente. Sería posible,  entonces, crear una herramienta que procese el resultado de CIMTool  agregando información de la  disposición de los elementos en un diagrama."	﻿diagramas de clase UML , model driven architecture	es	23073
42	Evaluación de las capacidades humanas en el proceso de desarrollo de software	﻿ En los modelos de proceso software actuales las personas son el factor menos formalizado, sin embargo,  éstas presentan un comportamiento no determinístico y subjetivo que influye decisivamente en los resultados de la  producción de software. Se ha desarrollado un Modelo del Proceso Software Orientado a las Capacidades que define los  elementos del proceso software: actividades, productos, técnicas y personas; más los originales de esta investigación:  estructuración del trabajo, roles y capacidades. El artículo se centra en el proceso de Evaluación de las Capacidades y en  el proceso de Asignación de Personas a Roles, definiéndose las relaciones capacidad-persona y capacidad-rol  involucradas en el desarrollo de software. Se proponen dos procedimientos en función de dichas relaciones: el  Procedimiento de Evaluación de las Capacidades, para determinar las capacidades que tienen los miembros de un  equipo de trabajo y el Procedimiento de Asignación de Personas a Roles que permite asignar personas para desempeñar  roles según las capacidades que aquéllas poseen y que requieren los roles. Asimismo se pretende comprobar la relación  persona-capacidad-rol mediante un experimento para probar la hipótesis de que asignar personas a roles según las  capacidades mejora el proceso de software.   Palabras clave : Modelización de procesos, proceso organizacional, proceso software, personas, equipo de trabajo,  capacidades.    1. Introducción  Este trabajo se centra en el proceso software. Hoy por hoy, un área que juega un papel central en la investigación del  proceso software es la modelización del proceso. Las personas son el factor menos formalizado en los modelos de  proceso software actuales. Sin embargo, su importancia es obvia: presentan un comportamiento no determinístico y  subjetivo que influye de forma decisiva en los resultados de la producción de software, que es una actividad  básicamente intelectual y social [11] [18].  En la actualidad, en el área de la evaluación del proceso software existe: a) el People-CMM [6], focalizado en el factor  recurso humano; b) el Proceso Software Personal [10], focalizado en el rendimiento individual; y c) el Proceso Software  del Equipo [12], que trata la mejora del proceso software a nivel de equipo. Sin embargo, en el área de la modelización  del proceso software, se observa una falta de conceptualización y formalización de la incorporación de los humanos y la  interacción en la que participan [7][8][14][15] [17].   Los modelos de proceso existentes no formalizan las habilidades y capacidades de cada miembro: gestores,  desarrolladores, personal de soporte, profesionales, expertos, clientes y usuarios, para que el equipo del proyecto sea  eficaz y eficiente [8]. La falta de especificación de las personas añade el riesgo que se ejecuten procesos no adecuados a  las capacidades de las personas de la organización.                                                                    1 Trabajo realizado en el marco del Proyecto “Gestión Integrada de las Organizaciones: Modelización de las Capacidades Humanas  en los Procesos de Software”, Código 23/C051, aprobado por el CICyT-UNSE.  La modelización del proceso software se encarga de la creación de modelos del proceso de desarrollo de software y  pretende minimizar algunos de los problemas encontrados en las organizaciones con respecto a su proceso de  desarrollo, tales como:  · Falta de correspondencia de la descripción del proceso con el proceso realizado;  · Descripción del proceso de muy alto nivel para ser útil en la práctica; y  · Descripción del proceso imprecisa, incompleta, ambigua o incomprensible.  Se ha desarrollado un Modelo del Proceso Software en el que se han incorporado procesos nuevos, los procesos  culturales y organizacionales [2]. En estos procesos se consideran las capacidades de las personas y de los roles  involucrados. En este trabajo se definen las relaciones capacidad-persona y capacidad-rol involucradas en el desarrollo  de software. Se proponen dos procedimientos basados en estas relaciones, respectivamente: el Procedimiento de  Evaluación de las Capacidades de las Personas para determinar las competencias que tienen los miembros de un equipo  de trabajo y el Procedimiento de Asignación de Personas a Roles que permite asignar personas para desempeñar roles  según las capacidades que aquéllas poseen y que requieren los roles.  Este trabajo se basa en un enfoque de Gestión de Recursos Humanos, la Lógica de la Competencia. Esta lógica se  practica actualmente en numerosas organizaciones para diferentes propósitos tales como la selección y contratación de  personal, el análisis de puestos de trabajo, el aprendizaje de la organización o la evaluación económica [9][15]. Sin  embargo, en el contexto del proceso software esta lógica no se aplica todavía.  El resto del artículo se estructura de la siguiente forma. En el apartado 2, se presenta el Modelo del Proceso Software  propuesto. En el apartado 3, se define la relación capacidad-persona y la relación capacidad-rol, luego se describe el  Procedimiento de Evaluación de las Capacidades de las Personas y el Procedimiento de Asignación de Personas a Roles  y asimismo se hace referencia a la validación empírica de estos procedimientos. Finalmente, en el apartado 4, se  presentan las conclusiones.    2. Modelo de l Proceso Software   El Modelo del Proceso Software propuesto involucra seis elementos: actividad, rol, persona, producto, técnica y  capacidad, definidos para cada subproceso del modelo.   · Actividad. Define las actividades que realizan los actores en cada subproceso para desarrollar un producto. Las  actividades se descomponen en otras actividades más elementales.   · Rol. Define un conjunto de responsabilidades y capacidades necesarias para realizar las actividades de cada  subproceso. Para el perfil de cada rol se definen las capacidades requeridas.   · Persona. Define los actores que tienen capacidades necesarias para desempeñar un rol determinado.  · Producto. Define los productos que alimentan y son generados por las actividades de los subprocesos. Un producto  puede estar compuesto de subproductos.  · Técnica. Definen los métodos y técnicas utilizados para la realización de las actividades de cada subproceso. Para  cada subproceso se han detallado o enumerado las técnicas pertinentes.  · Capacidad. Define la habilidad o atributo personal de la conducta de un sujeto que puede considerarse como  característica de su comportamiento, bajo la cual, el comportamiento orientado a la actividad puede clasificarse de  forma lógica y fiable.   Proceso de Configuración  Proceso de Verificación y  Validación  Procesos de Protección de la  Calidad  Procesos de Soporte del  Proyecto  Procesos de Modelización del Software  Procesos de Utilización  Procesos de Formalización  Procesos de Exploración  Proceso de E studio  del Dominio  Proceso de Estudio de  Viabilidad  Procesos de Conceptualización  Procesos de Gestión  del Proyecto  Proceso de Asignación   de Personas a Roles  Proceso de Gestión de   Calidad del Software  Proceso de Seguimiento y  Control del Proyecto  Proceso de Iniciación,  Planificación y   Estimación del Proyecto  Proceso de Selección del Modelo de   Ciclo de Vida del Software   Proceso de Detección de  Problemas del Trabajo  Proceso de Evaluación  de las capacidades  de  las personas  Proceso de  Evaluación y  Mejora  Procesos Organizacionales   Proceso de  Formación  Proceso de Documentación  Procesos de Comunicación  Proceso de Instrumentación del  Equipo  Proceso de Adquisición de  Información y Conocimientos  Figura 1. Modelo del Proceso Software   Estos elementos se definen para cada uno de los subprocesos que conforman el Modelo del Proceso Software,  representado en la Figura 1. En el modelo se han incorporado procesos completamente nuevos, los procesos culturales  tales como: el Proceso de Detección de Problemas del Trabajo y Proceso de Evaluación de las Capacidades dentro de  los Procesos Organizacionales, el Proceso de Asignación de Personas a Roles dentro de los Procesos de Gestión del  Proyecto y el Proceso de Instrumentación del Equipo dentro de los Procesos de Soporte del Proyecto; los que aparecen  sombreados en la Figura 1.                                El modelo anterior se ha enriquecido mediante la definición de los roles para cada uno de los subprocesos involucrados.   Por lo tanto, en este trabajo se han analizado el conjunto de actividades que realiza cada rol y su perfil de competencias.  En [2] se encuentra la descripción íntegra de las actividades de cada proceso que conforma el modelo (especificación de  documentos de entrada y de salida para cada actividad, técnicas propias, roles involucrados y sus capacidades  requeridas). Esta descripción permite que una persona interesada pueda seguir y aplicar el proceso software genérico  orientado a las capacidades a partir de un único documento. Sin embargo en este trabajo nos centramos en las  capacidades humanas.    3. Formalización de las Capacidades Humanas  Se ha realizado un estudio y análisis de las listas de capacidades o competencias conductuales de las personas que se  utilizan en la selección de candidatos para puestos de trabajo [1][4] validadas en el marco del Assessment Centre  Method [16]. En el presente trabajo se han seleccionado y adaptado las pertinentes al desarrollo de software,  caracterizando la relación capacidades-personas. Se han definido y clasificado 20 capacidades generales, consideradas  críticas en el desarrollo de software, con un enfoque que integra elementos teóricos funcionales con elementos  cognitivos. Estas capacidades son competencias conductuales generales, que están referidas exclusivamente a las  características o habilidades del comportamiento general de la persona utilizadas durante un proyecto de desarrollo de  software. La selección de capacidades se ha realizado mediante reunión de expertos donde han participado gestores de  equipos, desarrolladores y un psicólogo laboral y mediante encuestas a diez jefes de proyectos de organizaciones  desarrolladoras de software españolas y argentinas.  Las capacidades detectadas se han organizado en cuatro categorías tal como se muestra en la Tabla 1. Esta clasificación  se basa en los niveles de adquisición de las diferentes habilidades en el proceso de evolución profesional de los  miembros involucrados en el desarrollo de software en el marco de una estructura organizativa concreta.   La formalización de las capacidades humanas se basan en herramientas técnico-conceptuales de la ciencia del  comportamiento organizacional y de la gestión integrada de personas [9][15][16], que, a su vez, se basan en principios  bien establecidos de la psicología, en general, y de la psicología laboral, en particular.  Las capacidades se utilizan tanto en el Procedimiento de Evaluación de las Capacidades de las Personas como en el  Procedimiento de Asignación de Personas a Roles.     CATEGORÍAS    Habilidades  intrapersonales   Habilidades  organizativas   Habilidades  interpersonales   Habilidades  directivas  DE SC RI PC IÓ N  Se trata de competencias  conductuales de tipo  elemental, básicas en el  individuo, de cuyo  desarrollo se ocupan los  procesos de inculturación  básica y de formación, y  que resultan preparatorias  de un desempeño eficaz  para el desarrollo  profesional.  Se trata de competencias  conductuales relacionadas  con el desempeño eficaz de  una posición desde el punto  de vista tanto de la  actuación personal,  individual como de la  adaptación del profesional  a la vida de una  organización estructurada  para evolucionar dentro de  tal organización.  Se trata de competencias  conductuales que resultan  relacionadas con el éxito en  las tareas que suponen  contacto interpersonal para  el correcto desempeño de las  actividades del proceso. Este  tipo de habilidades está  íntimamente implicado con  la eficacia en posiciones de  contacto social.  Se trata de  competencias  conductuales que  resultan imprescindibles  para dirigir a otras  personas dentro de la  organización,  orientando su  desempeño, en  diferentes niveles de  supervisión y con  distintos grados de  responsabilidad.  CA PA CI DA DE S - Análisis  - Decisión  - Independencia  - Innovación/creatividad  - Juicio  - Tolerancia al estrés  - Auto-organización  - Gestión del riesgo  - Conocimiento del entorno  - Disciplina  - Orientación ambiental  - Atención al cliente  - Capacidad de negociación  - Empatía  - Sociabilidad  - Trabajo en  equipo/cooperación  - Evaluación de los  colaboradores  - Liderazgo de grupos  - Planificación y  organización   Tabla 1. Clasificación de capacidades    3.1. Formalización de las relaciones capacidad - persona y capacidad - rol  Para determinar los factores de la personalidad o conductas indicadoras de la personalidad que están asociadas con las  personas involucradas en los procesos de desarrollo de software, se propone una Tabla de Correspondencia entre cada  una de las 20 capacidades definidas y los factores de la personalidad de un test psicométrico. En este trabajo se utiliza el  Test de personalidad de tipo proyectivo: 16 PF-5, Cuestionario Factorial de Personalidad Forma 5 descrito por Russell y  Karol [19]. Este test evalúa la estructura de la personalidad, identificando los principales componentes de la misma, y  predice la conducta de las personas en diversas situaciones y actividades.   El 16 PF-5 mide 16 rasgos primarios de personalidad identificados por Cattell et al. [5] tales como afabilidad,  razonamiento y estabilidad, que describen conductas humanas de primer orden, y cinco dimensiones globales de la  personalidad tales como extraversión, ansiedad, dureza, independencia y auto-control.  Una vez detectados los rasgos primarios y las dimensiones globales de la personalidad del individuo (mediante el test  16 PF-5) se utiliza la Tabla de Correspondencia (Tabla 2) en el Procedimiento de Evaluación de las Capacidades de las  Personas para integrar y determinar las capacidades pertinentes al desarrollo de software. Esta correspondencia  relaciona los factores de personalidad del test 16 PF-5 y las capacidades de las personas. Cada uno de estos factores  tiene un polo bajo (representado con el signo “-”) y un polo alto (representado con el signo “+”). Por ejemplo,  Estabilidad- describe una persona reactiva y emocionalmente cambiable y Estabilidad+ una persona emocionalmente  estable, adaptada y madura. Este test psicométrico permite la determinación de conductas indicadoras de las  capacidades de la persona mediante el análisis factorial del conjunto de descriptores de la personalidad total.  Por ejemplo, para la fila de análisis (Tabla 2), la conducta de la persona indica que es una persona con elevada potencia  mental, perspicaz y de rápido aprendizaje (Razonamiento+), y a su vez denota una persona con sentido práctico y con  los pies en la tierra, es decir realista y práctica (Abstracción-).                              Para la definición de esta correspondencia se han realizado dos tareas: a) un análisis bidireccional tanto de los requisitos  de personalidad de cada capacidad como de las facetas conductuales de cada factor de la personalidad; y b) una síntesis  valorativa con la participación de psicólogos laborales de la Universidad Complutense de Madrid y de la empresa TEA,  España.  En la formalización de la relación capacidad-rol, para cada rol del proceso se han definido las capacidades necesarias para la  consecución con éxito de las actividades asociadas. Las capacidades propuestas son las obligatorias para cada rol. Esto no  implica que la persona pueda poseer otras capacidades deseables. Se determina así, la Tabla Roles-Capacidades (Tabla 3).  Esta tabla ha sido elaborada a través de un análisis de las competencias conductuales requeridas para el desempeño  eficaz de cada rol. Para ello, se ha llevado a cabo un proceso de reflexión que ha involucrado múltiples tareas de  análisis: a) análisis de las actividades realizadas por cada rol; b) análisis de las situaciones críticas para el éxito en el  desempeño de cada rol, clasificadas según las cuatro categorías de capacidades descritas anteriormente, considerando  así situaciones individuales, organizativas, grupales o directivas; c) análisis de las 20 competencias conductuales  propuestas, determinando las competencias que sean requeridas por cada una de dichas situaciones críticas para  alcanzar un resultado positivo, pensando en todo momento que se trata de situaciones críticas sin cuya realización  adecuada es imposible o muy improbable alcanzar el resultado deseado. De igual modo, se ha considerado que al hablar  de competencias conductuales requeridas nos referimos a aquellas que son realmente imprescindibles (y no sólo  Tabla 2. Correspondencia entre los factores de personalidad del test 16 PF-5 y las capacidades de las personas  Af ab ilid ad Ra zo na m ien to Es ta bil ida d Do m in an cia At en ció n  a  no rm as Sa ns ibi lid ad Vi gi la nc ia Ab str ac ció n Pr iva cid ad Ap re ns ión Ap er tu ra  a l c am bi o Au to su fic ie nc ia Pe rfe cc io ni sm o Te ns ión An sie da d Du re za In de pe nd en cia Análisis + Decisión + Independencia + + Innovación / Creatividad + + Juicio + Tenacidad + + Tolerancia al estrés - Auto-organización + Gestión del riesgo + Conocimiento del Entorno + + Disciplina + Orientación ambiental + Atención al cliente  + + Capacidad de negociación  + + Empatía  + Sociabilidad  + Trabajo en equipo/cooperación + Evaluación de los colaboradores + Liderazgo de gruposPlanificación y organización + - + Planificación y organización + Escalas y dimensiones del Test PF-5 Habilidades organizativas Habilidades interpersonales Habilidades directivas Habilidades intrapersonales Capacidades deseables o, incluso, importantes). Se trata de competencias conductuales que debe poseer la persona que desempeña el  rol y en cuya ausencia la situación crítica no puede realizarse completa o adecuadamente y, en consecuencia, el objetivo  del subproceso correspondiente no se alcanza. El resultado de esta reflexión se ha reflejado en tablas similares a la  Tabla 3 por situación crítica y por rol. Finalmente, mediante sesiones participativas se ha repasado en forma sistemática  y completa la lista de competencias conductuales indicando aquellas que resultan necesarias para resolver con éxito  cada una de las situaciones críticas y la relevancia o importancia (A=alta o M=media) de cada una de ellas. En estas  sesiones participativas han intervenido personas que realizan el rol que se está analizando, personas que desempeñan  roles que le suministran productos de entrada a este rol y personas que consumen los productos generados por el rol en  cuestión y la psicóloga laboral Marta Aparicio de la Universidad Complutense de Madrid, España, para la revisión de  las capacidades consensuadas por los expertos en el desarrollo de software.                           Tabla 3. Capacidades y rol por subproceso del modelo    3.2. Procedimiento de evaluación de las capacidades de las personas  El Procedimiento de Evaluación de Capacidades de las Personas se centra en cómo determinar las capacidades de las  personas involucradas en el proceso software. Este procedimiento está descrito en [3]. La salida del Procedimiento de  Evaluación de Capacidades de las Personas está formada principalmente por dos modelos (Figura 2): el Modelo de  Factores de Personalidad y el Modelo de Lista de Capacidades de las personas.   El primero, Modelo de Factores de Personalidad, involucra un perfil gráfico de los 16 factores primarios de la personalidad y  de las cinco dimensiones globales del test 16 PF-5. El Modelo de Lista de Capacidades de las personas, integra las  valoraciones analíticas anteriores de forma coherente y presenta las capacidades de las personas junto con su nivel de  requerimiento, determinadas a través de la Tabla de Correspondencia. Además, este documento involucra una  valoración sintética de las características de la personalidad y habilidades de la persona.   Las etapas del procedimiento se presentan en la Tabla  4. Es conveniente aclarar que este procedimiento no se realiza  para cada proyecto, sino la idea involucra lo siguiente: a) la existencia en la organización de una base de datos con las  capacidades del personal; b) la revisión de esta base cada año por las variaciones de personalidad individuales; y c) la  utilización de esta información por parte del jefe de proyecto cuando tenga que asignar personas a roles.                                              Figura 2. Modelo de Factores de Personalidad y Modelo de Lista de Capacidades de las Personas      3.3. Proceso de Asignación de Personas a Roles  El proceso descrito previamente está íntimamente relacionado con el Procedimiento de Asignación de Personas a Roles,  a través del elemento Capacidades. Éstas constituyen el elemento integrador entre el perfil de cada persona y el perfil de  cada rol. El Procedimiento de Asignación de Personas a Roles,  permite realizar la asignación de personas para  desempeñar roles, es decir para llevar a cabo las actividades de cada subproceso, según las capacidades que poseen las  personas y que requieren los roles. Para alcanzar su objetivo, el Procedimiento de Asignación de Personas a Roles  Etapas Actividad Salida  Identificación de  Factores de  Personalidad  Se desarrolla el Modelo de Factores de Personalidad, que contiene la lista  de conductas indicadoras de la personalidad de cada miembro  involucrado en el proceso de desarrollo de software, obtenida a través del  Test 16 PF-5.   Modelo de  Factores de  Personalidad  Determinación de  Capacidades de las  Personas   En esta etapa se organizan y estructuran los factores de personalidad en  un modelo que contendrá las capacidades o competencias conductuales  de las personas consideradas. Se utiliza la Tabla de Correspondencia  (Tabla 2).  Modelo de  Personas  Preliminar   Validación del Modelo  de Personas   Se verifica las capacidades que posee la persona mediante una entrevista  focalizada. La entrevista tiene como objetivo la comprobación correcta y  minuciosa de las capacidades que posee la persona para posteriormente  asignarle o reasignarle un determinado rol.   Modelo de  Personas  Validado    Tabla 4 . Procedimiento de Evaluación de Capacidades de las Personas  Informe Factores de Personalidad  Organización  ESPAÑOLA  E.D   Nombre del proceso que realiza: .............................................................................................   Rol que desempeña: ........................................... ......................................................................  Fecha de la evaluación cultural: 21/6/2000...........................  N° de informe: 60  Nombre de los evaluadores:  A. Datos personales  Edad: 43  Nacionalidad: Española   Lugar de orig en: Sevilla   Sexo:                 Varón                              Mujer  Residencia habitual: ..............................................................  Estado civil:                 Soltero/a                      Casado/a                     Separ ado/a                       Divorciado/a                            Viudo/a   B. Perfil gráfico                                               Observaciones: ........................................................................................................................ ..................................................  ....................................................................................................................................................................................................   C. Informe analítico sobre la persona  Efectúe una descripción redaccional con el detalle de apreciaciones sobre la persona evaluada, siguiendo cada factor de  personalidad y dimensión global del test 16 PF-5 así como los puntos débiles y fuertes de los factores característicos de la  persona detectados, de tal manera que pueda ser comprendida por cualquier profesional no especialmente familiarizado  con los métodos y la terminología de evaluación psicológica.   Características de las conductas indicadoras de la personalidad:   ..... Ver Volumen de Demostración de Desarrollos...............................     PD DE Afabilidad A 13 4 Razonamiento B 12 8 Estabilidad C 19 8 Dominancia E 10 4 Animación F 10 4 Atención a las normas G 21 9 Atrevimiento H 8 4 Sensibilidad I 9 4 Vigilancia L 2 2 Abstracción M 0 3 Privacidad N 14 7 Aprensión O 16 6 Apertura al cambio Q1 11 4 Autosuficiencia Q2 2 4 Perfeccionismo Q3 18 7 Tensión Q4 10 6 Manipulación de la imagen MI 22 8 Infrecuencia IN 1 7 Aquiescencia AQ 37 3 EXTRAVERSIÓN Ext 4,9 ANSIEDAD Ans 6,1 DUREZA Dur 7,7 INDEPENDENCIA Ind 3,6 AUTO-CONTROL AuC 8,3 1.5 3.5 5.5 7.5 9.5  1.5 3.5 5.5 7.5 9.5  Informe Lista de Capacidades  Organización  ESPAÑOLA  E.D. Nombre del proceso que realiza: .............................................................................................   Rol que desempeña: .................................................................................................................  Fecha de la evaluación cultural: 28/7/2000...........................  N° de lista: 60  Nombre de los evaluadores: Silvia Teresita Acuña y Marta Evelia Aparicio  A. Datos personales   Edad: 43   Nacionalidad: Argentina  Lugar de origen: Santiago del Estero  Sexo:                 Varón                              Mujer   Residencia habitual: ..............................................................  Estado civil:                Soltero/a                      Casado/a                     Separado/a                       Divorciado/a                            Viudo/a  B. Capacidades o competencias c onductuales   N° Competencia  Nivel de competencia   1  Análisis  Alto   2 Decisión Alto   3 Juicio Alto   4 Tenacidad Alto   5 Auto-organización  Alto   6 Gestión del riesgo Alto   7 Disciplina Alto   8 Trabajo en equipo/cooperación Alto   9 Evaluación de los colaborad ores Alto   10 Planificación y organización Alto   Observaciones: ..........................................................................................................................................................................  ....................................................................................................................................................................................................  C. Resumen general sobre la persona  Efectúe una descripción redaccional con el  resumen de apreciaciones integradas sobre la persona evaluada, siguiendo las  características de la personalidad y habilidades de la persona detectadas, de tal manera que pueda ser comprendida por  cualquier profesional no especialmente familiarizado con los métodos y la terminología de evaluación psicológica.  Características de la personalidad y habilidades de la persona:   Elena puede tener dificultades en trabajos que impliquen contacto interpersonal,  por lo que trabajos adecuados para  ella serían aquello s técnicos y de mantenimiento de sistemas, así como programadora o investigadora.  Puede destacar profesionalmente en trabajos que implique pensar y razonar, así como en tareas que conlleven  creatividad, docencia o alta exigencia de recursos intelectuales.   Se encontrará más a gusto si  trabaja cerca de personas dominantes que supervisen y organicen su trabajo.  Si no es así ,   su indecisión para asumir responsabilidades puede desmotivarla y generarle frustración. En esta línea, tiene una gran  capacidad para aceptar normas de sus superiores y puede desarrollarse profesionalmente en trabajos que requieran  autodisciplina y precisión.  Su perfil podría ser adecuado para desempeñar tareas de mando y gestión en organizaciones cuyos niveles jerárquicos  estén claramente d elimitados y no exista mucho contacto con sus subordinados; no es un líder nato.  Es una persona detallista lo que es adecuado para trabajos que requieran exactitud.  Puede desenvolverse con éxito en si tuaciones donde sea preciso disponer de diplomacia en el trato con los demás, así  como para venderse a sí misma.   Es una persona organizada y perfeccionista, pero su exceso de perfección puede suponer que a veces su trabajo se  ralentice.       conlleva un proceso de estructuración de los perfiles obtenidos en cuatro actividades: comparación,  evaluación,  seguimiento y consolidación y documentación.  A continuación se describe cada actividad.  Actividad “Comparar perfil personal-perfil de rol”: se analiza cada perfil de persona con cada perfil de rol buscando el  mayor grado de coincidencia entre el perfil personal y el del rol. Es decir, en esta actividad se trabaja con el Modelo de  Factores de Personalidad generado en el Procedimiento de Evaluación de las Capacidades de las Personas y el Perfil de  Capacidades. Se establece para cada rol la proporción entre el número de factores de personalidad individual que  coincide con los requeridos por el rol y el número de factores total requeridos por el rol. De este modo se intenta  predecir, con la mayor probabilidad posible, la conducta de la persona ante un determinado rol en función de unas  respuestas de la persona al cuestionario factorial de personalidad 16 PF-5. Se obtiene así, como salida de esta actividad,  la Tabla Persona-Subprocesos-Roles-Ajuste.  Actividad “Evaluar los resultados”: se decide si una persona se incluye o no en un rol considerando las siguientes  reglas de asignación: Si la persona se ajusta al rol con un acuerdo mayor o igual que el 50%, entonces la persona se  asigna al rol para su participación correspondiente en el proyecto. Si hubiera un grado de acuerdo similar con dos roles,  se le asignaría a uno de ellos en función de un mayor número de factores de personalidad clasificados en el nivel alto. Si  el grado de coincidencia entre el perfil personal con el perfil de rol es menor que el 50% y mayor o igual al 30%, la  probabilidad de un buen ajuste de la persona a un rol sería mucho menor, por lo cual, lo único que se puede hacer es  buscar el rol cuyo perfil fuese más similar al de la persona, entonces la persona es asignada al rol, pero participa  también en programas de formación para incorporarse después a un rol con un acuerdo mayor o igual al 50%. Caso  contrario, si el ajuste es menor que el 30%, la persona ingresa directamente a programas de formación para incorporarse  posteriormente a un rol del proceso software.   Actividad “Realizar el seguimiento y consolidación del desempeño”: donde se asegura que el efecto del ajuste rolpersona perdure, valorando si su asignación personalizada ha logrado mejorar el rendimiento actual, comunicando los  desvíos y asegurando que no disminuya aquel efecto en virtud de la aplicación de técnicas probadas, que apoyan y  sostienen la asignación realizada. Se genera el Informe Histórico del Desempeño.   Actividad “Documentación”: en esta actividad se archivan convenientemente los resultados de cada actividad.  Estas actividades no son estrictamente secuenciales, ni los documentos que producen pasan sólo a la siguiente, sino que  se relacionan, se retroalimentan unas a otras y se pueden ejecutar en diversas secuencias.      3.4. Validación de los procedimientos  La hipótesis planteada, que se comprobó a través de un experimento fue que: la distribución de roles según las  capacidades de las personas y las capacidades requeridas por el rol influye en la eficacia y eficiencia del desarrollo de  software. El experimento detallado en [3] consistió en dos muestras de 8 proyectos de software llevados a cabo en una  organización desarrolladora de software de Argentina. La experimentación se realizó sobre los Procesos de Iniciación,  Planificación y Estimación del Proyecto, Análisis de Requisitos, Diseño y Verificación y Validación para obtener la  modelización del sistema, realizada por personas con similar experiencia en el paradigma estructurado. En 4 de los  proyectos seleccionados al azar, se asignaron las personas según las preferencias del gestor del equipo (básicamente  respondían a las experiencias propias), es decir tal como se realiza habitualmente en los proyectos de desarrollo de  software. En los otros 4 proyectos las personas destinadas a la ejecución de cada uno de los procesos mencionados se  asignaron a los roles de planificador, especificador de requisitos, diseñador y validador según la propuesta de este  trabajo, es decir siguiendo el Procedimiento de Asignación de Personas a Roles, descrito anteriormente.   Las variables respuesta consideradas en este experimento y sus mediciones por criterio en cada proyecto y equipo son  las siguientes:  · Tiempo de Desarrollo. Se realizó el cociente entre el tiempo real y el estimado por 100 para cada equipo y proceso  de planificación, análisis de requisitos, diseño y verificación y validación. Además, esta misma medición se realizó  para los cuatro procesos en forma conjunta.  · Defectos en Revisiones Formales. Se efectuó medición de los defectos en la revisión formal de la Especificación de  Requisitos del Software y se computaron porcentaje de defectos sobre los puntos de función de cada proyecto y  porcentaje de defectos sobre número de requisitos de cada proyecto.  Para analizar los datos obtenidos se utilizó la técnica estadística: Prueba de la Suma de los Órdenes de Wilcoxon [13],  ya que los datos provienen de muestras independientes y no se distribuyen normalmente.  Los resultados para el criterio Tiempo de Desarrollo en cada proceso y en los cuatro procesos considerados en forma  conjunta (Proyecto) se muestran en la Tabla 5. Se establece como hipótesis nula que los desvíos con respecto al tiempo  estimado para los procesos son los mismos en los equipos con tratamiento que en los equipos sin tratamiento. La misma  hipótesis se formula para los procesos fusionados. Los resultados de esta tabla resultan estadísticamente significativos.  En consecuencia, se puede afirmar que en los cuatro procesos los equipos con tratamiento tuvieron un desvío menor en  el tiempo de desarrollo del proyecto con respecto al tiempo estimado que los sin tratamiento. El resultado con respecto a  los procesos fusionados es similar.    PROCESO  m n W p menor o igual que  Iniciación, Planificación y Estimación del Proyecto 4 4 10,5 * 0,029  Verificación y Validación 4 4 10 * 0,014  Análisis de Requisitos 4 4 10 * 0,014  Diseño  4 4 10,5 * 0,029  Proyecto 4 4 10* 0,014  Tabla 5. Resultados del criterio tiempo de desarrollo de cada proceso y del proyecto  Considerando el criterio Defectos en Revisiones Formales de la Especificación de Requisitos del Software, en el caso  donde se consideró número de errores/número de requisitos por 100 se obtuvo w=10 (m=4, n=4, p<=0,014), y en el de  número de errores/puntos de función ajustados por 100 se obtuvo w=12 (m=4, n=4, p<=0,057). Bajo la hipótesis nula  que los errores de los grupos con tratamiento tienen igual porcentaje de errores que los sin tratamiento, los resultados  obtenidos permiten afirmar que los grupos con tratamiento tienen menor porcentaje de errores que los sin tratamiento.     4. Conclusiones  Las personas influyen en los resultados del desarrollo de software y sus capacidades deben incorporarse como modelo  de proceso para la asignación de personas a roles según las capacidades que aquéllas poseen y que requieren los roles.  Esta propuesta aporta:  · Descripciones formalizadas de las capacidades tanto de las personas como de los roles y de sus interacciones en el  proceso software, mejorando la completitud y eficacia de los modelos existentes.  · Dos procedimientos, evaluados a través de experimentación, con etapas definidas, documentos detallados y guías  para permitir la incorporación de las características culturales, es decir las capacidades de las personas  involucradas en el proceso software y la asignación de roles según las capacidades de las personas y las  capacidades requeridas por cada rol definido en el proceso.   Sobre la base de los resultados obtenidos en el experimento realizado, se puede afirmar que la propuesta del Modelo del  Proceso Software Orientado a las Capacidades significa una mejora en la modelización del proceso software.  Concretamente, la aplicación del Procedimiento de Asignación de Personas a Roles permite mejorar el desarrollo,  proporcionando guías definidas para incorporar las capacidades de las personas involucradas en el proceso software,  facilitando la gestión de las personas en el desempeño de sus roles según sus capacidades al realizar en forma eficaz y  eficiente las actividades asignadas.  El cierre del problema abierto, aquí tratado, ha provocado distintos problemas y caminos que permiten continuar  profundizando en la línea aquí establecida. Entre estos caminos y problemas cabe destacar los siguientes:  · El refinamiento de la métrica de asignación de personas a roles. Se requiere llevar a cabo un diseño experimental  que permita comprobar la independencia y el significado estadístico de la métrica para asignar personas a roles.  · Por último, la definición de un método para organizar los equipos de trabajo considerando las capacidades personales y  las capacidades interpersonales a fin de alcanzar una sintonía al trabajar juntas, una resonancia en la comunicación  interpersonal, y un eficaz y eficiente comportamiento de equipo.	﻿equipo de trabajo , Modelización de procesos , proceso organizacional , proceso software , personas , capacidades	es	23080
43	Un modelo abstracto de diálogo sobre creencias para sistemas multiagente	﻿ de Diálogo Sobre Creencias para Sistemas Multiagente M. Julieta Marcos Marcelo A. Falappa Guillermo R. Simari Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET), Laboratorio de Investigación y Desarrollo en Inteligencia Artificial, Departamento de Ciencias e Ingeniería de la Computación, Universidad Nacional del Sur, Avenida Alem 1253,(B8000BCP), Bahía Blanca, Argentina Tel: (0291) 459-5135 / Fax: (0291) 459-5136 Email: {mjm,mfalappa,grs}@cs.uns.edu.ar Resumen Este trabajo muestra una relación entre dos áreas de investigación en Inteligencia Artificial: el modelamiento de Diálogos en Sistemas Multiagente por un lado, y la Teoría de Cambio de Creencias por el otro. Presentamos un modelo abstracto de diálogo sobre creencias, basado en operadores de cambio no priorizado. La abstracción se refiere tanto al sistema de razonamiento interno utilizado por los agentes, como al tipo particular de diálogo que se quiera modelar. Básicamente, vemos al diálogo como un proceso mediante el cual los agentes provocan sucesivos cambios sobre una base de conocimiento pública (que representa el estado del diálogo). Los agentes tienen metas que dictan qué conocimiento exponer en determinado momento. El modelo impone ciertas restricciones, como por ejemplo que las bases de conocimiento privadas de los agentes no se modifiquen durante el diálogo, y que todos los agentes tengan el mismo grado de credibilidad o autoridad. Palabras Clave: sistemas multiagente, diálogos entre agentes, cambio de creencias. 1. INTRODUCCIÓN En un sistema multi-agente los agentes necesitan comunicarse, por diferentes motivos: resolver diferencias de opinión o intereses en conflicto, cooperar para resolver dilemas o encontrar pruebas, o simplemente informarse uno a otro sobre hechos pertinentes. En muchos casos no alcanza con intercambiar mensajes aislados, sino que los agentes necesitan entablar diálogos (secuencias de mensajes sobre el mismo tema) [10]. Además, se puede mejorar la calidad de la interacción si los agentes exponen los argumentos que justifican lo que dicen [11], es decir, entablan diálogos basados en argumentación. Existe una gran variedad de interacciones con características diferentes que podrían quererse modelar. Una posible tipología, teniendo en cuenta el objetivo común del diálogo y las metas particulares de cada participante, es la siguiente [12]: Diálogo de Búsqueda de Información. Un agente busca la respuesta a una pregunta en el conocimiento de otro agente. Se supone que este último conoce la respuesta. Diálogo de Investigación. Todos los agentes colaboran para encontrar la respuesta a una pregunta. Se supone que ninguno de ellos conoce la respuesta. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1371 Diálogo Persuasivo. Un agente trata de convencer a otro para que se adhiera a cierta creencia o punto de vista. Negociación. Los agentes tratan de llegar a un acuerdo aceptable sobre la división de recursos escasos. Cada uno trata de maximizar su ganancia. La meta del diálogo puede estar en conflicto con las metas individuales de los agentes. Diálogo Deliberativo. Los agentes colaboran para decidir que acción realizar en cierta situación. Se puede hacer una distinción entre diálogos colaborativos y diálogos no-colaborativos. En un diálogo colaborativo los agentes no tienen metas individuales más allá de la meta común del diálogo; por lo tanto, todos colaboran en aras del mismo fin. En un diálogo no-colaborativo, en cambio, los agentes tienen metas individuales que podrían estar en conflicto. Por ejemplo, un diálogo de investigación es colaborativo (el único objetivo compartido por todos los agentes investigadores es descubrir la verdad) pero una negociación no lo es (cada agente negociador tiene como meta maximizar su propia ganancia). Un diálogo deliberativo podría ser colaborativo o no (dependiendo de si los agentes tienen algún interés particular por tomar cierto curso de acción). Un diálogo persuasivo puede ser visto como un diálogo semi-colaborativo, donde el agente que persuade tiene una meta particular, pero el persuadido podría no tenerla. Los agentes colaborativos expondrán toda la información que consideren relevante, mientras que los agentes no-colaborativos podrían ocultar información, sabiendo que es relevante, porque no favorece el cumplimiento de sus metas individuales. Otra posible diferenciación es entre aquellos diálogos que son sobre creencias, y aquellos que no lo son. En un diálogo sobre creencias los participantes hablan sobre la verdad de cierta proposición. A esta categoría corresponden los primeros tres tipos de diálogo: búsqueda de información, investigación y diálogo persuasivo. Sin embargo, un diálogo persuasivo no es necesariamente sobre creencias (podría ser, por ejemplo, sobre acciones). Este trabajo está dedicado principalmente a diálogos sobre creencias. Se han realizado varios trabajos con el objetivo de modelar formalmente estas interacciones. Sin embargo, las soluciones propuestas son ad hoc y carecen de una fundamentación teórica sólida. En [11], por ejemplo, se investiga un tipo particular de diálogo: la negociación basada en argumentación, identificando y describiendo elementos necesarios para su modelamiento (tanto internos como externos a los agentes). En [10], por otro lado, se concentran en diálogos de investigación, de búsqueda de información y persuasivos. Definen un conjunto de locuciones para que los agentes puedan intercambiar argumentos, un conjunto de actitudes que marcan una relación entre los argumentos que puede construir un agente y las locuciones que puede realizar (intuitivamente, los agentes “menos atrevidos” sólo afirman proposiciones soportadas por “buenos argumentos”), y definen también un conjunto de protocolos para llevar a cabo los diálogos. Creemos que una falencia del trabajo citado es que construyen el modelo de diálogo sobre la base de un sistema argumentativo particular. El objetivo de nuestro trabajo es mostrar como puede construirse un modelo (limitado) de diálogo con un mayor nivel de abstracción, en cuanto al sistema de razonamiento interno utilizado por los agentes. Para ello, utilizaremos el basamento teórico que brinda la Teoría de Cambio de Creencias. La misma estudia la dinámica del conocimiento en agentes o mundos, es decir, los cambios provocados en una base de conocimiento por el arribo de nueva información. De esta manera, modelaremos el diálogo como un proceso mediante el cual los agentes provocan sucesivos cambios sobre una base de conocimiento pública (que representa el estado del diálogo). El trabajo está estructurado de la siguiente manera. La Sección 2 repasa conceptos básicos de la Teoría de Cambio de Creencias. En la Sección 3 presentamos un modelo parametrizable XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1372 de diálogo, basado en operadores de cambio. Se presenta un algoritmo de diálogo, y se analizan algunos aspectos importantes y sus limitaciones. Finalmente, en la Sección 4, se muestra cómo puede utilizarse el algoritmo propuesto para modelar diferentes tipos de interacción. 2. TEORÍA DE CAMBIO DE CREENCIAS La Teoría de Cambio de Creencias estudia la dinámica del conocimiento, esto es, la forma en que se actualiza el conocimiento de un agente después de que recibe información nueva. Un estado epistémico es una representación del conocimiento de un agente en un momento del tiempo. Existen, principalmente, dos alternativas para representar estados epistémicos: conjuntos de conocimiento [1] o bases de conocimiento [6]. Los primeros son conjuntos clausurados bajo algún operador de consecuencia lógica. Los segundos son conjuntos no clausurados, y son los que utilizaremos en este trabajo. Una actitud epistémica describe el estado de varios elementos del conocimiento que están contenidos en un estado epistémico. En los modelos clásicos de la Teoría de Cambio se consideran tres actitudes epistémicas: aceptación, rechazo e indeterminación. Una entrada epistémica es una pieza de información externa que puede producir cambios en un estado epistémico. Las operaciones de cambio epistémico que utilizaremos en este trabajo son: expansión [1] notada con “+”, mezcla [4] notada con “◦” y consolidación [8] notada con “!”. El significado intuitivo de cada una de ellas es el siguiente: Expansión. Se incorpora conocimiento sin importar si el estado resultante es consistente. Mezcla. Se combinan dos estados de conocimiento, buscando que el resultado sea consistente. Consolidación. Se eliminan inconsistencias de un estado de conocimiento. La expansión es la operación más simple. Cuando el estado epistémico se representa con bases, una expansión consiste en una simple unión de conjuntos. Si K es una base de creencias y α una entrada epistémica, entonces la expansión se define como K + α = K ∪ {α} [6]. La consolidación es, en realidad, un caso particular de otra operación: la contracción [1]. La operación de contracción elimina una creencia de un estado de conocimiento. En una consolidación la creencia a eliminar es ⊥ (la contradicción). Entre varios tipos de contracciones, nos enfocaremos en: Partial Meet Contraction [1] y Kernel Contraction [7]. En base a éstas se definen dos formas de consolidación [8]: Partial Meet Consolidation y Kernel Consolidation. Las consolidaciones de tipo Partial Meet se basan en subconjuntos maximales consistentes y funciones de selección. Sea K la base a consolidar. Una función de selección selecciona uno o más de todos los subconjuntos maximales consistentes de K. Luego, se define la consolidación partial meet como la intersección de todos los subconjuntos elegidos por al función de selección. Las consolidaciones de tipo Kernel se basan en subconjuntos minimales inconsistentes y funciones de incisión. Sea K la base a consolidar. Una función de incisión selecciona una o más creencias para eliminar de cada subconjunto minimal inconsistente de K, buscando restaurar la consistencia del mismo. Luego, se define la consolidación kernel como la base original sin las creencias seleccionadas por la función de incisión. Existen algunas propiedades intuitivas que deberían ser satisfechas por un operador de consolidación [8]: Inclusión. Para toda base K, debe ser K! ⊆ K. Consistencia. K! debe ser consistente. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1373 Relevancia y Retención de Núcleo. Buscan captar (en menor y mayor medida, respectivamente) la noción de no eliminar de más, es decir, no eliminar creencias que no contribuyen a que la base sea inconsistente. Luego, pueden definirse las operaciones de consolidación partial meet y kernel en función de las propiedades anteriores [8]: ! es un operador de partial meet consolidation si y solo si, para toda base K, ! satisface inclusión, consistencia y relevancia. ! es un operador de kernel consolidation si y solo si, para toda base K, ! satisface inclusión, consistencia y retención de núcleo. La propiedad de relevancia implica la propiedad de retención de núcleo. Por esta razón, todo operador de partial meet consolidation es también un operador de kernel consolidation. No es cierta la relación inversa. La operación de mezcla puede definirse en términos de la operación de consolidación, y viceversa. Si deseamos combinar una base K con una nueva base H, se define la mezcla como K ◦ H = (K ∪ H)! [4]. Notemos que no hay inconveniente en hacer una mezcla de bases previamente inconsistentes. También puede definirse la consolidación de K, en términos de una mezcla, como K! = (K ◦K) = (K ◦ ∅) [4]. La mezcla es un operador de cambio no priorizado. Esto significa que no se asigna ninguna prioridad especial al conocimiento nuevo, por lo que podría o no pertenecer al resultado. Otros operadores de cambio no priorizado son, por ejemplo, la semi-revisión definida en [8], la revisión por conjuntos de sentencias definida en [2], la revisión selectiva definida en [3], la screened revision definida en [9], etc.. Existe otro tipo de operadores, llamados operadores de cambio priorizado, mediante los cuales se asegura que el nuevo conocimiento pertenecerá al estado resultante, como por ejemplo el operador de revisión definido en [1]. Estos últimos no se presentarán porque no serán utilizados en este trabajo. 3. UN MODELO DE DIÁLOGO BASADO EN OPERADORES DE CAMBIO En el modelo que presentaremos, el diálogo se desarrolla en torno a una base de conocimiento pública (nos referimos a ésta como el estado del diálogo) que contiene todo el conocimiento expuesto por los agentes hasta el momento. Una consolidación del estado del diálogo representa el consenso alcanzado en un momento dado (nos referimos a ésta como el estado consensuado del diálogo). Los agentes exponen conocimiento modificando el estado actual, y tienen metas que dictan qué conocimiento exponer para lograr el efecto deseado en el estado consensuado del diálogo. Asumiremos los siguientes elementos para construir el modelo: 1. Un lenguaje de representación de conocimiento L (al menos proposicional), junto con una noción de consistencia de una base de conocimiento K ⊆ L, y un mecanismo de inferencia () para derivar conclusiones a partir de subconjuntos de L. 2. Operadores de cambio definidos sobre bases de conocimiento K ⊆ L: un operador “+” de expansión, un operador “!” de kernel consolidation, y el operador “◦” de mezcla asociado. 3. Un conjunto de metas SG (la noción de meta es tratada de manera abstracta). XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1374 A continuación introducimos algunas nociones preliminares: Definición 1 (Agente de Diálogo) Un agente de diálogo es un par A = (K, G), donde K ⊆ L es la base de conocimiento privada del agente (asumimos que K es consistente) y G ∈ SG es la meta del agente en el diálogo. Definición 2 (Estado del Diálogo) Un estado del diálogo es un conjunto E ⊆ L posiblemente inconsistente. Definición 3 (Criterio de Éxito) Un criterio de éxito es una función booleana ϕ que toma como entrada un agente de diálogo A = (G, K) y un estado E. Decimos que la meta del agente A se cumple según ϕ en el estado E si ϕ(A, E) = verdadero, y que no se cumple si ϕ(A, E) = falso. Cuando no haya lugar a dudas sobre el criterio de éxito usado, diremos directamente que la meta del agente A se cumple o no se cumple en el estado E. Definición 4 (Entorno de Diálogo) Un entorno de diálogo es un par (SA, ϕ) donde SA es un conjunto de agentes de diálogo y ϕ es un criterio de éxito. La idea básica es que un agente Ai buscará en su base de conocimiento privada un subconjunto minimal capaz de expandir el estado actual E y obtener un nuevo estado en el que su meta Gi se cumpla. El diálogo termina cuando cada agente o bien alcanza su meta o bien descubre que no tiene medios para alcanzarla (es decir, no existe un subconjunto de su conocimiento capaz de hacer una expansión exitosa). El Algoritmo 1 describe como se desarrollaría un diálogo entre los agentes A1 . . . An con bases de conocimiento K1 . . . Kn y metas G1 . . . Gn. Algoritmo 1 : Diálogo entre los Agentes {A1 = (K1, G1), . . . , An = (Kn, Gn)} 1: E ← ∅ 2: Repetir 3: Ai ← algún agente del conjunto {A1 . . . An} tal que Gi no se cumple en E, pero existe X ⊆ Ki (X minimal) tal que Gi se cumple en E + X 4: E ← E + X 5: Hasta que no exista tal agente Ai 6: Retornar E! Podemos ver que hay elementos sin especificar en este algoritmo. La noción de meta y el criterio de éxito de una meta en un estado son tratados en forma abstracta porque dependen del tipo de diálogo que se quiera modelar. En la sección siguiente mostraremos una forma de definir estos elementos para modelar algunos tipos de diálogos. El operador de consolidación adecuado también depende de las características particulares de la interacción que se quiera modelar, por eso su implementación no es especificada en el modelo. El modelo sí especifica que el cambio es no priorizado. Sin embargo, existen algunas situaciones en las que sería más adecuado un operador de cambio priorizado. Supongamos que un agente A tienen más autoridad o credibilidad que otro agente B, entonces el conocimiento que expone A debería tener prioridad con respecto al conocimiento que expone B. Por otro lado, supongamos que un agente rectifica su conocimiento durante el diálogo porque recibió percepciones (provenientes de una fuente externa al diálogo) más acertadas , entonces las nuevas XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1375 creencias expuestas por el agente deberían tener más prioridad que las creencias previas expuestas por ese mismo agente. Por lo tanto, el uso de cambio no priorizado en este trabajo se justifica en las siguientes suposiciones: 1. Todos los agentes tienen el mismo grado de autoridad (o credibilidad). 2. Los agentes no reciben nuevas percepciones (externas al diálogo) mientras dialogan. Es decir, sus bases de conocimiento internas no se modifican durante el diálogo. Otro aspecto observable en este algoritmo es que es no determinístico, por dos razones: pueden existir varios agentes Ai, y varios conjuntos X ⊆ Ki, que verifican la condición del paso 3. Diferentes caminos pueden conducir a resultados distintos. Si bien los agentes no se turnan estrictamente para hablar, se puede asegurar que un agente nunca hablará dos veces consecutivas (ya que su meta se cumple inmediatamente después de hablar). La ejecución termina cuando se alcanza un estado en el que cada agente, o bien cumple su meta o bien descubre que no puede cumplirla. Definición 5 (Estado Final) Un estado E es un estado final para un entorno de diálogo (SA, ϕ) si y solo si para cada agente Ai = (Ki, Gi) ∈ SA se verifica alguna de las condiciones siguientes: 1. Gi se cumple en E, o bien 2. No existe X ⊆ Ki tal que Gi se cumpla en E + X Es fácil ver que el algoritmo siempre termina. En un caso extremo, los agentes exponen la totalidad de su conocimiento, alcanzando un estado final. El resultado devuelto por el algoritmo es la consolidación del último estado alcanzado, y esto debe interpretarse como el consenso al que llegaron los agentes mediante el diálogo. La siguiente definición relaciona un entorno de diálogo con un posible resultado devuelto por el algoritmo. Definición 6 (Diálogo) Un diálogo es un par (Γ, E) donde Γ es un entorno de diálogo y E es un estado final para Γ. El estado del diálogo mantiene todo el conocimiento que ha sido expuesto (notemos que en el paso 4 del algoritmo el estado E se modifica mediante una expansión). Hacer expansiones en lugar de mezclas brinda dos ventajas: (1) los agentes pueden hacer uso implícito del conocimiento publicado por otros con anterioridad, y (2) se controla implícitamente que los agentes no cometan falacias [5] en el diálogo. Con respecto al estado inicial del diálogo (llamémoslo E0), si bien en el algoritmo anterior asumimos E0 = ∅, esto podría ser de otra manera si resultara más adecuado. Supongamos, por ejemplo, que los agentes recuerdan diálogos pasados. Entonces podrían comenzar el diálogo con un conjunto no vacío de conocimiento público, proveniente de diálogos anteriores entre esos mismos agentes. Finalmente, podemos destacar algunas restricciones impuestas por el modelo: 1. El conocimiento público es un subconjunto del conocimiento privado de los agentes. Es decir, que los agentes no dicen nada que no forme parte explícitamente de sus bases de conocimiento privadas. 2. Los agentes no pueden retractarse arbitrariamente sobre locuciones pasadas, sino sólo a través del operador no priorizado de mezcla. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1376 3. No se modifican las bases de conocimiento privadas de los agentes. El conocimiento que expone un agente sólo se ve reflejado en el estado público del diálogo, pero no afecta a las bases privadas de otros agentes. Creemos que estas restricciones no impiden modelar con naturalidad la mayoría de los diálogos sobre creencias, pero podrían ser demasiado fuertes para otros tipos de diálogo, como la negociación o el diálogo deliberativo. 4. MODELANDO DIFERENTES TIPOS DE DIÁLOGO El Algoritmo 1 intenta capturar el esquema general de cualquier tipo de diálogo: en un momento dado (un estado del diálogo) un participante considera que debe exponer cierto conocimiento, entonces lo hace y se produce un cambio de estado. Creemos que la diferencia entre distintos tipos de diálogo radica, en parte, en el criterio usado por los participantes para determinar si tienen algo para decir (y para elegir qué decir); y usamos la noción de meta para representar ese criterio. Veremos como pueden modelarse algunos tipos de diálogo de los presentados en la Sección 1 definiendo de manera adecuada las metas de los agentes. Por ejemplo, en un diálogo persuasivo la meta del agente que persuade podría ser una sentencia α ∈ L y el criterio de cumplimiento, para un estado E, podría ser E!  α. En este trabajo usaremos la siguiente representación de metas: Definición 7 (Meta) Una meta es un par (α, actitud), con α ∈ L y actitud ∈ {⊕, , ?}, donde ⊕ representa una actitud a favor de α,   representa una actitud en contra de α, y ? representa una actitud imparcial con respecto a α. De esta manera queda definido un conjunto SG de metas para un lenguaje L. En lo que resta del trabajo, asumiremos siempre este conjunto de metas. No deben confundirse estas actitudes de los agentes en el diálogo con las actitudes epistémicas mencionadas en la Sección 2. Si bien existe cierta relación entre las actitudes epistémicas de aceptación/rechazo y las metas (α,⊕)/(α, ), no sucede lo mismo con la actitud epistémica de indeterminación y la meta (α, ?). La siguiente definición clarifica el significado de las actitudes mencionadas en la Definición 7. Definición 8 (Primer Criterio de Exito) Definimos el siguiente criterio ϕ1 de cumplimiento de metas: una meta G=(α,⊕) se cumple en un estado E ⊆ L si y solo si E!  α una meta G=(α, ) se cumple en un estado E ⊆ L si y solo si E!  ¬α una meta G=(α, ?) de un agente A = (K, G) se cumple en un estado E ⊆ L si y solo si se verifican las siguientes condiciones: 1. E!  α ⇔ (E ◦K)  α 2. E!  ¬α ⇔ (E ◦K)  ¬α Observación 1 Dada una sentencia α, una meta (α, ?) es equivalente a una meta (¬α, ?), y una meta (α, ) es equivalente a una meta (¬α,⊕). XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1377 Intuitivamente, si un agente tiene una meta (α,⊕) significa que tiene una inclinación particular por concluir que la sentencia α es verdadera. Por el contrario, si un agente tiene una meta (α, ) significa que tiene una inclinación particular por concluir que la sentencia α es falsa. Por último, si un agente tiene una meta (α, ?) significa que su actitud en el diálogo es imparcial y su único objetivo es averiguar la verdad sobre la sentencia α. Las metas (α,⊕) y (α, ) son adecuadas para agentes no-colaborativos, mientras que las metas (α, ?) son adecuadas para agentes colaborativos. Nos concentraremos en diálogos en los cuales todos los agentes tienen metas referidas a la misma sentencia α, como se define a continuación. Definición 9 (Entorno de Diálogo Sobre α) Sea α ∈ L. (SA, ϕ) es un entorno de diálogo sobre α si y solo si Gi = (α, actitudi) para todo agente Ai ∈ SA. Definición 10 (Diálogo Sobre α) (Γ, E) es un diálogo sobre α si y solo si Γ es un entorno de diálogo sobre α. Las metas (α, ?) apuntan (aunque no siempre lo logran, como veremos más adelante) a obtener diálogos con una propiedad especial: que las conclusiones resultantes coincidan con lo que se concluiría de la unión consolidada de las bases de conocimiento privadas de los agentes. Formalizamos esta propiedad de los diálogos con la siguiente definición. Definición 11 (Diálogo Completo) Un diálogo sobre α (Γ, E), con Γ = ({A1 = (K1, G1), . . . , An = (Kn, Gn)}, ϕ), es un diálogo completo si y solo si se verifican las siguientes condiciones: 1. E!  α ⇔ (⋃1≤i≤n{Ki})!  α 2. E!  ¬α ⇔ ( ⋃ 1≤i≤n{Ki})!  ¬α Ahora podemos definir formalmente algunos tipos de diálogo: Definición 12 (Diálogo de Investigación) Sea α ∈ L. Un Diálogo de Investigación (sobre α) es un diálogo sobre α en el cual todos los agentes participantes tienen la misma meta (α, ?). Definición 13 (Diálogo de Búsqueda de Información) Un Diálogo de Búsqueda de Información (sobre α) es un Diálogo de Investigación (sobre α) en el cual existe por lo menos un agente participante Ai = (Ki, Gi) tal que: o bien Ki  α o bien Ki  ¬α. Definición 14 (Diálogo Persuasivo) Sea α ∈ L. Un Diálogo Persuasivo (sobre α) es un diálogo sobre α en el cual existe por lo menos un agente participante Ai = (Ki, Gi) tal que Gi = (α, actitudi) y actitudi = ?. Esta definición no respeta exactamente la caracterización previa (Sección 1) de Diálogo de Investigación, en la cual se menciona que ningún participante conoce la respuesta a la pregunta en cuestión. Decidimos adoptar esta visión más general y ver al Diálogo de Búsqueda de Información como un caso particular de Investigación. Las metas y diálogos así definidos son solamente ejemplificaciones sobre como podrían modelarse algunas interacciones con el modelo propuesto. A continuación, se ilustra con ejemplos de Diálogos de Investigación y Diálogos Persuasivos el funcionamiento del Algoritmo 1, y se muestran algunos problemas que podrían surgir. Por simplicidad consideramos diálogos entre dos agentes y utilizamos Lógica Clásica Proposicional como lenguaje de representación de XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1378 conocimiento. Las consolidaciones son realizadas de manera arbitraria, dado que esta simplificación no afecta la ilustratividad de los ejemplos. La representación de metas es de acuerdo a las Definiciones 7 y 8. Ejemplo 1 Supongamos un Diálogo de Investigación en un entorno ({A1 = (K1, G1) y A2 = (K2, G2)}, ϕ1) con: K1 = {a, a → c, b → c, c → d} K2 = {a ∧ c → ¬d, b ∧ c → d} G1 = G2 = (d, ?) Supongamos además que el operador ! de partial meet consolidation se comporta de la manera especificada a continuación, cuando es aplicado al siguiente estado: {a, a → c, c → d , a ∧ c → ¬d}!  ¬d (1) De acuerdo a esto, se muestra a continuación una posible ejecución del algoritmo. En cada paso indicamos qué agente interviene y qué conocimiento publica (respetando una posible elección del paso 3 del Algoritmo 1), y también indicamos el efecto en el estado consensuado del diálogo. Notaremos con Ei el estado del diálogo en la iteración i: 1. E1 = ∅ 2. El agente A1 dice: {a, a → c, c → d} E2!  d 3. El agente A2 dice: {a ∧ c → ¬d} E3!  ¬d 4. Termina el diálogo en el estado E3 El diálogo termina en el estado E3 porque ambos agentes cumplen sus metas. En general, podemos decir que: Observación 2 En un diálogo de investigación todos los agentes cumplen sus metas al terminar el diálogo (es fácil ver que si una meta Gi = (α, ?) de un agente Ai no se cumple en E entonces debe existir X ⊆ Ki tal que Gi se cumple en E + X). En este caso se concluye, por (1), que la sentencia d es falsa. Esta misma conclusión se obtiene de (K1 ∪ K2)! (el diálogo es completo). El Ejemplo 2 muestra que esto no siempre es así. Hay casos en los que los agentes quedan en una situación de bloqueo sin poder exponer toda la información relevante (porque no advierten que es relevante). El Ejemplo 3 muestra un diálogo de investigación en el que, a diferencia de este, la conclusión final contradice la opinión individual de todos los agentes. Ejemplo 2 Supongamos que modificamos el entorno del Ejemplo 1 agregando sentencias a ambas bases: K1 = {a, a → c, b → c, c → d, f ∧ g → ¬d} K2 = {a ∧ c → ¬d, b ∧ c → d, f, f → g, g → d} G1 = G2 = (d, ?) Supongamos además que el operador ! de partial meet consolidation se comporta de la manera especificada a continuación, cuando es aplicado a cada uno de los estados siguientes: XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1379 {a, a → c, c → d, a ∧ c → ¬d}!  ¬d (1) {a, a → c, c → d, a ∧ c → ¬d, f, f → g, g → d}!  d (2) {a, a → c, c → d, a ∧ c → ¬d, f, f → g, g → d, f ∧ g → ¬d}!  ¬d (3) De acuerdo a esto, se muestra a continuación una posible ejecución del algoritmo. 1. E1 = ∅ 2. El agente A1 dice: {a, a → c, c → d} E2!  d 3. Termina el diálogo en el estado E2 En este caso podemos ver que el diálogo resulta en la aceptación de la sentencia d, pero si ambos agentes expusieran todo su conocimiento se obtendría una conclusión diferente. Notemos que el agente A2 considera, en el estado E2, que no tiene nada relevante para decir. Esto se debe a que dicho agente advierte, por (2), que aún aportando todo su conocimiento no cambiaría la conclusión sobre d. Lo que el agente A2 no alcanza a advertir es que, por (3), la conclusión sí cambiaría en una iteración posterior, luego de la intervención del agente A1. Esta situación (la llamamos situación de bloqueo) es poco deseable para agentes con metas (α, ?), ya que estos agentes buscan idealmente diálogos completos. Observemos que, por (1), existe en este caso un subconjunto propio del conocimiento privado del agente A2 capaz de cambiar la conclusión pública sobre d. Una forma de reducir las situaciones de bloqueo es redefinir el criterio de cumplimiento de las metas (α, ?) de la siguiente manera: Definición 15 (Redefinición del Criterio de Éxito de Metas (α, ?)) Una meta G=(α, ?) de un agente A = (K, G) se cumple en un estado E ⊆ L si y solo si, para todo X ⊆ K, se verifican las siguientes condiciones: 1. E!  α ⇔ (E ◦X)  α 2. E!  ¬α ⇔ (E ◦X)  ¬α Llamaremos ϕ2 el criterio de éxito ϕ1 modificado según la Definición 15. El cumplimiento de una meta (α, ?) según ϕ2 implica trivialmente el cumplimiento de una meta (α, ?) según ϕ1. La diferencia entre un diálogo de investigación con el criterio ϕ1 y uno con el criterio ϕ2 es que en el segundo caso los agentes publicarán más información, evitando algunas situaciones de bloqueo. Ejemplo 3 Reconsideremos el Ejemplo 2, pero ahora usando el criterio ϕ2 en lugar de ϕ1: K1 = {a, a → c, b → c, c → d, f ∧ g → ¬d} K2 = {a ∧ c → ¬d, b ∧ c → d, f, f → g, g → d} G1 = G2 = (d, ?) A continuación se muestra una posible ejecución del algoritmo, asumiendo el mismo operador de consolidación que en el Ejemplo 2: 1. E1 = ∅ 2. El agente A1 dice: {a, a → c, c → d} E2!  d XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1380 3. El agente A2 dice: {a ∧ c → ¬d, f, f → g, g → d} E3!  d 4. El agente A1 dice: {f ∧ g → ¬d} E4!  ¬d 5. Termina el diálogo en el estado E4 En este caso el diálogo termina concluyendo que la sentencia d es falsa, a pesar de que cada agente creía individualmente lo contrario. El resultado coincide con lo que se concluiría de (K1 ∪K2)! (el diálogo es completo). Veremos con el Ejemplo 4 que en algunos casos la situación de bloqueo persiste a pesar de la redefinición de criterio del éxito. Ejemplo 4 Supongamos un caso de Diálogo de Investigación extremadamente simple: K1 = {a} K2 = {a → b} G1 = G2 = (b, ?) Sería deseable concluir que la sentencia b es verdadera. Sin embargo, el modelo de diálogo propuesto no logra obtener esta conclusión (ya sea con el criterio de éxito ϕ1 o ϕ2 ). Se puede ver fácilmente que se genera un diálogo vacío en el que ninguno de los agentes puede empezar a hablar. Ejemplo 5 Por último, veremos un ejemplo de Diálogo Persuasivo. Tomaremos el entorno del Ejemplo 3 pero modificaremos la meta del agente A1, de la siguiente manera: K1 = {a, a → c, b → c, c → d, f ∧ g → ¬d} K2 = {a ∧ c → ¬d, b ∧ c → d, f, f → g, g → d} G1 = (d,⊕) G2 = (d, ?) A continuación se muestra una posible ejecución del algoritmo: 1. E1 = ∅ 2. El agente A1 dice: {a, a → c, c → d} E2!  d 3. El agente A2 dice: {a ∧ c → ¬d, f, f → g, g → d} E3!  d 4. Termina el diálogo en el estado E3 La conclusión alcanzada es que la sentencia d es verdadera. Podemos ver que el diálogo no es completo. Sin embargo, la incompletitud no es un problema en este caso, sino que es causada intencionalmente por el agente A1 para cumplir su meta individual. 5. CONCLUSIONES Y TRABAJO FUTURO El problema abordado en este artículo es el modelamiento de diálogos en sistemas multiagente. Propusimos un algoritmo abstracto no determinístico que simula una interacción entre dos o más participantes. Creemos que los siguientes son aspectos positivos de la solución propuesta: (1) el uso de Operadores de Cambio nos permite abstraernos de la teoría lógica subyacente (lenguaje y mecanismo de inferencia), y (2) el uso de la noción abstracta de meta nos permite XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1381 parametrizar las actitudes de los agentes en el diálogo, abstrayéndonos, en cierta medida, del tipo de diálogo. Identificamos también algunas limitaciones del modelo propuesto: (1) está principalmente orientado a diálogos sobre creencias, (2) todos los agentes tienen el mismo grado de autoridad (o credibilidad), (3) los agentes no reciben nuevas percepciones (externas al diálogo) mientras dialogan, y (4) el trabajo brinda un aporte teórico, pero no práctico, dada la complejidad computacional del algoritmo propuesto. Nuestro trabajo futuro estará orientado a: (1) investigar el modelamiento de otros tipos de diálogo (como la negociación y el diálogo deliberativo), (2) analizar el uso de lógicas argumentativas como lenguaje de representación de conocimiento (creemos que esto podría facilitar el hallazgo del conjunto X del paso 3 del Algoritmo 1), (3) buscar estrategias para solucionar las situaciones de bloqueo mencionadas en la Sección 4, y (4) profundizar sobre implementaciones adecuadas del operador de consolidación en diferentes tipos de diálogo.	﻿diálogos entre agentes , sistemas multiagente , cambio de creencias	es	23085
44	Desarrollo de sistemas inteligentes aplicados a redes eléctricas industriales	﻿ Este trabajo sintetiza el desarrollo de un Sistema Inteligente de Control (SIC) para la Automatización de Sistemas Eléctricos de Potencia (SEPs) del tipo industrial. Las tareas automatizadas son detección y aislación de fallas, y automatización de métodos de back-up que soporten posibles pérdidas de integridad en los componentes involucrados. Para este sistema se propone una arquitectura multiagente altamente escalable y flexible, y se decriben los agentes necesarios para realizar las distintas tareas. Se implementan prototipos de dos de los agentes que integran el SIC utilizando el modelo BDI (Belief-Desire-Intention) y se evalúa su comportamiento. Keywords: Diseño Orientado a Agentes, Agentes BDI, Sistemas Eléctricos de Protencia. 1. INTRODUCCIÓN El objetivo de un Sistema Eléctrico de Potencia (SEP) es proveer energía eléctrica con criterios de calidad, seguridad y fiabilidad acorde a las normas vigentes. En este trabajo se hace especial referencia a las redes eléctricas industriales1. En la actualidad estos sistemas presentan la posibilidad de mejorar la calidad de prestación. Para ello es necesario disponer de medios de protección y control que se adapten a distintas condiciones de hardware y de operación del SEP. La condición básica de todo SEP es el equilibrio entre generación y demanda. El mundo y en especial la Argentina se enfrenta al problema que los recursos energéticos son limitados y el margen de reserva de generación es cada vez menor, por lo que es necesario agudizar los mecanismos de control para evitar colapsos generales o parciales. Para los ingenieros de SEPs el principal factor a tener en cuenta, es la calidad del servicio, en el cual uno de los elementos más importantes es la continuidad del mismo, siendo una preocupación la pérdida total del suministro y en un segundo lugar se encuentran las pérdidas parciales del servicio. Para dar soporte a este problema, los sistemas inteligentes son una importante alternativa a explorar. Los sistemas basados en técnicas de la Inteligencia Artificial y en particular la tecnología de los sistemas multi-agentes, son una herramienta valiosa a la hora de desarrollar sistemas para problemas reales complejos, de carácter distribuido y donde se espera que sus componentes tengan cierta autonomía. Existen diversos trabajos enfocados al desarrollo de sistemas inteligentes en este dominio de aplicación, por ejemplo [14] y [15]. 1Comprenden a los grandes consumidores de energía eléctrica, tales como las industrias del acero, químicas, papel, etc.; que generalmente reciben el suministro eléctrico en alta o media tensión. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1383 1.1. Dominio de aplicación Un SEP es un conjunto de equipos que permiten energizar cargas en forma segura y confiable, en distintos niveles de tensión, ubicados generalmente en diferentes lugares físicos. Entre los componentes que se encuentran en un SEP, podemos mencionar principalmente a Protecciones Eléctricas (Relays), Interruptores, Transformadores, Barras, Generadores, Líneas y Cargas. Las protecciones eléctricas tienen un rol fundamental en el proceso de adquisición y análisis de datos. Una protección eléctrica (también llamada relé) es un dispositivo que sensa variables de redes, para procesarlas mediante algoritmos adecuados. A su vez ordena acciones frente a situaciones anormales. Existen distintos tipos de protecciones que se clasifican en base a su función. Para el trabajo realizado se considera principalmente el relé denominado de máxima corriente y direccional. Cada relé tiene asignada una “corriente de seteo” (Ic). Ic es superior a la máxima corriente que puede circular por el relé en condiciones normales en al menos un 30%, para tener en cuenta errores de calibración. Se pueden definir las principales señales provenientes de un relé de la siguiente manera: S1 = { 1 si I ≥ Ic 0 si I < Ic S2 = { 1 si I va de Barra a Línea 0 en I va de Línea a Barra S3 = { 1 si el Relé presenta alguna falla interna 0 si no Si la corriente (I) que circula es mayor (o igual) que Ic, entonces el relé se excita (también suele decirse que opera). Esta información se obtiene mediante S1. Mediante S2 obtenemos la dirección en que circula la corriente y mediante S3 podemos saber si el relé presenta alguna falla interna (por motivo desconocido). Otros tipos de protecciones que consideramos son las denominadas protecciones propias de transformador (PPT). Estas presentan señales de excitación y falla-relé (falla interna). También ordenan disparos. Las protecciones de máxima corriente, tienen un tiempo de seteo ta. Si la protección se encuentra en estado de excitación durante el tiempo ta, inmediatamente ordena la apertura del interruptor asociado (se dice que ordena disparo o “trip”). Un interruptor (de potencia) es un dispositivo utilizado para desconectar una carga o una parte del sistema eléctrico, tanto en condiciones de operación normal como en condición de cortocircuito. La operación de un interruptor puede ser automática o manual, accionada por la señal de un relé encargado de vigilar la correcta operación del sistema eléctrico donde está conectado o de un operador a distancia. Cada interruptor suele estar asociado a un relé. Las barras pueden ser consideradas como nodos del SEP. Son puntos de maniobra pues a ella concurren las líneas, se conectan los transformadores y las cargas. Las cargas son elementos que transforman la energía eléctrica en otra forma de energía: mecánica, calórica, etc; pueden ser por ejemplo: motores, hornos, equipos de iluminación, etc. Las líneas cumplen el rol de conductores y transmisores de la energía. Un transformador es un equipo que permite aumentar o disminuir la tensión en un circuito eléctrico. Un generador es una fuente productora de energía eléctrica. Es muy importante mantener el suministro de energía en forma adecuada, pues los daños ocasionados pueden ser de costos muy elevados e incluso se puede ver afectada la vida de seres humanos, si consideramos por ejemplo un sistema de ventilación. Resulta necesaria entonces la existencia de sistemas de control de SEPs. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1384 1.2. Problemática actual En la actualidad se cuenta con sistemas SCADA para la adquisición de datos, aplicaciones aisladas que realizan análisis de aspectos específicos y también software para soporte de decisión. La complejidad actual de los SEPs radica en que no es posible disponer de una regulación (seteo) única para todas las condiciones operativas factibles. En las redes actuales se pueden presentar operaciones no consideradas previamente debido a servicios prestados bajo severidades extremas motivadas por salidas de servicio de una parte de las instalaciones y ante la necesidad de continuar con el suministro, se alteran las condiciones preestablecidas. Entre ellas se pueden citar sobrecargas en líneas que llevan a desconexiones en cascada. Es necesario que para lograr un funcionamiento adecuado de las protecciones eléctricas, se produzca una variación en su seteo en forma automática (relé adaptivo), teniendo en cuenta estados de operación de elementos adyacentes y lejanos [2]. Con el fin de desarrollar un sistema de control acorde a las necesidades vigentes se utilizaron en este proyecto herramientas de la Inteligencia Artificial, principalmente se hizo uso de tecnología multi-agente y de Sistemas de Razonamiento Procedural (PRS) [12]. El resultado de este trabajo es el diseño de un Sistema Inteligente de Control (SIC) aplicado a redes eléctricas y la implementación de dos de sus agentes (el agente Detección y Aislación de Fallas (DAF) y el agente Back-Up) utilizando el modelo BDI. Esta presentación tiene la siguiente estructura: en la sección 2 se describen los requerimientos que debe verificar el SIC. En las secciones 3 y 4 se desarrollan respectivamente la arquitectura del SIC y el diseño de sus agentes. Finalmente, las secciones 5 y 6 corresponden a observaciones sobre la experimentación y conclusiones. 2. REQUERIMIENTOS Por medio de un trabajo de Ingeniería de Conocimiento se realizó la extracción de requerimientos para el sistema planteado. Mediante sucesivas reuniones con el experto en el dominio de aplicación 2 se logró refinar y comprender los objetivos del mismo. En primera instancia se identifica el siguiente objetivo general del sistema de control: actuar ante una falla, tratando de mantener la integridad del mayor área posible bajo condiciones de calidad adecuadas. Luego, se puede descomponer el objetivo global en las siguientes tareas o subojetivos: detectar fallas; aislar fallas; respaldar componentes que presenten falla interna (protecciones e interruptores); restaurar el servicio a zonas desenergizadas y mantener un modelo del SEP. En este trabajo se centra la atención en las tres primeras tareas. También debe considerarse un importante requerimiento temporal. Los tiempos de actuación en caso de existir una falla en el SEP deben ser del orden de 100 milisegundos. Si bien el presente trabajo no pretende necesariamente que la implementación verifique este requerimiento dado que se trata de un prototipo, debe ser considerado, ya que una implementación final sí debería verificarlo. 2Ing. Luis A. Krapf , Escuela de Ingeniería Eléctrica, Facultad de Cs. Exactas, Ingeniería y Agrimensura, UNR XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1385 Un aspecto que también se debe tener en cuenta es la capacidad de adquirir datos provenientes del SEP, realizar algún tipo de preprocesamiento (e.g., filtrado3) y luego transmitir eventos significativos a las entidades interesadas. Otro requerimiento, no funcional, involucra la posibilidad o facilidad para integrar sistemas de análisis preexistentes o “legacy”. El sistema a desarrollar deber permitir una fácil integración. Para continuar con el proceso de adquisición del conocimiento, se obtuvieron una serie de escenarios, también llamados casos de uso. Cada caso de uso representa una situación de falla y determina las acciones necesarias que deben efectuarse correspondientemente. Esto permite realizar un refinamiento en los requerimientos. Por cada falla analizada se genera un caso de uso, el cual a su vez es representante de un conjunto de situaciones que deben ser tratados de forma equivalente. Esta agrupación se debe sencillamente que dependiendo de la configuración de cada protección se puede excitar un mayor o menor número de protecciones. Las protecciones que deben tener un comportamiento común para toda instancia de una clase de casos de uso son aquellas más próximas a la falla, salvo en casos de incertidumbre en los que estas protecciones pueden no dar indicaciones (considerando de esta manera fallas internas, funcionamientos anómalos o errores de calibración). A continuación se describe un escenario a modo de ejemplo. La situación que se desea detectar se describe en forma gráfica (ver Fig. 1). Para simplificar la notación, no se introducen los nombres de los interruptores. Se asume en términos generales que el relé Rj está asociado al interruptor Ij . Además pueden aparecer flechas debajo de algunos componentes. Estas corresponden a indicaciones (o señales) del sentido de la corriente provistas por los relés. Si bien un relé puede dar este tipo de indicación sin estar excitado, debe asumirse que lo está (también con el fin de simplificar la notación). Figura 1: Caso 1. Falla en LINEA En el caso ilustrado en la figura 1 se presenta la falla en una línea. Las acciones necesarias para resolverlo son: disparar los interruptores Ibc y Icb. 3evitando por ejemplo, el envío de señales ante una intermitencia en las mismas XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1386 si no abre Ibc se debe disparar Iba y Iab. También deben desconectarse las cargas involucradas, o sea, abrir ITfb y las cargas colgadas del transformador (TfB) deben desconectarse recursivamente (empezando por la más cercana). análogamente, si no abre Icb se debe disparar Icd y Idc. También deben desconectarse las cargas involucradas, o sea, abrir ITfC . Las cargas colgadas del transformador (TfC) deben desconectarse recursivamente (empezando por la más cercana). se deben impedir (bloquear) los disparos de los demás relés que se excitaron. En este trabajo concretamente se identificaron nueve casos de uso, abarcando entre ellos existencia de incertidumbre. Se presenta incertidumbre cuando la información recolectada es incompleta para alcanzar una solución (por ejemplo el caso en que una protección no envíe señal cuando en realidad debiera hacerlo) o cuando la información obtenida es errónea y genera inconsistencias (por ejemplo el envío de señales incorrectas, como ser la dirección de corriente opuesta a la real) debiéndose esto a fallas internas o funcionamientos anormales. 2.1. Características del problema El problema en el cual nos concentramos tiene algunas características importantes a considerar. Los datos utilizados para el análisis del SEP, se encuentran distribuidos geográficamente, puesto que la información puede ser recolectada de diferentes estaciones remotas. Estos datos tienen a su vez una naturaleza altamente dinámica. Por otro lado, la complejidad del problema puede ser atacada mediante tareas independientes. Estas características, claramente nos conducen a que la aplicación de la tecnología de sistemas Multi-Agentes [10] es altamente adecuada para resolver un problema como el propuesto. 3. ARQUITECTURA MULTI-AGENTE Durante los últimos años se han desarrollado varias metodologías para el análisis y diseño orientado a agentes (AOSE). Ejemplos de esto son DESIRE [7], Gaia [11], Prometheus [8] y para el desarollo de agentes BDI [1]. Si bien hay similitudes, cada metodología tiene aspectos muy interesantes, por lo que se extrajeron conceptos de las mismas para avanzar hacia la etapa diseño. 3.1. De los Requerimientos hacia la Arquitectura El primer paso seguido es la descomposición de tareas (según la metodología DESIRE [7]). Este proceso tiene como fin la obtención de tareas o funcionalidades requeridas. Las funcionalidades deben ser lo más simples posible de manera de tratar con un único aspecto o comportamiento esperado del sistema. Por lo tanto podemos identificar las siguientes funcionalidades del sistema: identificar fallas; bloquear disparos innecesarios; aislar fallas identificadas; administrar eventos; respaldar componentes que presenten falla (Back-Up) y restaurar el servicio. Al definir una funcionalidad también es importante definir la información que requiere y la información que produce. Para describir o especificar una funcionalidad se utilizan descriptores que contienen el nombre de la misma, una descripción informal breve, una lista de acciones, una lista de percepciones, datos usados y producidos, y una breve descripción de las interacciones con otras funcionalidades. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1387 Esta forma de identificar funcionalidades, da la posibilidad de estudiar la interacción entre ellas, lo cual es necesario para el siguiente paso. Este consiste en identificar cuáles serán los agentes que van a existir en el sistema. Veamos la descripción de una funcionalidad: Nombre: Detector de fallas. Descripción: Determina la presencia de falla, su localización y su tipo. Acciones: -. Percepción. Estado de protecciones. Acceso a Datos: Lectura del estado de protecciones, topología de red. Escritura de datos con la región donde se encuentra la falla y su clasificación. Interacciones: Provee la región generada a las funcionalidades Bloqueo y Aislación. A esta última también le provee la clase y locación de falla. En el diseño de un sistema Multi-Agente, resulta fundamental definir los agentes que lo componen, así como también las interacciones entre ellos. Se puede considerar a un agente como una combinación de funcionalidades. Luego, se puede determinar qué agentes existirán evaluando diferentes combinaciones según criterios provenientes del diseño orientado a objetos y reutilizados en AOSE. Se pueden asignar funcionalidades a agentes de modo de encontrar una configuración en la que se consiga un bajo nivel de acoplamiento y un gran nivel de cohesión. Por ejemplo, si distintas funcionalidades usan los mismos datos entonces es una indicación de que se deben agrupar dado que habrá una gran interacción entre ellas. Si distintas funcionalidades están claramente no relacionadas es una razón para no agruparlas, así como también si existen en distintas plataformas de hardware. Utilizamos un diagrama de interacción entre funcionalidades (ver figura 2) para mostrar las funcionalidades (rectángulos), los datos (elipses) y conexiones entre ellos (flechas). Una flecha desde una funcionalidad hacia datos indica que la funcionalidad produce los datos. Figura 2: Interacción entre funcionalidades Del análisis de las funcionalidades antes identificadas y del diagrama de interacción resulta claro y natural agrupar las funcionalidades de forma tal que se obtienen los siguientes agentes: Detección y Aislación de fallas (DAF) Back-Up (BK) Manejador de Eventos (ME) Reposición de Servicio (RS) XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1388 Además de los mencionados consideramos dos tipos de agente, cuya representación está motivada por correspondencia directa con elementos muy importantes para el dominio de aplicación: Agente de Protección (AP) Agente de Interrupción (AI) En el presente trabajo no se describe el diseño de los agentes RS y ME. Sí se consideran aspectos relacionados a la interaccón con el agente ME, dado que éste constituye el nexo entre los agentes DAF y Back-Up y el sistema eléctrico subyacente (SEP). El Agente de Protección es un tipo de agente que representa una protección eléctrica. Puede actuar autónamente para proteger dispositivos del SEP. Consiste principalmente de reglas, lógica de relés, y características de operación. El Agente de Interrupción representa el comportamiento de un interruptor. A continuación se describe un agente tal como aparece en la guía de agentes, siguiendo el estilo de notación empleado en Prometheus [8]. Nombre: Agente de Detección y Aislación de Fallas (DAF) Descripción: Analiza los eventos provenientes de una capa inferior con el objetivo de detectar y encontrar con la mayor precisión posible la locación de una falla eventual para luego poder aislarla y evitar una desenergización innecesaria. Cardinalidad: 1 Funcionalidades incluidas: Identificar región de falla, bloquear disparos innecesarios, aislar fallas Lee datos: Estado de los componentes (modelo propio) Escribe datos: Estado de los componentes (modelo propio) Interactúa con: ME (para obtener eventos de interés y ordenar acciones), Back-Up (obtener correcciones en la información sobre componentes con falla interna) 3.2. Interacción entre Agentes Tan importante como definir los agentes que forman el sistema, es describir la interacción que habrá entre ellos. La figura 3 muestra un diagrama de interacción entre los agentes propuestos. Este diagrama simplemente vincula agentes que tienen algún tipo de interacción, ya sea mediante datos compartidos o mensajes. La existencia de múltiples agentes de un mismo tipo se representa en el diagrama mediante rectángulos superpuestos. Resulta útil este diagrama para considerar el nivel de acoplamiento entre agentes. Figura 3: Interacción básica entre Agentes Mediante un mecanismo de suscripción, el Agente Manejador de Eventos (ME), mantiene registros de cuáles son los eventos de interés para cada agente existente en el sistema y es su función enviarlos. Por lo tanto es importante, definir cuáles serán los eventos generados como resultado de la información obtenida del entorno (SEP). Los eventos generados serán los percibidos por los agentes XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1389 Figura 4: Diagrama de Colaboración. Figura 5: Mensajes entre agentes. de análisis como por ejemplo, los agentes DAF y Back-Up. También se deben definir los eventos generados por estos últimos. Los agentes se comunican entre sí mediante mensajes. Un lenguaje para la comunicación de agentes (ACL) es un medio adecuado de encapsulación de mensajes para su transporte [6]. La interacción entre agentes puede ser capturada mediante distintas representaciones. Una representación posible está dada por “diagramas de colaboración”, otra esta dada por “diagramas secuenciales”. Los diagramas secuenciales hacen énfasis en la secuencia cronológica de la comunicación mientras que los diagramas de colaboración hacen énfasis en las asociaciones entre agentes. La figura 4 y el cuadro de la figura 5 ejemplifican cómo es la interacción entre los agentes. Los números de referencia que se encuentran sobre los líneas de asociación dan una orden a una secuencia general de interacción posible, que está descripta en la tabla adyacente. Mediante un diagrama secuencial (ver figura 6) se puede permitir una mejor visualización de la forma en que interactúan o colaboran los agentes en el caso particular en que el agente DAF (suscripto al servicio de corrección de lecturas provista por el agente Back-Up) ordena la apertura de un interruptor con falla interna. En este tipo de diagramas, se puede leer la secuencia temporizada de interacción desde arriba hacia abajo (o sea, el tiempo se incrementa hacia abajo). En este caso se simplificó la notación agrupándose a los Agentes de Protección y Agentes de Interrupción dentro de SEP. Figura 6: Diagrama secuencial. Presencia de falla interna en interruptor. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1390 A partir de la arquitectura de este sistema multiagente denominado SIC, en la próxima sección se avanza en el diseño e implementación de dos de sus agentes: el agente DAF y el agente Back-Up. 4. AGENTES DAF Y BACK-UP En esta sección se describen los puntos relevantes del diseño e implementación de los agentes DAF y Back-Up. Del proceso de adquisición del conocimiento resulta natural pensar en un conjunto o jerarquía de procedimientos para determinar cómo estos agentes llevarán a cabo sus tareas. Este proceso deliberativo para decidir qué acción deberá tomar el agente en cada caso, se puede trasladar a la toma de decisión en torno a las intenciones en un sistema intencional. Además, los agentes DAF y Back-Up deben combinar acciones de naturaleza reactiva y proactiva, por lo cual se decide modelizarlos bajo el paradigma BDI, utilizando una arquitectura PRS. La arquitectura PRS4 (originalmente desarrollada Georgeff y Lansky [12]) fue quizás la primera arquitectura basada en el paradigma BDI y se ha convertido en una de las más conocidas. Ha sido utilizada en varias aplicaciones reales (e.g. [9], [13]). Entre las ventajas principales de PRS se destacan: la representación de planes y procedimientos, la posibilidad de utilizar planes parciales, comportamiento Reactivo y Proactivo, y la capacidad de Meta-Razonamiento, es decir planes que razonan o gestionan otros planes. Durante los últimos años se ha construido un gran número de plataformas o entornos para el desarrollo de agentes y en particular de PRS. De las plataformas existentes se buscaron aquellas de código abierto5 y escritas fundamentalmente en lenguaje C. Esta última preferencia se basa en que el código C ha mostrado no tener inconvenientes bajo restricciones de tiempo real duro. La plataforma de desarrollo elegida resultó ser OpenPRS (OPRS) [4], una versión de código abierto de PRS [5]. 4.1. Sistema de Razonamiento Procedural (PRS) Un Sistema de Razonamiento Procedural es un conjunto de herramientas y métodos para la representación y ejecución de planes y procedimientos. Una arquitectura PRS consiste básicamente en: (1) una base de datos con las creencias actuales sobre el entorno; (2) un conjunto de objetivos o deseeos actuales a alcanzar; (3) una librería de planes o procedimientos, que describen secuencias particulares de acciones y pruebas que pueden realizarse para alcanzar ciertos objetivos o para reaccionar ante ciertas situaciones; y (4) una estructura de intenciones, que consiste de un conjunto ordenado (parcialmente) de todos los planes elegidos para ejecución. Estos componentes y sus interacciones se ilustran en la figura 7. Un intérprete (mecanismo de inferencia) manipula estos componentes. Recibe nuevos eventos y objetivos internos; selecciona un plan apropiado teniendo en cuenta los nuevos eventos, objetivos y creencias; ubica el plan dentro de la estructura de intenciones (grafo); elije un plan (intención) en la estructura y finalmente ejecuta un paso del plan activo. Esto puede resultar en una acción primitiva o en la formulación de un nuevo objetivo. El sistema interactúa con el entorno a través de su base de datos y de las acciones básicas o primitivas. Los algoritmos utilizados y el ciclo principal que presenta el Intérprete de OPRS, considerando algunos supuestos razonables, permiten garantizar una cota superior sobre el tiempo de reacción. Por ejemplo, se debe considerar un máximo en la frecuencia de llegada de eventos y se debe verificar que la cardinalidad del conjunto de planes aplicables se decrezca monótonamente. Un análisis detallado de esto puede encontrarse en [3]. 4Procedural Reasoning System 5Open Source XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1391 Figura 7: PRS. Una arquitectura basada en el paradigma BDI 4.2. Diseño Para diseñar un agente con arquitectura PRS es necesario y fundamental especificar sus creencias y también su librería de planes. Con estos componentes, el intérprete (cuya implementación está provista por la plataforma OPRS) genera dinámicamente los componentes de objetivos e intenciones. Las creencias de un agente son básicamente una base de datos de formato simple, la cual consiste en una lista de expresiones (predicados). Un ejemplo de archivo que representa parte de la base de datos del agente DAF en un instante dado es: ( (EXCITED R3 1) (EXCITED R4 1) (POSITION I3 CLOSED) (POSITION I4 CLOSED) (ASSOCIATED-RI R5 I5) (ASSOCIATED-RI R3 I3) ) Podemos observar entonces que el agente DAF cree o tiene conocimiento de los siguientes hechos: los relés R3 y R4 se encuentran excitados, los interruptores I3 e I4 están cerrados y los relés R3 y R5 están asociados a los interruptores I3 e I5 respectivamente. La plataforma OPRS permite especificar los planes mediante una herramienta gráfica. Un ejemplo de plan generado con esta herramienta es la figura 8. Por último, se puede considerar a cada agente como un núcleo (OPRS Kernel) compuesto por un conjunto de planes y por una base de conocimientos. Siguiendo la metodología Prometheus [8], una forma de documentar elegantemente el diseño de un agente, consiste en determinar sus capacidades. Estas pueden pensarse como módulos dentro del agente y pueden estar anidadas dentro de otras capacidades. En el nivel inferior de anidamiento las capacidades se describen en términos de eventos internos, planes y estructuras de datos. Para fines de documentación, cada capacidad puede ser definida mediante un Descriptor de Capacidad, el cual brinda información sobre la interfaz de la capacidad (los eventos que sirven como entrada y los eventos producidos), información sobre interacción con otras capacidades, las capacidades incluidas, y referencias a datos de lectura y escritura. Las funcionalidades descriptas en la sección 3.1 forman un conjunto inicial de capacidades, el cual puede ser refinado si así se quisiera. A continución se describe una de las capacidades del agente DAF a modo de ejemplo y también uno de los planes que comprende esta capacidad (figura 8). Nombre: Aislación. Descripción: Aisla la falla detectada, con posible manejo de incertidumbre. XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1392 Eventos de salida: Ordena disparos (apertura de interruptores). Eventos de entrada: Estado de las protecciones. Capacidades incluidas: Planes: AISLAR-F-B, AISLAR-F-L, AISLAR-F-T, AISLAR-F-TB, APERTURA-CNTRL, APERTURA-REC, APERTURA-VEC, APERTURA-MUL-VEC, ASSOCIATED-LRI, INIT-AIS, INIT-AIS-INCERT, INIT-AIS-INCERT2, OBTENER-RAF-TOTAL Acceso a Datos: Lectura del estado de las protecciones, topología de red. Interacciones: Necesita la información sobre la falla (clase y locación) producida por Detector de Fallas. Figura 8: Un plan para controlar la apertura de un interruptor 5. EXPERIMENTACIÓN Y SIMULACIÓN Se consideraron dos modalidades de prueba. En la primera se supone que el agente DAF parte de un estado en que cuenta con la información correspondiente a situaciones específicas de interés. Basta con enviarle un mensaje de excitación del relé que dará inicio a la detección. En la segunda se considera que el agente DAF no tiene la información que describe cada situación sino que el agente XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   VIII Workshop de Agentes y Sistemas Inteligentes _________________________________________________________________________     1393 ME envía la información simulando un comportamiento posible del SEP para alcanzar dicha situación de prueba. Todos los casos tratados tuvieron la respuesta esperada. Por último se destaca que la herramienta OPRS utilizada brinda una interfaz gráfica (X-OPRS) la cual permite interactuar con cada agente, analizar su respuesta ante estímulos manuales y también observar la evolución de las estructuras cognitivas que lo representan. Esta herramienta visual, disminuye notoriamente la performance (tiempos de respuesta), pero esto carece de importancia puesto que la utilización de la interfaz gráfica es opcional y sus fines consisten en permitir una mejor concepción y comprensión del comportamiento del SIC. 6. CONCLUSIONES En este trabajo se analizó y describió la arquitectura general de un Sistema Inteligente de Control (SIC) para SEPs y la arquitectura propia de dos agentes, el agente de Detección y Aislación de Fallas y el agente de Back-Up. Como resultado se obtiene una arquitectura sumamente flexible, ya que permite realizar cambios en cualquiera de los componentes estudiados en este trabajo, sin afectar de manera importante al resto. Esto vale para el caso del sistema multiagente y también a nivel de agente. A nivel de sistema multi-agente, se cuenta con características importantes como por ejemplo: cooperación de agentes autónomos, lo cual permite el manejo de tareas independientemente, clara interacción entre los agentes a través de mensajes y la posibilidad de trabajar con información de manera descentralizada. A nivel de diseño de los agentes, la arquitectura PRS hace que los agentes desarrollados posean cualidades muy ventajosas. En primer lugar su incrementabilidad, pues permite muy fácilmente satisfacer nuevos requerimientos o cambios en los mismos. Además los procedimientos gozan de una representación muy ventajosa ya que pueden ser parciales, delegando al intérprete la responsabilidad de satisfacer los subobjetivos que estos requieran alcanzar y permitiendo un refinamiento en las tareas. La noción de objetivo es fuerte en PRS, dado que el intérprete considera todos los procedimientos que unifiquen con él para alcanzarlo, antes de considerar que falla. Por otra parte, la combinación de comportamiento proactivo y reactivo (en tiempo acotado) es fundamental para este desarrollo, pues se necesita comportamiento reactivo cada vez que el SEP de indicaciones de fallas potenciales y se requiere comportamiento proactivo para realizar por ejemplo, tareas de detección bajo incertidumbre. Otra ventaja es la posibilidad de controlar parte del ciclo principal mediante meta-razonamiento, utilizando meta-planes. Por último, se destaca que como resultado final de la etapa de diseño se obtiene directamente la implementación del sistema.	﻿sistemas eléctricos de protencia , diseño orientado a agentes , agentes BDI	es	23086
45	Combinando clustering con aproximación espacial para búsquedas en espacios métricos	﻿ El modelo de espacios métricos permite abstraer muchos de los problemas de búsqueda por proximidad. La búsqueda por proximidad tiene múltiples aplicaciones especialmente en el área de bases de datos multimedia. La idea es construir un índice para la base de datos de manera tal de acelerar las consultas por proximidad o similitud. Aunque existen varios índices prometedores, pocos de ellos son dinámicos, es decir, una vez creados muy pocos permiten realizar inserciones y eliminaciones de elementos a un costo razonable. El Árbol de Aproximación Espacial (dsa–tree) es un índice recientemente propuesto, que ha demostrado tener buen desempeño en las búsquedas y que además es totalmente dinámico. En este trabajo nos proponemos obtener una nueva estructura de datos para búsqueda en espacios métricos, basada en el dsa–tree, que mantenga sus virtudes y que aproveche que en muchos espacios existen clusters de elementos y que además pueda hacer un mejor uso de la memoria disponible para mejorar las búsquedas. Palabras Claves: búsqueda por similitud, espacios métricos, bases de datos y algoritmos. 1. Introducción y Motivación Con la evolución de las tecnologías de información y comunicación, han surgido almacenamientos no estructurados de información. No sólo se consultan nuevos tipos de datos tales como texto libre, imágenes, audio y vídeo; sino que además, en algunos casos, ya no se puede estructurar más la información en claves y registros. Aún cuando sea posible una estructuración clásica, nuevas aplicaciones tales como la minería de datos requieren acceder a la base de datos por cualquier campo y no sólo por aquellos marcados como “claves”. Estos tipos de datos son difíciles de estructurar para adecuarlos al concepto tradicional de búsqueda. Así, han surgido aplicaciones en grandes bases de datos en las que se desea buscar objetos similares. Este tipo de búsqueda se conoce con el nombre de búsqueda aproximada o búsqueda por similitud y tiene aplicaciones en un amplio número de campos. Algunos ejemplos son bases de datos no tradicionales, búsqueda de texto, recuperación de información, aprendizaje de máquina y clasificación; sólo para nombrar unos pocos. Como en toda aplicación que realiza búsquedas, surge la necesidad de tener una respuesta rápida y adecuada, y un uso eficiente de memoria, lo que hace necesaria la existencia de estructuras de datos especializadas que incluyan estos aspectos. El planteo general del problema es: existe un universo U de objetos y una función de distancia positiva d: U × U → R+ definida entre ellos. Esta función de distancia satisface los tres axiomas que hacen que el conjunto sea un espacio métrico: positividad estricta (d(x, y) = 0 ⇔ x = y), simetría (d(x, y) = d(y, x)) y desigualdad triangular *Este trabajo ha sido financiado parcialmente por el Proyecto RIBIDI CYTED VII.19 (todos los autores) y por el Centro del Núcleo Milenio para Investigación de la Web, Grant P04-067-F, Mideplan, Chile (último autor). (d(x, z) ≤ d(x, y)+ d(y, z)). Mientras más “similares” sean dos objetos menor será la distancia entre ellos. Tenemos una base de datos finita S ⊆ U que puede ser preprocesada (v.g. para construir un índice). Luego, dado un nuevo objeto del universo (un query q), debemos recuperar todos los elementos similares de que se encuentran en la base de datos. Existen dos consultas básicas de este tipo: Búsqueda por rango: recuperar todos los elementos de S a distancia r de un elemento q dado. Búsqueda de k vecinos más cercanos: dado q, recuperar los k elementos más cercanos a q en S. La distancia se considera costosa de evaluar (por ejemplo, comparar dos huellas dactilares). Así, es usual definir la complejidad de la búsqueda como el número de evaluaciones de distancia realizadas, dejando de lado otras componentes tales como tiempo de CPU para computaciones colaterales, y aún tiempo de E/S. Dada una base de datos de |S| = n objetos el objetivo es estructurar la base de datos de forma tal de realizar menos de n evaluaciones de distancia (trivialmente n bastarían). Un caso particular de este problema surge cuando el espacio es un conjunto D-dimensional de puntos y la función de distancia pertenece a la familia Lp de Minkowski: Lp = ( ∑ 1≤i≤d |xi−yi|p)1/p. Existen métodos efectivos para buscar sobre espacios D-dimensionales, tales como kd-trees [1] o R-trees [5]. Sin embargo, para 20 dimensiones o más esas estructuras dejan de trabajar bien. Nos dedicamos en este trabajo a espacios métricos generales, aunque las soluciones son también adecuadas para espacios D-dimensionales. Es interesante notar que el concepto de “dimensionalidad” se puede también traducir a espacios métricos: la característica típica en espacios de alta dimensión con distancias Lp es que la distribución de probabilidad de las distancias tiene un histograma concentrado, haciendo así que el trabajo realizado por cualquier algoritmo de búsqueda por similaridad sea más dificultoso [2, 4]. Para espacios métricos generales existen numerosos métodos para preprocesar la base de datos con el fin de reducir el número de evaluaciones de distancia [4]. Todas aquellas estructuras trabajan básicamente descartando elementos mediante la desigualdad triangular, y la mayoría usa la técnica dividir para conquistar. El Árbol de Aproximación Espacial Dinámico (dsa–tree) es una estructura de esta clase propuesta recientemente [7], basado sobre un nuevo concepto: más que dividir el espacio de búsqueda, aproximarse al query espacialmente, y además es completamente dinámica. El dinamismo completo no es común en estructuras de datos métricas [4]. Además de ser desde el punto de vista algorítmico interesante por sí mismo, se ha mostrado que el dsa–tree da un buen balance espacio-tiempo respecto de las otras estructuras existentes sobre espacios métricos de alta dimensión o consultas con baja selectividad, lo cual ocurre en muchas aplicaciones. A diferencia de algunas otras estructuras de datos métricas [3], el dsa–tree no saca mayor provecho si el espacio métrico posee clusters, ni puede mejorar las búsquedas a costa de usar más memoria. Nos proponemos obtener un nuevo índice para búsqueda en espacios métricos, basado en el dsa– tree, que mantenga sus virtudes, pero que aproveche que en muchos espacios existen clusters de elementos y que además haga un mejor uso de la memoria disponible para mejorar las búsquedas. 2. Árbol de Aproximación Espacial Dinámico Describiremos brevemente aquí la aproximación espacial y el dsa–tree, más detalles en [6, 7]. Se puede mostrar la idea general de la aproximación espacial utilizando las búsquedas del vecino más cercano. En este modelo, dado q ∈ U y estando en algún elemento a ∈ S el objetivo es moverse a otro elemento de S más cercano “espacialmente” de q que a. Cuando no es posible realizar más este movimiento, estamos posicionados en el elemento más cercano a q de S. Las aproximaciones son efectuadas sólo vía los “vecinos”. Cada a ∈ S tiene un conjunto de vecinos N(a). Para construir incrementalmente al dsa–tree se fija una aridad máxima para el árbol y se mantiene información sobre el tiempo de inserción de cada elemento. Cada nodo a en el árbol está conectado con sus hijos, los cuales forman el conjunto N(a), los vecinos de a. Cuando se inserta un nuevo elemento x, se ubica su punto de inserción comenzando desde la raíz del árbol a y realizando el siguiente proceso. Se agrega x a N(a) (como una nueva hoja) si (1) x está más cerca de a que de cualquier elemento b ∈ N(a), y (2) la aridad del nodo a, |N(a)|, no es ya la máxima permitida. En otro caso, se fuerza a x a elegir el vecino más cercano en N(a) y se continúa bajando en el árbol recursivamente, hasta que se alcance un nodo a tal que x esté más cerca de a que de cualquier b ∈ N(a) y la aridad de a no haya alcanzado la máxima permitida, lo que eventualmente ocurrirá en una hoja del árbol. En ese punto se agrega a x como el vecino más nuevo en N(a), se le coloca a x la marca del tiempo corriente y éste se incrementa. En cada nodo a del árbol se mantiene la siguiente información: el conjunto de vecinos N(a), el tiempo de inserción del nodo tiempo(a) y el radio de cobertura R(a) que es la distancia entre a y el elemento de su subárbol que está más lejos de a. La Figura 1 ilustra el proceso de inserción. Se sigue sólo un camino desde la raíz del árbol al padre del elemento insertado. La función se invoca como Insertar(a,x), donde a es la raíz del árbol y x es el elemento a insertar. El dsa–tree se puede construir comenzando con un único nodo a donde N(a) = ∅ y R(a) = 0, y luego realizando sucesivas inserciones. Insertar (Nodo a, Elemento x) 1. R(a)← máx(R(a), d(a, x)) 2. c← argminb∈N(a)d(b, x) 3. If d(a, x) < d(c, x) ∧ |N(a)| < MaxArity Then 4. N(a)← N(a) ∪ {x} 5. N(x)← ∅, R(x)← 0 6. tiempo(x)← CurrentT ime 7. Else Insertar (c,x) Figura 1: Inserción de un nuevo elemento x dentro del dsa–tree con raíz a. MaxArity es la máxima aridad del árbol y CurrentT ime es el tiempo actual, que luego se incrementa en cada inserción. Notar que no podemos asegurar que un nuevo elemento x sea vecino del primer nodo a que satisfaga estar más cerca de a que de cualquier elemento b ∈ N(a). Es posible que la aridad de a fuera ya máxima y x fuera forzado a elegir un vecino de a, lo cual tiene consecuencias en la búsqueda. Se ha conseguido demostrar experimentalmente en [7], que el desempeño de la construcción mejora a medida que se reduce la aridad máxima del árbol, siendo mucho mejor que la construcción estática del árbol de aproximación espacial (sa–tree). Por lo que la aridad reducida es un factor clave para bajar los costos de construcción. La idea de la búsqueda por rango es replicar el proceso de inserción de los elementos relevantes para la consulta. Es decir, se procede como si se quisiera insertar q pero considerando que los elementos relevantes pueden estar a distancia hasta r de q. Así, en cada decisión, al simular la inserción de q se permite una tolerancia de±r; por lo tanto, puede ser que los elementos relevantes fueran insertados en diferentes hijos del nodo corriente, y sea necesario hacer backtracking. Durante las búsquedas se debe considerar que cuando se insertó un elemento x, un nodo a en su camino pudo no haberse elegido como padre porque su aridad ya era máxima, como se indicó previamente. Entonces, en la búsqueda debemos seleccionar la mínima distancia sólo entre N(a). Otro hecho importante a considerar es que, cuando se insertó x los elementos con timestamp más alto no estaban presentes en el árbol; así, x pudo elegir su vecino más cercano sólo entre los elementos más viejos que él. En otras palabras, consideremos los vecinos {v1 . . . vk} de a de más viejo a más nuevo, sin tener en cuenta a, entre la inserción de vi y la de vi+j pudieron aparecer nuevos elementos que eligieron a vi porque vi+j no estaba aún presente; así, estos elementos se deben tener en cuenta en la búsqueda y no se deben descartar por la existencia de vi+j . Las búsquedas se optimizan usando la información almacenada sobre el tiempo de inserción y el radio de cobertura, dado que nos permiten podar cierta parte del árbol. La Figura 2 muestra el algoritmo de búsqueda, que se invoca inicialmente como BúsquedaporRango(a,q,r,CurrentT ime), donde a es la raíz del árbol. Notar que d(a, q) siempre se conoce, excepto la primer invocación. En las líneas 1 y 6 se puede observar la optimización de la búsqueda por medio del tiempo de inserción y el radio de cobertura, como se destacó en el parrafo anterior. A pesar de la naturaleza cuadrática de la iteración implícita en las líneas 4 y 6, la query, desde luego, se compara sólo una vez contra cada vecino. BúsquedaporRango (Nodo a, Query q, Radio r, Timestamp t) 1. If tiempo(a) < t ∧ d(a, q) ≤ R(a) + r Then 2. If d(a, q) ≤ r Then informar a 3. dmin ← ∞ 4. For bi ∈ N(a) en orden creciente de timestamp Do 5. If d(bi, q) ≤ dmin + 2r Then 6. k ← mín {j > i, d(bi, q) > d(bj , q) + 2r} 7. BúsquedaPorRango (bi,q,r,tiempo(bk)) 8. dmin ← mín{dmin, d(bi, q)} Figura 2: Consulta de q con radio r en un dsa–tree. Experimentalmente [7], se puede concluir que la mejor aridad para la búsqueda depende del espacio métrico, pero la regla, grosso modo, es que aridades bajas son buenas para dimensiones bajas o radios de búsqueda pequeños. Las eliminaciones son más complicadas porque los cambios a realizar en la estructura son más costosos, y no son localizados. Sin embargo, en [7, 8] se muestran distintas posibilidades de eliminación. Podemos concluir que el dsa–tree es completamente dinámico, como ya hemos destacado, y además respecto al sa–tree logra mejorar la performance del costo de construcción. En algunos casos mejora el desempeño de las búsquedas, mientras que en otras paga un pequeño precio por su dinamismo. Por lo tanto, el dsa–tree se convierte en una estructura muy conveniente. 3. Nuestra Propuesta El dsa–tree es una estructura que realiza la partición del espacio considerando la proximidad espacial, pero si el árbol lograra agrupar los elementos que se encuentran muy cercanos entre sí, lograría mejorar las búsquedas al evitarse el recorrido del árbol para alcanzarlos. Así, nos hemos planteado el estudio de una nueva estructura de datos que realice la aproximación espacial sobre clusters o grupos de elementos, en lugar de elementos individuales. Podemos pensar entonces que construimos un dsa–tree, con la diferencia que cada nodo representa un grupo de elementos muy cercanos (“clusters”); y de este modo, logramos relacionar los clusters por su proximidad en el espacio. Por lo tanto, cada nodo de la estructura sería capaz de almacenar varios elementos de la base de datos. La idea sería que en cada nodo se mantenga un elemento, al que se toma como el centro del cluster correspondiente, y se almacenen los k elementos más cercanos a él, cualquier elemento a mayor distancia del centro que los k elementos, pasaría a formar parte de otro nodo en el árbol, que podría ser un nuevo vecino en algunos casos. Al igual que en el dsa–tree para cada nodo n, se mantiene el radio de cobertura R(n), el conjunto de vecinos del nodo N(n), y el tiempo de inserción del nodo tiempo nodo(n). Como cada nodo representará un cluster, se mantendrán el elemento que será el centro del cluster centro(n), los k elementos más cercanos al centro cluster(n), junto con las distancias entre los elementos del cluster y su centro. Dado que como se mantienen la distancias entre el centro del cluster centro(n) y sus elementos, conocemos el radio del cluster rc(n), es decir la distancia al elemento más alejado del centro. Durante las búsquedas, se pueden utilizar ambos radios, R(n) y rc(n), para permitirnos descartar zonas completas del espacio. A continuación daremos detalles de esta nueva estructura. 3.1. Inserción Para construir incrementalmente nuestra estructura de datos, mantenemos las consideraciones del dsa–tree, es decir que fijamos una aridad máxima para el árbol y almacenamos información del tiempo de inserción de cada nodo; pero además, mantenemos el tiempo de inserción tiempo(a) para cada elemento a que se encuentra en el árbol. Al intentar insertar un nuevo elemento en un nodo cuyo cluster ya tiene sus k elementos presentes, debemos decidir cuáles son los k elementos más cercanos al centro que deberían quedar en el cluster para mantener el mínimo volumen del mismo, esto nos permitirá mejorar la performance de las búsquedas. Entonces, para cada elemento en el cluster se podrían almacenar las distancias al centro y mantener los elementos del cluster ordenados por esta distancia; evitando así recalcular distancias para decidir quién queda y, por consiguiente, quién sale del cluster. Además estas distancias juegan un rol importante en el proceso de búsqueda. Debido a la aproximación espacial, al insertar un nuevo elemento x, deberíamos bajar por el árbol hasta encontrar el nodo n tal que x esté más cerca de su centro a que de los centros de los nodos vecinos en N(n). Si en el cluster de ese nodo hay lugar para un elemento más, se lo insertaría junto con su distancia d(x, a). Si no hay lugar, elegiríamos el elemento más distante b entre los k elementos del cluster y x, es decir el k + 1 en el orden de distancias con respecto al centro a. A continuación deberíamos analizar dos casos posibles: Si b es x: x se debería agregar como centro de un nuevo nodo vecino de n, si la aridad de n lo permite; en otro caso, debería elegir el nodo cuyo centro, entre todos los nodos vecinos en N(n), es el más cercano y continuar el proceso de inserción desde allí. Si b es distinto de x: b debería elegir al centro más cercano c entre a y los centros de los nodos vecinos en N(n) que sean más nuevos que b, debido a que cuando b se insertó no se tuvo que comparar con ellos. Luego, si c es a, el proceso que sigue es idéntico a lo realizado cuando b es x; en otro caso, si c no es a, se continúa con la inserción de b desde el nodo cuyo centro es c. La Figura 3 ilustra el proceso de inserción. La función es invocada como InsertarCluster(a,x), donde a es la raíz del arbol y x es el elemento a insertar. Como se puede observar, al igual que en el dsa–tree, sólo seguimos un camino desde la raíz del árbol hasta el cluster o el nodo padre, en caso de que sea un nuevo centro vecino, donde va a ser insertado el elemento. En la línea 3 se decide si el elemento x cae en el cluster del correspondiente nodo a, en primer lugar x debe estar más cerca del centro de a que de los centros de sus vecinos, y posteriormente determinamos si hay lugar dentro del cluster o si su distancia determina que debe ir dentro de él. Aquí se pueden observar los dos casos posibles que analizamos previamente, siempre y cuando el cluster ya contenga sus k elementos. El primer caso corresponde con la rama del Else, línea 11, de la condición que determina si el elemento debe pertenecer al cluster (línea 3). Luego, se determina si al elemento se lo puede ubicar como el centro de un nuevo nodo vecino b, siempre y cuando la máxima aridad lo permita; en caso contrario, la inserción va a continuar por el centro vecino más cercano. El segundo caso que analizamos, se corresponde la rama del Then (línea 4) de la condición, donde el elemento se inserta en el cluster del nodo, pero el elemento más distante se reinsertará en el árbol; líneas 8, 9 y 10. Podemos observar que cuando se inserta un nuevo elemento x en el cluster se almacena la distancia al centro d′(x) y su tiempo de inserción tiempo(x), lineas 5 y 6 respectivamente. Cabe destacar que cuando se reinserta un elemento y, por quedar afuera del cluster (línea 10), éste ya tendrá su tiempo de inserción. Por lo tanto, este valor será reutilizado en vez de asignarle uno nuevo. Además el elemento y sólo se compara con los centros vecinos más nuevo que él en la línea 2. InsertarCluster (Nodo a, Elemento x) 1. R(a)← máx(R(a), d(centro(a), x)) // Sea rc(a) la distancia desde centro(a) al elemento más alejado en cluster(a) 2. c← argminb∈N(a)d(centro(b), x) 3. If (d(centro(a), x) < d(centro(c), x)) ∧ ((|cluster(a)| < k) ∨ (d(centro(a), x) < rc(a))) Then 4. cluster(a)← cluster(a) ∪ {x} 5. d′(x)← d(centro(a), x) 6. tiempo(x)← CurrentT ime 7. If (|cluster(a)| = k + 1) Then 8. y ← elemento en cl(a) con mayor d′ 9. cluster(a)← cluster(a)− {y} 10. InsertarCluster(a,y) 11. Else 12. If d(centro(a), x) < d(centro(c), x) ∧ |N(a)| < MaxArity Then 13. N(a)← N(a) ∪ {b} //b es el nuevo nodo vecino de a, con x como centro 14. centro(b)← x 15. N(b)← ∅, R(b)← 0 16. cluster(b)← ∅ 17. tiempo(x)← CurrentT ime 18. tiempo nodo(b)← CurrentT ime 19. Else 20. InsertarCluster (c,x) Figura 3: Inserción de un nuevo elemento x en el árbol con raíz a. MaxArity es la máxima aridad del árbol, k la capacidad del cluster y CurrentT ime el tiempo actual. El árbol se puede construir comenzando con un único primer nodo a, donde contiene el elemento centro centro(a), N(a) = ∅, R(a) = 0 y cluster(a) = ∅; y luego realizando sucesivas inserciones. Los siguientes k inserciones, desde el primer único nodo se tomarán como los k objetos del cluster del nodo raíz y luego, al insertar el k +2, recién se creará un nuevo nodo cuyo centro será el elemento más distante del cluster del nodo raíz. Teniendo en cuenta la manera en que se realizan las inserciones, es posible observar que ninguna inserción modificaría el centro de un cluster y además que es posible que durante una inserción se cree a lo sumo un nuevo nodo y que ésta afecte posiblemente a varios nodos. 3.2. Búsquedas En la búsqueda de un elemento q con radio r procedemos similarmente al dsa–tree, es decir realizando aproximación espacial entre los centros de los nodos. También consideramos los dos hechos importantes que influyen en el proceso de búsqueda del dsa–tree. El hecho de que cuando insertamos (o reinsertamos) un elemento x, un nodo a pudo no haberse elegido como padre por que su aridad ya era máxima; y que x pudo elegir su centro vecino más cercano sólo entre los nodos más viejos que x. Además de las consideraciones previamente mencionadas, al tener clusters en los nodos debemos verificar si hay o no intersección entre la zona consultada y el cluster. Más aún, si no la hay se pueden descartar todos los elementos del cluster sin necesidad de compararlos contra q. A través del radio del cluster de un nodo a, rc(a), verificamos si existe dicha intersección. Si el cluster no se pudo descartar para la consulta, usamos al centro de cada nodo como un pivote para los elementos xi que se encuentran en el cluster, ya que mantenemos las distancias d′(xi) respecto del centro del nodo a. De esta manera es posible que, evitemos algunos cálculos de distancia entre q y los xi, si |d(q, centro(a)) − d′(xi)| > r. Es importante destacar que si la zona consultada cae completamente dentro de un cluster, podemos estar seguros que en ninguna otra parte del árbol encontraremos elementos relevantes para esa consulta. La Figura 4 muestra el algoritmo de búsqueda, que se invoca inicialmente como BúsquedaRangoCluster(a,q,r,CurrentT ime), donde a es la raíz del árbol. En la línea 3 del algoritmo, se determina si existe intersección entre el cluster y la zona consultada, posteriormente en caso de que haya interseccion la línea 7 determina si la búsqueda debe continuar o no. BúsquedaRangoCluster (Nodo a, Query q, Radio r, Timestamp t) 1. If tiempo nodo(a) < t ∧ d(centro(a), q) ≤ R(a) + r Then 2. If d(centro(a), q) ≤ r Then Informar a 3. If (d(centro(a), q) − r ≤ rc(a)) ∨ (d(centro(a), q) + r ≤ rc(a)) Then 4. For ci ∈ cluster(a) Do 5. If |(d(centro(a), q) − d′(ci)| ≤ r Then 6. If (d(ci, q) ≤ r Then Informar ci 7. If d(centro(a), q) + r < rc(a) Then Terminar búsqueda 8. dmin ← ∞ 9. For bi ∈ N(a) en orden creciente de timestamp Do 10. If d(centro(bi), q) ≤ dmin + 2r Then 11. k ← mín {j > i, d(centro(bi), q) > d(centro(bj), q) + 2r} 12. BúsquedaRangoCluster (bi,q,r,time(bk)) 13. dmin ← mín{dmin, d(centro(bi), q)} Figura 4: Consulta de q con radio r en nuestra nueva estructura. 4. Resultados Experimentales Para los experimentos hemos seleccionado dos espacios métricos diferentes. Imágenes de la NASA: 40700 vectores de características, en dimensión 20, generado de imágenes descargadas de la NASA. Usamos la distancia Euclídea. http://www.dimacs.rutgers.edu/Challenges/Sixth/software.html. Éste es un espacio fácil (el histograma de distancia no es tan concentrado). Palabras: un diccionario con 69069 palabras en inglés. La distancia es la distancia de edición, esto es, el mínimo número de inserciones, eliminaciones o cambios de caracteres necesarios para que dos palabras sean iguales. Se usa en recuperación de textos para manejar errores de ortografía, de escritura y reconocimiento óptico de caracteres (OCR). Es de dificultad media baja. En ambos casos, construimos los índices con el 90 % de los elementos y usamos el 10 % restante (seleccionado aleatoriamente) como consultas. Hemos considerado en las consulta por rango recuperar el 0.01 %, el 0.1 % y el 1 % de la base de datos, cuando la función de distancia es continua. Esto quiere decir radios 0,605740, 0,780000 y 1,009000 para las imágenes de la NASA. Como las palabras tienen una distancia discreta, entonces usamos radios de 1 a 4, el cual recupera el 0.00003 %, 0.00037 %, 0.00326 % y 0.01757 % de la base de datos, respectivamente. Las mismas consultas fueron usadas para todos los experimentos sobre la misma base de datos. Realizamos 10 ejecuciones de cada experimento sobre permutaciones distintas de la base de datos; por lo tanto, los resultados exhibidos corresponden a los valores medios obtenidos. Como existen algoritmos de rango óptimos para las búsquedas de los k-vecinos más cercanos, no hemos realizado experimentos para esta clase de búsquedas separadamente. Algunos de los experimentos que hemos realizado comparan el costo de construcción incremental de esta estructura contra el dsa–tree y el sa–tree para conjuntos crecientes de la base de datos. Experimentamos con los tamaños de cluster 10, 50, 100, 150 y 200, y con las aridades 2, 4, 8, 16, 32 y sin límite de aridad. De los resultados obtenidos podemos deducir principalmente que la aridad reducida es un factor clave para bajar los costos de construcción como ocurria con el dsa–tree, pero además el tamaño del cluster es también un factor importante para el costo de construcción, ya que a medida que el cluster crece los costos decrecementan significativamente. En la Figura 5 se muestran los costos de construcción para el espacio de vectores de imágenes de la NASA, comparando el comportamiento de los distintos tamaños de cluster para cada aridad. En la Figura 6 se muestran los costos de construcción para el espacio de palabras, comparando también el comportamiento de los distintos tamaños de cluster para cada aridad.  0  100  200  300  400  500  600  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=40700 vectores, dim. 20 y Aridad=2 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=40700 vectores, dim. 20 y Aridad=4 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=40700 vectores, dim. 20 y Aridad=8 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=40700 vectores, dim. 20 y Aridad=16 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=40700 vectores, dim. 20 y Aridad=32 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=40700 vectores, dim. 20 y sin limite de Aridad Estatico K=10  K=50 K=100 K=150 K=200 Figura 5: Costos de construcción para el espacio de vectores de imágenes de la NASA, utilizando distintas aridades y comparando los distintos tamaños de cluster.  0  100  200  300  400  500  600  700  800  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=69069 palabras y Aridad=2 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  700  800  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=69069 palabras y Aridad=4 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  700  800  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=69069 palabras y Aridad=8 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  700  800  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=69069 palabras y Aridad=16 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  700  800  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=69069 palabras y Aridad=32 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  0  100  200  300  400  500  600  700  800  10  20  30  40  50  60  70  80  90  100 Ev al ua ci on  d e  di st an ci as  ( x  10 00 0) Porcentaje de la base de datos usada Costo de Construccion para N=69069 palabras y sin limite de Aridad Estatico K=10  K=50 K=100 K=150 K=200 Figura 6: Costos de construcción para el espacio de palabras, utilizando distintas aridades y comparando los distintos tamaños de cluster. En las Figuras 7 y 8 se muestran los costos promedios de búsqueda de un elemento para el espacio de vectores de imágenes de la NASA. La Figura 7 compara el comportamiento de los distintos tamaños de cluster para cada aridad. Como puede observarse en este espacio para todas las aridades e incluso no limitando la aridad, el mejor tamaño de cluster para las búsquedas es k = 10. Así, en Figura 8 mostramos la comparación usando k = 10, para las distintas aridades (a la izquierda). Claramente la mejor aridad para todos los radios de búsqueda considerados es 8. A la derecha de Figura 8 comparamos nuestra estructura con sus mejores parámetros contra el dsa–tree para todas las aridades. Es destacable que hemos obtenido un mejor desempeño en las búsquedas que el dsa–tree y el sa–tree. En las Figuras 9 y 10 se muestran los costos promedios de búsqueda de un elemento para el diccionario. La Figura 9 compara los distintos tamaños de cluster para cada aridad y la Figura 10 muestra los mismos resultados con los mejores tamaños de cluster observados, es decir para k = 10 y k = 50. En aridades altas mejora el comportamiento usando k = 50 para radios pequeños. Si usamos k = 10 mejoran las búsquedas en aridades bajas para todos los radios y en aridades altas para radios grandes. En este espacio también los mejores parámetros para las búsquedas parecen ser k = 10 con aridad ilimitada, o con aridad 32, porque se comportan mejor que k = 50 para los radios 1, 2 y 3. Podemos observar que hemos logrado mejorar el desempeño de la búsqueda del sa–tree en el diccionario, donde el dsa–tree no lo lograba [7]. La aridad 32 e ilimitada son las que obtienen mejor  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as   Porcentaje de la base de datos recuperada Costo de Consulta por elemento para N=40700 vectores, dim. 20 y Aridad=2 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as   Porcentaje de la base de datos recuperada Costo de Consulta por elemento para N=40700 vectores, dim. 20 y Aridad=4 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as   Porcentaje de la base de datos recuperada Costo de Consulta por elemento para N=40700 vectores, dim. 20 y Aridad=8 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as   Porcentaje de la base de datos recuperada Costo de Consulta por elemento para N=40700 vectores, dim. 20 y Aridad=16 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as   Porcentaje de la base de datos recuperada Costo de Consulta por elemento para N=40700 vectores, dim. 20 y Aridad=32 Estatico Dinamico K=10  K=50 K=100 K=150 K=200  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as   Porcentaje de la base de datos usada Costo de Consulta por Elemento para N=40700 vectores, dim. 20 y Sin Limite de Aridad Estatico K=10  K=50 K=100 K=150 K=200 Figura 7: Costos de búsqueda para el espacio de vectores de imágenes de la NASA, utilizando distintas aridades y comparando los distintos tamaños de cluster. desempeño superando ampliamente al sa–tree y al dsa–tree. 5. Conclusiones y Trabajo Futuro Hemos presentado una nueva estructura para búsqueda en espacios métricos que permitiendo agrupar los elementos de la base de datos y manteniendo el dinamismo del dsa–tree, logra reducir significativamente los costos de búsqueda. Además hemos mejorado también el costo de contrucción. Para los costos de construcción el tamaño del cluster juega un rol importante, mientras mayor sea el cluster menor sera el costo. La aridad reducida también es un factor clave para reducir el costo de construcción. La situación para las búsquedas es distinta, el tamaño de cluster y la aridad dependen del espacio métrico en particular. Aunque el tamaño de cluster para obtener un mejor desempeño en las búsquedas dependa del espacio, si comparamos con el sa–tree y dsa–tree, podemos asegurar que independientemente del tamaño del cluster elegido lograremos reducir o mantener el costo de construcción. Entre los temas que pretendemos analizar a futuro, podemos citar la siguientes: Experimentar con otros espacios métricos y analizar los resultados obtenidos. Analizar y experimentar con las distintas técnicas de eliminación existentes para el árbol de aproximación espacial.  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as Porcentaje de la base de datos recuperada Costo de Consulta por Elemento para N=40700 vectores, dim. 20 y K=10 Estatico sin limite de aridad Aridad 2 Aridad 4 Aridad 8 Aridad 16 Aridad 32  5000  6000  7000  8000  9000  10000  11000  12000 0.01 0.1 1 Ev al ua ci on  d e  di st an ci as Porcentaje de la base de datos recuperada Costo de Consulta por Elemento para N=40700 vectores, dim. 20 Aridad 8- k=10 Aridad 2 Aridad 4 Aridad 8 Aridad 16 Aridad 32 Figura 8: Costos de búsqueda para el espacio de vectores de imágenes de la NASA, utilizando tamaño de cluster k = 10. Dado que los nodos tienen tamaño fijo, esta estructura parecería adecuada para memoria secundaria. Pero, ¿sería realmente eficiente en memoria secundaria? ¿Es posible mantener el cluster separado del nodo, y permitir que en cada nodo se almacene el centro, con su información asociada y los centros vecinos? ¿Sería una mejor opción para memoria secundaria? ¿Sería más adecuado mantener los clusters con cantidad fija de elementos o con radio fijo? ¿Existen otras maneras de combinar técnicas de clustering con aproximación espacial para búsquedas en espacios métricos?	﻿espacios métricos , búsqueda por similitud	es	23138
46	Una propuesta para la selección de pivotes en indices métricos	﻿ Muchas aplicaciones en computación tienen por objetivo buscar objetos en una base de datos que sean similares a uno dado. Todas estas aplicaciones pueden tratarse en abstracto con el formalismo de espacio métrico. Este método encapsula las propiedades de los objetos de la base de datos y permite construir índices genéricos. Existen muchas técnicas de construcción de índices para realizar búsquedas de objetos similares. En este trabajo nos hemos centrado en las técnicas basadas en pivotes, las cuales construyen el índice en torno a un grupo de puntos estratégicos de la base de datos denominados pivotes. El grupo de pivotes utilizado en la construcción del índice no afecta en absoluto la efectividad del mismo, pero es crucial para su eficiencia. Es por esta razón que el tema de selección de un buen grupo de pivotes está siendo ampliamente estudiado. En este artículo presentamos el diseño de dos nuevas técnicas para la selección de un buen grupo de pivotes. Palabras claves: Bases de Datos, Espacios Métricos, Indices, Selección de Pivotes. *Este trabajo ha sido parcialmente subvencionado por el proyecto Tecnologías Avanzadas de Bases de Datos (22/F314), Universidad Nacional de San Luis. 1. Introducción Con la evolución de los sistemas y las tecnologías de información, las bases de datos actuales han incluido la capacidad de almacenar datos no estructurados tales como imágenes, sonido, video, huellas digitales, entre otros. Las búsquedas que son de interés en ese tipo de bases de datos, son aquellas que recuperan objetos similares a un elemento dado. Este tipo de búsqueda se conoce con el nombre de búsqueda por similitud y surge en áreas tales como reconocimiento de voz, reconocimiento de imágenes, compresión de texto, recuperación de texto, biología computacional, por nombrar algunas. El conjunto X de todos los objetos sobre los cuales se puede realizar la búsqueda, junto con una función de distancia d que mide la similitud entre los elementos de X , se denomina espacio métrico, y se denota (X, d). La base de datos será un conjunto U ⊆ X . La función de distancia d : X×X → R+ debe satisfacer las propiedades que caracterizan a una métrica, a saber: 1. ∀x, y ∈ X : d(x, y) ≥ 0 (positividad) 2. ∀x, y ∈ X : d(x, y) = d(y, x)( simetría) 3. ∀x, y, z ∈ X : d(x, y) ≤ d(x, z) + d(z, y) (desigualdad triangular) Si bien existen distintos tipos de búsquedas por similitud, una de las más comunes es la búsqueda por rango. Esta búsqueda, que denotaremos con (q, r)d, consiste en recuperar todos los elementos de U cuya distancia a un elemento q dado no supere un rango de tolerancia r; en símbolos (q, r)d = {u ∈ U : d(q, u) ≤ r}. Nos referiremos a q como elemento de consulta o query. Hay tres factores que afectan el tiempo necesario para resolver una búsqueda por similitud; por un lado tenemos la cantidad de evaluaciones de la función de distancia d que se realizaron durante el proceso de búsqueda; por otro lado tenemos una cierta cantidad de operaciones adicionales que implican un tiempo extra de CPU; finalmente, tenemos un tiempo de I/O determinado por la cantidad de accesos a memoria secundaria, si es que fuera necesario. En consecuencia, el tiempo total de resolución de una búsqueda se puede calcular de la siguiente manera: T = # evaluaciones de d× complejidad(d) + Tpo de CPU + Tpo de I/O En muchas aplicaciones el cálculo de la función de distancia d es tan costoso que las demás componentes que afectan el tiempo pueden ser despreciadas. En estos casos, la medida de complejidad es la cantidad de evaluaciones de la función de distancia d realizadas al momento de resolver una consulta. Una búsqueda por similitud puede ser resuelta con O(n) evaluaciones de distancias examinando exhaustivamente la base de datos. Para evitar esta situación, se preprocesa la base de datos por medio de un algoritmo de indexación con el objetivo de construir una estructura de datos o índice, diseñada para ahorrar cálculos en el momento de resolver una búsqueda. Un algoritmo de indexación se considera eficiente si puede responder una búsqueda por similitud haciendo una cantidad pequeña de cálculos de distancia, sublineal en la cantidad de elementos de la base de datos. En [7] se presenta un desarrollo unificador de las soluciones existentes en la temática. En dicho trabajo se muestra que todos los enfoques para la construcción de índices en espacios métricos consisten en: particionar el espacio en clases de equivalencia. indexar las clases de equivalencia. durante la búsqueda, usando el índice y la desigualdad triangular, descartar algunas clases y buscar exhaustivamente en las restantes. La diferencia entre los distintos algoritmos radica en cómo construyen estas clases de equivalencia. Básicamente se pueden distinguir dos enfoques: algoritmos basados en pivotes y algoritmos basados en particiones compactas. En este trabajo nos hemos centrado sobre algoritmos basados en pivotes [1, 2, 3, 5, 6, 7], dedicándonos al estudio de la problemática de selección de un buen grupo de pivotes. Este artículo se organiza de la siguiente manera. Comenzamos dando una introducción a algoritmos de indexación basados en pivotes. Luego, en la sección 3, describimos los trabajos existentes sobre la problemática de selección de un buen grupo de pivotes. En la sección 4, presentamos nuestro aporte que consiste en el diseño de dos nuevos algoritmos para la selección de pivotes. Finalizamos el artículo en la sección 5 dando las conclusiones y el trabajo futuro. 2. Algoritmos Basados en Pivotes Este grupo de algoritmos definen una relación de equivalencia basados en la distancia de los elementos a un conjunto de elementos preseleccionados que llamaremos pivotes. Sea {p1, p2, . . . , pk} el conjunto de pivotes, dos elementos son equivalentes si y solo si están a la misma distancia de todos los pivotes: x ∼{pi} y ⇔ d(x, pi) = d(y, pi), ∀i = 1 . . . k Gráficamente, cada clase de equivalencia está definida por la intersección de varias capas de esferas centradas en los puntos pi (ver figura 1, izquierda). Durante la indexación, se seleccionan k pivotes {p1, p2, . . . , pk}, y se le asigna a cada elemento a de la base de datos, el vector o firma: Φ(a) = (d(a, p1), d(a, p2), . . . , d(a, pk)) Durante la búsqueda, se usa la desigualdad triangular junto con la firma de cada elemento para filtrar objetos de la base de datos sin medir su distancia a la query q. Dada (q, r)d, se computa la firma de la query q, Φ(q) = (d(q, p1), d(q, p2), . . . , d(q, pk)), y luego se descartan todos aquellos elementos a, tales que para algún pivote pi se cumple que | d(q, pi)− d(a, pi) |> r, es decir: max1≤i≤k{| d(a, pi)− d(q, pi) |} = L∞(Φ(a),Φ(q)) ≤ r Los elementos no descartados forman parte de una lista de candidatos, que posteriormente se comparan directamente con la query q. Esto significa que la cantidad total de cálculos de la función de distancia d queda determinada por la cantidad de pivotes k más la cardinalidad de la lista de candidatos. Notar que la relación de equivalencia definida por un conjunto de k pivotes también puede verse como una proyección al espacio vectorial Rk. La i-ésima coordenada de un elemento es la distancia al i-ésimo pivote. Esto significa que hemos proyectado el espacio métrico original (X , d) en el espacio vectorial Rk con la función de distancia L∞ (ver figura 1). La mayoría de los índices basados en pivotes seleccionan en forma aleatoria elementos de la base de datos, para formar su grupo de pivotes. Sin embargo, se sabe que la política usada en la selección de pivotes afecta notablemente la performance de la búsqueda [4, 7, 9, 10]. Esto significa que si tenemos dos conjuntos de pivotes del mismo tamaño elegir el mejor de los dos para indexar la base de datos, puede reducir la cardinalidad de la lista de candidatos y, en consecuencia, reducir el tiempo de u13 u4 u11 u2 u12 u3 u7 u1 u15 u14 u6 u8 u8 u11 u3 u7u4 u8 u9 u10 u11 u6 u15 q u5 u5 q u10 u9 u13 u1 u14 u2 u12 Figura 1: Un ejemplo de la relación de equivalencia inducida por la intersección de anillos centrados en dos pivotes, u8 y u11 (izquierda); y su correspondiente transformación en un espacio vectorial de dimensión 2 (derecha). También se ilustra la transformación de una búsqueda (q, r)d. búsqueda. Por otro lado, un grupo pequeño de pivotes bien elegidos puede resultar tan eficiente como un grupo de mayor cantidad de pivotes pero elegidos aleatoriamente. El grupo de pivotes utilizados para la construcción del índice no afecta en absoluto la efectividad del mismo pero es crucial para su eficiencia. Por lo tanto, el tema de selección de un buen grupo de pivotes para indexar un determinado espacio métrico está siendo ampliamente estudiado. 3. Trabajos Relacionados 3.1. Selección Incremental de Pivotes En [4] se proponen tres técnicas para la selección de un buen grupo de pivotes. Dichas técnicas tratan de maximizar la media µD de la distribución de D, donde: D([x], [y]){p1,...,pk} = max1≤i≤k{ | d(x, pi)− d(y, pi) | } De las tres técnicas allí presentadas, la que muestra un mejor desempeño es selección incremental. Este método consiste en tomar una muestra de N elementos de la base de datos y seleccionar como primer pivote p1 a aquel elemento que tenga el máximo valor para µD. El segundo pivote p2 se elige de otra muestra de N elementos de forma tal que {p1, p2} tenga el máximo valor para µD. Este proceso se repite hasta terminar de elegir los k pivotes necesitados. En dicho trabajo se muestra que un buen grupo de pivotes tiene dos características básicas: los pivotes están alejados unos de otros, es decir, la distancia media entre pivotes es mayor que la distancia media entre elementos tomados al azar del espacio métrico. los pivotes están alejados del resto de los elementos del espacio métrico. Los elementos que tienen estas dos propiedades se denominan outliers. También se observa que, si bien buenos pivotes tienen la propiedad de ser outliers, no todos los outliers son eficientes como pivotes. A partir de los resultados experimentales se concluye que los conjuntos de outliers tienen un buen desempeño en espacios vectoriales uniformemente distribuidos, pero tienen una baja performance en espacios métricos generales, aún peor que una selección aleatoria. 3.2. Selección Dinámica del Conjunto de Pivotes En [8] se presenta un enfoque alternativo al problema de selección de pivotes que consiste en una selección dinámica del conjunto de pivotes y, por consiguiente, del índice sobre el que se resolverá la consulta. Para lograr esto, en lugar de seleccionar durante la construcción del índice un grupo de pivotes que sea efectivo para todo el espacio métrico, se selecciona durante la búsqueda un grupo de pivotes que sea efectivo para la query q. Para ello, se construyen varios índices sobre el espacio con distintos grupos de pivotes (elegidos aleatoriamente); luego, durante una búsqueda (q, r)d se selecciona aquel índice que sea más adecuado a q de acuerdo al conjunto de pivotes con el que fue construido. Esta selección dinámica de índices permite además realizar consultas en paralelo, si cada uno de los índices creados se mantiene en distintas máquinas de una red. Supongamos que se generan M índices de k pivotes cada uno. Si sólo tomamos en cuenta la cantidad de evaluaciones de distancia, esta idea pierde al compararla con la opción de tener un sólo índice de Mk pivotes. Pero si tomamos en cuenta tiempo extra de CPU y tiempo de I/O, la idea de varios índices pequeños aventaja a la opción de un sólo índice con mayor cantidad de pivotes. En [8] se presentan varias heurísticas para la selección del índice adecuado, ellas son: selección por votos, pivote más cercano, pivote más lejano, menor masa total, pivote de menor masa y votación global. A partir de la evaluación experimental de las mismas se muestra que las de mejor desempeño son las heurísticas pivote de menor masa y votación global: PIVOTE DE MENOR MASA: Dado un pivote p y una búsqueda (q, r)d sabemos que los elementos que no pueden ser eliminados por p son aquellos x tales que d(x, p) ∈ [d(p, q)− r, d(p, q) + r]. Consideremos el histograma local del pivote p (ver figura 2). Este histograma permite ver la distribución de los elementos u del espacio respecto de p. El eje x representa los distintos valores para d(u, p) y el eje y muestra la cantidad de elementos del espacio que están a una determinada distancia de p. En esta figura la cantidad de elementos que no podrán eliminarse ante una búsqueda (q, r)d, corresponde al área sombreada. Llamaremos a esta zona la masa de p para la query q, y la denotaremos con m(p, q). Suponiendo que se han generado M índices de k pivotes cada uno, la heurística de selección de pivote de menor masa selecciona aquel índice que contenga el pivote de menor masa para una query q dada. Si denotamos con pij al j−ésimo pivote del índice i, podemos expresar lo anterior de la siguiente manera (∀i)1≤i≤M (∀j)1≤j≤k : calcular m(pij, q) Seleccionar aquel índice i tal que m(pij, q) sea mínima para algún j, con 1 ≤ j ≤ k. Dado que durante la construcción de un índice se calculan las distancias de todos los elementos de la base de datos a todos los pivotes, en ese momento es posible obtener y almacenar el histograma local de todos los pivotes. De esta manera, durante la búsqueda, no hay costo adicional de tiempo por cálculo de histogramas. VOTACIÓN GLOBAL : Todas las heurísticas planteadas (selección por votos, pivote más cercano, pivote más lejano, menor masa total y pivote de menor masa) permiten no sólo seleccionar un índice sino ordenar todos los índices según la conveniencia para una búsqueda (q, r)d. 2 r d (p, q) d (p, u) Figura 2: Histogramas local de un pivote p. El área sombreada representa los elementos que no pueden ser eliminados por p ante una búsqueda (q, r)d Dada una query q y una heurística h, puede suceder que h cometa un error y, en consecuencia, el índice que resulta seleccionado no es el óptimo para q. Esto significa que si miramos los índices ordenados según la visión de h para q , el índice óptimo queda desplazado del primer lugar. La técnica de votación global, mediante la asignación de votos a los índices, trata de captar la información dada por cada una de las heurísticas con el objetivo de bajar la probabilidad de error en la selección del índice. Sea 〈Ij1, Ij2, . . . , IjM 〉 la secuencia de índices ordenados por una heurística h de acuerdo a su conveniencia para la query q. Luego, Ijs recibe una cantidad de votos que depende de la probabilidad de que la técnica considerada desplace a la posición s el índice óptimo para q. Dada una query q, se realiza el proceso de votación con las heurísticas selección por votos, pivote más cercano, pivote más lejano, menor masa total y pivote de menor masa. El índice que finalmente resulta electo es aquel que recibe la mayor cantidad de votos totales. En [8] también se analizan las características que presentan los pivotes de los índices más competitivos, siendo la observación más importante que los mejores índices son aquellos cuyo grupo de pivotes tienen una mayor varianza. 4. Nuevos Algoritmos para la Selección de Pivotes Basándonos en los resultados que se muestran en [4] y [8] diseñamos dos nuevas técnicas para la selección de un buen grupo de pivotes. Estas técnicas permiten elegir elementos de la base de datos que tienen característica apropiadas para ser considerados buenos pivotes. Explicamos a continuación cada una de ellas y realizamos el análisis de complejidad de las mismas, midiendo la cantidad de evaluaciones de distancia. 4.1. Selección incremental maximizando la varianza Tal como se muestra en [4], la selección incremental maximizando µD resulta efectiva. Por otro lado en [8] se señala que los índices más competitivos tienen pivotes que maximizan la varianza. La política de selección que proponemos combina estos resultados permitiendo realizar una selección incremental maximizando la varianza. Básicamente el proceso es el siguiente. El primer pivote p1 se elije de una muestra de N elementos de manera tal que ese único pivote maximice la varianza σD. Habiendo fijado p1, el segundo pivote p2 se elije de otra muestra de N elementos de manera tal que el conjunto {p1, p2} maximice la varianza σD. Este proceso se repite hasta completar los k pivotes requeridos. Seleccionar Máxima Varianza( in k); 1. P = Ø 2. Para i = 1 hasta k hacer 3. pi = Seleccionar pi(P) 4. P = P ∪ pi 5. Fin Para i 9. Retornar P Seleccionar pi( in P); 1. Elegir en forma aleatoria el conjunto {u1, u2, . . . , uN} 2. máx = −∞ 3. Para j = 1 hasta N hacer 4. Si σD{P ∪uj} > máx entonces 5. máx = σD{P ∪uj} 6. pi = uj 7. Fin Si 8. Fin Para j 9. Retornar pi Figura 3: Algoritmo de selección incremental maximizando la varianza La figura 3 muestra este proceso de selección. El algoritmo sólo necesita como parámetro de entrada la cantidad k de pivotes a seleccionar y retorna como salida el conjunto P que contendrá los k pivotes seleccionados. En el paso i, el conjunto P contendrá los i−1 pivotes ya seleccionados y se procederá a seleccionar el i−ésimo pivote. Para la selección del pivote pi se requiere calcular σD{P∪uj} para los distintos uj. Si el espacio tiene n elementos, el cálculo exacto de esta varianza implicaría O(n2) evaluaciones de distancias. Siguiendo las ideas dadas en [4], podemos evitar este costo estimando σD{p1,...,pi} de la siguiente manera: Seleccionar en forma aleatoria A pares de elementos (a1, a′1), (a2, a′2), . . . , (aA, a′A). Para cada par (at, a′t) calcular Dt = D([at], [a′t]){p1,...,pi} = max1≤j≤i{ | d(at, pj)− d(a′t, pj) | } Esto produce como resultado el conjunto de valores {D1, D2, . . . , DA}. Estimar la media µD{p1,...,pi} de la siguiente forma: µD{p1,...,pi} = 1 A ∑ 1≤j≤A Dj Estimar la varianza σD{p1,...,pi} de la siguiente forma: σD{p1,...,pi} = 1 A ∑ 1≤j≤A (Dj − µD{p1,...,pi}) 2 Una observación importante es que el mismo conjunto de A elementos debe usarse en todo el proceso de selección de los k pivotes, caso contrario los resultados obtenidos en cada estimación no serían comparables porque corresponderían a distintas muestras de elementos. Dado que hay A pares de elementos que se comparan con los i pivotes, esta estimación de la varianza requiere 2Ai evaluaciones de distancia. Como A es una constante del algoritmo y el mayor valor posible para i es k, tenemos que lo anterior es O(k). Para la selección de un pivote, se realiza esta estimación N veces, lo que implica NO(k) evaluaciones de distancia, que es O(k) (N es una constante del algoritmo). Como en total se seleccionan k pivotes, podemos concluir que la complejidad del proceso de selección maximizando la varianza tiene una complejidad O(k2). 4.2. Selección incremental por votos Esta técnica realiza una adaptación de las heurísticas más prometedoras presentadas en [8] a fin de poder realizar una selección estática de pivotes. Para seleccionar un grupo de k pivotes, se realizan los siguientes pasos: Se seleccionan M grupos de h pivotes cada uno (con h < k). Se selecciona un grupo de A queries. Para cada query, se calcula cuál de los M grupos contiene el pivote de menor masa para la query. A ese grupo se le asigna un voto. El grupo que resulta con mayor cantidad de votos pasa a formar parte del conjunto de pivotes. Se repiten los pasos anteriores hasta completar los k pivotes requeridos. En la figura 4 se muestra el pseudocódigo de este proceso. El proceso votar (in q) , toma como entrada un query y selecciona aquel grupo g que contiene el pivote de menor masa para q, asignándole un voto a dicho grupo. Cuando cada una de las A queries ha realizado el proceso de votación, se agregan al conjunto P los pivotes del grupo más votado. Cabe señalar que M , h y A son constantes para este algoritmo. Veamos ahora cuál es el costo de este proceso de selección, medido en cantidad de evaluaciones de distancias. El primer punto donde se realizan evaluaciones de distancia es en el momento de calcular los histogramas locales de los pij (paso 6). Si en el espacio métrico hay n elementos, el cálculo de los histogramas implicarían Mhn = O(n) evaluaciones de distancia. Para bajar este costo, los histogramas de los pivotes se pueden aproximar tomando sólo una muestra de elementos del espacio [7]. De esta manera, el cálculo de los histogramas requiere sólo Mh evaluaciones de distancia. El proceso de votación requiere calcular la masa de q a cada uno de los Mh pivotes, lo que implica calcular la distancia de q a cada uno de esos pivotes. Por lo tanto el proceso de votación requiere Mh evaluaciones de distancia. Como la votación se invoca con cada una de las A queries, en total tenemos MhA evaluaciones de distancia. Como los pasos descriptos se realizan en total k/h veces, tenemos que la cantidad total de evaluaciones de distancias hechas en este proceso de selección es k/h(Mh + AMh) que es O(k). Seleccionar por Votos (in k); 1. Elegir en forma aleatoria un conjunto de A queries {q1, q2, . . . , qA} 2. P = Ø 3. Repetir 4. Para i = 1 hasta M hacer 5. Para j = 1 hasta h hacer 5. Elegir en forma aleatoria pij 6. Calcular el histograma local de pij 5. Fin Para j 6. Fin Para i 7. Para t = 1 hasta A hacer 8. votar (qt) 9. Fin Para t 10. Sea i el grupo con mayor cantidad de votos: P = P ∪ {pi1, . . . , pih} 11. Hasta que |P| = k 12. Retornar P Votar (in q); 1. mín = +∞ ; g = 0 2. Para i = 1 hasta M hacer 3. Para j = 1 hasta h hacer 4. Si m(pij , qt) < mín entonces 5. mín = m(pij , qt) 6. g = i 7. Fin Si 8. Fin Para j 9. Fin Para i 10. votos(g)++ Figura 4: Algoritmo de selección por votos, minimizando la masa 5. Conclusiones y Trabajo Futuro Basándonos en los resultados que se muestran en [4] y [8] hemos diseñamos dos nuevas técnicas para la selección de un buen grupo de pivotes. Estas técnicas permiten, con un bajo costo, elegir elementos de la base de datos que tienen característica apropiadas para ser considerados buenos pivotes. Los dos algoritmos presentados tienen constantes cuyos valores no hemos especificado. En el caso de selección por máxima varianza existen dos constantes N y A cuyos valores pueden ser heredados de los valores que se recomiendan en [4] para la selección incremental maximizando la media. Algo similar se puede realizar con las constantes M , h y A de la selección por votos, tomado los valores recomendados en [8]. Como trabajo futuro nos proponemos estudiar experimentalmente el comportamiento de estas técnicas sobre espacios métricos reales. Nos proponemos estudiar dos tópicos. Por una lado analizaremos experimentalmente si los valores para N y A recomendado en [4], y los valores para M , h y A recomendados en [8] son adecuados para estas nuevas políticas de selección. En caso de que no lo sean, estableceremos experimentalmente los valores más adecuados. Por otra parte analizaremos el efecto de estas políticas de selección sobre la eficiencia de los índices basados en pivotes.	﻿selección de pivotes , espacios métricos	es	23144
47	Descripción semántica de los objetos de aprendizaje para la potenciación de su reusabilidad	﻿ En el diseño de actividades para los Sistemas de Aprendizaje Basados en la Web, el concepto de Objeto  Didáctico u Objeto de Aprendizaje, se ha perfilado como el eje de un nuevo paradigma que se preocupa  por la reutilización de contenidos y actividades, por su organización desde lo conceptual, el uso de  metadatos en formatos conocidos y por el establecimiento de estándares. En este artículo se analiza el  concepto de reusabilidad en el contexto de los sistemas mencionados y se proponen los Mapas  Conceptuales a la Sowa y los Mapas Conceptuales Hipermediales como esquemas aptos para la  representación de conocimiento, ya que permite una clara visualización y tienden a habilitar  funcionalidades automatizadas conocidas, de manera precisa. Los mencionados recursos están  orientados a completar la información de los metadatos en lo referente al aspecto semántico.    Palabras Clave: aprendizaje basado en la Web - objeto de aprendizaje - semántica - reusabilidad    1. Introducción  La educación basada en la Web en los últimos años ha evolucionado según un eje conformado por  los llamados objetos de aprendizaje (OA), en torno al cual se estructura un conjunto de tecnologías y  estándares. Si bien no existe consenso en relación a una única definición del concepto OA, se puede  observar que en todas se hace algún tipo de referencia, ya sea implícita o explícitamente, a la  reusabilidad y a la reutilización de los mismos, constituyéndose en unas de sus potencialidades más  importantes. “Desde un punto de vista económico, es fácil construir modelos de costo-beneficio para  justificar el diseño de OA, análogos en cierta medida a los modelos que han justificado la Ingeniería del  Software basada en componentes” [24].  La posibilidad de estructurar nuevos OA a partir de otros más elementales y en función de  decisiones que se toman en forma dinámica es lo que permite crear estrategias de aprendizaje  complejas a partir de otras más sencillas. Para que ello sea posible, hay que asegurar una correcta  conexión y compatibilidad entre los diferentes OA y se torna indispensable el uso de estándares para su  descripción. Es importante asegurar un lenguaje de metadatos común y lo suficientemente amplio y  rico para poder expresar toda la información necesaria que permitirá luego hacer las composiciones que  se necesiten. De la misma forma, es necesario un sistema basado en reglas para poder expresar las  relaciones entre OA y poder construir secuencias o itinerarios de aprendizaje a partir de ellos [30].    2. Sobre los objetos de Aprendizaje   Actualmente el diseño de los Sistemas Educativos basados en la Web (SEBW), se fundamenta en  gran medida en el uso los OA, piezas elementales que se organizan para conformar experiencias  educativas. Dichas piezas, caracterizadas como de grano fino, componen los cursos que son  estructurados según una planificación didáctica algorítmica (WebQuest). Los SEBW cerrados  contienen sólo OA computacionales, mientras que en los abiertos puede haber de distinto tipo.  La idea es que para crear una experiencia educativa y luego ponerla a disposición de los usuarios se  debe contar con OA, creados especialmente u obtenidos en alguno de los almacenes existentes, y  componerlos dando lugar a recursos educativos más complejos. El uso de piezas elementales y la  posibilidad de ensamblarlas a voluntad para construir con ellas modelos agregados de estructura  superior al estilo de las piezas de un mecano es una de las características más atractivas de esta  tecnología. Ahora bien, si para conformar una experiencia educativa debemos ensamblar OA, se  presentan cuestiones fundamentales, tales como:  a) Los OA deben ser fácilmente accesibles y reutilizables: se deben desarrollar en forma  independiente del contexto en el que se usarán en una primera instancia. Lo ideal es que estos los OA  se construyan como componentes reutilizables normalizados, lo que beneficia tanto a los  desarrolladores de material educativo, como a quienes arman las diferentes WebQuest.   b) La estructuración de la experiencia debe responder a una planificación didáctica que contemple  las diferentes características individuales de los participantes y que pueda plasmarse en forma  algorítmica.   Se pueden citar numerosos esfuerzos para desarrollar estándares sobre la creación y utilización de  OA; en ello se han involucrado numerosas organizaciones e instituciones entre las que se pueden  destacar: el grupo LTSC de IEEE, la Advanced Distributed Learning (ADL) initiative del  Departamento de Defensa de los Estados Unidos, el Consorcio para el aprendizaje Global learning  consortium (IMS), el Aviation Industry Computer Based Training Committee (AICC), ISO (ISOSC36), el Dublin Core Metadata Initiative (DCM); y proyectos tales como GESTALT, PROMETEUS,  ARIADNE, CEN-ISSS o GEM [1], [8], [32].    2.1. Reutilización y reusabilidad de los OA  La reutilización de un OA es un hecho observable que puede darse dentro de una misma  organización o involucrar a varias de ellas. Mientras que la reusabilidad de un OA es uno de sus  atributos y puede ser usado como una medida de calidad a priori. “En el caso de la reusabilidad – como  ocurre en la disciplina de la Ingeniería del Software – no existen medidas precisas, sino sólo  indicadores, que podrán o no ser confirmados por tasas de reutilización elevadas a posteriori, sin que  desdigan en nada la potencial reusabilidad” [27].   La reusabilidad de un OA es un concepto que abarca fundamentalmente tres aspectos: el formato, la  interpretación y la adecuación desde lo didáctico. Los estándares y especificaciones actuales cubren el  primero de los aspectos, pero son necesarias mejoras y nuevos conceptos en los dos segundos aspectos.  Por otro lado, hay diferentes puntos de vista – incluyendo perspectivas de valor y calidad –, que  justifican el esfuerzo añadido que requiere la producción de los metadatos para los OA. El diseño de los  cursos en el marco del paradigma de OA está claramente orientado por la reutilización. Los criterios de  diseño y de descripción técnico- pedagógica y de organización de los OA, tanto para los autores como  para quienes los organizan para la EBW se basan en este concepto de reutilización   Un OA debe ser propenso al reuso, es una propiedad fundamental, intrínseca a su caracterización.  De un modo u otro, este aspecto aparece en las definiciones del término que se manejan actualmente.  Por ejemplo, para Wiley un OA es “cualquier recurso digital que puede ser reutilizado para  proporcionar aprendizaje” [34], para Polsani un OA es “una unidad independiente y auto contenida de  contenido de aprendizaje que esta predispuesta a ser reutilizada en múltiples contextos de aprendizaje”  [18]. Obviamente, desde una perspectiva económica, “el uso repetido es la fuente de valor y de  economía de escala para el caso de los proveedores de contenidos” [26].  Es importante tener en cuenta que como la EBW está cada vez más difundida y que tal tendencia es  marcadamente positiva, la reusabilidad de los OA estará en función de la facilidad que se tenga para  accederlos y ensamblarlos; no depende sólo de su contenido sino también de lo que se conoce como  metadatos, sobre todo si pensamos que en un futuro cercano el acceso a los OA se realizará casi  exclusivamente a través de herramientas de software. Los metadatos son especificaciones que nos  permiten encontrar los OA que necesitamos. Pero esas descripciones deberían ser orientadas  fundamentalmente a las máquinas, y no a la lectura humana. “Pensar en la reutilización a gran escala  sin la mediación de software especializado es perder la perspectiva del fenómeno que pretendemos  caracterizar” [25]. Si se busca reusabilidad hay que pensar en metadatos que vayan más allá un registro  compatible con LOM (IEEE, 2002) o un paquete compatible SCORM [27]. Es cierto que la creación de  metadatos LOM o SCORM es útil, pero no garantiza la reusabilidad, ya que permiten metadatos que  aunque estén completos y sean correctos, no necesariamente son aptos para el procesamiento  automatizado, que es lo que se necesita si se piensa en la universalización de la EBW. Es sabido que no  todo los formatos que permiten un procesamiento humano necesariamente hacen posible su manejo  computacional.  Si bien algunos formatos (SCORM, por ejemplo) permiten el intercambio de contenidos entre  plataformas aptas para la EBW, eso no pasa de ser un pase de archivos, es decir se trata de una  reutilización técnica. Obviamente, como el intercambio es posible, el formato proporciona reusabilidad,  aunque se puede dar el caso de un contenido SCORM sea reutilizado muy pocas veces por ser  semánticamente muy general o demasiado específico para la experiencia educativa en cuestión. Es  decir, adhiriendo a LOM o SCORM, se pueden lograr OA con alto grado de reusabilidad aunque no es  una condición suficiente, para lograrlo es necesario que los metadatos tengan formatos adecuados para  su procesamiento automatizado. Un ejemplo en esta dirección lo constituye el “diseño por contrato” de  OA que se esmera en proporcionar una semántica clara de los metadatos para que las herramientas de  software puedan seleccionarlos y combinarlos, verdaderas tareas de reutilización que hasta hoy algunos  diseñadores de experiencias educativas y muchos tutores y profesores siguen resolviendo en forma  artesanal (cortar y pegar).  Los principales aspectos de la reusabilidad que se deben considerar son:  a) Un aspecto técnico de formato que implica que los materiales estén formateados de acuerdo a  ciertas reglas y convenciones. Con los estándares actuales se ha avanzado notablemente en este punto.  b) Un aspecto técnico de interpretación referido a que los metadatos utilizados permitan habilitar en  forma automatizada y de manera precisa ciertas funcionalidades conocidas. LOM no es suficiente en  esta área, pero puede extenderse con técnicas y prácticas especiales.  c) Un aspecto de diseño instruccional, de manera tal que el diseño de los contenidos y su  granularidad esté orientado a la reutilización, pensando en posibles entornos de usos futuros. Sobre este  tópico se ha propuesto un esbozo de caracterización del concepto donde se relaciona la reusabilidad en  diferentes contextos educativos del OA con la reusabilidad total [25].    2.2. Diseño de OA y evaluación de la reusabilidad   El paradigma de los OA tiene un valor superior a otras aproximaciones existentes para el diseño de  contenidos y actividades educativas, se puede destacar:  a) Desde lo que significa una generación de valor: el valor puede referirse a elementos de carácter  económico o a la capacidad de servicio, entre otros. Si se expresarse en términos de la relación costobeneficio sería el valor de la adquisición o de la producción de los OA necesarios para la experiencia  educativa vs. el incremento en las competencias y/o conocimiento de los destinatarios. Las actividades  didácticas en el contexto organizativo, forman parte de un ciclo de adaptación, por lo que el valor se  conceptualiza como un incremento final en la capacidad competitiva de la organización. Además, la  economía de escala a la que pueden llegar los fabricantes de OA estandarizados seguramente llevará a  una reducción de los costos globales de producción.  b) Desde la perspectiva de la conformidad técnica: para los sistemas de e-learning, los estándares  actuales son la base de la interoperabilidad de contenidos y actividades educativas. Su capacidad en  esta área está probada, y los organismos de especificación continúan una actividad intensa para abarcar  más áreas de conformidad.  c) Desde el punto de vista de la adecuación pedagógica: para el diseñador de experiencias  educativas que busca OA para una situación concreta, la disponibilidad de herramientas automatizadas  de búsqueda y composición le ahorran tiempo de diseño y le amplían las posibilidades de encontrar OA  adecuado a sus necesidades concretas. Cabe destacar en este punto que las posibilidades efectivas de  reuso se incrementan en forma proporcional a la calidad descriptiva de los metadatos Es importante  notar que el uso de metadatos estructurados y con interpretaciones no ambiguas abre un escenario  completamente diferente a la construcción de herramientas de búsqueda.   d) Desde la calidad proveniente del uso repetido: encuadrados en un marco de investigación-acción, los  contenidos y actividades educativas se evalúan y perfeccionan con la práctica, en este caso con su uso  repetido, esto es, la evaluación permanente y la experiencia repetida permite incrementar la calidad de  los OA. Existen emprendimientos que añaden una dimensión de meta-información sobre calidad y  adecuación que posee ya que se trata de un valor intrínseco muy importante.    3. Propuesta para la descripción semántica de los OA  Esta propuesta se centra en el uso de los Mapas Conceptuales Hipermediales (MCH) y de los Mapas  Conceptuales a la Sowa (MCS) como esquemas de representación de conocimiento [22]. Cuando se  examina el límite computacional sobre el razonamiento automatizado y su efecto sobre la representación  del conocimiento, se advierte que no se razona correctamente y con igual facilidad sobre los distintos  lenguajes de representación. Además, generalmente el grado de dificultad aumenta en forma paralela  con el poder expresivo de los lenguajes. El esquema de representación de conocimiento que se presenta  es suficientemente flexible para el manejo humano y riguroso para poder realizar razonamiento  automatizado.     3.1. Sobre Mapas Conceptuales Hipermediales y Mapas Conceptuales a la Sowa   Los MCH se basan en los Mapas Conceptuales de Novak (MC) e incorporan la flexibilidad y riqueza  que permite la tecnología hipermedial. En el área educativa, ambos esquemas han sido probados con  éxito como potentes estructuras capaces de contribuir con la construcción de aprendizajes significativos  en las personas. Se destaca el valor del recurso hipermedial, no sólo en el aspecto operacional sino en  los planos relacionados con la percepción y la abstracción [36]. Se detectan falencias al querer extender  el modelo para realizar gestión automática con base semántica. Por esa razón, se realiza una extensión  de los MC incorporando elementos de los Grafos Conceptuales de Sowa. Se definen entonces, los MCS  y una arquitectura para la representación de una base de conocimiento, con capacidad para realizar  razonamiento en forma automática.    3.2 Esquema de la Base de Conocimiento   Los MCH constituyen una representación exitosa entre agentes humanos, pero es incompleta como  esquema de representación en ambientes de aprendizaje mixtos (compuestos por agentes humanos y de  software). Dichas falencias se centran en la jerarquización de los conceptos, en la definición de clases e  individuos y en el manejo de la aridad de las relaciones. Para solucionarlo se enriquece el modelo de la  siguiente manera: se crea un esquema de clases y un esquema de representación de proposiciones [22].  El esquema de clases es un reticulado representado por medio de un MCH basado en el modelo de los  MC de Novak. Para la representación de las proposiciones se migra a un modelo fundamentado en los  GC de Sowa [29], para los cuales ya están resueltos los problemas anteriormente planteados. Se elige el  modelo de los GC por ser intuitivo, por la simpleza de su notación, su impacto visual, su capacidad para  ser visualizado y por la lógica subyacente. Los GC forman una base fuerte para el razonamiento lógico,  se pueden usar las relaciones y conceptos resultantes y mantener la consistencia. Se definen así, los MCS  y se presentan reglas canónicas y operaciones lógicas para la formación de nuevos MCS a partir de otros  existentes. Se logra una representación que es equivalente a la notación del cálculo de predicados y que  permite razonar con mayor facilidad.    3.3. Esquema de clases: La herencia es una herramienta natural para representar el conocimiento  en forma taxonómicamente estructurada. Esta organización garantiza que todos los miembros de una  clase hereden las propiedades adecuadas, asegurando consistencia con la definición de las clases. Con  esta estrategia se reduce el tamaño de la base de conocimiento, y se permite la implementación de  valores por defecto y excepciones. Los valores por defecto se heredan simplemente desde las  superclases apropiadas. Un modelo que es capaz de representar aquellas jerarquías que permiten una  multiplicidad de clases padre es más expresivo. Aunque estas jerarquías de herencia múltiples pueden  introducir dificultades en la definición de los lenguajes de representación, sus beneficios son grandes en  relación con estas desventajas [11]. Los reticulados constituyen una forma común para el caso de  herencia múltiple. Se establece un orden parcial en el conjunto de las clases, indicado por el símbolo ⊆  (⊆ representa la inclusión entre clases). Se definen los conceptos de subclase y superclase, y como se  trata de un reticulado, las clases pueden tener múltiples padres y múltiples hijos. Sin embargo, cada par  de clases debe tener una superclase común mínima y una subclase común máxima. La superclase común  mínima de una colección de clases es el lugar apropiado para definir las propiedades que son comunes  sólo a esas clases. Para resolver el problema que se presenta cuando hay clases que no tienen  superclases o subclases comunes naturales, se incorporan dos clases especiales que cubren esas  funciones. Se logra así que la ⊆ de clases sea un verdadero reticulado.   En esta propuesta la jerarquía de clases es representada por un MCH que cuenta con dos clases  estándares: una clase Universal como superclase de todas las clases y una clase Absurda como subclase  de todas las clases. Siguiendo las convenciones de MC, si C2 es una subclase de C1, C1 aparece en la  representación en un nivel superior al de C2. Las clases quedan vinculadas a través de la relación “es  un”, por lo tanto es necesario el dibujo explícito de una flecha, como se muestra en la figura 1.    Figura 1    Para jerarquías de gran número de clases, el uso de MCH marca una diferencia importante en lo  operacional.    3.4. Representación de las proposiciones: Cada proposición simple se representa por medio de  un MCS que es un grafo dirigido finito caracterizado por:  Los nodos del mapa representan conceptos, gráficamente los conceptos se dibujan como elipses  rotuladas.  Todas las relaciones son binarias. Se mantiene la representación tradicional para las relaciones de los  MC, es decir mediante arcos etiquetados con el nombre de la relación.  Los nodos representan objetos del universo de discurso; pueden ser concretos o abstractos. Los  conceptos concretos incluyen conceptos genéricos y conceptos específicos.   Las proposiciones verbales se representan de la siguiente manera: el concepto verbo es raíz del MCS que  representa la proposición. Por ejemplo, para la proposición El oso toma agua, la relación agente vincula  el concepto toma con el concepto oso y la relación objeto vincula el concepto toma con el concepto agua  como puede observarse en la figura 2.  En las proposiciones nominales, el concepto al que se le asocia una propiedad es el concepto raíz del  MCS. Por ejemplo para la proposición Pájaro color azul el MCS asociado es el de la figura 3.        Figura 2 Figura 3    En los MCS que representan las proposiciones, siguiendo la propuesta de los GC, cada concepto  sustantivo es un individuo único de una clase particular. Puede tratarse de un individuo genérico o de un  individuo específico. La notación para los distintos casos en las respectivas elipses es la siguiente:    sin marcador genérico < Nombre de clase > individuo genérico  usando marcador genérico < Nombre de clase > : *  usando nombre <Nombre de clase> : <Nombre de individuo> individuo específico  usando marcador <Nombre de clase> : # <Número de individuo>   Cada individuo en el mundo del discurso tiene asociado un único token, llamado marcador numérico,  que lo identifica plenamente. Esto permite indicar individuos específicos pero sin nombre. Los MCS  permiten el uso de variables con nombre. Éstas son representadas por un asterisco seguido del nombre  de la variable (por ejemplo *X). Esto es útil si dos elipses distintas indican el mismo individuo, pero se  trata de un individuo no especificado. El mapa de la figura 4 representa la afirmación El niño apoya la  frente sobre sus rodillas. Aunque no se especifica cuál es el niño al que se refiere la proposición, la  variable *X indica que la frente y las rodillas pertenecen al mismo niño. También permiten nodos  proposicionales para representar proposiciones subordinadas o coordinadas. De tal forma, además de  usar los MCS para definir relaciones entre objetos del mundo se puede también definir relaciones entre  proposiciones. Un nodo proposicional se representa como un nodo del mapa que está etiquetado con un  MCS que representa una proposición, es decir se indica como una elipse que contiene otro MCS. Por  ejemplo la sentencia Juan cree que el pájaro es azul se representa por el MCS que muestra la figura 5. En  este caso cree es una relación que toma como argumento una proposición. Cada MCS representa una  proposición simple. Los MCS pueden ser arbitrariamente complejos, pero son siempre finitos. Una base  de conocimiento típica contendrá un cierto número de estos mapas, además del MCH que representa el  esquema de clases. Los conceptos proposicionales pueden ser usados con relaciones apropiadas para  representar conocimiento acerca de proposiciones. Se muestra así cómo los MCS con nodos  proposicionales pueden ser usados para expresar los conceptos modales de conocimiento y creencia.     Figura 4 Figura 5    3.5. Creación de nuevos MCS   Para crear nuevos MCS a partir de MCS existentes se incluyen operaciones que permiten tratar la  generalización y la especialización (reglas de formación canónica) y operaciones lógicas. En el primer  grupo se encuentran las operaciones copiar, restringir, unir y simplificar y en el segundo, negación  conjunción y disyunción.     3.5.1. Reglas de formación canónica  Dados los MCS m1 y m2, el resultado de aplicar cada una de las reglas da como resultado un nuevo MCS  como se indica en la figura 6.  La regla de restricción puede usarse para hacer que aparezca una correspondencia entre dos conceptos y  así se pueda aplicar luego la regla Unir. Las reglas Restringir y Unir juntas permiten la implementación  de la herencia. El reemplazo de un marcador genérico por uno individual implementa la herencia de una  de las propiedades de la clase a un individuo. Por ejemplo en el MCS m3, el profesor Juan hereda la  propiedad de nacionalidad argentina originalmente definida en m1 para un individuo genérico de la clase  profesor. El reemplazo de la etiqueta de una clase por la etiqueta de una subclase define la herencia  entre una clase y una subclase. Es el caso de la propiedad heredada por la subclase profesor en m4 desde  la clase persona en m2.  Además, uniendo un MCS con otro y restringiendo ciertos conceptos, se puede implementar herencia de  una variedad de propiedades. Como los MCS se basan en el modelo de los GC, también se puede aplicar  a ellos unión y restricción para implementar supuestos plausibles que juegan un rol importante en la  comprensión del lenguaje común, por ejemplo de la sentencia María y Tomás salieron juntos a comer  pizza puede ser modelado con MCS.          m1 m2          m3 = Restringir(m1) m4 = Restringir(m2)      m5 = Unir(m3,m4) m6 = Simplificar(m5)  Figura 6    Como en el caso de los GC, la unión y la restricción de MCS son reglas de especialización. Ellas definen  un orden parcial sobre el conjunto de los MCS derivables. Si un MCS m1 es una especialización de m2  entonces se puede decir que m2 es una generalización de m1. Como Luger ha mencionado, las jerarquías  de generalización son importantes en la representación de conocimiento; ellas, junto con la provisión de  bases para herencia y otros esquemas de razonamiento del sentido común, se usan en muchos métodos  de aprendizaje. Obviamente, no se trata de reglas de inferencia, ellas no garantizan que desde MCS  verdaderos se derivarán siempre MCS verdaderos. En la restricción del mapa m3 de la figura 6 el  resultado podría no ser verdadero, por ejemplo si Juan no es un profesor. Otro ejemplo representativo de  no preservación de la verdad lo constituye m5 en la misma figura, ya que el profesor que está en el  gabinete podría ser una persona distinta del que corrige el examen. Estas operaciones no preservan  verdad pero tienen la importante propiedad de preservar la condición de significatividad, es decir las  reglas de formación canónica no permiten formar MCS sin sentido desde otros que sí lo tienen, lo que  constituye una propiedad importante. “Aunque ello no suena a reglas de inferencia, las reglas de  formación canónica forman las bases para muchos de los razonamientos plausibles realizados en  comprensión del lenguaje natural y razonamiento del sentido común” [11].     3.5.2Operaciones Lógicas  a) Negación: La existencia de los nodos proposicionales en los MCS hace que se pueda implementar  con facilidad la negación de una proposición. Se define una operación llamada neg que toma como  argumento un concepto proposicional y afirma ese concepto como falso. Para la representación gráfica,  se muestra la proposición que se quiere negar como un nodo proposicional, y para establecer la  negación se usa un nodo ficticio desde el que parte la relación neg hacia el nodo proposicional. El uso  de ese nodo ficticio es al solo efecto de tratar a la operación neg como binaria.   b) Conjunción: se pueden formar MCS que representen aserciones disyuntivas. Si cada uno de los  MCS que representan las proposiciones a coordinar tiene un nodo raíz, se puede establecer la relación  “y” vinculando ambos nodos raíz; de lo contrario puede representarse cada una de las proposiciones a  vincular por medio de un nodo proposicional y luego relacionarlos con “y”.  c) Disyunción: De acuerdo con las reglas de la lógica, usando negación y conjunción se pueden  formar MCS que representen aserciones disyuntivas. Para simplificar esto también se puede definir una  relación “o” la cual toma dos proposiciones y representa su disyunción en forma análoga a como se  representa la conjunción.    3.5.3 Cuantificación de variables  a) Se asume que en los MCS los conceptos genéricos están existencialmente cuantificados. Por  ejemplo en el caso del MCS de la figura 3, el concepto genérico pájaro representa una variable  existencialmente cuantificada. Este MCS se corresponde con la expresión lógica:  ∃X ∃Y (pájaro(X) ∧ color(X,Y) ∧ azul(Y))  b) Se puede representar cuantificación universal mediante el uso de negación y cuantificación  existencial. Por ejemplo, Para el MCS que representa la negación de la proposición El pájaro es amarillo  se tiene la siguiente expresión lógica:  ∀X ∀Y (¬(pájaro(X) ∧ color(X,Y) ∧ amarillo(Y)))  c) Un MCS que hace referencia a un individuo particular, por ejemplo el que representa la  proposición El oso Simón es de color marrón, se corresponde con la siguiente expresión del cálculo de  predicados  ∃X1(oso(Simón) ∧ color(Simón,X1) ∧ marrón(X1))    3.6. Poder expresivo de los MCS  Como los ejemplos precedentes sugieren, existe una correspondencia directa desde los MCS hacia la  notación del cálculo de predicados. Los MCS resultan equivalentes al cálculo de predicados en su poder  expresivo. El siguiente algoritmo permite obtener la expresión del cálculo de predicados equivalente a  un MCS dado.    Algoritmo Expresión Lógica Equivalente  Entrada: MCS m  Salida: expresión del cálculo de predicados equivalente a m    Pasos:  1- Asignar una única variable X1, . . . ,Xn a cada uno de los n conceptos genéricos en m.  2- Asignar una única constante a cada concepto individual en m. Esta constante puede  simplemente ser el nombre o el marcador usado para indicar el referente del concepto.  3- Representar cada nodo concepto por un predicado un-ario con el mismo nombre del tipo de ese  nodo y cuyo argumento es la variable o constante asignada a ese nodo.   4- Representar cada relación conceptual en m como un predicado binario cuyo nombre es el  mismo que el de la relación. Esto permite que cada argumento del predicado sea la variable o la  constante asignada al correspondiente nodo concepto vinculado a tal relación.  5- Tomar la conjunción de todas las sentencias atómicas formadas en los puntos 3 y 4. Éste es el  cuerpo de la expresión del cálculo de predicados. Todas las variables en la expresión son  existencialmente cuantificadas.      Es importantante destacar que aunque los MCS, así como los GC, pueden reformularse usando la  sintaxis del cálculo de predicados, ellos soportan un número de mecanismos de inferencia de propósito  especial tales como unión y restricción que no son normalmente parte del cálculo de predicados.    4. Conclusiones  El paradigma de los SEBW basados en OA pone el énfasis en la reusabilidad de los contenidos y de las  actividades orientadas al aprendizaje. Se espera que los OA puedan ser encontrados, visualizados,  agregados para poder construir experiencias educativas en el marco del e-learning y en base a este  paradigma, por diseñadores didácticos y docentes que no cuentan con una formación informática  específica. En este artículo se ha tratado el concepto de reusabilidad en el contexto del modelo de  SEBW basado en OA y se proponen los Mapas Conceptuales a la Sowa y los Mapas Conceptuales  Hipermediales como esquemas aptos para la representación de conocimiento. Se muestra su potencial  para una clara visualización y para habilitar funcionalidades automatizadas de manera precisa tales  como búsquedas orientadas por la semántica o composición desde lo conceptual. Los mencionados  esquemas se proponen para completar la información de los metadatos actuales en lo referente al  aspecto semántico.	﻿semántica , aprendizaje basado en la Web , objeto de aprendizaje , reusabilidad	es	23275
48	Utilización de excepciones para implementar predicados opacos en técnicas de ofuscación de código intermedio	"﻿   Microsoft’s .NET Framework, and JAVA platforms, are based in a just-in-time compilation  philosophy. Software developed using these technologies are executed in a hardware  independent framework, which provides a full object-oriented environment, and in some  cases allows the interaction of several components written in different programming  languages.  This flexibility is achieved by compiling into an intermediate code which is platform  independent. Java is compiled into ByteCode, and Microsoft .NET programs are compiled  into MSIL (Microsoft Intermediate Code). However, this flexibility comes with a price. It is  really easy, with tools available for free in the web, to decompile this intermediate code and  obtain a working, readable version of the original source code.  Of all techniques developers can use to protect their intellectual property, obfuscation is the  most accepted and commercially available one.  In the present work, we propose the use of try-catch mechanisms available in .NET as a way  to improve the quality of one of the building blocks of obfuscation: opaques predicates.    Keywords: Obfuscation. Obfuscation Transformation. Opaque Predicates.    Resumen    La plataforma .NET de Microsoft se basa en una filosofía de just-in-time compilation  (compilación bajo demanda al momento de la ejecución). Los programas desarrollados de  esta manera se ejecutan en un entorno o framework independiente de la plataforma, basado  en objetos y, en algunos casos, permitiendo que interactúen componentes desarrollados en  distintos lenguajes de programación.  La clave de esta flexibilidad se da en que, tanto en JAVA como en las plataformas .NET, la  compilación resulta en un código intermedio, independiente de la plataforma (bytecode y  MSIL respectivamente). Sin embargo, dicha flexibilidad tiene un costo. Hoy en día, y  utilizando herramientas gratuitas que pueden descargarse desde Internet, es sumamente fácil  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1827 aplicar tecnologías de ingeniería inversa a las dos plataformas de desarrollo más populares:  JAVA y .NET.  De todas las técnicas que los desarrolladores pueden utilizar para proteger su propiedad  intelectual, la ofuscación es la técnica más aceptada y de hecho, es la única utilizada  comercialmente.  En este trabajo, proponemos el uso de los mecanismos de excepción (bloques try-catch) que  brinda la plataforma .NET como una manera de mejorar la calidad de uno de los bloques  básicos de la ofuscación, los predicados opacos.    PALABRAS CLAVES: Ofuscadores. Código Intermedio. Transformaciones de Ofuscación.  Predicados Opacos.    1 Introducción    Tanto las aplicaciones JAVA, como aquellas desarrolladas para ejecutarse sobre cualquiera de las  versiones del Framework de .NET, poseen la vulnerabilidad de que, con herramientas gratuitas y  con solo conocimientos básicos de informática, es posible para cualquier persona que posea los  distribuíbles de la aplicación obtener, de manera completa y con solo ligeras variaciones, el código  fuente original completo de la aplicación.  La  apropiación del código fuente de un software desarrollado por una organización en manos de  personal no autorizado y con intenciones evidentemente ilegales, podría tener las siguientes  consecuencias:   • Pérdida a manos de la competencia del dinero invertido en I+D1. La competencia puede, de  forma desleal, lanzar al mercado el mismo producto, con un “lavado de cara” y  aprovechando la inversión de la organización original.  • Una organización competidora podría descubrir fallas en el producto y utilizarlas en su  beneficio.  • En el caso puntual de los algoritmos de encriptación modernos, cuya seguridad está basada  en la existencia de una clave desconocida y no de un algoritmo en particular, el acceso al  algoritmo por parte de manos malintencionadas podría servir para, previa modificación de  los mismos, intentar ataques de fuerza bruta contra los datos cifrados.  • El acceso al código fuente de una aplicación facilita el “crackeo” de sistemas anti piratería,  como ser la registración de software mediante keys, expiration dates, hardlocks, etc.  • Un empleado con conocimientos de informática podría, descompilar la aplicación de gestión  administrativa de la empresa, modificar los strings de selección a las bases de datos de  manera de eliminar restricciones y filtros, recompilarla, ejecutarla, y obtener acceso  irrestricto a la base de datos de clientes. Esto podría resultar en la pérdida de valiosos  secretos comerciales.    Lo anterior indica problemas económicos, pero algunos gobiernos como el de los EEUU identifican  al problema como de seguridad nacional [4]. En el ámbito privado, se sabe que el 75% de las  empresas Fortune 500[11] utilizan de una manera u otra el paquete de desarrollo Microsoft Visual  Studio 2005[5].  Viendo que las principales amenazas a la seguridad provienen, hoy por hoy, no de  agentes externos a las organizaciones sino de elementos internos de la misma con acceso a los  recursos de la empresa desde adentro (empleados, personal contratado, consultores, etc.), el  problema no es menor [6].                                               1 I+D: Investigación y Desarrollo  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1828   Nuestra línea de trabajo, iniciada en [12], muestra a las técnicas de ofuscación como la rama de la  seguridad informática que puede brindar un nivel de protección superior al de las alternativas  existentes. En este artículo mostramos cómo, mediante la utilización de excepciones, es posible  incrementar de manera notoria la calidad de los predicados opacos, mejorando en consecuencia la  calidad de la ofuscación y por ende la protección de la propiedad intelectual.    La estructura del trabajo es la siguiente. A continuación, se brinda una descripción introductoria de  la ofuscación y de sus conceptos claves. Luego, se profundiza en las construcciones denominadas  predicados opacos. En la sección 4, presentamos el aporte de este trabajo. Se describen las técnicas  de generación de predicados opacos más avanzadas y se presenta la innovación de reemplazar el uso  de sentencias de salto condicional por bloques del tipo try-catch. Finalmente, en la sección 5, se  reportan las conclusiones y se presentan algunas líneas de trabajo futuro.    2 Ofuscación, conceptos claves  En términos generales, se entiende por ofuscar un código fuente o un código intermedio, un proceso  mediante el cual se transforma utilizando diversos algoritmos de reescritura, un código  perfectamente legible y entendible por una persona en otro de funcionalidad equivalente en un  ciento por ciento, pero, en términos ideales, totalmente ilegible e incomprensible para un lector  humano.  En la figura 1 se muestra, de manera esquemática, el proceso y concepto de aplicación de  ofuscación de código intermedio.    Código Fuente Compilación Código Intermedio fácilmente descompilable Máquina Virtual: Ejecución Proceso de Ofuscación Código Intermedio Ofuscado Máquina Virtual: Ejecución Funcionalmente equivalentes   Figura  1: Ofuscación de Código Intermedio    En pocas palabras, algunas de las técnicas de ofuscación más comunes consisten en la inclusión de  bucles irrelevantes, cálculos innecesarios, comprobaciones fuera de contexto, nombres de funciones  y de variables que no tienen nada que ver con su cometido, funciones que no sirven para nada,  interacciones inverosímiles entre variables y funciones, etc. Otras técnicas, sin embargo, son mucho  más potentes, en el sentido de que requieren un conocimiento superior de las características del  lenguaje, e incluso pueden estar diseñadas para burlar a herramientas de ingeniería inversa  específicas.  A continuación, discutimos conceptos claves presentados por Collberg[1].  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1829 2.1 Transformación de ofuscación  Sea  'PP ⎯→⎯τ   la transformación de un código fuente o intermedio P en un código fuente o  intermedio P’     'PP ⎯→⎯τ  es una transformación de ofuscación si P y P’ tiene el mismo comportamiento  observable, entendido desde el punto de vista de lo que percibe el usuario.     Más precisamente, para que τ sea una transformación válida se debe verificar que:    • Si P falla al terminar dada una entrada, P’ podría o no terminar.  • De otra forma, dada una entrada, P’ debe terminar y producir el mismo resultado que P.    Idealmente, P’ debería tener características que dificultan la compresión del código fuente. Se  define, de manera informal, a comportamiento observable como la percepción del usuario acerca de  la entrada y salida del usuario. P’ podría tener comportamientos diferentes, cuyo objetivo es  confundir a un posible desofuscador y/o a un hacker, que serían válidos mientras no sean parte de la  experiencia del usuario de P’.    Las transformaciones de ofuscación pueden clasificarse en cuatro categorías o tipos [1]:    • Léxicas o de Estructura: renombramiento de identificadores, cambio de formatos.  • Ofuscaciones de Datos: encriptación de recursos embebidos, encriptación de metadatos,  encriptación de cadenas almacenadas, modificación de jerarquías, unificación de variables.  • Ofuscaciones de Control: reconversión de flujos de control, reordenamientos de  sentencias, bucles y expresiones, extensión de condiciones de loop.  • Ofuscaciones Preventivas: destinadas exclusivamente a provocar malfuncionamiento a  herramientas de descompilación.    2.2 Calidad de una Transformación de Ofuscación  La calidad de una transformación de ofuscación se evalúa según los siguientes criterios: cuanto más  difícil se vuelve de entender el código fuente por un lector humano (potencia), qué tan difícil resulta  para una herramienta automática revertir la transformación (resistencia), qué tan bien las  modificaciones introducidas se disimulan o mezclan con el resto del programa (stealth) y cuanto  costo extra computacional se agrega a raíz de la aplicación de la transformación (costo).  2.2.1 Potencia  La potencia de una transformación indica, de una manera orientativa, en qué medida el código  ofuscado es más difícil de comprender por un lector humano. Si bien el concepto “más difícil de  comprender” no puede ser cuantificado objetivamente, se utilizan métricas de la ingeniería de  software que miden la claridad conceptual y la mantenibilidad de un código como referencia.  A efectos prácticos se clasifica la potencia de cualquier transformación en baja, media y alta.  2.2.2 Resistencia  Un atacante merecedor de consideración seguramente dispondrá de herramientas configurables de  ingeniería inversa, llegando al extremo de desarrollar las mismas y poder adaptarlas a las técnicas de  ofuscación que vaya detectando. Debido a esto, la resilience o resistencia puede expresarse como la  combinación de dos medidas:  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1830   Esfuerzo de Programación: la cantidad de tiempo que llevaría construir una herramienta de  ingeniería inversa que efectivamente pueda reducir la potencia de una transformación τ.    Esfuerzo de Desofuscación: los recursos (tiempo / espacio) necesarios para que dicha herramienta  efectivamente reduzca la potencia de una transformación τ.    Es importante entender la distinción entre potencia y resistencia. Una transformación es potente  si  logra confundir a un lector humano, mientras que es resistente  si torna difícil la construcción de un  desofuscador o hace que la ejecución del mismo se torne impráctica en entornos reales.    Las transformaciones más resistentes son aquellas que son irreversibles. Consisten generalmente en  la eliminación de información presente en el programa pero que no es necesaria para la ejecución  del mismo, como los nombres significativos de identificadores, entre otros.  Otras transformaciones, como el agregado de código basura que no afecta el comportamiento  observable del programa, podrían ser revertidas con distintos niveles de dificultad.  2.2.3 Stealth   Es posible crear técnicas de ofuscación que modifiquen un programa de manera de hacerlo muy  difícil de comprender (alta potencia) y que a su vez no sea fácil extraerlas para obtener el código  original (alta resistencia).  Un ejemplo podría ser modificar la codificación de valores de variables. En vez que asignar a una  variable el valor escrito en el código fuente, una técnica de ofuscación podría asignar valores  enormes (del rango de los millones) y aplicar las mismas fórmulas a los valores con los que  interactúa esta variable. De esta manera, comparaciones simples del tipo while (I <= 10)  podrían transformarse en while ((I * f(I) – 234)^12 <= 5748951478) siendo la  ejecución equivalente.  Este último código ofuscado, sin embargo, salta a la vista como sintético y forzado, y un atacante  experimentado lo identificará enseguida como resultado de la aplicación de una técnica de  ofuscación. Para mejorar el stealth, el código incorporado por un ofuscador debería parecerse lo más  posible al código original, lo cual es un desafío dado que un código que podría ser stealthy en un  programa podría no serlo en otro de estilo y dominio diferente.  2.2.4 Costo   La aplicación de muchas técnicas de ofuscación, como la del ejemplo de la definición de stealth,  implican de manera clara un overhead en el tiempo de la ejecución del programa debido a mayores  operaciones (overhead termporal). Otro tipo de ofuscaciones podrían aumentar los requerimientos  de recursos espaciales (típicamente memoria) de un programa ofuscado con respecto a su versión  original, mientras que algunas transformaciones no incluyen overhead, como el renombrado de  identificadores. A medida que aumenta el costo de una transformación disminuye la calidad de la  misma.    3 Predicados Opacos  El presente trabajo se enfoca en los predicados opacos, que son el bloque básico de las  transformaciones de ofuscación que oscurecen el programa modificando el flujo de control del  mismo[2].  Las transformaciones de flujo de control generalmente realizan alguna de estas tres acciones:    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1831 • Ocultar el verdadero flujo de control de un programa entre sentencias irrelevantes que no  contribuyen a la ejecución del programa.  • Introducir, en el código intermedio, secuencias de control sin correspondencia en el lenguaje  de alto nivel en el que originalmente fue escrito el programa.  • Remover construcciones reales del flujo de control y/o introducir construcciones falsas.    Los predicados opacos son expresiones que no pertenecen al programa a ofuscar sino que son  introducidos por el ofuscador. La realidad indica que, es el predicado opaco el que realmente hace  que se ejecute el código que el programador pretendía ejecutar, y no el código basura o irrelevante  insertado por la herramienta de ofuscación.  Informalmente, una variable V es opaca si tiene alguna propiedad que es conocida a priori por el  ofuscador, pero es difícil de deducir para un ingeniero inverso.  Lo mismo puede decirse para un predicado P cuyo valor booleano es conocido por el ofuscador,  pero no por el ingeniero inverso.   Por ejemplo, una variable opaca V introducida por el ofuscador  de valor “10” puede usarse para  generar expresiones verdaderas o falsas preguntando, por ejemplo si V == 10, si V < 6, etc. El  ofuscador conoce el valor de V en cada momento dado que es el encargado de asignarla. Sin  embargo, cuanto más difícil sea para el ingeniero inverso deducir que en tal punto del programa V  tiene valor “10”, mejor funcionará el predicado opaco.  La creación de predicados opacos que sean difíciles de deducir por el ingeniero inverso es uno de  los desafíos más importantes para el creador de herramientas de ofuscación. De hecho, los  predicados opacos son la clave para la resistencia de las transformaciones de control.  Utilizaremos las mismas medidas que han sido definidas para transformaciones (potencia,  resistencia, stealth  y costo) para los predicados opacos.    3.1 Uso de Predicados Opacos  Los predicados opacos son claves en las transformaciones de ofuscación que consisten en:    • Inserción de código muerto o irrelevante. El código muerto nunca debe ejecutarse. Su  existencia solo tiene por objeto confundir a un posible atacante. La no ejecución de código  muerto se deja en manos de un predicado opaco. Por ejemplo, el código muerto podría  situarse en el bloque else de un if cuya expresión sea un predicado opaco que evalúa  siempre a verdadero.  • Extensión de Condiciones de loop. Es posible oscurecer un bucle en el programa haciendo  más compleja su condición de terminación. La idea consiste en utilizar predicados opacos de  valor conocido para extender la expresión que determina el bucle.  • Conversión de flujo de control de Reducible a No Reducible: los lenguajes en los cuales se  basa la necesidad de la existencia de la ofuscación, como son el Java y aquellos de la  plataforma .NET, son compilados o traducidos a un lenguaje intermedio, denominados  bytecode y MSIL respectivamente. La característica fundamental de estos lenguajes  intermedios es que son más poderosos que los lenguajes originales. Esto debe ser así, debido  a que no es posible que existan construcciones en los lenguajes de alto nivel que no puedan  ser transformadas a código intermedio. La idea es, construir en lenguaje intermedio un flujo  de control que no tenga equivalente de alto nivel, pero preservando la ejecución correcta  mediante el uso de predicados opacos. Un ejemplo sería una bifurcación condicional a una  sentencia que esté en el medio de un bloque while, protegida mediante un predicado opaco  que evalúa siempre a falso de manera que la verdadera ejecución no es alterada.    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1832 3.2 Construcción de Predicados Opacos  Los predicados opacos son vitales hasta el punto que  la resistencia de las transformaciones está  relacionada directamente con la calidad de dichos predicados  Los predicados obvios que podrían pensarse, como P == 0, Q != null, tienen resistencia a lo  sumo, débil, siendo la mayoría triviales. Esto significa que un desofuscador automático podría  deducir su valor mediante un análisis estático, local o global, sin insumir mucho esfuerzo.  Obviamente, es necesario un nivel de protección mayor, identificando expresiones opacas cuyo  esfuerzo requerido, en el peor caso, sea exponencial con respecto al tamaño del programa, pero que  solo requiera un tiempo lineal o polinomial para construirlos.    3.3 Técnicas avanzadas  Existen varias técnicas avanzadas de construcción de predicados opacos. Sin embargo, tienen dos  problemas fundamentales. Uno de ellos es el costo y el otro es la visibilidad del salto del flujo de  control.  A continuación, describimos las técnicas más avanzadas de construcción de predicados opacos.  3.3.1 Construcción de Predicados Opacos usando Objetos y Alias  Los análisis estáticos de cualquier tipo sobre un programa se tornan significativamente costosos  cuando existe la posibilidad de que existan alias para los objetos. Se ha demostrado que el análisis  estático de código fuente cuando existe aliasing de objetos es NP-Hard [9] o incluso indecidible  [10].   La idea básica es construir una estructura dinámica compleja con punteros y alias y mantener un  conjunto de punteros a esta estructura. De esta manera, el ofuscador sabe si p es igual a q (siendo p  y q punteros de una estructura) pero un desofuscador no podría saberlo realizando un análisis  estático. En la figura 2, mostramos un posible ejemplo de generación de predicados opacos.    XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1833   Figura  2: Predicados opacos construidos a partir de objetos y alias.  Se construye una estructura dinámica formada por Nodos. Cada nodo cuenta con un campo booleano Token y  dos campos puntero que pueden apuntar a otros nodos. La estructura está diseñada por dos componentes  conectados, G y H.  Existen dos punteros globales, g y h, que apuntan a G y a H respectivamente.    3.3.2 Construcción de Predicados Opacos usando Threads    Los programas paralelos o multiproceso son mucho más difíciles de analizar estáticamente que sus  contrapartidas secuenciales. La razón es el intercalamiento: si n segmentos de código pueden  ejecutarse de manera paralela, la cantidad de formas en las que puede ejecutarse es n!. Si n es  grande, rápidamente puede intuirse la dificultad de realizar un análisis estático.   La técnica es la misma que la del punto anterior, pero sumando la complejidad que brinda el  multithreading.    3.4 Problemas detectados   Las técnicas analizadas arriba son las técnicas de construcción de predicados opacos más avanzadas  del estado del arte. Sin embargo, en este trabajo sugerimos que no son prácticas ni viables porque  implican un alto costo y no logran el stealth que sería necesario para justificarlo.  3.4.1 Costo  En ambas técnicas (Objetos-Alias y Multithreading) se observa la creación y el mantenimiento de  una estructura de grafos que es totalmente ajena al programa original solo al efecto de preguntar, en  algún momento, si P == Q o alguna pregunta similar. Puede verse que la técnica de objetos-alias  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1834 implica un costo temporal (el tiempo de procesamiento requerido para generar y mantener la  estructura) y también un costo espacial (la cantidad de memoria).  Para el caso del multithreading, que consiste en generar lo mismo mediante threads, hay que  considerar también el overhead de los cambios de contexto.   Además, en ambos casos, hay una consideración práctica: los programas suelen fallar. Cuando esto  ocurre, los entornos informan al usuario con un dump del stack trace, lo cual suele ser útil al  desarrollador para identificar las causas de la falla. En el caso de una falla generada por un error del  programa es posible que el stack trace esté contaminado por las llamadas del ofuscador, lo que  también es un problema.  3.4.2 Visibilidad  En este trabajo sugerimos que, en realidad, no es tan importante la complejidad del cálculo del  predicado opaco sino su visibilidad.  Es posible mantener una estructura enorme, compleja y cara para que el ingeniero inverso no pueda  nunca llegar a saber si P == Q, pero eventualmente este ingeniero inverso detectará que P y Q no  tienen nada que ver con el programa que está analizando y llegará a la conclusión de que P == Q  es un predicado opaco, por más que no pueda conocer su valor en cada momento que es invocado.  El hecho de identificar predicados opacos ya es una información invaluable a los efectos de  desofuscar el programa, dado que el ingeniero sabe que algunos de los dos casos (verdadero o falso)  esconde código resultado de transformaciones, y ese conocimiento podría ser suficiente para lograr  los propósitos de la ingeniería inversa.  Una herramienta inicial para detectar predicados opacos es, sencillamente, identificar a todas las  sentencias de salto condicional del código intermedio,  sabiendo que algunas corresponden a la  lógica del programa y que otras son introducidas por el ofuscador.  Esto es casi trivial. De hecho, la herramienta ildasm.exe provista por Microsoft que extrae el código  intermedio a partir de ejecutables portables .NET ya marca con un espacio aquellas instrucciones  que son un salto, condicional o no, haciendo muy fácil identificar los predicados opacos potenciales.    4 Predicados Opacos Superiores: utilización de bloques  try-catch-finally    En esta sección, introducimos nuestra propuesta para generar predicados opacos de mayor calidad.   4.1 Bloques Try-Catch-Finally  Los lenguajes sobre los que trata este trabajo, implementan el manejo de excepciones basado en  bloques try-catch-finally. No es el objetivo de este trabajo describir en detalle esta construcción.  Basta comentar que try, catch y finally son palabras clave que delimitan bloques disjuntos de código  de manera que, si ocurre una excepción, de cualquier tipo, dentro del bloque try (la excepción puede  ocurrir incluso en otro método de otra clase, o incluso en otro módulo que puede estar en un archivo  binario distinto) la ejecución se deriva, automáticamente, a la primera sentencia del bloque catch. Si  no ocurre ninguna excepción, el bloque catch no se ejecuta.   El bloque finally, que es optativo, se ejecuta en todos los casos y suele utilizarse para realizar tareas  de limpieza que deben realizarse tanto si ocurrió una excepción, como si no. El ejemplo clásico de  un bloque finally consiste en el cierre de una conexión a una base de datos.  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1835 4.2 Predicados Opacos utilizando Excepciones  La propuesta de este trabajo consiste en la utilización de predicados opacos simples y de  relativamente poco costo (como q == 0, p == null) pero prescindiendo del uso de una  sentencia de salto condicional para implementarlo y, en cambio, forzando una excepción.  De esta manera, si bien los valores son simples, a un ingeniero inverso le resultaría difícil identificar  los posibles lugares donde efectivamente ocurre el salto condicional mediante un análisis estático.  A continuación mostramos ejemplos utilizando el lenguaje de código intermedio de .NET, llamado  MSIL. Está fuera del alcance de este trabajo explicar el funcionamiento del MSIL, remitiendo a las  especificaciones de la ECMA al respecto [7, 8].    Ejemplo: Predicado Opaco utilizando una sentencia IF. Corresponde a un if (Predicado Opaco  False) then (código real) else (código bogus)      IL_0000:  ldarg.1    IL_0001:  brtrue.s   IL_003e  /*** salto condicional ***/    ........... /* Código real de la aplicación */    IL_0033:  ldstr      ""Codigo real""    IL_0038:  call       void [mscorlib]System.Console::Write(string)    IL_003d:  ret    ........... /* Código falso */    IL_003e:  ldstr      ""Codigo Bogus introducido por el ofuscador""    IL_0043:  call       void [mscorlib]System.Console::Write(string)    IL_0048:  ret      Nótese la línea IL_0001 (resaltada), con la sentencia de salto condicional brtrue sobre el argumento  1 del método (ldarg.1). Es importante destacar que, por muy complicada que sea la manera en la que  el ofuscador oculta que ese parámetro es cero, es suficiente con saber que el predicado opaco está  allí, quedando solamente analizar los bloques then y else y el comportamiento de los mismos.    Ejemplo: Predicado Opaco Utilizando Bloques Try-Catch      IL_0000:  ldc.i4.0    IL_0001:  stloc.0    .try    {    ........... /* Código real */        IL_0032:  ldstr      ""Codigo real""      IL_0037:  call       void [mscorlib]System.Console::Write(string)      IL_003c:  ldloc.0      IL_003d:  ldarg.1      IL_003e:  div      ........... /* Código falso */      IL_003f:  call       string [mscorlib]System.Convert::ToString(int32)      IL_0044:  call       void [mscorlib]System.Console::Write(string)      IL_0049:  ldstr      ""Codigo Bogus introducido por el ofuscador""      IL_004e:  call       void [mscorlib]System.Console::Write(string)      IL_0043:  leave.s    IL_0032    }  // end .try    catch [mscorlib]System.Object     {   ........... /* Código real */        IL_0055:  pop      IL_0056:  ldstr      ""Codigo real""      IL_005b:  call       void [mscorlib]System.Console::Write(string)      IL_0050:  leave.s    IL_0032    }  // end handler    IL_0052:  ret  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1836   Un ejercicio interesante podría ser intentar identificar en que instrucción se encuentra el predicado  opaco sabiendo que el valor del parámetro es, al igual que en el caso anterior, un entero de valor  cero.   La respuesta es, en la línea IL_003e:  div. Lo que esta sentencia hace es provocar división por cero.  El resultado neto es, entonces, la continuación de la ejecución en el bloque catch donde reside el  resto del código real.  Lo importante de este esquema es que el error inducido puede estar en cualquier parte dentro de un  bloque de código, sin tener como requerimiento el uso de una sentencia de salto condicional que son  fácilmente identificables. Esto convierte al predicado opaco en una sentencia más, no identificable  estáticamente.   Algunas de las excepciones que podrían inducirse son división por cero, uso inválido de null, error  de conversión de tipos, valores fuera de rango, cast inválidos, entre otros.    4.3 Predicados Opacos utilizando Excepciones con Stealth mejorado  Intentemos llevar este concepto aún más allá. Dado que la bifurcación del flujo de control que puede  manipular el ofuscador surge de la generación inducida y controlada de un error en runtime, un buen  ofuscador podría utilizar construcciones comunes del programa siendo ofuscado e inducir errores  gracias a los valores opacos conocidos, pero generando estructuras exactamente iguales a la que  utilizó el programador de la aplicación.  4.3.1 Ejemplo: uso inválido de null  Supongamos que el ofuscador detecta que el programa a ofuscar hace uso intensivo de objetos que  son instancias de una clase que llamaremos CACIC y un método llamado Compartir(). Esto quiere  decir que, las sentencias en IL del tipo A.Compartir, siendo A una variable instancia de la clase  CACIC son comunes y frecuentes.  El ofuscador podría entonces ingresar predicados opacos de la forma A.Compartir(), pero en un  momento en el cual conoce que A tiene valor nulo.   La ofuscación del flujo de control se realiza perfectamente: para el ingeniero inverso es todo un  bloque de código coherente con construcciones normales y muy utilizadas, como por ejemplo  A.Compartir().  Sin embargo, desde el momento que A es un valor nulo, la ejecución se bifurca hacia el bloque  catch, lo cual es muy difícil de determinar automáticamente con un análisis estático, y es muy difícil  de detectar con un análisis visual del código, dado que el salto se produce en una sentencia  totalmente común al programa. En consecuencia, éste predicado goza de altos niveles de stealth.  4.3.2 Ejemplo Dos: colaboración con el programador.  Si la aplicación utilizara de manera intensiva acceso a datos, con una pequeña colaboración del  desarrollador, la excepción podría ser generada mediante llamados SQL erróneos. Esto confundiría  aún más a un ingeniero inverso desprevenido, ya que es, a priori, impensable que un mecanismo  automático como un ofuscador introduzca sentencias de llamado a datos.    4.4 Algunas limitaciones  Este esquema presenta algunas limitaciones. Una de ellas consiste en que los bloques try-catchfinally pueden anidarse pero nunca solaparse, de manera que no es posible realizar cualquier  construcción arbitraria. Otra limitación es, claramente, que es necesario respetar los bloques trycatch-finally que son verdaderamente parte del programa, respetando el funcionamiento del mismo.  XIII Congreso Argentino de Ciencias de la Computación ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯   Congreso General _________________________________________________________________________     1837 5 Conclusiones    La ofuscación es la técnica estándar para la protección del código fuente en ambientes de desarrollo  modernos. Dentro de las técnicas de ofuscación, los predicados opacos son el bloque de  construcción básico y su fortaleza determina en gran medida la calidad de las transformaciones de  ofuscación aplicadas.  Hemos analizado las técnicas de generación de predicados opacos más avanzadas, como la  utilización de estructuras utilizando alias y multithreading y concluímos que por su elevado costo,  tanto temporal como espacial, no son la solución óptima.  La propuesta de este trabajo ha sido la creación de predicados opacos utilizando el mecanismo de  manejo de excepciones como una manera de generar predicados de mayor calidad mediante la  reducción del costo y el aumento del stealth.   Como líneas de trabajo futuro podemos mencionar el estudio en mayor detalle del uso de  características avanzadas de los ambientes modernos para la protección de la propiedad intelectual,  como podrían ser el uso de atributos personalizados y el mencionado manejo de excepciones.  También hay un gran campo por explorar en el estudio y análisis del concepto de “funcionalidad  equivalente” entre un programa sin ofuscar y un programa ofuscado."	﻿transformaciones de ofuscación , obfuscation transformation , opaque predicates , código intermedio , predicados opacos	es	23376
49	Estudio del desempeño de OLSR en una red mallada inhalámbrica en un escenario real	"﻿ El objetivo de este trabajo es analizar el comportamiento del protocolo  OLSR sobre una red mallada configurada con firmware OpenWrt utilizando distintos  equipos de hardware. Se presentan los resultados empíricos de varias pruebas  utilizando el mismo escenario. El escenario que se presenta es una red de mundo real  (no de laboratorio) con pruebas reales y no simulaciones. OpenWrt  es un software  perfectamente válido que puede ser utilizado en una gran variedad de dispositivos y su  configuración para utilizarlo con protocolo OLSR es sencilla de realizar y no presenta  problemas de funcionamiento con dicho protocolo.   Keywords: Redes Malladas Inalámbricas, Redes Mesh, Protocolos, OLSR, OpenWrt.  1   Introducción  El objetivo de este trabajo es analizar el comportamiento del protocolo OLSR sobre una red  mallada configurada con firmware OpenWrt utilizando distintos equipos de hardware.  Las redes malladas inalámbricas (Wireless Mesh Networks) han tenido un gran éxito en  la historia de las ciencias de la computación y de la ingeniería. Sus aplicaciones son  numerosas en el dominio industrial, militar y comercial. Son en particular un dominio  rápidamente creciente y esto trae muchos desafíos. En particular, un desafío difícil e  inmediato es el enrutamiento efectivo debido a la volatilidad típica de tráfico en topologías  complejas. Muchos estudios han intentado resolver el problema de enrutamiento mediante  métodos heurísticos, pero este enfoque no proporciona los límites de cuán bien se asignan  los recursos.  Sin embargo, este tipo de investigación generalmente asume que el tráfico de  demandas de la red es estático y conocido de antemano. Como resultado, estos algoritmos  tienden a sufrir un desempeño pobre. De hecho, trabajos recientes han demostrado que el  tráfico inalámbrico es muy variable y difícil de caracterizar. Comprender el impacto de la  incertidumbre de la demanda en el ruteo y el diseño de algoritmos de enrutamiento para  proporcionar robustez, es relativamente un problema de investigación aún incipiente.   Las redes Mesh abiertas son redes ad-hoc descentralizadas que no se basan en  infraestructuras previas, como routers o puntos de acceso. En su lugar, cada nodo participa  en el enrutado, siendo él mismo un router y enviando datos de otros, y de ese modo la  determinación de las rutas se hace dinámicamente, basándose en la conectividad que va  surgiendo. Para ello, necesitan de protocolos que viabilicen ese comportamiento.  Es de suma importancia el análisis de la perfomance de diferentes protocolos de  comunicación que deben interactuar con diversos dispositivos que hacen al enlace de los  1066 nodos de la red a los fines de establecer la integración tecnológica disponible. No menos  importante es la determinación de la relación costo/beneficio de una determinada  implementación. El conocimiento en tiempo real de la configuración topológica de la red,  mediante el uso de distintas herramientas de hardware y software, nos permite el monitoreo  del comportamiento y sus alcances. Todo ello posibilita optimizar la red para que brinde un  mejor servicio. En general, la optimización se basa en lograr el mejor camino para enrutar  los paquetes de datos, sin demoras o con una demora mínima en función de lograr un mejor  aprovechamiento de los recursos utilizados.   En la Sección 2 se presentan algunos conceptos básicos sobre redes malladas y  protocolos de ruteo. En la Sección 3 se describe el escenario, hardware y software  utilizados. En la Sección 4 las pruebas realizadas. Finalmente, se presentan las  conclusiones.  2   Conceptos Básicos  Una Red Mallada Inalámbrica (Mesh) es una red compuesta por nodos organizados en una  topología de malla. Son redes en las cuales la información es pasada entre nodos en una  forma de todos contra todos y en una jerarquía plana, en contraste a las redes centralizadas.  Toda variación no prevista en el diseño, puede cambiar su topología, afectar a la  distribución de carga de la red y al rendimiento general [1].   Las ventajas que presenta frente a otras redes son el bajo costo al utilizar enlaces  inalámbricos, la facilidad de aumentar el área de cobertura incluyendo nuevos nodos, ya no  es necesario cambiar infraestructuras como en el caso de las redes cableadas, la robustez  que presenta ante fallos al disponer de rutas alternativas y la capacidad de transmisión que  permiten aplicaciones a los usuarios en tiempo real de voz, video y datos. Por tanto se  puede incluir un nuevo nodo en cualquier momento y lugar. Como consecuencia el costo de  este tipo de redes inalámbricas es mucho menor que en las redes cableadas, ya que no hay  que invertir en materiales de cableado y en estudios enfocados a la unión más óptima de los  nodos. En la realidad, la topografía raramente viene en forma de anillo, línea recta o  estrella. En terrenos difíciles, sean remotos, rural o urbano, donde no todos los usuarios ven  uno o algunos puntos centrales, lo más posible es que el usuario solo vea a uno o más  usuarios vecinos.   En una red mallada un conjunto de nodos se comunican entre sí de manera directa  transmitiendo la información de nodo a nodo hasta que llega a su destino final. La  información atraviesa múltiples saltos y no hay necesidad de una unidad centralizada que  controle el modo de transmisión. La comunicación se realiza entre los nodos directamente.  Cada nodo puede ser origen y destino de los datos o encaminar la información de otros  nodos. Las redes malladas inalámbricas son robustas al tener varios caminos disponibles  entre el nodo origen y el destino, de modo que el servicio no se ve afectado por la caída de  un nodo o por la ruptura de un enlace.   Dado que la forma de operar que tienen estas redes consiste en que los datos pasan de un  nodo a otro hasta que llegan a su destino, los algoritmos de ruteo dinámico necesitan que  cada nodo comunique información de ruteo a otros nodos en la red. Cada nodo determina  qué hacer con los datos que recibe, ya sea pasarlos al próximo nodo o quedárselos,  dependiendo del protocolo utilizado. El algoritmo de ruteo usado siempre debería asegurar  que la información tome el camino más apropiado de acuerdo a una métrica. Una métrica es  1067 el valor por el cual los protocolos determinan cuál ruta tomar o con cuál nodo comunicarse.  Una de las debilidades y limitaciones de las redes Mesh es la latencia (el retardo de  propagación de los paquetes), que crece con el número de saltos. Los efectos del retardo  son dependientes de la aplicación. Por ejemplo los correos electrónicos no son afectados  por grandes latencias, mientras que los servicios de voz son muy sensibles a los retardos.  Otra debilidad es la disminución del rendimiento en todas las redes multisalto, esto es, a  mayor número de saltos, se tiene menor rendimiento.    Con respecto al hardware, prácticamente cualquier nodo inalámbrico puede convertirse  en un nodo Mesh simplemente mediante modificaciones de software.    Protocolos de Encaminamiento  La principal función de los protocolos de encaminamiento es seleccionar el camino entre el  nodo fuente y destino de una manera rápida y fiable. Las redes malladas inalámbricas  pueden utilizar los protocolos de encaminamiento de otras redes ya existentes, pero  modificándolos para que funcionen correctamente con ellas. Si se elige esta opción, el  protocolo de encaminamiento modificado debe asegurar las principales características que  son el número de saltos, el rendimiento, la tolerancia a fallos, el equilibrado de carga, la  escalabilidad y el soporte adaptativo.   Otra opción es diseñar un nuevo protocolo de encaminamiento para las redes malladas  inalámbricas. Esta solución es más costosa ya que cuando se desarrolla un nuevo protocolo  hay que probarlo, modificarlo y solucionar los fallos. Por tanto el tiempo de realización es  mayor que si nos centramos en un protocolo ya experimentado.  En este trabajo utilizamos el protocolo OLSR para el encaminamiento en la red mesh  dado que es uno de los más difundidos en este tipo de redes inalámbricas, a continuación  una breve reseña.  OLSR: Optimized Link State Routing Protocol ([2], [3]) es un protocolo proactivo que se  basa en el estado de los enlaces. Se utiliza la técnica MPR (Multipoint Relaying) que  consiste en elegir un conjunto de nodos vecinos que cubran el acceso de nodos distantes a 2  saltos o más. Se adapta bien en redes con un gran número de nodos y de alta movilidad. El  formato del paquete es igual para todos los datos del protocolo, así es fácil la extensión del  mismo. Para saber el estado de un enlace se envían mensajes de  HELLO. Cada nodo tiene  asociado a cada vecino el estado del enlace. Cuando un nodo detecte la aparición de un  nuevo vecino se debe incluir una nueva entrada a la tabla de encaminamiento e incluir el  estado del enlace. Además si se detecta una variación en el estado de un enlace, se debe  comprobar en la tabla de encaminamiento que el cambio ha sido reflejado. Si no se recibe  información de un enlace durante un tiempo determinado se elimina de la tabla de  encaminamiento el enlace y el vecino correspondiente. Para calcular las rutas, cada nodo  contiene una tabla de encaminamiento con el estado del enlace y el nodo. El estado del  enlace se mantiene gracias al intercambio de mensajes periódicos. La tabla de  encaminamiento se actualiza si se detecta algún cambio en el campo de enlace, de vecino,  de vecino de dos saltos o en la  topología.    1068 3   Escenario y Tecnologías Utilizadas  Se montó una red experimental distribuida en tres edificios del campus de la Universidad a  los efectos de tener un campo de pruebas más parecido a la realidad de las redes mesh. En  la Figura 1 se muestra la distribución del equipamiento y métricas del protocolo OLSR.  Al momento de montar la red mesh, se realizó un análisis del campo electromagnético en  la frecuencia 2.4 ghz. Para esto se utilizó un analizador de frecuencia de Ubiquiti AirView2  ext. Se detectó que el canal 11 no estaba siendo utilizado por la red inalámbrica de  infraestructura. A raíz de esto se eligió esta frecuencia para la red mesh.    Fig. 1. Escenario  Tabla 1. Hardware utilizado.    1069 En el montaje de esta red se utilizaron equipos de las marcas Linksys (WRT54GL),  Ubiquiti (Nonostation 2, Nanostation Loco M2), TP-Link (TL-WR743ND, TLWR842ND)  como se muestra en la Tabla 1. Se eligieron por la gran popularidad y su bajo precio.    Se utilizó como sistema de operativo OpenWrt [4] que es una distribución de Linux usada  para dispositivos embebidos tales como routers personales. El soporte fue limitado  originalmente al modelo Linksys WRT54G, pero desde su rápida expansión se ha incluido  soporte para otros fabricantes y dispositivos. OpenWrt utiliza principalmente una interfaz  de línea de comando, pero también dispone de una interfaz web en constante mejora. El  soporte técnico es provisto como en la mayoría de los proyectos de Software Libre, a través  de foros y su canal IRC. El desarrollo de OpenWrt fue impulsado inicialmente gracias a la  licencia GPL, que obligaba a todos aquellos fabricantes que modificaban y mejoraban el  código, a liberar éste y contribuir cada vez más al proyecto en general.   Como se puede ver en la tabla de dispositivos se utilizaron distintas versiones de SO:  - OpenWrt en su versión 10.03.1, que es la estable más reciente, solamente con el  agregado del protocolo OLSR versión 0.6.1-3 que es la que viene standart con esa versión  de openwrt  - Freifunk [5] que es una adaptación basada en OpenWrt hecha por grupos de usuarios  alemanes que la utilizan para el montado de redes mesh en varias ciudades de ese país. Este  sistema operativo presenta varias adaptaciones específicas para redes mesh y entre ellas una  muy útil como es la graficación de los enlaces de toda la red y los valores de las métricas de  OLSR para cada una como se puede ver en la Figura 1, viene instalada por defecto. La  versión de OLSR es la 0.6.0.  Commotion [6] es otra adaptación de OpenWrt hecha especialmente para montado plug and  play de redes mesh. Está basada en las últimas versiones de OpenWrt (10.03 en adelante) y  para ser usada principalmente en equipos Ubiquiti de la serie M. Al igual que Freifunk (de  hecho muchas aplicaciones vienen de esta distribución) presenta varias herramientas y  utilidades para el análisis y la visualización del comportamiento de la red. También utiliza  protocolo OLSR por defecto en su versión 0.6.5.4. Si uno no utiliza las configuraciones por  defecto para el armado de la mesh presenta algún grado de dificultad para realizar la  configuración que uno desee.  En todas estas versiones de OpenWrt se utilizaron las versiones de OLSR que se instalan  por defecto desde los repositorios.  Todas estas versiones de OpenWrt utilizan un interface web que permite la configuración  de todas las opciones para que la mesh funcione.  4 Pruebas realizadas  Utilizando el escenario, se realizaron pruebas para medir la efectividad del protocolo. En la  ejecución de estas pruebas se utilizó el camino formado por los nodos 20, 7, 9, 16 y 14. Las  métricas de rendimiento son: El tiempo de ida y vuelta (Round-trip time - RTT), Jitter, la  probabilidad de error y testeo de ancho de banda.   RTT: es el tiempo que le lleva a un paquete alcanzar un nodo remoto y regresar.  Está  relacionado con la latencia de la conexión. Cuanto más bajo es el RTT, mejor es la  conexión.  1070 Jitter: es la variación en la latencia de paquetes recibidos de un nodo remoto. Cuanto más  bajo es, mejor conexión. Es importante cuando se utiliza aplicaciones de voz sobre IP.  Probabilidad de error: Los errores en una red causan que los paquetes se pierdan,  corrompan, se dupliquen o queden fuera de servicio. Cuando ocurre un error es importante  saber la probabilidad con la que suceden y el tiempo entre ellos. Lo ideal es no tener  errores, pero una tasa baja es aceptable.  Ancho de banda: es la tasa de transmisión de un enlace o sistema de transporte de datos  y se puede definir como la capacidad de un enlace o sistema para transmitir datos. Se  expresa en bit por segundo.  Nuestra herramienta principal de testeo fue iperf para todas las métricas a excepción de  RTT, que se midió con ping. Iperf es una herramienta que se utiliza para hacer pruebas en  redes informáticas. El funcionamiento habitual es crear flujos de datos TCP y UDP y medir  el rendimiento de la red.  Iperf permite al usuario ajustar varios parámetros que pueden ser  usados para hacer pruebas en una red o para optimizar y ajustar la red. Puede funcionar  como cliente o como servidor y puede medir el rendimiento entre los dos extremos de la  comunicación, unidireccional o bidireccionalmente. Es software de código abierto y puede  ejecutarse en varias plataformas incluyendo Linux, Unix y Windows. Cuando se utiliza el  protocolo UDP, Iperf permite al usuario especificar el tamaño de los datagramas y  proporciona resultados del rendimiento y de los paquetes perdidos.  Cuando se utiliza TCP,  Iperf mide el rendimiento de la carga útil. Típicamente la salida de Iperf contiene un  informe con marcas de tiempo con la cantidad de datos transmitidos y el rendimiento  medido.  Para medir el valor de RTT se utilizó la herramienta ping.  Ping es el acrónimo de Packet  Internet Groper, que significa ""Buscador o rastreador de paquetes en redes"". Es un utilitario  que analiza el estado de la comunicación entre un host local y uno o varios remotos por  medio del envío de paquetes. Se utiliza para  diagnosticar el estado, velocidad y calidad de  una red determinada. En nuestras pruebas siempre se utilizó como tamaño de paquete 1016.    La Figura 2 muestra los resultados de RTT obtenidos utilizando el protocolo OLSR. Los  resultados muestran un patrón donde RTT se incrementa con el número de saltos.      1071 Fig. 2. Evaluación de Round-trip Time en OLSR   La Figura 3 muestra los resultados de la variación del retardo (jitter) obtenidos. A medida  que aumenta el número de saltos aumenta el valor de retardo  y el aumento se torna  significativo para 3 y 4 saltos.    Fig. 3. Evaluación de Jitter en OLSR   La Figura 4 muestra los resultados de Probabilidad de error usando OLSR. Se observa  que  la pérdida de paquetes crece  con el número de saltos y se vuelve significativa a partir de 3  saltos.      Fig. 4. Evaluación de la Probabilidad de Error en OLSR   1072 La Figura 5 muestra los resultados obtenidos de las pruebas con TCP y UDP.  En ambos  casos el comportamiento es similar. El ancho de banda sufre un decrecimiento a medida que  se incrementa el número de saltos.      Fig. 5. Evaluación del ancho de banda en OLSR con UDP y TCP  Además, utilizando el mismo escenario, se realizaron pruebas para medir el tiempo de  recuperación del protocolo cuando cae un nodo de la red y para medir el tiempo que tarda el  protocolo en entrar en funcionamiento cuando se incorpora un nuevo nodo a la red.  Para medir el tiempo de recuperación cuando cae un nodo de la red, se procedió al  apagado del nodo 9 y se midió cuánto tiempo tardaba la red mallada en encontrar un  camino alternativo. El promedio obtenido fue de 25,63 segundos.  Para medir el tiempo de arranque, se apagó el nodo 14, luego se volvió a arrancar este  nodo registrando la hora de encendido. Este procedimiento se realizó ejecutando un script  para evaluar cuantos segundos demoraba en re arrancar. Se tomó la hora de la ejecución del  primer ping inalcanzable y luego la hora del primer 0% paquetes perdidos. El promedio  obtenido fue de 42,46 segundos.   Para complementar las pruebas con servicios reales se montaron sobre la red mesh, una  central telefónica IP Elastix con cinco teléfonos internos: tres internos utilizando un ATA  (Linksys phone adapter PAP2-NA) y dos por medio de software cliente de centrales IP.  También se montó una cámara IP sobre uno de los nodos más alejados. En estas pruebas  (video y voz sobre IP) sobre la red se pudo visualizar un desempeño aceptable de la misma  para dichos servicios aunque la aplicación de video no responde eficientemente a cambios  bruscos.  5 Conclusiones  En este trabajo se presentó un estudio sobre el rendimiento del protocolo OLSR. Se  presentaron los resultados empíricos de varias pruebas utilizando el mismo escenario. El  escenario que se presenta es una red de mundo real (no de laboratorio) con pruebas reales y  1073 no simulaciones. Cuando se trata con este tipo de entornos, los experimentos son cada vez  más difíciles de repetir en forma exacta al anterior.   Por lo observado se confirma que el rendimiento de la red decrece con el número de  saltos. Su valor, que en nuestro caso es bajo para cuatro saltos, depende mucho de la  ubicación y la conectividad entre dispositivos, como así también de la actividad  radioeléctrica circundante.  A la luz de los resultados hay algunos descubrimientos interesantes: a) OpenWrt  es un  software perfectamente válido que puede ser utilizado en una gran variedad de dispositivos  y b) su configuración para utilizarlo con protocolo OLSR es sencilla de realizar y no  presenta problemas de funcionamiento con dicho protocolo.    Ex-profeso se utilizó hardware variado para demostrar que se puede realizar una mesh  con los dispositivos disponibles en mercado (como los TP-LINK) e incluso algunos más  antiguos, como es el caso de los Linksys WRT54GL. De todas maneras en el  relevamiento  de redes existentes realizado y en el grado de desarrollo de los firmware, se pudo observar  que los equipos más utilizados son las distintas versiones de Nanostation de la marca  Ubiquiti.   Las pruebas con servicios reales sobre la red no tienen rigor de investigación y fueron  hechas a los efectos de visualizar en forma sencilla el comportamiento de la red y la validez  de la provisión de estos servicios sobre la misma.  Cabe aclarar que por tratarse de una red montada sobre un escenario real y no de  laboratorio hemos tenido que escoger adecuadamente los horarios de realización de las  pruebas dado que las otras redes inalámbricas instaladas en el edificio y la circulación de  personas tienen una marcada influencia en el funcionamiento de la red mallada."	﻿redes malladas inalámbricas , redes mesh , OLSR , open WRT	es	31363
50	IP core para redes de Petri con tiempo	﻿ En este trabajo, se presenta un procesador de Redes de Petri con  Tiempo, el que es la evolución del Procesador de Petri Temporizado. Este  procesador es programado directamente con las matrices y vectores del  formalismo de Petri, lo que permite aprovechar el poder de las redes de Petri para  modelar sistemas de tiempo real y verificar formalmente sus propiedades,  evitando errores de programación al implementar el programa a ejecutar.  Este desarrollo ha sido realizado como un IP-cores y es usado en un sistema  Multi-core. De esta manera, es posible realizar la implementación del sistema  utilizando este IP-core, lo que asegura las propiedades del modelo realizado con  la red de Petri con Tiempo, que verifican los requerimientos del modelo que  representa al sistema real, sean cumplido.  Key words: Multi-core, Red de Petri, Procesador  1 Introducción  Los sistemas informáticos son complejos tanto en su estructura como en su  comportamiento, más aun cuando tienen un gran número de estados y numerosas  combinaciones de datos y eventos de entrada.  Abordar soluciones de sistemas complejos y crítico, para dar solución a sistemas en  tiempo real, tiene problemas como: la complejidad inherente de la especificación, la  coordinación de tareas concurrentes, la falta de algoritmos portables, entornos  estandarizados, software y herramientas de desarrollo.   Y teniendo en cuenta, las tendencias inequívocas en el diseño de hardware, que  indican que un solo procesador no puede ser capaz de mantener el ritmo de incrementos  de rendimiento. Por lo que la evolución de los procesadores, que es consecuencia de la  mayor integración y la composición de distintos tipos de funcionalidades integradas en  un único procesador. Más aun, hoy la disponibilidad de transistores ha hecho factible  construir en una sola pastilla varios núcleos de procesador que ha resultado en el  desarrollo de la tecnología Multi-core [1].  La obtención de rendimiento decreciente del paralelismo a nivel de instrucción (ILP)  y el costo del incremento en la frecuencia debido principalmente a las limitaciones de  potencia (se sugiere que un 1% de aumento de velocidad de reloj resultados en un  aumento de potencia del 3% [2]) ha motivado el uso de los Multi-core.  1097 Por lo cual los procesadores Multi-core son una propuesta para obtener aumento de  rendimiento. Lo que se traduce principalmente en menores tiempos de ejecución,  consumo ruido, densidad de energía, latencia y más ancho de banda en las  comunicaciones inter-core. Si también consideramos a los Multi-core heterogéneos que  tienen como ventaja emplear cores especializados, diseñados para tareas específicas.  Es decir, optimizado según la necesidad. Estos tienen la capacidad de usar los recursos  de hardware disponibles donde el software específicamente lo requiere. [3]  Con el fin de aumentar el desempeño, estos sistemas hacen uso colaborativos de  multi-hilos y/o multi-tarea, lo que permite aprovechar los múlti-núcleos. Pero se  requiere de más trabajo en el diseño de las aplicaciones, ya que emergen con fuerza la  problemática de los sistemas concurrentes.  Por lo que con estos procesadores, la programación paralela es indispensable para la  mejora del desempeño del software en todos los segmentos de desarrollo y con más  razón en el segmento de sistemas de tiempo real.  Para dar solución a los sistemas reactivos, paralelos y de tiempo real, en relación con  los siguientes aspectos:   Problemas de concurrencia que emergen en la programación paralela, por no ser  componible, es decir, no se puede obtener un programa paralelo de la composición  directa de dos programas secuenciales.   Que el hardware de soporte a la implementación de sistemas concurrentes,  permitiendo mejorar los algoritmos paralelos.   Asegurar los requerimientos temporales en los sistemas de tiempo real, es decir, los  intervalos mínimos y máximos para la ocurrencia de un evento. Para lo cual el  hardware facilite la programación de estas restricciones en forma directa.   Tareas de codificación, que se requieren para la implementación de un modelo,  conducen a errores e incrementan el esfuerzo, por lo que es muy valorable que no  exista ninguna tarea entre el modelo y el software a ejecutar.  2 Objetivo  2.1 Objetivo Principal  El objetivo principal de este trabajo es diseñar e implementar un procesador de Redes  de Petri con Tiempo, que ejecute la semántica temporal y se programe en forma directa  a partir de las ecuaciones de estado del modelo.  2.2 Objetivos Secundarios  Los objetivos secundarios de este trabajo son:    Describir brevemente las Redes de Petri con Tiempo con el fin de realizar su  implementación por hardware.   Mantener la ejecución de las Redes de Petri ordinarias con parámetros temporales  en dos ciclos de reloj.    Implementar el procesador de Redes de Petri en un IP-core.  1098 3 Redes de Petri con Tiempo  En estas redes, cada transición con tiempo tiene asociado un intervalo de tiempo [a,  b] que establece el intervalo de tiempo dentro del cual puede ser disparada la transición,  con el fin de homogenizar las definición matemática definimos  transiciones inmediatas  con límite inferior cero. [4]  3.1 Definición Matemática  Una Red de Petri con Tiempo (TPN)  [5] y marcada, se define matemáticamente  como una 8-tupla de la siguiente manera:  {𝑃, 𝑇,  𝐼+,  𝐼−, 𝐻, 𝐶,𝑚0, 𝐼𝑆 }    Donde {𝑃, 𝑇,  𝐼+,  𝐼−, 𝐻, 𝐶,𝑚0} es una red de Petri plaza transición marcada con  brazos inhibidores y plazas acotadas, y 𝐼𝑆 es la función estática de intervalos [𝑎, 𝑏]  asociados a cada transición.   Dónde:   𝑷: es un conjunto finito y no vacío de plazas.  𝑻: es un conjunto finito y no vacío de transiciones, P y T son conjuntos disjuntos    𝑰+,  𝑰− : son las matrices de incidencia positiva y negativa. La matriz 𝑰 es las  diferencias entre  𝑰+,  𝑰−.  𝑃𝑥𝑇 →  𝑍  H: es la matriz de brazos inhibidores.  𝑃𝑥𝑇 → {0,1}  C: es el vector de cota de plaza  𝐶 → 𝑁  IS: es la función estática de intervalos asociados a cada transición.   𝑇 → ℚ+ × (ℚ+ ∪∞)  La función IS asocia a cada transición un par de valores que representan los límites  temporales máximó y mínimo entre los cuales la transición podrá ser disparada. De  manera tal que  𝐼𝑆(𝑡) =  [𝑚𝑖𝑛,𝑚𝑎𝑥] ∀ 𝑡 ∈ 𝑇    Como la función IS representa un intervalo temporal, para cada transición t  sensibilizada se introduces el valor 𝑡𝑖𝑚𝑒𝑟𝑡 , que se auto incrementa con el tiempo, si la  transición esta sensibilizada y se cumplir: min ≤  𝑡𝑖𝑚𝑒𝑟𝑡  ≤ 𝑚𝑎𝑥 el disparo sea  posible.  Estas cotas deben cumplir las siguientes condiciones:   0 ≤ min < ∞   0 ≤ max ≤ ∞   𝑚𝑖𝑛 ≤ max 𝑠𝑖 max ≠ ∞   𝑚𝑖𝑛 < max 𝑠𝑖 max = ∞  1099 Al valor min lo lamamos Earliest Firing Time EFT (Instante de disparo más  temprano). Y, al valor max se le llama Latest Firing Time LFT (Instante de disparo más  tardío).  Existen dos tipos de intervalos destacables:   Intervalo puntual [𝑎, 𝑎]. En este caso, el tiempo de disparo es fijo, después de  sensibilización se espera un tiempo 𝑎.  Un disparo inmediato es representado por α = 0 y se comporta como en las Redes de  Petri plaza transición.   Intervalo sin restricción temporal, [𝑎,∞]. Se disparara en algún momento después  de sensibilizarse y un tiempo a.  3.2  Estados en una Red de Petri Temporizada  En las Redes de Petri con tiempo, el estado de la red es definido por el vector de  marcado 𝑚𝑖 y por el vector de valores de intervalos de transición 𝑡𝑖𝑚𝑒𝑟 de la red, que  lleva la cuenta de tiempo de cada transición sensibilizada. Por lo tanto el estado es:   𝐸 =  (𝑚𝑖, 𝑡𝑖𝑚𝑒𝑟 )  3.3 Transición Sensibilizada y Disparo de una Transición  Cundo nos referimos a una transición hay que distinguir las siguientes cuestiones:  transición habilitada o sensibilizada, transición no habilitada y disparo de una  transición.  En una Red de Petri marcada, con una marca 𝒎𝒌, se dice que una transición tj se  encuentra habilitada o sensibilizada si y solo si (sii) todos los lugares del conjunto de  plazas • tj de entrada a la transición tienen al menos la cantidad de marcas igual al peso  de los arcos ( 𝒘(𝒑𝒊, 𝒕𝒋)) de entrada a la transición tj, esto es:      𝑝𝑖 ∈   • 𝑡𝑖 , 𝑚(𝑝𝑗) ≥ 𝑤(𝑝𝑖 , 𝑡𝑗)    Si el 𝑡𝑖𝑚𝑒𝑟 de la transición es cero, se debe habilitar 𝑡𝑖𝑚𝑒𝑟𝑡  para que se auto  incremente con el tiempo.  Las transiciones sensibilizadas pueden ser disparadas en el intervalo [a, b], y su  disparo provoca un nuevo marcado es decir un cambio de estado. La ecuación para  calcular el cambio de estado o la nueva marca alcanzada por el disparo de tj es  𝜕 (𝑚𝑘 , 𝑡𝑗), y se define por la siguiente expresión:  𝜕 (𝑚𝑘, 𝑡𝑖𝑚𝑒𝑟𝑡) = {     𝑚𝑘+1(𝑝𝑖) =  𝑚𝑘(𝑝𝑖) − 𝑤𝑖𝑗    𝑚𝑘+1(𝑝𝑖) =  𝑚𝑘(𝑝𝑖) + 𝑤𝑗𝑖    min ≤  𝑡𝑖𝑚𝑒𝑟𝑡  ≤ 𝑚𝑎𝑥          𝑚𝑘+1(𝑝𝑖) =  𝑚𝑘(𝑝𝑖)     , ∀ 𝑝𝑖 ∈  𝑡𝑗 • , ∀ 𝑝𝑖 ∈  𝑡𝑗 • , 𝑒𝑛 𝑒𝑙 𝑟𝑒𝑠𝑡𝑜 𝑑𝑒 𝑙𝑜𝑠 𝑐𝑎𝑠𝑜𝑠;   𝑚𝑖𝑛 = 𝑎,𝑚𝑎𝑥 = b   Donde el 𝑡𝑖𝑚𝑒𝑟𝑡𝑗se incrementa en cada ciclo de reloj mientras la transición se  encuentra sensibilizada.  1100 4 Arquitectura y Funcionamiento del Procesador de Petri con  Tiempo  El procesador ejecuta la ecuación de cambio de estado resolviendo solo un disparo  de una transición a la vez, esto permite resolver todos los casos de disparos, los simples  (disparo único) y los disparos múltiples, realizándolos como una secuencia de disparos  simples, esto simplifica el hardware.   Las disparos son transmitidos por los hilos que se ejecutan en los cores a través del  bus del sistema, según las solicitudes emergentes del sistema que se está ejecutando.  Esto disparos son recibidos por el Procesador de Petri con Tiempo y almacenado en la  cola de disparos de entrada. Existe una cola FIFO por cada transición, la salida de este  conjunto de colas es una palabra con tamaño igual a la cantidad de transiciones, la cual  tiene unos en las posiciones correspondientes a las transiciones con disparos solicitados,  el orden del bit en la palabra es igual al número de la transición que solicita el disparo.  Los bits, que se corresponden con las transiciones que no tiene disparo solicitado, son  cero, es decir no hay solicitud de disparo.   La cola de salida tiene una estructura similar, pero comunica los disparos resueltos a  los hilos.  En la Fig. 1 se muestran los distintos módulos que componen el procesador,  resaltando las principales diferencias con versiones anteriores. [6]  El módulo de I/O Datos gestiona el acceso de los cores a las matrices y vectores que  programan el sistema.   Bus del Sistema Multi-Core Cola de  Salida Cola de  Entrada I/O Datos Ma riz I Matriz H Cotas de  Plazas Transicion es Auto Vector  EFT Vector  LFT   Vector Timer Marcado Estado de  Transición Matriz Cal  proximo estado Estado  Sensibilizado  Detect Red  Activa  Matriz de  Prioridad Modulo de  Calculo de la  Ecuación de  Estado Fig. 1. Procesador de Petri con Tiempo.  El programa del sistema son las matrices y vectores descriptas en la ecuación de  estado, esto permite programar el procesador en forma directa a partir de la Red de Petri  con Tiempo.  Aquí se han agregado la matriz de Brazos Inhibidores y el vector de Cota de Plaza  que no figuran en la ecuación de estado presentada en este trabajo, pero son los mismos  que en el Procesador de Petri presentado en otros trabajos [7].  1101 La responsabilidad del Módulo de Cálculo de la Ecuación de estado es la siguiente:  1. Calcular el nuevo estado que resultaría por disparar solamente una transiciones una  vez, por lo que resultan tantos vectores de estados calculados como transiciones, y  se almacenan. Esto se realiza en paralelo sumando al estado actual a cada columna  de I y almacenando todos los vectores resultantes, los que serán evaluados para  determinar si son los posibles nuevos estado. Esta operación es realizada siempre  que cambia el estado del procesador, vector Marcado.  2. Determinar que transición esta sensibilizada. Se toma todos los vectores calculados  en 1 y se verifica que se cumpla que ninguna plaza tenga marcado negativo y  tampoco supere la cota de plaza, estas son las transiciones sensibilizadas.  3. Se arranca o para los Timer𝑡  . Si en una transición sensibilizada Timer𝑡 = 0 se  arranca Timer𝑡  y si Timer𝑡 ≠ 0 no se hace nada.   4. Disparo de una transición. Las transiciones que cumplen con:    𝑉𝑒𝑐𝑡𝑜𝑟 𝐸𝐹𝑇 ≤  Vector Timer𝑡  ≤ 𝑉𝑒𝑐𝑡𝑜𝑟 𝐿𝐹𝑇   Las transiciones que cumplen con esta condición y han recibido por la cola de  entrada un disparo o el disparo están programado como automático, conforman un  conjunto de disparos posibles  De este conjunto se selecciona el de mayor prioridad y se ejecuta la transición.  Según la transición ejecutada se actualiza el vector de estado, y se pone Timer𝑡   a cero.  5. Se ejecuta como un ciclo continuo los pazos 1, 2, 3 y 4.  El sistema posee una unidad que detecta cuando ninguna transición esta sensibilizada  y Vector Timer supera el tiempo máximo; esta condición genera una interrupción que  comunica que el sistema ha finalizado o esta interbloqueado, esta característica es de  suma utilidad para verificar el diseño e implementación del sistema.  La Tabla 1 muestra las diferencias significativas, desde el punto de vista de la  ejecución de las distintas semánticas, estas son:  Tabla 1. Comparación entre Semánticas Temporales.    Con Tiempo Temporizada  1 Interrumpible Si No  2 Representa las dos semánticas Si No  3 Matrices usadas I I+, I-  4 Permite contener subredes No Si    De este cuadro se desprenden las siguientes observaciones:   1. Siendo que las TPN son interrumpibles y las Redes de Petri Temporizadas (TdPN)  no lo son, para el caso de múltiples disparos y transiciones en conflicto, un TPN lo  resuelve según el intervalo de tiempo; en cambio una TdPN lo hace explícitamente  en la matriz de prioridad. Esto hace más complejo el modelado con TdPN e  indispensable incluir en el procesador una matriz de prioridades.  1102 Dado que la mecánica de ejecución de las TdPN requiere de un estado más para  no ser interrumpibles los tokens son retirados inmediatamente de la plaza y no  pueden ser solicitados por otra transición.  2. Dada una red con TPN, una transición, que por semántica es interrumpible, puede  transformarse en una no interrumpible modificando la red. Esto se logra encerrando  con una transición inmediata la transición temporiza. Lo que tiene como impacto un  incremente de una plaza y una transición adicional por cada transición no  interrumpible.  3. Para realizar el cálculo de un nuevo estado las TPN lo hacen con una matriz de  enteros con signo mientras que las TdPN lo realizan con dos matrices de enteros sin  signo; por lo cual debemos analizar dos casos:  a. Si los pesos de los arcos son uno:  i. Las TPN requieren de una matriz con 2 bit por elemento.  ii. Las TdPN requieren de dos matrices binarias.  b. Si los pesos de los arcos son uno o mayor a uno:  i. Las TPN requieren de una matriz de enteros con signo.  ii. Las TdPN requieren dos matrices de enteros sin signo.  En el primer caso los recursos utilizados son similares. Por lo que la selección de  uno u otro procesador depende de la semántica a utilizar. Mientras que, en el  segundo caso los recursos utilizados por las TdPN son mayores. La ventaja de una  con respecto a la otra en cuestión de recursos está determinada por incremento de  la matriz de incidencia dada por la conversión de las transiciones Time a su  equivalente no interrumpibles.  4. El procesador que implementa la semántica TdPN utiliza dos estados para realizar  el cálculo de los toquen que entran de una transición y los que salen de esta. Esta  diferenciación de estados nos permite insertar una nueva red de Petri entre los dos  estados de una transición, lo que posibilita que el procesador puede ser extendido a  redes de Petri jerárquicas; ya sea haciendo uso de la semántica TdPN o de las redes  de Petri ordinarias. Esto en la actualidad es motivo de una nueva investigación.  Las dos semánticas son investigadas, puesto que las TPN requieren de menos  recursos para resolver problemas no interrumpibles (que son los más habituales).  Mientras que las TdPN presentan potencial de mejora al permitir construir redes de  Petri jerárquicas. [8].  5 Análisis de Rendimiento  La implementación de sistema ha sido realizada en una plataforma Atlys™ Spartan6, los cores utilizados son los MicroBlaze ver8.40 [9] que ejecuta un Sistema Operativo  XilKernel ver5.01a. Interconectado con el Procesador de Petri Temporizado por un bus  AXI [10].  Para comprobar correcto funcionamiento del IP Core y analizar los tiempos de  sincronización, se realizaron mediciones para distinto número de iteraciones y numero  1103 de hilos tratando de acceder a una variable compartida en exclusión mutua. Luego se  compararon el Procesador de Petri con una implementación utilizando semáforos,  ambos resolviendo un mismo problema. La elección de este segundo método de  sincronización se basa en que son el mecanismo más ligero para realizar éstas tareas.  A partir de estas mediciones se calculó el Speedup, los resultados se muestran en la  Fig. 2, se puede observar que, para todos los casos, el procesador de Petri es en  promedio es entre un 15% y un 30% más rápido que el uso de semáforos para resolver  el problema de sincronizar múltiples hilos que desean escribir sobre una variable  compartida e incluso, se alcanzan picos de hasta un 70%.    Fig. 2. Tiempos de sincronización por iteración  Estas mediciones se realizaron con tiempos 𝐸𝐹𝑇 𝑦 𝐿𝐹𝑇 cero, de manera que el  rendimiento es el mismo obtenido en el procesador de Redes de Petri sin la semántica  temporal. Esto es válido ya que el tiempo de una transición es parte del modelo, es  decir, es el mismo para el procesador de Petri como para la implementación con  semáforos y el propósito es medir únicamente los tiempos de sincronización.  Además, como se observa en la Fig. 3, el procesador necesita únicamente un semiciclo de reloj, desde que el contador alcanza el valor EFT hasta que el disparo se coloca  en la cola de salida. La demora introducida es despreciable en relación con el tiempo  que tiene un δt de un ciclo de reloj.  Teniendo en cuenta lo despreciable de la latencia y tomando el tiempo como parte  del modelo es posible analizar el rendimiento sin tener en cuenta los vectores EFT y  LFT.    Fig. 3. Ejecución en hardware.  1,35 1,73 1,74 1,10 1,24 1,241,16 1,26 1,26 0,0 0,5 1,0 1,5 2,0 10 1000 10000 Sp ee d  u p 2 Escritores 4 Escritores 5 Escritores 1104 6 Crecimiento del IP Core  Se analizó el crecimiento del procesador en función de los parámetros que posee.  Para esto se generaron procesadores de 8x8, 16x16, 32x32, 48x48 y 64x64 (Plazas por  Transición) con capacidad de 7 bits por plaza y elementos de tiempo de 48 bits y se  graficaron los resultados, los que se pueden observar en la Fig. 4.  Se observa que el crecimiento del IP Core no es algo para despreciar, puesto que la  cantidad de elementos empleados crese rápidamente con el producto de las Plazas por  las Transiciones.  Fig. 4. Crecimiento del IP Core    Por otra parte, ya que es posible sintetizar un procesador para cada semántica es  deseable determinar y comparar el consumo de recursos para cada uno. La Fig. 5  muestra la comparación del crecimiento entre las distintas implementaciones.  Se puede observar que ambos procesadores utilizan aproximadamente la misma  cantidad de Flip-Flops pero la implementación para redes temporizadas utiliza un 90%  mas LUTs para el mismo número de plazas y transiciones.    Fig. 5. Recursos usados por distintas semánticas.  1105 7 Conclusión y Aportes  En el presente trabajo, se desarrolla el Procesador de Petri con Tiempo, que permite  desacoplar los la concurrencia del procesamiento secuencial. Teniendo en cuanta que  el Procesador de Petri con Tiempo permite utilizar Redes de Petri Temporizadas, este  procesador puede remplazar a su predecesor y preserva sus particularidades.  El modelo de Petri es adecuado para implementar, validar y verificar un sistema  paralelo con concurrencia, este tiene una representación algebraica que este procesador  usa como el código ejecutable. Las ventajas de este procesador son la disminución de:   Esfuerzo de programación, la ecuación de estado es ejecutada directamente en el  procesador, y no se requiere programación adicional.   El gap entre las restricciones temporales y sus programaciones. Puesto que se trata  de los vectores temporales propios de la semántica usada por el procesador.	﻿procesador , red de Petri	es	31453
51	Posicionamiento indoor determinado por la distancia en función de la potencia medida de balizas bluetooth	﻿ El presente artículo presenta una experiencia de captura de datos de  dispositivos que emiten señales bluetooth usando una placa tipo Arduino Mega  2560. Se analizan los datos con algunas técnicas y se detalla la magnitud de  errores encontrados en las diferentes muestras. Además, se proponen algunas  nuevas medidas para intentar alcanzar una mejor precisión en el  posicionamiento.  Keywords: Posicionamiento indoor, Balizas Bluetooth, RSSI.  1 Introducción  Los métodos de posicionamiento fueron evolucionando en el tiempo. Los fenicios  usaban el sol, la luna y las estrellas para guiarse [1][2]. En la actualidad, se usan  micro dispositivos electrónicos [3]. A finales del siglo XX, la aparición de  calculadoras y computadoras electrónicas, facilitó grandemente el cálculo; pero la  aparición del GPS, poco después, revolucionó la forma de localizar un objeto [4].   Esta tecnología para el posicionamiento en exteriores carece de utilidad en un  espacio cerrado como puede ser un edifico. Es difícil usar esta tecnología para  distinguir en que habitación o en que planta se encuentra ubicado una persona. Es por  ello, que en los últimos años se han presentado diversas soluciones de  posicionamiento en espacios interiores. Entre ellas, se encuentra Bluetooth la cual se  experimenta aquí y se analiza con diversas técnicas para lograr posicionar un objeto  en un ambiente interior.  Bluetooth utiliza frecuencias de radio del orden de 2.4 Ghz, representa una  tecnología económica [5], pero es de corto alcance, de esta manera, para cubrir la  zona de un recinto, se necesitarían varios dispositivos. El error asociado a la  estimación puede encontrarse en torno a los 1.5 metros de precisión. En este sentido,  el parámetro RSSI Received Signal Strength Indicator, (Intensidad de la Señal  Recibida) no es preciso, motivo por el cual, no se puede estimar con exactitud la  1119 ubicación de un dispositivo, sino que se identifica el entorno en el que se encuentra,  en un radio determinado.  En general, los sistemas de posicionamiento heredan características dependientes  del tipo de sensor. Algunas de ellas son: Retardo en la propagación, difracción,  reflexión y la dispersión que afectan a todos los tipos de señales.  Las características  propias de la señal son las siguientes [6]:  • Atenuación por distancia: A mayor separación entre el emisor y el  receptor, la potencia de la señal decrece con el tiempo de forma  logarítmica, si el receptor se encuentra a una distancia corta del emisor, la  potencia decrece rápidamente y si el receptor se encuentra en un rango de  alcance medio, la señal decrece a una velocidad menor.  • Absorción de la señal: Cuando la señal atraviesa algún material, la  potencia de la misma se debilita o atenúa en mayor o menor intensidad,  dependiendo de las características físicas del material y de la frecuencia  propia de la onda.  • Reflexión: Este fenómeno ocurre, cuando una onda, choca con un  obstáculo, parte de la potencia de la señal no se absorbe, sino que es  reflejada y la misma puede tener distinta fase que la señal original,  dependiendo de las características propias del obstáculo.  • Dispersión: Este fenómeno ocasiona que parte de la energía sea irradiada  en numerosas direcciones diferentes y ocurre cuando el medio por el cual  viaja la señal, está formado por objetos con dimensiones pequeñas,  comparados con la longitud de onda propia de la señal. Es el fenómeno  contrario a la reflexión, la cual ocurre cuando los objetos poseen  dimensiones grandes.   • Difracción: Cuando una señal impacta con el borde de un obstáculo, se  originan diferentes frentes de onda en distintas direcciones. Los factores  de los cuales dependen la intensidad de este fenómeno son: la calidad y  tipo del material con el que está compuesto el obstáculo, así como  también de la amplitud o fase de onda.    Los tres últimos fenómenos citados anteriormente, dan lugar un fenómeno  denominado: Multitrayecto (Multipath) que origina que la señal llegue al receptor a  través de diferentes caminos y por lo tanto a diferentes tiempos ocasionando retardos  e interferencias en las transmisiones. De esta manera, las comunicaciones  inalámbricas en interiores se caracterizan por este fenómeno, donde no solamente  existen señales directas entre el emisor y el receptor, sino que también se encuentran  señales difractadas, dispersadas y reflejadas por los diferentes obstáculos y objetos  que se encuentran en el medio.  En el trabajo desarrollado en [7] se dispone de tres ordenadores portátiles que  actúan como emisores de señal Bluetooth y un dispositivo móvil como receptor de la  señal Bluetooth. El dispositivo móvil es quien calcula la posición donde se encuentra  mediante triangulación de la señal que emiten los tres portátiles.  En [8] se presenta una arquitectura en la que los emisores son de bajo costo y el  receptor es un dispositivo móvil. El artículo quiere enfatizar en la ventaja de usar este  tipo de arquitectura pasiva de bajo costo.  1120 La plataforma Alipe [9] mezcla diversas topologías para obtener la posición. En  esta plataforma por un lado hay dispositivos Bluetooth que envían información sobre  su ubicación al realizarle una petición por parte de otro dispositivo Bluetooth cliente.  Si el dispositivo al que se le ha realizado la petición no está adaptado para comunicar  su posición, el dispositivo cliente que ha realizado la petición registra la dirección  Bluetooth remota y busca en una base de datos centralizada la ubicación asociada a  esa dirección Bluetooth.  “Follow me” [10] presenta una aplicación práctica para un sistema de  posicionamiento en interiores. El sistema consiste en dispositivos Bluetooth  rastreadores cuya ubicación es conocida. Estos dispositivos rastreadores están  constantemente escaneando dispositivos Bluetooth, cada vez que se detecta un  dispositivo se almacena su dirección Bluetooth junto con su ubicación, que es la  ubicación del dispositivo rastreador, en una base de datos centralizada. La aplicación  ofrecida al usuario es poder obtener su ubicación en el edificio, consultando esa base  de datos y publicar la ubicación en una aplicación web como puede ser Twitter.  En el pabellón de Finlandia en la expo de Shangai 2010 se desarrolló una  aplicación para móvil [11] en la que los usuarios podían obtener su posición dentro  del pabellón mediante puntos de acceso Bluetooth que calculaban posiciones  mediante distribuciones de probabilidad del indicador RSSI.  Los sistemas de posicionamiento en interiores no sólo se limitan el uso de  tecnologías inalámbricas también, como se puede ver en [12], se ha desarrollado un  sistema de posicionamiento en interiores basado en visión por ordenador, en el que se  cuenta con una base de datos de imágenes georefenciadas que se usa para buscar  coincidencias con lo que está viendo el dispositivo móvil en ese momento.  En la mayoría de los sistemas de posicionamiento indoor analizados se opta por  estimar la posición usando el indicador de Fuerza de Señal de Recepción (RSSI),  recolectando medidas desde distintos puntos para inferir un modelo probabilístico que  estima las posiciones una vez que el sistema está en funcionamiento.  Albert Huang [13] utiliza la infraestructura de computadoras existentes en un  edificio agregando 30 balizas BT en los puertos USB de las mismas, logrando una  exactitud de 3-10 metros, dependiendo de la densidad de baliza en la zona.  Fernández Gorroño [14] utiliza un sistema de posicionamiento en interiores basado  en la tecnología Bluetooth para aplicaciones en Dispositivos Móviles (DM), con el  objeto de proveer información en estaciones de subterráneos. La característica de este  sistema es que el peso de la lógica está en el DM y las balizas son pasivas y de bajo  costo.  Sudarshan S. Chawathe [15] estudia los retrasos en la fase de descubrimiento de  balizas Bluetooth y propone métodos para aliviarlos de forma de poder utilizarlos   para aplicaciones de localización en interiores como, complejos de edificios grandes o  terminales de aeropuertos usando coordenadas adecuadas al lugar como número de  piso y número de habitación o terminal del aeropuerto y dársena.   La sección 2 presenta el proceso de adquisición de datos, la 3 muestra el análisis  que se hace a los datos recavados con algunas técnicas propuestas, aquí se muestran  los resultados a los que se llegó luego de procesar los datos. Por último, la 4, presenta  las conclusiones y futuros trabajos.  1121 2 Toma de muestras y experimentos realizados  Se diseña un escenario para la toma de muestras en una de las oficinas del edificio  INTIA/INCA, de la Facultad de Ciencias Exactas perteneciente a la Universidad  Nacional del Centro de la Provincia de Buenos Aires. Se hacen marcas de precisión  en el suelo cada un metro llegando a completar 25 metros. Se toman 100 muestras de  cada punto. Las primeras marcas, hasta los 3 metros corresponden al interior de una  oficina y hasta los 7 metros a otra oficina contigua. Luego hasta los 25 metros las  tomas se hacen en un pasillo del edificio.   Para la toma de datos se especifica un sistema de medición de potencia de  dispositivos Bluetooth (BT) remotos (baliza BT) utilizando comandos AT. Para ello,  se desarrolla un programa de control residente en la placa Arduino Mega que setea al  dispositivo Bluetooth local en modo “master”. De esta forma, se interroga a los  dispositivos detectados, se identifican por su dirección MAC y se mide su potencia.  La figura 1 muestra un diagrama de los componentes del sistema de adquisición.                  Figura 1. Diagrama de componentes del sistema  Los dispositivos que se usan para adquirir los datos son módulos transceptores de  tecnología inalámbrica Bluetooth RS232 TTL V2.0 con chipset RSE.  Para el procesamiento de los datos se desarrolla un programa en Ansi C que, por  medio del puerto USB conectado al Arduino, recibe cien datos de medición de  potencia y los almacena en un archivo de texto plano.  3 Análisis de los datos con las técnicas propuestas  Los datos obtenidos se vuelcan en una plantilla de cálculo donde se obtienen por cada  grupo de datos: la moda, el valor máximo, el mínimo y el promedio. Luego se  proponen algunas técnicas novedosas a implementar aplicadas en [16][17][18].  En la figura 2 se observa el comportamiento de cada una de las técnicas  aplicadas. Cabe aclarar que el mínimo no se muestra en el gráfico ya que tiene valores  extremos que no permiten visualizar con claridad el resto de los resultados obtenidos.       PC     Arduino    Mega 2560   BT   Baliza     BT  d  1122 160 165 170 175 180 185 190 195 200 205 210 215 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Distancia (metros) RS SI   (dB m ) moda promedio max   Figura 2. Gráfico de la potencia en función de la distancia    La figura 2 presenta un comportamiento similar para las tres medidas. Cuando la  distancia aumenta, el indicador de fuerza de la señal (RSSI) disminuye. Hasta los 3  metros puede verse como el comportamiento es ideal para la moda ya que cada uno de  los valores es menor a medida que se aleja de la baliza. No pasa lo mismo con las  otras medidas. Al pasar a la oficina contigua, y esto puede ser debido a la presencia de  paredes que obstaculizan la señal, la moda oscila bruscamente pero el promedio y los  máximos mantienen consistencia hasta los 8 metros inclusive. Desde allí y hasta los  12 metros, el comportamiento es similar para todas las medidas. A los 13 y 14 metros,  prácticamente coinciden en valor las tres. Luego, se observan muchas oscilaciones en  todas las medidas, esto se puede atribuir a los rebotes de la señal en las paredes del  pasillo y en el amoblamiento de las oficinas que se encuentra al paso de la señal.   Ahora si se calcula la regresión lineal de cada una de las medidas se puede  observar la distancia entre la medida “ideal” y la real. En el caso de la figura 3, se  observa la regresión lineal para el promedio, en la 4 para la moda; y finalmente en la  5, para el máximo. También se muestran las ecuaciones de las rectas en cada caso y el  valor de R2 que representa cuanto se ajusta la recta a los valores reales obtenidos.  Cuanto mayor es este valor, significa que mejor se ajusta la regresión a los valores  reales.   1123 y = -0.9796x + 201.85 R2 = 0.8234 160 165 170 175 180 185 190 195 200 205 210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Distancia (metros) RS SI   (dB m ) promedio Lineal (promedio)   Figura 3. Promedio y su regresión lineal  y = -0.9262x + 202.2 R2 = 0.7179 160 165 170 175 180 185 190 195 200 205 210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Distancia (metros) RS SI   (dB m ) moda Lineal (moda)   Figura 4. Moda y su regresión lineal  1124 y = -0.8346x + 206.25 R2 = 0.7753 170 175 180 185 190 195 200 205 210 215 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Distancia (metros) RS SI   (dB m ) max Lineal (max)   Figura 5. Máximo y su regresión lineal  Según lo analizado la mejor regresión, es decir, el mayor R2 se da para el  promedio. Por lo tanto, el mejor estimador estadístico para el caso estudiado que más  se asemeja a un comportamiento lineal es el promedio. Entonces si se aplica a los  datos de entrada (RSSI) la ecuación x = (y - 201.85) / -0.9796 que se despeja de la  que se observa para la regresión (y = -0.9796x + 201.85), se puede tener una salida  (metros) donde se obtenga una distancia a la baliza de tal manera de estimar su  posición relativa.   Las siguientes tres figuras (6, 7 y 8) analizan la magnitud del error cometido  entre la medida tomada y la que indicaría la regresión lineal. Esto se realiza luego de  aplicar la ecuación despejada para la entrada (x) como se mostró en el caso del  promedio. En la figura 6 se muestra el error del promedio, en la 7 de la moda y en la 8  del máximo.  0 1 2 3 4 5 6 7 8 Error (metros) 1 3 5 7 9 11 13 15 17 19 21 23 25 Distancia (metros) Magnitud del error   Figura 6. Magnitud del error entre el promedio y su regresión lineal  1125 0 1 2 3 4 5 6 7 8 9 10 Error (metros) 1 3 5 7 9 11 13 15 17 19 21 23 25 Distancia (metros) Magnitud del error   Figura 7. Magnitud del error entre la moda y su regresión lineal  0 1 2 3 4 5 6 7 8 9 10 Error (metros) 1 3 5 7 9 11 13 15 17 19 21 23 25 Distancia (metros) Magnitud del error   Figura 8. Magnitud del error entre el máximo y su regresión lineal  El error máximo para todos los casos está cercano a los 9 metros cuando la  distancia al dispositivo es de 14 metros aproximadamente. El mínimo es 0 para  algunos casos. En las tres estimaciones se denota que en el rango medio de distancia  es donde se obtiene la mayor magnitud de error. Este fenómeno se puede deber a los  obstáculos que debe sortear la señal para llegar al dispositivo y obviamente cuando  más se aleja, menor es la recepción. También se observa que cercano al dispositivo de  captura el error medido es alto pero no se tienen detalles de su ocurrencia.  1126 4 Conclusiones y trabajos futuros  Se ha presentado un sistema ad-hoc para posicionamiento indoor usando un entorno  libre como es Arduino con la posibilidad de ser montado en cualquier móvil. Las  técnicas utilizadas muestran que es posible estimar la distancia con un cierto grado de  error y por medio de triangulación, localizar un objeto en un ambiente interior. Los  errores máximos se encuentran en los 9 metros cuando la distancia es casi del doble.  Calculando el promedio de las medidas se obtiene un buen estimador pero no es  suficiente para una buena precisión. La moda si bien es la que menos se ajusta a su  regresión lineal, también es un buen estimador aunque el error acumulado es el más  grande de los tres.  Como futuros trabajos se prevé el desarrollo de técnicas que permitan mejorar la  estimación de la distancia a partir de la fuerza de la señal recibida. Se va a tomar mas  cantidad de muestras con rangos de distancias que van de a 50cm para verificar la  aplicabilidad de las técnicas propuestas en este trabajo y de las futuras a desarrollar.  También se va a utilizar el tiempo de vuelo de la señal para verificar los datos  obtenidos. Se van a hacer pruebas en un ambiente sin obstáculos para ver el  comportamiento de la señal en un ambiente ideal. Además, se van a realizar tomas de  datos en un ambiente exterior de manera de analizar su comportamiento.   Como se mencionó anteriormente, algunas de las técnicas que se proponen  implementar son: filtro de Kalman con ajuste de la desviación estándar, lógica difusa  y redes neuronales. Con ello se busca aumentar la precisión de posicionamiento.	﻿posicionamiento indoor , balizas bluetooth , RSSI	es	31458
52	Experiencia de utilización de herramientas colaborativas para la enseñanza y el aprendizaje de la programación de computadoras	﻿ En  este  trabajo  se  presentan  algunos  resultados  y  conclusiones  preliminares sobre una experiencia de trabajo colaborativo apoyado en recursos  tecnológicos provistos y/o compatibles con el EVEA Moodle. Las experiencias  se  aplicaron  a  un  curso  introductorio  de  enseñanza  y  aprendizaje  de  programación en la Lic. en Sistemas de la Universidad Nacional de Río Negro   (UNRN) - Sede Atlántica.  Se exponen algunas referencias teóricas que sostiene  la propuesta y se realiza una descripción del  contexto de aplicación. Finalmente  se presentan los  resultados de la implementación y conclusiones obtenidas. Keywords: trabajo colaborativo, enseñanza, programación, 1   Enseñar y Aprender en colectivo Maldonado  Pérez  [1]  expresa  la  importancia  de  reconocer  el  carácter  social  que  implica el enseñar y aprender en estos tiempos, donde el esquema convencional que  posiciona al docente en el rol de enseñante y al alumno en su rol de aprendiz en forma  exclusiva,  ya  no  tiene  lugar.  Para  la  autora,  el  aprendizaje  es  un  proceso  social,  construido a través de la interacción no solo del docente con los alumnos, sino entre  alumnos y teniendo en cuenta el contexto y el significado que cada uno le asigna a lo  que aprende. Esta forma de aprendizaje, responde a los postulados del psicólogo Jean  Piaget,  quien  sostenía  que  el  aprendizaje  consiste  en  la  generación  de  estructuras  cognoscitivas que se crean a través de la modificación de los reflejos iniciales del   recién nacido y que se van enriqueciendo a través de la interacción del individuo con  el medio. A través de estas estructuras, el individuo adquiere información, usando los  procesos de asimilación y acomodamiento de la misma. De esta forma, el proceso de  aprendizaje no se basa en la memorización de la información,  sino en asimilar  o   incorporar  información a esquemas que poseen una información previa. El enfoque  de Piaget se ve complementado, desde la perspectiva teórica de Vygotsky [2], que  hacía  énfasis  en  la  interacción  social  como factor  clave  para  el  aprendizaje  y  la  transmisión de cultura [1].  Según Johnson et al; [3],  Vygotsky sostenía el  carácter  social del conocimiento y su construcción a partir de los esfuerzos cooperativos por  1517 aprender, entender y resolver problemas. Un concepto clave, definido por Vygostky  [2], es el de la zona de desarrollo próximo, entendiéndola como aquella zona situada  entre lo que un estudiante puede hacer solo y lo que puede lograr si trabaja guiado por  un instructor  o en colaboración con otros pares más avanzados. Así, la enseñanza y  como consecuencia el  aprendizaje,  sólo tiene lugar en la zona en la que el sujeto   puede  desarrollar  una  actividad  en  colaboración  con  otro  [2].   En  este  sentido,  Johnson [3], sostiene que a menos que los alumnos trabajen de manera cooperativa,  no crecerán intelectualmente; por lo tanto, debe reducirse al mínimo el tiempo que los  alumnos pasan trabajando solos en las actividades académicas. Maldonado Pérez [1],  basándose en la teoría de Vygostky afirma que los procesos que desarrolla un grupo  en interacción serán internalizados por cada uno de sus miembros, formando de esta   manera parte de su propio aparato cognoscitivo.  Por otra parte, destaca el espacio  fundamental  que  ocupan  los  lenguajes  y  los  procesos  de  comunicación  en  esta  interacción.   En  cuanto  al  docente,  la  misma  autora  [1]  señala   que  es  su  responsabilidad  alentar,  promover  y  crear  el  espacio  adecuado  que  permita  la  construcción del conocimiento.  En este sentido,  se organizará la enseñanza y el uso  de  estrategias  y  metodologías  apropiadas,  que  permitan  la  creación  de  nuevos  espacios de interacción humana y tecnológica. 2   Ambientes Colaborativos &  Enseñanza y Aprendizaje de la  Programación En las carreras vinculadas a la Informática, la enseñanza de la programación es una  base  fundamental  y  uno  de  los  primeros  cursos  que  deben  tomar  los  alumnos  ingresantes  [4].  La  enseñanza  y  aprendizaje  de  programación  es  una  actividad  intelectual compleja y dificultosa, tanto para los alumnos como para quienes llevan  adelante la enseñanza; más aún cuando su impacto es muy importante en la mayoría  de las asignaturas sucesivas y en el campo profesional del futuro egresado [5,6]. En  el  ámbito  educativo,  las  actividades  de  aprendizaje  colaborativas  buscan  desarrollar en los alumnos un conjunto de habilidades que se relacionan en forma  directa  con  el  objetivo  que  persigue  la  educación  moderna,  la  formación  en  competencias  que  le  permiten  al  alumno  integrarse  en  una  esta  nueva  sociedad  mediada  por  tecnologías  digitales,  donde  el  docente  desde  su  lugar  debe  ser,  dinamizador, orientador y asesor de todo el proceso de enseñanza y aprendizaje [7].   En este sentido, Estévez [8], sostiene que los ambientes colaborativos pueden ofrecer  un  importante  soporte  a  los  alumnos  durante  las  actividades  aprendizaje  de  la  programación. Y agrega que la resolución de problemas a través de la colaboración  promueve la reflexión, un mecanismo que estimula el proceso de aprendizaje. Para el  desarrollo  de  una  actividad  grupal  los  alumnos  necesitan  comunicarse,  discutir  y  emitir opiniones a otros miembros del grupo, alentando de esta forma una actitud de  reflexión que conduce al aprendizaje. En cuanto a las características de una herramienta que esté orientada tanto para el   aprendizaje como para el  desarrollo colaborativo del  software,  algunos autores [9]  señalan que deben estar incluidas: las actividades comunes, el entorno compartido y el  espacio/tiempo. Por actividades comunes se entiende a  aquellas tareas comunes que  los participantes del grupo llevan a cabo; el entorno compartido brinda la posibilidad  1518 de tener informado a cada miembro del proyecto sobre el estado de éste, lo que cada  miembro  está  trabajando,  etc.;  y  el  espacio/tiempo soporta  que  la  interacción  del  grupo  de  trabajo  se  produzca  en  el  mismo  lugar  y  momento.  En  cuanto  a  la  interacción es posible encontrar dos tipos: síncrona o asíncrona, que a su vez puede  ser distribuida o centralizada. A  continuación  se  describen  las  herramientas  digitales  que  se  utilizan  en  la  experiencia que relata el artículo. 2.1 Virtual Programming Lab (VPL) VPL es un producto de software de código abierto creado por el Departamento de  Informática  y  Sistemas,  de  la  Universidad  de  Las  Palmas  de  Gran  Canaria;  que  permite  la  gestión  de  prácticas  de  programación  sobre  el  entorno  virtual  de  enseñanza-aprendizaje (EVEA) Moodle[18], incorporando el ambiente de desarrollo  de  software  al  aula  virtual  de  las  materias  donde  se  utiliza.  Su  arquitectura  está  compuesta de un módulo Moodle, un applet editor de código fuente y un demonio  Linux que permite la ejecución remota de programas de forma segura.  VPL tiene  como propósitos el ahorro de tiempo y mejorar la gestión general  de este tipo de  actividades, tanto en los cursos de programación que se dictan en forma online como  usando B-Learning, además de permitir la realización de las prácticas utilizando solo  un  navegador.  La  intención  de  la  herramienta  es  facilitar  el  seguimiento  y  la  orientación  personalizada  y  continua  del  proceso  de  aprendizaje  del  alumno,  contribuyendo de esta forma a tratar las dificultades a las que se enfrenta éste en la  realización de las actividades de programación [11]. A nivel profesional, las herramientas comerciales que se utilizan para el desarrollo del   software,  presentan  una  amplia  cantidad  de  opciones  y  de  información  que  los  alumnos  que  recién  se  inician  en  la  práctica  de  la  programación,  no  pueden  comprender  tan  fácilmente  porque  aún  no  tienen  los  conceptos  necesarios  para  manipularlas [10].  Así, VPL busca proveer a los alumnos novatos de un entorno de  desarrollo que sea simple. Sus características más destacadas son: la posibilidad de  editar  el  código  fuente  y  ejecutar  las  prácticas  de  forma  interactiva  desde  el  navegador,  ejecutar  pruebas que revisen las  prácticas  y analizar  la  similitud entre  prácticas  para  el  control  del  plagio  para  algunos  lenguajes  de  programación  soportados  [11].  La  versión  2.0 de  VPL incorpora  características  que  permiten  el  trabajo en grupo.  Así, cada grupo dispone de un repositorio compartido de entregas,  donde cualquier integrante puede agregar una nueva versión del programa que están  realizando y el resto del grupo recibirá el resultado de la evaluación. 2.2  Herramientas de Moodle:  Foros y Wiki Moodle dispone de una serie de herramientas que permite la colaboración dentro del  aula virtual entre ellas los foros y wiki. A través de los foros, Moodle da lugar al  planteo de debates y discusiones, posibilitando además la comunicación asincrónica.  Los  foros  pueden  estructurarse  de  diferentes  maneras,  y  cada  mensaje  puede  ser  1519 evaluado por los participantes. Existen diferentes formas de visualizar los mensajes y  los  mismos  permiten  la  inclusión  de  imágenes  y  adjuntar  archivos.  Cuando  los  participantes de un curso, se suscriben a un foro, recibirán copias de cada mensaje en  su bandeja de correo. El participante con rol de profesor puede forzar la suscripción a  todos los participantes. En Moodle hay dos categorías de foros: Foro general (Se encuentra en la sección 0 del  curso) y Foro de aprendizaje (Son foros de alguna sección específica del curso). Una  wiki  es  un  espacio  web  colaborativo  que  puede  ser  editado  por  varios  participantes, es decir todos pueden crear, modificar o eliminar contenido de forma  interactiva; permitiendo así la escritura colaborativa [13] en [12] Moodle dispone de la herramienta wiki, la cual se puede configurar al momento de  crearla de un determinado tipo. Este tipo determina el ámbito de la misma y quien  puede escribir y editar los cambios. Los tres tipos de wiki son: estudiante, grupo y  profesor. La wiki puede funcionar en modo: sin grupos, grupos separados o grupos  visibles al igual que los foros [14]. 3 Actividades Prácticas Colaborativas Se  describe  la  implementación  de  la  propuesta  de  enseñanza  y  de  aprendizaje  destinada a los alumnos ingresantes a la Licenciatura en Sistemas de la UNRN que  tomen el curso de Programación I. Programación  I,  es  una  materia  perteneciente  al  área  Algoritmos  y  Lenguajes  de  Programación; que se dicta en forma presencial en el primer cuatrimestre del primer  año con un total de 96 horas. Tiene como objetivos generales que los alumnos puedan  analizar problemas resolubles con computadora, poniendo énfasis en la modelización,  la abstracción de funciones y en la descomposición funcional de los mismos, a partir  de un paradigma procedural/ imperativo. Se realiza una introducción de las nociones  de estructuras de datos, tipos de datos y abstracción de datos.   En cuanto a los alumnos, en su mayoría son ingresantes a la universidad,  egresados   recientemente del nivel medio, cuyas edades oscilan entre los 17 y 21 años, y que  toman contacto por primera vez con la actividad de programación.  Son varios los  alumnos  que  llegan  al  curso  con  netbooks.  Esto  hace  suponer  que  tienen  cierto  manejo de recursos tecnológicos como navegadores de internet y redes sociales  tipo  facebook entre otros.   Por otra parte,  es común verlos con sus teléfonos celulares  navegando, escuchando música o mirando videos, aún dentro del espacio presencial  de las clases. El curso está dividido en clases teóricas y prácticas. En las primeras se desarrollan los  conceptos  teóricos  previstos  en  el  plan  de  estudio  (resolución  de  problemas,  estructuras  de  control,  modularización,  estructuras  de  datos)  haciendo  uso  de  ejemplos prácticos que permitan la aplicación de los conceptos analizados. Respecto a  las clases prácticas, las mismas tienen como objetivo la aplicación de los conceptos  trabajados en las clases de teoría, en la resolución de problemas computacionales, a  través  del  diseño  algoritmos.  En  un  paso  siguiente  estas  soluciones  serán  implementadas en un lenguaje de programación de alto nivel tipo Pascal.  El énfasis  1520 de la asignatura está puesto en la parte práctica, ya que para desarrollar la habilidad de  resolver  problemas  usando  algoritmos  es  fundamental  el  entrenamiento.  Con este  objetivo se diseñan actividades prácticas que enfrentan a los alumnos con situaciones  problemáticas  en  las  que  tienen  que  decidir  sobre  la  naturaleza  del  problema,  seleccionar  una representación que  ayude a  resolverlo (modelo)  y,  monitorear  sus  propios pensamientos (metacognición) y estrategias de solución [15]. El  programa consta  de  seis  unidades  didácticas,  cada  una  con  su  correspondiente  trabajo  práctico  y  tres  Actividades  Prácticas  Entregables  (APE)  integradoras,  las  cuales deben ser entregadas y evaluadas para poder acceder al examen parcial. Las  fechas de publicación de la APE,  están establecidas en el cronograma  de actividades  de la materia. Las APE consisten en la resolución colaborativa en equipos de trabajo, de problemas  de  mediana  complejidad,  cuya  solución  es  un  programa  computacional  que  se  implementará en el lenguaje de programación elegido por la cátedra.  En el caso de  Programación I, se utiliza el lenguaje Pascal. La consigna de trabajo que se proponen a través de la APE, incluye la definición del  problema, formas de entrega, consistente en un cronograma de actividades, fechas de  previstas para cada etapa del proceso de resolución y recursos que se proponen para  su desarrollo, información sobre pruebas, es decir se definen o se proporcionan los  datos con los que serán puestos a prueba los programas y consideraciones especiales. El desarrollo de las APE se propone que se realice combinando la forma de trabajo  colaborativo y herramientas TIC, promoviendo de esta forma, la participación de los  alumnos y el desarrollo de competencias transversales tales como el  razonamiento  crítico,  la  capacidad  de  análisis,  el  trabajo  en  equipo,  la  autorregulación  y  la  comunicación.  Teniendo en cuenta que los alumnos de Programación I son jóvenes  que tienen cierto manejo de la tecnología,  la intención de las APE es también que  ellos las apropien como un recurso útil  para construir y enriquecer su aprendizaje.  Haciendo uso de las funcionalidades provistas por el entorno Moodle (Foro, Wiki,  mensajería)  y  del  laboratorio  virtual  de  programación  (VPL);  se  posibilita  el  desarrollo colaborativo del análisis y diseño de la solución y de la implementación del   programa computacional  que resuelve  el  problema propuesto  en  la  APE. Las tres  herramientas  TIC  se  configuran  de  manera  tal  que  cada  grupo  disponga  de  una  instancia de las misma, de manera que los participantes solo puedan ver y editar las  asociadas a su equipo. Las consignas de las APE se presentan a través del aula virtual (archivo en formato  .pdf),  a  la  vez  que  se  habilitan  los  espacios  de  Foro  y  Wiki  y  se  indica  el  tutor   asignado al grupo. El tutor es responsable de hacer el seguimiento del grupo y puede  ser un docente de la teoría o de la práctica de la materia. A continuación se describen las cinco etapas involucradas en el desarrollo de las APE;  a saber: Debate inicial:   En la clase presencial siguiente a la presentación de la consigna  en el aula virtual, se reserva una hora de la misma para que los grupos junto a los  tutores asignados puedan debatir acerca del problema. Así, se busca atender dudas y  1521 consultas sobre la consigna. Entre la fecha que se habilita la consigna de la APE sobre  la plataforma y la fecha prevista para el debate, los estudiantes disponen de al menos  3 días para realizar una lectura crítica del problema en forma personal y grupal de  manera de llevar  a  la clase de debate inicial  dudas y consultas  sobre la  consigna  propuesta. Análisis y Diseño: en esta etapa se modela la solución al problema, el diseño  modular y las estructuras de datos.  Se propone utilizar una wiki y un foro ambas  herramientas provistas por el entorno  Moodle.   Implementación: en esta fase se propone continuar usando la wiki y el foro. Y  se suma el laboratorio virtual VPL, con la opción de trabajo en grupo.  A través de  VPL, es  posible que los grupos editen,  compilen y ejecuten sus programas.  Cada  grupo tiene un repositorio compartido de entregas, y cualquier integrante del grupo  puede entregar  una nueva versión y todos los  miembros de un grupo recibirán la  misma devolución. Presentación  y  defensa:  en  esta  fase  se  propone  la  elaboración  de  una  presentación que resume la solución al problema. La misma pueden subirla a la wiki  de cada grupo y su exposición se desarrollará en una clase presencial de manera de  poder  compartir  y  debatir  con el  resto de  los  grupos las  producciones realizadas;  propiciando así la capacidad de comunicación. Evaluación:  La  evaluación  de  las  APE  se  desarrolla  en  tres  partes:  una  evaluación del programa computacional a través del entorno virtual usando VPL, otra  evaluación  en  forma  presencial  a  modo  de  exposición  y  defensa  de  la  solución  propuesta y una tercera evaluación en forma de encuesta que permite que los alumnos  evalúen el proceso de desarrollo de la APE, evaluando su propio desempeño, el del  grupo  y  el  del  tutor  asignado.  De  esta  forma  se  propone  evaluar  la  experiencia  tomando en cuenta no sólo el resultado final de las APE - el programa computacional   que resuelve el problema-, sino también el proceso de aprendizaje a nivel grupal e  individual  que dan lugar al  mismo.  Este proceso esta  soportado a través  del  aula   virtual  de  la  materia  y  de  las  herramientas  wiki,  foros  y  VPL entre  otras.  Las  evaluaciones de las APE servirán de información para los docentes y de orientación  para el alumno. La evaluación que hacen los alumnos de sus compañeros de grupos,  se apoya en la idea de un grupo de investigadores del Departamento de Informática y  Sistemas Universidad de Las Palmas de Gran Canaria, España que entienden este tipo  de evaluación como un complemento valioso que permite integrar al alumno en el  proceso de evaluación del aprendizaje. De esta esta forma los alumnos pueden evaluar  las competencias desarrolladas por sus pares  durante el desarrollo de la actividad  educativa. Los investigadores señalan que este tipo de evaluación requiere del alumno  una mayor responsabilidad y el desarrollo de habilidades que le permitan  valorar el  trabajo de sus compañeros de equipo [16]. Respecto a la organización de los grupos de trabajo, en función de la complejidad que  presentan las APE  y teniendo en cuenta que desde los inicios de la carrera, en el año  2009,  los alumnos inscriptos en el curso no supera los 50 en promedio, se propone  que los equipos de trabajo no superen los 4 alumnos.  En cuanto a su conformación,  en  esta  experiencia  se  propuso  para  la  APE1  que  los  alumnos  decidan  como  1522 agruparse, luego en las siguientes APE los equipos fueron re-armados por el equipo  docente de acuerdo al seguimiento realizado. 4   Resultados El programa de la materia contempla el desarrollo de tres APE durante la cursada.   Esta experiencia se inició con un grupo de 40 alumnos. De los cuales, para la APE1 se  dividieron en 13 grupos de los cual completaron todas las etapas de la actividad 12  grupos. Para la APE2 se formaron 14 grupos y completaron la actividad 6 grupos.  Para la APE3 se formaron 11 grupos y completaron la actividad 4 grupos. Respecto al   desgranamiento que  se  observa,  está  asociado  en  parte  con el  hecho que  muchos  alumnos a medida que cursan las materias del primer año de la carrera,  están también  cursando las asignaturas introductorias de “Razonamiento y resolución de problemas”  (RRP)  e “Introducción a la lectura y escritura académica” (ILEA) que la UNRN ha  dispuesto como un recorrido previo de ingreso universitario. De esta forma quienes no  aprobaron estas materias antes del inicio del primer cuatrimestre,  pueden cursarlas  durante  el  mismo  y/o  rendirlas  en  forma  libre  en  las  fechas  establecidas  por  el  calendario académico. Cómo se  indicó  anteriormente,  al  finalizar  la  fecha  de  entrega  de  cada  APE los  alumnos  respondieron  a  una  encuesta  anónima,  que  permitió  evaluar  su  propio  desempeño, el de su grupo y el del tutor asignado. A continuación se exponen algunos  resultados  de  las  mismas.  En  cuanto  a  la  autoevaluación  los  gráficos  1,  2  y  3,  muestran  el  análisis  de  datos  realizados  hasta  el  momento.  El  gráfico  1  permite  observar que los alumnos indicaron que su nivel de participación aumentó en cada  APE. Solo para la primer APE, un poco más del 10% manifestó una participación  nula. Cuando se les consultó a los alumnos acerca de porque no habían participado,   algunos  indicaron  que  por  falta  de  coordinación  y  de  organización.  Otros  manifestaron  que  no  podían encontrarle  utilidad  al  uso  de  la  wiki  o  el  foro,  que  preferían reunirse en forma personal. Gráfico  1 –  Autoevaluación:  nivel  de  participación en las e-actividades Gráfico 2 – Autoevaluación: dominio de los  temas tratados Siempre Casi Siempre Alguna Vez  Nunca 0 20 40 60 APE1 APE2 APE3 Po rce nta je Siempre Casi Siempre Alguna Vez  Nunca 0 20 40 60 APE1 APE2 APE3 Po rce nta je 1523 El gráfico 2, presenta el resultado de la autoevaluación que hicieron los estudiantes  respecto  a  si  tenían  dominio  (conocimiento  y  manejo)  de  la  información  que  se  discutía en cada APE. Se puede observar que en cada APE se produjo un incremento  del  mismo.  En  este  sentido  vale  destacar  que  cada  APE  era  integradora  de  los  conceptos vistos en las unidades involucradas más los analizados en la APE anterior.  Así por ejemplo para la APE2 el 53% manifiesta haber tenido siempre dominio de los  temas tratados. El gráfico 3,  muestra las percepciones de los alumnos respecto a si el desarrollo de   las  APE  les  permitió  una  mejor  comprensión  de  los  conceptos  involucrados.  Se  observa que más del 50% consideran que las APE contribuyeron en forma normal en  las dos primeras actividades y en la última se observa que la contribución superó el   70%. Esta APE resultó una experiencia de investigación para los grupos, ya que la  resolución  del  problema  planteado  requirió  de  conceptos  no  analizados  en  clase  ( Pilas y Colas). Luego de finalizada la entrega de la APE 3 y tomado el examen parcial de la materia,  se consultó a los alumnos  acerca de si esta propuesta de trabajo les había permitido   prepararse mejor para rendir el examen, aquí casi el 80% de los alumnos respondió  positivamente,  aún entre quienes indicaron  no haber aprobado el examen. Gráfico 3 – Autoevaluación: comprensión  de los conceptos Gráfico 4 - Evaluación del grupo de trabajo En cuanto a las evaluaciones que los alumnos realizaron sobre el grupo, en el gráfico  4 se observa que consultados acerca de si volverían trabajar con ese equipo, para las  tres APE más del 40% respondió afirmativamente.  Sólo en aquellos casos donde se  observó a través del aula virtual y/o de la encuesta, que sus miembros no deseaban  seguir  trabajando  juntos  y/o  que  los  alumnos  lo  manifestaron  en  forma  personal  equipo al docente, se hicieron cambios para la siguiente APE. Ante la pregunta: ¿qué fue lo mejor y lo peor de trabajar en este grupo?, en general las  respuestas  coinciden y señalan la  falta  de  dominio sobre  los  temas  tratados.  Esta  situación ponía a los alumnos  en distintos niveles, la despreocupación y la negativa   de algunos integrantes  a  utilizar  las  herramientas  TIC propuestas.  En cuanto a  lo  mejor resaltaron el debate, la puesta en común de las posibles soluciones y el respeto   hacia las opiniones de los demás. En la etapa de presentación y defensa de las APE,  resulta  importante  destacar  cómo  evolucionaron  las  presentaciones  digitales  que  realizaron los grupos y como los mismos interactuaron no solo con los docentes sino  con los demás grupos, en la defensa oral de las actividades. Nada Poco Normal Mucho 0 50 100 APE1 APE2 APE3 Po rce nta je Si, con todos Si, con algunos No 0 20 40 60 APE1 APE2 APE3 Po rce nta je 1524 5   Conclusiones Se presentó una propuesta de enseñanza de programación de computadoras basada en  actividades  prácticas  colaborativas  usando  herramientas  compatibles  con  Moodle.  Para la primera implementación se puede observar que: ●Resultó difícil que los alumnos adoptarán el uso de foros y wiki, como un recurso   para  el  desarrollo  de  las  actividades  didácticas  específicas  del  área  de  programación, no así el uso del laboratorio virtual VPL. Para la mayoría de los  alumnos ingresantes esta forma de trabajo y algunas de las herramientas TIC que  la  soportan,  representan  una  experiencia  novedosa.  Así  acordamos  con  otras  investigaciones [17] que resaltan la necesidad de un tiempo de maduración de la  misma por parte de los alumnos. Donde el mismo, debe estar en todo momento  acompañado por la participación activa del tutor.  En este sentido se propone a   futuro el desarrollo de un curso pre-ingreso que trabaje sobre el uso de las mismas  aplicadas al área específica de estudio. ●Hubo una evolución en el desarrollo de las presentaciones digitales y la calidad de  las exposiciones a medida que avanzaban en la cursada. ●Desde los inicios de la Lic. en Sistemas en la UNRN Sede Atlántica en el año   2009, se puede observar que en las materias de programación de primer año el  desgranamiento es muy alto de esta forma los alumnos que logran llegar al final  del curso son muy pocos en relación a la cantidad de inscriptos.  En este sentido la  propuesta presentada, puede re-pensarse de manera que pueda convertirse en una  herramienta  que  permita  sostener  a  los  alumnos a  lo  largo  de  la  cursada.   El   desarrollo  de  actividades  colaborativas  en  programación  favorece  el  futuro  desarrollo profesional de los estudiantes, se les presenta situaciones similares a las  que van a tener que desarrollar. Para la próxima implementación se trabajará en el ajuste de la propuesta teniendo en  cuenta las observaciones de los estudiantes y docentes de la cátedra. Como trabajo futuro se analizarán los tipos de problemas a  resolver y su adecuación  para el desarrollo de actividades colaborativas de programación.	﻿programación , trabajo colaborativo	es	31831
53	Generando entornos de investigación y desarrollo utilizando redes inalámbricas de sensores WSN	"﻿ Las Redes Inalámbricas de Sensores (WSN) jugarán un papel preponderante tanto en las actividades académicas y como en el desarrollo en nuestro país. En este artículo se reportan actividades desarrolladas en dos universidades argentinas, utilizando a la tecnología WSN como disparador de actividades de investigación, desarrollo. La aplicación de ésta tecnología de última generación se enmarcan en el dominio de “Internet de las Cosas” y “Ciudades Inteligentes”. Las actividades teórico-prácticas desarrolladas introdujeron a un  número cada vez mayor de interesados en la experimentación, hacia el desarrollo de soluciones frente a situaciones de la vida cotidiana, buscando soluciones  utilizando WSN. Se presentan resultados de proyectos de la vida real, por lo que  amerita considerar la implementación del estudio de las WSN en las curriculas  de las carreras con contenido de las Tecnologías de la Información y Comunicación.  Palabras clave: Sensores inalámbricos; Redes ad hoc; Internet del Futuro; Internet de las Cosas.   1 Introducción y Propósitos  Las redes ad-hoc son sistemas sumamente complejos. En ellos conviven y participan  muchos conceptos, protocolos, tecnologías, algoritmos, y elementos que deben ineludiblemente trabajar conjuntamente. La aplicación de las redes móviles ad hoc  (MANET) y las WSN es sumamente diversa, yendo desde pequeñas redes estáticas  limitadas en su existencia por la necesidad de disponibilidad de energía, a redes a gran  escala con gran dinámica y mucha movilidad. Si bien los nodos sensores han sido  utilizados desde hace décadas, el desarrollo de la tecnología ha sido exponencial a  partir de 1998, con el proyecto SmartDust.  Las redes de sensores inalámbricos, proveen una tecnología que permite operar de  forma autónoma a cada uno de los nodos, sin depender de infraestructura alguna. Son  parte de aquellos objetos cooperantes, que residiendo en el dominio de la computación ubicua; permite desarrollar una gran variedad de aplicaciones prácticas. La  1535 computación ubicua es un modelo de interacción de personas y equipos, en los cuales  el procesamiento ha sido asimilado invariablemente a los elementos y actividades del  día a día. Cada uno de los objetos con los cuales interactuamos, tienden a ser integrados con sensores de alguna naturaleza. Son redes auto-configurables de pequeños  nodos desplegados en cantidades suficientes de tal manera de interaccionar con el  mundo. Es posible mencionar casos de éxito como el monitoreo de hábitats naturales  de aves, control de las migración de animales, vulcanología, contralor de parámetros  en viñedos, calidad de vida de los internos en residencias geriátricas, eventos deportivos multitudinarios, y diversos otros dominios y entornos. La solución implementada  en cada caso ha sido más competitiva que las existentes, ya que se puede implementar  como red fija o móvil.   Computación ubicua es un modelo posterior de la interacción personacomputadora de escritorio, y es un término ya utilizado por Mark Weiser en los años  1990’s [1]. Hoy las WSN forman la columna vertebral de una nueva Internet, principalmente ubicua, como parte indivisible de ""Internet de las cosas (IoT)"" [2], dominio  en el cual cada “cosa” existente en el mundo físico también puede convertirse en un  elemento que está conectado a Internet. Las “cosas” se pueden caracterizar como  pequeños dispositivos capaces de diferentes tareas “inteligentemente”.   Si a la brecha digital se la define como la diferencia entre los que tienen acceso regular y eficaz de las tecnologías digitales y los que no, entonces la brecha científica se  puede definir como la brecha entre quienes tienen acceso a los datos científicos y los  que no. Estamos persuadidos que el uso de WSN en los países en vías de desarrollo  puede ayudar a llenar este vacío en el estado del arte y de la técnica. Abogamos por la  utilización de WSN para el desarrollo en el dominio de las ICT4D (ICT for Development) ya que se convierte, aplicándose a diversos escenarios [3], en una herramienta  válida para reducir la brecha científica y tecnológica existente.   La implementación de las WSN aporta nuevas fortalezas al diseño curricular de carreras en Informática, agregando valor actualizado para los propósitos educativos en  aquellas instituciones académicas que avancen sobre el tema.  En nuestro país existen hoy 2,9 investigadores, tecnólogos y becarios por cada  1.000 personas de la población económicamente activa. Se espera aumentar a 4,6 en  2020, según el escenario más pesimista [4], logrando duplicar la cantidad de científicos en 7 años.   Uno de los ejes principales del plan es la “focalización” en sectores que se consideran estratégicos, la agroindustria, el ambiente y el desarrollo sustentable, el desarrollo  social, las energías, la industria, y la salud. En todos esos ambientes es esperable que  las WSN jueguen un rol sumamente importante.   La capacidad de realizar mediciones directas y determinar las estrategias de reconocimiento y caracterizar patrones; conjuntamente con una acertada explotación de  los recursos computacionales disponibles en la tecnología; representan retos ingenieriles muy interesantes de abordar en el ámbito científico académico. Para validar la  tecnología se hace necesario un amplio portafolio de aplicaciones como una prueba  del concepto. Para ello las redes desplegadas deben adecuarse al medio bajo estudio e  investigación, debiendo considerarse no solo el impacto científico potencial, sino  también el impacto en la sociedad. Las bondades y potenciales aplicaciones de las  1536 WSN, coadyuvará en la adhesión de mayor cantidad de voluntades, siendo fundamentales aquellas que se encuentren relacionadas con actividades de desarrollo de hardware.  2 Génesis y Contexto  La Facultad de Ciencias Exactas, Químicas y Naturales (FCEQyN) de la Universidad  de Misiones (UNaM), el Centro de Investigación en Tecnologías de la Información y  Comunicaciones (CITIC) de la Universidad Gastón Dachary (UGD), y el Parque Tecnológico Misiones (PTMi) han trabajado activamente en la difusión y promoción de  actividades centradas en tecnologías de bajo costo y alto impacto en la industri y la  sociedad, siendo uno de los objetivos la formación de personas que puedan convertirse en propagadores de los conocimientos y las capacitaciones recibidas.  Por convenios con la Universidad de Lübeck (Alemania), el LINTI –Laboratorio  de Investigación de Nuevas Tecnologías Informáticas de la Universidad Nacional de  La Plata y la FCEQyN se ha desarrollado el Proyecto “Hacia una Red Global de Sensores Interconectados. Un ensayo experimental Argentino-Alemán”, aprobado en el  marco del Programa de Cooperación Científico-Tecnológico entre el Ministerio de  Ciencia, Tecnología e Innovación Productiva de la República Argentina (MinCyT) y  el Bundesministerium für Bildung und Forschung (BMBF) de Alemania. La meta  principal del proyecto ha sido obtener resultados valederos tanto desde el punto de  vista práctico como desde lo teórico y académico, promoviendo cursos y actividades a  nivel de grado y postgrado, los que no son tan comunes en las actuales curriculas de  las universidades argentinas. El carácter multidisciplinario de la actividad involucrada  en el proyecto ha logrado aportes significativos al estudio y modelización de tráfico  en las redes ad-hoc, como así también lo concerniente a encaminamiento en las citadas redes. En éste marco se realizaron capacitaciones sobre WSN en la UNLP y la  UNaM impartidas por científicos y académicos alemanes.   De la actividad participaron 10 estudiantes de postgrado de la UNLP, otorgando 3  créditos válidos para Maestrías y Doctorados. En la UNaM tomaron dicho curso 12  personas de las carreras de Informática e Ingeniería Electrónica de la UNaM. Como  corolario de la actividad, se ha defendido una tesis de Doctorado en Ciencias Informáticas en la UNLP  [5] en el año 2011.  De la sinergia establecida entre grupos de investigación en la UNaM y la UGD, se  han implementado charlas orientadas a docentes y alumnos de la carrera Ingeniería en  Informática denominándoselas “Internet de las Cosas e Inteligencia Ambiental”. En  ese contexto se concretaron los talleres “Programación de WSN” y “Hacia la Internet  del Futuro, Programando WSNs” y “Redes de Sensores Inalámbricos e Internet del  Futuro”. Estas actividades incluyeron importante carga horaria en actividades prácticas, que ha sido posible abordar por disponer un Laboratorio desplegado con una  WSN de 15 nodos.  1537 3 Talleres Desarrollados  Los objetivos, dinámica, contenidos, práctica, tecnologías utilizadas, y algunas  consideraciones relativas a la evaluación de los talleres se indican a continuación.  3.1 Objetivos  Objetivo General.   Concientizar sobre el potencial de la tecnología WSN, insistiendo en el hecho  consisten en dispositivos de bajo costo; resultando apropiada fundamentalmente  para aplicaciones en el medio ambiente.  Objetivos Particulares.    Proporcionar comprensión general sobre ésta nueva tecnología y el nuevo paradigma de las redes data-céntricas.   Apreciar de la naturaleza interdisciplinaria de WSN revelando sus potenciales  aplicaciones para la región.   Inculcar habilidades prácticas a través de la auto-motivación, la formación práctica no rutinaria, y de actividades de diseño en equipo, utilizando pensamiento  crítico, trabajo en equipo y habilidades de comunicación.   Desarrollar una estructura sostenible generando la infraestructura necesaria para  formar a una futura generación de capacitadores, capaces de interactuar a nivel  local, para así coadyuvar al desarrollo tecnológico.   Fomentar con un enfoque regional, el desarrollo de un sentido de comunidad  entre los participantes, despertando el interés por la aplicación de la tecnología  WSN como una herramienta válida para resolver problemas locales.   Promover la conformación de un grupo de trabajo sobre WSN en la UNaM y la  UGD integrado por investigadores, profesionales y estudiantes.   Elaborar documentos técnicos sobre dispositivos sensores   Dictar charlas y conferencias de sociabilización en distintos estamentos de nuestra comunidad.  3.2 Dinámica y Contenidos Tratados  Los talleres comprendían sesiones de desarrollo de contenidos teóricos, como prácticas llevadas a cabo sobre los nodos disponibles. Los participantes han estado en contacto desde el primer momento con los nodos sensores, lo cual potenció aún más el  rendimiento de las actividades. El curso se basado en la aplicación de algoritmos de  complejidad creciente, los que se esclarecían a medida que se incorporaban nuevos  conceptos y normas de programación de los nodos. Al final de cada una de las sesiones se realizaba una puesta en común de las actividades, logros y dificultades, las que  se atendían clarificaban en el próximo encuentro. En la tabla 1 se resumen las sesio1538 nes planificadas, temas tratados y objetivos perseguidos con el abordaje teórico; así  como también las actividades prácticas establecidas.  Tabla 1. - Contenidos desarrollados en Capacitaciones  Sesión 1: Introducción  Teoría: Disparadores de WSN. Principio de funcionamiento de WSN. Aplicaciones.  Práctica: Programación C++, Estructuras básicas para nodos iSense. Herramienta iShell.  Sesión 2: Arquitecturas WSN  T: Nodos WSN. Optimización. Caracterización. Principios de diseño. Redes data céntricas.  P: Despliegues, sinks, intermedios y leaf, Códigos  Sesión 3: Hardware  T: Componentes. Diferencias. Criterios de selección. Factores limitantes. Costos  P: Configuración diferentes módulos  Sesión 4: Aplicaciones y Firmware para nodos  T: Programación, Capacidades de SO. Jerarquía de Clases.  P: Instalación del iSense SDK. Instalaciones en nodo sensor.   Sesión 5: Sistemas Operativos  T: SO en WSN. Funcionalidades básicas. Control de eventos. Soluciones.  P: Ciclos en el S.O. Arranque, eventos y tareas. Temporizadores. Manejo de Memoria.  Sesión 6: Encaminamiento  T: Ruteo. Convergencia. Métodos. Multisalto  P: Algoritmos de enrutamiento, Flooding and hop-based. Tree Routing.   Sesión 7: Sincronización  T: Necesidad. Limitaciones. Interacciones usuario, inter WSN, Mundo real. Desafíos.  P: pruebas de protocolos. Pruebas de algoritmos (LTS, TPSN and HRTS),  Sesión 8: Simulación  T: Fundamentos. 802.15.4a. Simulaciones: factores a considerar.  P: Simuladores en WSN. Instalación y ejecución.  3.3 Actividades de Formación Práctica  Se ha pretendido que la formación práctica representara un distintivo de calidad de los  talleres sin descuidar la profundidad y rigurosidad de la fundamentación teórica y la  reflexión, como componentes del aprendizaje  Estas tareas en todos los casos han sido ejecutadas con éxito por los participantes.  Se listan a continuación algunos de los prototipos actualmente en evolución como  parte de proyectos de I+D del grupo de trabajo constituido:   A) Simulación de Redes de Sensores inalámbricos mediante interfaz Web. La  simulación por computadora ha permitido a los científicos e ingenieros experimentar  fácilmente con ambientes virtuales, alcanzando un nuevo nivel de detalle el análisis  de las aplicaciones naturales y artificiales; que fuera desconocido en las primeras  etapas del desarrollo científico. Se sabe que modelar analíticamente a las WSN es una  tarea complicada, dado que se tiende a realizar análisis simplificados. Toda simulación requiere de un modelo apropiado basado en fundamentos teóricos y sobre todo,  de fácil implementación práctica [6], dado que los resultados de la simulación se extrapolan del escenario particular de análisis, con determinadas presunciones, que ciertas veces no encierran al comportamiento real, comprometiendo seriamente con ello  la credibilidad de las simulaciones.  1539 En éste proyecto se pretende: a) avanzar en el estado del arte en cuanto simulación  WSN, b)Analizar propiedades y eventos necesarios para reproducir el comportamiento de una Red; c) Diseñar la interfaz web de obtención de parámetros, incorporación  de los archivos particulares del proyecto y visualización de resultados de simulación;  d) Diseñar una solución del lado del servidor Web para procesamiento de datos colectados y generación de resultados de simulación; e) Desarrollar un prototipo de interfaz  de simulación de WSN basado en la Web que a través de una interfaz pueda obtener  parámetros de simulación, incorporar archivos particulares del proyecto, procesar los  datos ingresados y generar los resultados de la simulación; f) Realizar pruebas para  comprobar el funcionamiento correcto del sistema.  Se pretende integrar el potencial de la herramienta de simulación Shawn [7] con  todas las ventajas de los sistemas basados en la Web. Se plantea incrementar la capacidad del servidor utilizado en las simulaciones por medio de procesamiento distribuido.   B) Sistema Basado en Redes de Sensores Inalámbricos para la Optimización de  Recolección de Residuos Domiciliarios en Ciudades Inteligentes. Sabido es que  más de la mitad de la población mundial vive hoy en ciudades, y Naciones Unidas  estima que el 70% de la población habitarán en centros urbanos en el año 2050. Es  primordial por ello, mantener la armonía entre los aspectos espacial, social y ambiental de las localidades, así como entre sus habitantes. En este nuevo escenario sociológico y demográfico, con claros efectos económicos, políticos y medioambientales,  cobra fuerza el concepto de ciudad inteligente.  El fin de éste proyecto es diseñar un prototipo de sistema, que utilice los datos generados por WSNs, para determinar que contenedores de residuos urbanos  [8] ameritan ser recogidos; calculando con ello una ruta óptima de recolección, pretendiendo  resolver problemas de gestión con implementaciones inteligentes.   Aquí se estudian antecedentes sobre Ciudades Inteligentes, Internet del Futuro, y  WSN; pretendiendo caracterizar el funcionamiento del sistema de recolección de  residuos de la ciudad de Posadas; definir componentes de hardware y software a utilizar en el proyecto, desarrollar software para los nodos sensores, que permita detectar  el nivel de llenado de un contenedor; desarrollar un prototipo para captura de datos  desde la Red de Sensores Inalámbricos para el cálculo de ruta óptima, y la visualización de la ruta en un mapa; realizar pruebas de laboratorio verificando el funcionamiento completo del sistema.  C) Plataforma para la publicación de datos de Redes de Sensores Inalámbricos, orientada a la visión de la Internet de las Cosas. Se pretende concebir una  plataforma para la captura, almacenamiento y publicación de datos de Redes de Sensores Inalámbricos, persiguiendo específicamente: estudiar las tecnologías WSN;  evaluar las alternativas existentes para la publicación de datos de WSN útiles para la  visión de la IoT; diseñar un prototipo de plataforma que permita la captura, almacenamiento y publicación de los datos obtenidos de la WSN; probar el prototipo en un  escenario donde se verifiquen las posibilidades de aplicación práctica como una solución valedera a problemas existentes en diferentes ámbitos.  1540 3.4 Tecnologías Utilizadas  Como plataforma base para los proyectos descriptos se han utilizado equipos con un  módulo principal iSense. El hardware iSense se proporciona junto a un firmware operativo y de red modular, permitiendo la generación de aplicaciones pequeñas pero  completas; proveyendo una base sólida para el desarrollo rápido de aplicaciones.  Brinda una API C++ para el nodo hardware, funcionalidades de sistema operativo y  una amplia variedad de protocolos de red.      Ilustración 1. Nodo sensor iSense  En los equipos de desarrollo se utilizó una plataforma PC+Linux en sus distribuciones Ubuntu y Debian. Se instalaron los paquetes make, cmake and gcc++. La plataforma iSense, que incluye al microprocesador Jennic, precisa para el desarrollo de  aplicaciones el compilador ba-elf-g++, que asegura la integración perfecta de las librerías.   3.5 Evaluación  Para evaluar el éxito/fracaso de los talleres realizados, se consideraron los siguientes  parámetros: Características multidisciplinarias de los participantes capacitados; Cantidad de participantes rechazados por elevada cantidad de postulantes; Calidad de los  participantes, expresando el número de participantes con un alto potencial de propagar la capacitación recibida; y la retroalimentación de los participantes.   Los resultados de los diferentes talleres organizados han revelado: interés en el tema WSN; exceso de candidatos a participar en los talleres; buen nivel de los participantes, quienes han aplicado lo aprendido para resolver un problema de la vida real  específico.  Como seguimiento de las actividades de formación propuestas, se ha mantenido  contacto con todos los participantes, de los cuales el 60% se encuentra actualmente  involucrado en diferentes proyectos referidos a las WSN.  1541 Por otra parte, se han identificado como principales debilidades y amenazas al costo de equipos; desconocimiento de tecnología a nivel de decisión y a la infraestructura  de soporte en los lugares de implementación.  4 Impactos  4.1 Incorporación de WSN a la Currícula en Carreras de Informática  En el contexto descripto, al observarse el interés de los estudiantes por las competencias prácticas adquiridas en las sesiones de capacitación, y habiendo identificando  también una genuina motivación por parte de los alumnos en volcar estas competencias en la producción de sus propias tesinas de grado, la UGD decidió incorporar al  nuevo plan de estudios de su carrera Ingeniería en Informática en el área de Tecnologías Aplicadas, una nueva asignatura electiva referida a la Tecnología WSN e Internet  de las Cosas.  La decisión tiene que ver con la evolución que se propone actualmente a nivel internacional desde la Joint Task Force on Computing Curricula (ACM & IEEE-CS)  [9], como también en los ámbitos de debate sostenidos en nuestro país promovidos  por la RedUNCI y la RIISIC para la estandarización de contenidos básicos e intensidad de formación práctica de las carreras en Informática.  La asignatura tiene relación directa con Redes y Comunicaciones siguiendo un enfoque bottom-up, construyendo la comprensión de las redes desde sus niveles más  bajos o físicos hacia los más altos en los modelos de referencia, con una modalidad  organizativa del tipo taller, basada en el Aprendizaje Basado en Problemas (ABP)  [10]. Las metas a alcanzar son: Comprender y aplicar conocimientos inherentes a las  WSN, en función de los desafíos presentados por la tecnología, a saber: Eficiencia  Energética, Capacidad limitada de procesamiento, ancho de banda y almacenamiento,  altos niveles de error, escalabilidad y robustez.   Por otra parte, los contenidos sintéticos definidos incluyen: Motivación para el estudio de las WSN. Sensores. Génesis de WSN. Desafíos. Aplicaciones. Arquitectura  de nodos. Sistemas Operativos. Modelo de Referencia. Capa física. Control de acceso  al medio. Capa de red. Gestión de energía. Sincronización. Localización. Seguridad y  Programación en WSN.  En todos los casos se prioriza la realización de actividades de resolución de problemas abiertos, de proyecto y diseño, en la búsqueda de inculcar un espíritu de investigación tendiente a producir descubrimientos que permitan nuevos desarrollos, articulándose perfectamente con el Taller de Tesis de Grado/Trabajo Final de carrera  vigente en la universidad.  4.2 Radicación de Nuevos Proyectos de I+D  El desarrollo de los talleres de WSN ha consolidado la conformación de un grupo  de investigación y desarrollo, integrado por docentes e investigadores de la UNaM y  UGD, habiéndose planteado varias (6) tesinas de grado en el contexto [11], incluyendo algunas con relación directa con la industria [12] .   1542 La UGD ha asignado recursos para que varios alumnos en etapa de preparación de  su tesis de grado sean asimilados como becarios en el mencionado proyecto, recursos  estos que son solventados con fondos propios de la universidad, logrando con ello  involucrar a estudiantes de grado en proyectos de investigación, desarrollo e innovación tecnológica con beneficios reales y concretos para su formación, exponiendo a  los estudiantes a interesantes proyectos en su contexto regional socio-productivo y al  proceso reflexivo crítico involucrado en la investigación científico-tecnológica.  5 Conclusiones y Trabajos Futuros  Se considera que los talleres de implementación de WSN descriptos en este trabajo  han resultado exitosos, ya que han promovido la conformación de un grupo de I+D,  en cuyo contexto se desarrollan diferentes tesinas de grado y proyectos fomentado el  desarrollo local de soluciones que buscan resolver problemas del entorno regional.  Los talleres han resultado la fuente de actualización curricular de una de las carreras  en Informática que ofrecen las universidades involucradas en la experiencia reportada.  El objetivo último ha sido siempre conducir a cada uno de los participantes a la obtención de una solución para la problemática particular propia, descartando en todo  momento las actividades conductoras a un único tipo determinado de proyecto, tendiendo a la co-creación del conocimiento [13]. En este mismo sentido, se ha desarrollado una Wiki, con documentación construida de forma cooperativa en los talleres de  capacitación ofrecidos y en la que se proveen además guías prácticas para la instalación del entorno de desarrollo SDK de iSense y la instalación del simulador Shawn.  Finalmente, es destacable mencionar que en los próximos talleres a desarrollar se  prevé enfocarse en IPv6 y seguridad en WSN, con la certeza que la demanda de este  tipo de capacitación es creciente, estando persuadidos acerca de que la red IPng será  soporte indefectible de todas las redes de sensores, alentando el advenimiento de Internet de las cosas."	﻿internet de las cosas , sensores inalámbricos , redes ad hoc , internet del futuro	es	31833
54	Procesamiento de señales SAR algoritmo RDA para GP GPU	﻿En este trabajo se presenta una solución secuencial y una paralela del algoritmo RDA (Range Doppler Algorithm) para el procesamiento de señales de radares SAR (Synthetic Aperture Radar). La solución paralela se desarrolló en C CUDA para GP-GPU (General Purpose Graphic Processing Units). Se describe la solución desarrollada, se muestran los primeros resultados y se describen las futuras optimizaciones para dicho algoritmo. Keywords: Procesamiento Paralelo, HPC, CUDA, GP-GPU, Procesamiento de señales, radares SAR. 1. Introducción Los radares de apertura sintética (SAR) son radares de pequeñas dimensiones que se acoplan a aeronaves (aviones o satélites) y aprovechan el desplazamiento de dicha aeronave para obtener imágenes de alta resolución. Se utilizan para formar imágenes de la superficie terrestre, detectar objetivos, realizar el seguimiento de objetivos móviles, etc. En este trabajo la información recolectada y almacenada por el radar es procesada para formar imágenes de la superficie terrestre. El modo de operación de estos radares se basa en que el radar avanza con la trayectoria de la aeronave mientras envía sucesivos pulsos y almacena sus ecos. Toda la información almacenada se sintentiza para formar una única imagen [3] [4] [9] [11] [12]. Debido a la frecuencia de envío de pulsos y al muestreo de los ecos recibidos, los datos recolectados son almacenados en arreglos de 2 dimensiones: las filas corresponden a cada pulso enviado (dimensión llamada acimut) y cada eco es muestreado y almacenado en una fila (el muestreo en rango define las columnas, dimensión llamada rango). Debido a las altas frecuencias con que se opera (en 174 2 Denham et al. rango y acimut), las matrices suelen ser de grandes dimensiones, normalmente almacenan millones de datos. A estos datos se los conoce como datos crudos. En la actualidad, existen diversos algoritmos para el procesamiento de señales SAR [4] [6] [9] [11] [12]. Los algoritmos más conocidos y utilizados son: Range Doppler Algorithm (RDA), Chirp Scaling Algorithm (CSA), Omega-K Algorithm, Back-projection Algorithm, etc. Dichos algoritmos se basan en aplicar filtros sobre los datos, transformadas de Fourier, antitransformadas, etc., todas operaciones con altos costos computacionales. Como se especificó anteriormente, estos algoritmos operan sobre una gran cantidad de datos. Estas características obligan a desarrollar soluciones desde la tecnología HPC (High Performance Computing). Además, es frecuente que este procesamiento esté ligado a aplicaciones de tiempo real haciendo aún más necesaria la implementación de algoritmos con bajos tiempos de respuesta, sin perder calidad de las imágenes generadas. En este trabajo se utilizan procesadores gráficos de tipo GPGPU como arquitectura de ejecución paralela. Estas son placas que nacen para procesamiento gráfico (en el mercado de video juegos) pero su alto poder de cómputo, alto rendimiento y bajo costo ha hecho que se desarrollen placas gráficas de uso general (GPGPU: General Purpose Graphic Processing Unit). Este trabajo es la continuación de [7], donde se enumeran las principales características del procesamiento de datos crudos SAR, como así también los pasos de los algoritmos RDA y CSA. Además, se muestran los rasgos más importantes de las arquitecturas GPGPU, poniendo énfasis en la organización de los componentes de las placas gráficas, la jerarquia de threads y jerarquia de memorias. Además se introduce las principales características de la programación en C CUDA. En las siguientes secciones se presentan los aspectos principales de la solución secuencial y paralela del algoritmo RDA. La solución paralela es una primera aproximación en la que aún no se consideran aspectos más avanzados del hardware [2] [5]. Luego se presentan resultados obtenidos con dichos algoritmos y una comparación y análisis de los mismos. Además se plantean los pasos futuros que corresponden con optimizaciones del algoritmo. 1.1. Procesamiento de Señales SAR: Range Doppler Algorithm El algoritmo RDA fue desarrollado en 1978 para procesar datos del primer radar SAR (SEASAT SAR) y hasta la actualidad es uno de los algoritmos más utilizados. Su principal característica es que usa operaciones en el dominio de la frecuencia en ambas dimensiones (rango y acimut), operando de esta forma en 1 dimensión, logrando mayor simplicidad [4]. El algoritmo se basa en la aplicación de tres operaciones: compresión en rango, RCMC (Range Cell Migration Correction), y compresión en acimut. Con estas operaciones se enfocan los datos en rango, luego se corrige migración de celdas en rango y por último se comprimen los datos en acimut para enfocar los datos en esta segunda dimensión. 175 Procesamiento de Señales SAR: Algoritmo RDA para GPGPU 3 La compresión de rango se lleva a cabo realizando una convolución rápida en cada fila de la matriz: se realiza una FFT para llevar los datos al dominio de la frecuencia, se multiplica por un filtro y por último se realiza una IFFT para llevar los datos al dominio del tiempo nuevamente. Asumiendo el caso de radares transportados en aviones, donde las distancias son cortas, se puede asumir que la superficie terrestre es plana. Considerando un objetivo puntual, es facil observar que a medida que avanza el radar en su recorrido, la distancia del radar a dicho objetivo cambia con el tiempo (acimut). Esta diferencia en la distancia en función del tiempo en acimut genera migración de celdas en rango en los datos crudos. Es necesario entonces corregir esta migración de celdas. Dicha corrección se lleva a cabo mediante un corrimiento de celdas en función de la migración producida. Por último, se realiza la compresión en acimut: cada columna de la matriz se pasa al dominio de la frecuencia (aplicando una FFT), se aplica un filtro, y se realiza una antitransformada para llevar los datos al dominio del tiempo. 2. RDA en C y C CUDA Como punto de partida se ha desarrollado dicho algoritmo en MATLAB. Esta primer implementación aportó claridad y experiencia en las distintas operaciones del algoritmo y sus posibles implementaciones. Además, dicha implementación permite verificar la correctitud de las operaciones a cada paso, haciendo que el desarrollo en C y C CUDA se desarrollen con plena seguridad. El algoritmo secuencial en el lenguaje C se desarrolló con el objetivo de tener una métrica con la cual poder comparar el rendimiento del algoritmo paralelo. Particularmente, se ha trabajado con una librería concreta para las operaciones de FFT e IFFT desarrollada por Mark Borgerding. Esta librería implementa de forma rápida las FFT e IFFT y se puede utilizar con datos de simple o doble precisión (http://sourceforge.net/projects/kissfft/). Además de los algoritmos secuenciales se ha desarrollado el algoritmo en paralelo, especialmente diseñado e implementado para su ejecución en GPGPU. Paralelización de RDA Actualmente se dispone de una primer versión de RDA en C CUDA, la cual será explicada y evaluada. Este primer desarrollo de la solución paralela no se considera que sea óptima, pero constituye un punto de partida para llegar a soluciones más eficientes. En el corto plazo, se buscarán modificiones al algoritmo paralelo para, principalmente, realizar una distribución de trabajo eficiente y una utilización de la jerarquia de memorias que logre rendimiento óptimo. Como ya se ha expuesto, muchas de las operaciones de RDA se basan en aplicación de filtros, convoluciones, FFT, IFFTs. La paralelización del algoritmo se basa en: las operaciones FFT e IFFT se resuelven utilizando las operaciones 176 4 Denham et al. paralelas de la librería cuFFT y las aplicaciones de filtros, convoluciones, normalizaciones, se paralelizan creando bloques de threads y haciendo que cada thread compute un único elemento del arreglo (señal). Libreria cuFFT [8] es una librería desarrollada por NVIDIA [1] que resuelve las Transformadas Rápidas de Fourier (y antitransformadas). Las operaciones en esta librería se implementan con la estrategia divide y vencerás y logran algoritmos eficientes para conjuntos de valores reales y complejos. La librería cuFFT provee interfaces simples que permiten al usuario hacer uso de la potencia de cálculo y poder de paralelismo de las placas GPU, de forma transparente. 2.1. Compresión en Rango Compresión en rango secuencial Esta operación se desarrolla de forma secuencial, fila por fila, realizando una convolución rápida: FFT, aplicación de filtro, IFFT. Como ya se ha expuesto, las operaciones FFT e IFFT se resuelven de forma secuencial con operaciones de la librería KISS FFT. A su vez, el filtrado de la señal se resuelve con un producto punto también secuencial. A continuación se muestra un pseudocódigo simplificado de esta operación. Compresión de Rango Secuencial ... for (cada fila) { FFT; producto punto; IFFT; } La transformada discreta de Fourier (TDF) tiene orden de complejidad O(n2). A su vez, las implementaciones FFT (Fast Fourier Transform) son algoritmos recursivos que siguen la estrategia divide y vencerás, y pueden llegar a reducir este orden de complejidad a O(n log n). El producto punto tiene orden de complejidad O(n). Estas operaciones se ejecutan por cada una de las filas, llegando a un orden de complejidad que O(n2 log n). Teniendo en cuenta las dimensiones de las matrices de datos crudos, decrementar este orden de complejidad es un requisito fundamental para mejorar el rendimiento de esta aplicación. Compresión en rango paralelo En esta primer propuesta paralela, el procesamiento también se realiza por cada una de las filas de la matriz: las operaciones FFT e IFFT se realizan utilizando operaciones de la librería cuFFT de CUDA. Como ya se ha dicho, las operaciones de dicha librería están optimizadas para obtener máximo rendimiento en GPU. 177 Procesamiento de Señales SAR: Algoritmo RDA para GPGPU 5 Luego de la FFT, se realiza un producto punto de toda la fila. Para dicha operación se implementa un kernel el cual se ejecuta con tantos threads como elementos tenga la fila. Todos los threads se ejecutan de forma concurrente (con un elevado paralelismo) y cada uno resuelve un único producto y lo almacena en un lugar de la fila resultante (figura 1). Figura 1. Kernel CUDA: cada thread calcula el producto de un elemento del vector resultante. Una de las características de la utilización de GPUs y CUDA es que el programador puede crear un número de threads muy elevado, asumiendo que no hay restricción máxima de cantidad de threads. Por esto mismo, crear un thread por cada elemento de la fila, que compute un único producto es natural, independientemente de las grandes dimensiones que pueda tener la estructura de datos con la que se opera. De esta forma, todos estos productos se resuelven “de forma simultánea” y este es un de los aspectos principales de las placas gráficas. Como se mencionó anteriormente, esta primer propuesta paralela realiza estas operaciones por cada una de las filas. A futuro, se preve implementar el procesamiento de todas las filas a la vez, aprovechando la ausencia de dependencia entre los datos y su procesamiento. 2.2. Range Cell Migration Correction (RCMC) Por cada fila de la matriz se calcula la cantidad de celdas enteras a corregir debido a la migración. Además se calcula la migración a nivel de fracción de celda, como se propone en [4]. RCMC Secuencial Por cada fila, se calcula la cantidad de celdas que el objetivo haya migrado en los datos crudos, dependiendo del tiempo en acimut. Luego se realiza un corrimiento de todos los elementos de la fila en función del cálculo previo. A su vez, se ajusta la migracion fraccionaria utilizando un kernel de interpolación propuesto en [4]. La fila se convoluciona con dicho kernel para realizar un ajuste más preciso. A continuación se muestra un pseudocódigo de RCMC. 178 6 Denham et al. Corrección de migración de celdas en rango secuencial ... for (cada fila) { cálculo de migración de celdas enteras; cálculo de migración de fracción de celdas; /* corrección de migración de celdas */ corrimiento de toda la fila; convolución con el kernel de interpolación; } RCMC Paralelo A nivel de celda entera se realiza el corrimiento con un kernel paralelo: cada uno de los threads copia un dato en la fila teniendo en cuenta el corrimiento. Se copian en paralelo todos los datos de la fila, evitando una iteración sobre la misma. La corrección de migración de celdas a nivel de fracción de celdas se ejecuta en paralelo teniendo un thread por elemento de la fila resultado y cada thread realiza la multiplicación y suma correspondiente. Nuevamente, se evita iterar sobre los elementos de la fila. De forma similar a la operación anterior, se observa que una posible mejora es implementar todos los corrimientos de todas las filas de forma concurrente. Esto evita iterar sobre cada una de las filas. 2.3. Compresión en Acimut Por cada una de las columnas de la matriz resultante de las dos operaciones anteriores se realiza: FFT de la columna, producto punto entre la columna y el filtro (el mismo se encuentra en dominio de la frecuencia) y por último IFFT del resultado, para llevar los datos resultantes al dominio del tiempo. Compresión en Acimut secuencial Esta operación es similar a la compresión de rango, pero se trabaja columna a columna. Se trabaja en el dominio de la frecuencia, por lo que el filtro se transforma al comienzo del procesamiento, y cada columna se transforma antes del producto. Compresión en acimut secuencial ... for (cada columna) { FFT columna producto punto columna y filtro IFFT resultado } 179 Procesamiento de Señales SAR: Algoritmo RDA para GPGPU 7 Compresión en Acimut paralelo Nuevamente, esta operación aprovecha las operaciones de la librería cuFFT, las cuales garantizan operaciones eficientes para resolver las transformadas (antitransformadas) de Fourier. Por otro lado, el producto punto se resuelve en paralelo, utilizando un thread por cada elemento del vector. Esto se realiza de forma secuencial sobre cada una de las columnas de la matriz, lo cual se prevee modificar para realizar de forma concurrente todas las columnas (no existe dependencia de datos). En las próximas secciones se mostrarán y analizarán los primeros resultados obtenidos con ambas implementaciones. 3. Resultados Se realizará la comparación de rendimiento del código secuencial y el rendimiento de la primer implementación paralela. La arquitectura utilizada está compuesta por un host multicore de 4 procesadores Intel(R) Core i5-2500 CPU @ 3.30GHz, con 7.7GB de memoria y sistema operativo Ubuntu de 64 bits. En dicha CPU se encuentra conectada una placa gráfica de tipo GeForce GTX 550Ti con 192 cores CUDA (dispuestos en 4 multiproceadores) de fabricante NVIDIA. Se ha experimentado con 3 imágenes: un objetivo puntual (imagen sintética: un único objetivo en el medio de la imagen), y 2 fotos reales, una de las cuales es la vista de un aeropuerto y otra de un crater de un volcán. Para disponer de los datos crudos de dichas imágenes se ha utilizado un simulador que simula el proceso de generación de los datos crudos. Dicho simulador fue desarrollado por uno de los miembros del equipo de trabajo. Como ejemplo, la figura 2 muestra la foto del aeropuerto utilizada y la imagen obtenida luego de procesar los datos crudos con el algoritmo RDA implementado. Figura 2. Imagen de aeropuerto, foto aérea e imagen obtenida con el algoritmo RDA. Para los 3 casos de experimentación, se han tenido en cuenta los siguientes valores: el tiempo de exposición del objetivo es 3.4 segundos, y se trabaja a una frecuencia de envío de pulso de 600Hz. En total se toman 2000 pulsos. 180 8 Denham et al. Por otro lado, el eco del pulso recibido se muestrea para su almacenamiento y posterior procesamiento (valores discretos). La frecuencia de muestreo es de 120MHz. Debido a la distancia que se desea cubrir en rango, y al resto de valores propios de la geometría SAR, se almacenan 8000 datos en rango. Esto es, el eco recibido se muestrea y se obtienen 8000 valores. Estos valores definen matrices de 2000 filas de 8000 muestras (columnas). Esto corresponde a la matriz de datos crudos, matrices intermedias e imagen enfocada (imagen final). Como el algoritmo RDA está bien definidos en 3 operaciones bien visibles, los primeros análisis se han realizado sobre cada una de estas operaciones. Se obtuvieron los tiempos de ejecución de las operaciones: compresión en rango, RCMC y compresión en acimut. Los tiempos obtenidos se muestran en el cuadro 1 para las 3 operaciones del algoritmo RDA (en segundos). Se puede observar que los tiempos son muy similares para las 3 imágenes, por lo que se expondrán dichos tiempos en figuras sólo para un caso, valiendo el análisis para el resto. En dicho cuadro, RC es compresión en rango, RCMC es corrección de migración de celdas en rango y AC es compresión en acimut. Por cada imagen se muestra los tiempos secuenciales (Sec) y los tiempos del algoritmo paralelo (Par). Objetivo Puntual Aeropuerto Volcan Sec Par Sec Par Sec Par RC 97.32 4.88 97.20 4.92 97.35 4.91 RCMC 0.57 0.04 0.56 0.04 0.57 0.04 AC 4.64 0.68 4.65 0.68 4.65 0.68 Cuadro 1. Tiempos obtenidos para las 3 imágenes (en segundos). Para continuar con el análisis, la figura 3 muestra los tiempos del algoritmo RDA con una de las imágenes (objetivo puntual). Figura 3. Tiempos de ejecución del algoritmo RDA con la imagen del objetivo puntual. 181 Procesamiento de Señales SAR: Algoritmo RDA para GPGPU 9 En dicha figura se puede distinguir con exactitud la ganancia de paralelizar la compresión en rango, pero las otras dos operaciones no se puede determinar debido a la escala del eje Y (tiempos en segundos). Por esto se muestran en la figura 4 estas operaciones en gráficos distintos, para poder comparar de forma precisa la diferencia de tiempos del algoritmo secuencial y paralelo. Considerar el cambio de escala del eje Y para dichos gráficos. Figura 4. Tiempos de las operaciones RCMC y compresión en acimut en detalle. En todos los casos es posible observar que la compresión en rango es la operación que más tiempo requiere cuando se resuelve de forma secuencial. Dicha operación resuelve una FFT, un producto punto y una IFFT por cada una de las filas. Debido a la operación FFT que se realiza, se realiza zero padding a los datos (filas), por esto se trabaja con 16000 valores en rango. Al finalizar esta operación, se vuelven a seleccionar los datos centrales de la antitransformada, que es donde se concentra la información útil (y se sigue el procesamiento usando 8000 muestras en rango). Lo anteriormente dicho no sucede en las operaciones de corrección de celdas (se trabaja con filas de 8000 complejos) ni en compresión en acimut (columnas de 2000 complejos). Tomando los tiempos secuenciales de cada operación como referencia, para la compresión de rango se observa un 95 % de reducción en el tiempo, en la operación RCMC un 93 % de reducción y en la compresión en acimut se observa un 86 % de reducción (aproximadamente). Como se ha mencionado anteriormente, la solución paralela propuesta es una primer aproximación, para la cual no se consideran aspectos fundamentales en la optimización de códigos CUDA: cantidad y jerarquia de threads en cada kernel, movimiento de datos a memorias de más rápido acceso (local, compartida, textura o constante) [2] [5]. El próximo paso a seguir en esta línea es modificar estos aspectos y así lograr algoritmos paralelos más eficientes. 182 10 Denham et al. 4. Conclusiones Este trabajo presenta los primeros resultados del algoritmo RDA secuencial y paralelo utilizando C CUDA en GPGPU. El algoritmo RDA cumple con los requerimientos necesarios para que su implementación y ejecución en GPU sea conveniente: alta carga computacional, independencia de datos, mínima transferencia de datos entre CPU y GPU (solo al comienzo y al final del procesamiento), no existen secciones críticas, los datos se mantienen en matrices logrando mapear las estructuras de datos con la disposición de los threads en los grids en cada kernel. Los primeros resultados obtenidos muestran que la paralelización del algoritmo logra reducir significativamente los tiempos en las 3 operaciones del algoritmo RDA. Esto muestra que la paralelización es efectiva, y, teniendo en cuenta posibles optimizaciones, el algoritmo RDA promete un muy buen rendimiento en GPGPU. A corto plazo se espera optimizar estas tres operaciones para obtener algoritmos más eficientes aún. Este trabajo muestra un primer paso abordando la comparación del algoritmo secuencial y del algoritmo paralelo. Como trabajo futuro también se estima la posibilidad de evaluar algoritmos propuestos en la literatura, con el fin de realizar comparaciones entre distintos algoritmos paralelos.	﻿procesamiento paralelo , HPC , GP GPU , procesamiento de señales , radares SAR	es	31851
55	Un método de sintonización para mejorar la salida de un modelo computacional de cuenca de ríos	﻿ Los modelos computacionales que simulan fenómenos naturales se pueden comportar de manera muy próxima a la real, pero debido a múltiples factores los resultados simulados difieren de los resultados reales. Una fuente de error es la falta de certeza en los valores de los parámetros de entrada. Este trabajo constituye un primer paso en el enunciado de una metodología que busca mejorar la capacidad de predicción de un simulador, aplicado a un modelo computacional de cuenca de ríos y, en particular, utilizando el modelo del cauce del Río Paraná. Se presenta un método computacional para la sintonización de los valores de los parámetros de entrada de dicho modelo, con el objetivo de minimizar el error entre la salida del simulador y la realidad observada. Este proceso de sintonización se lleva adelante aplicando una técnica de simulación paramétrica, la cual conlleva a ejecutar un gran número de simulaciones haciendo necesario utilizar recursos de cómputo de alto rendimiento y técnicas de paralelización. 1 Introducción La modelización y la simulación de inundaciones provocadas por el desborde de ríos brindan modelos computacionales para el estudio y la predicción de estos fenómenos naturales con el objetivo de estudiar, simular y predecir su comportamiento. El ingrediente esencial de cada modelo son las variables y los parámetros. Las variables son cantidades físicas y los parámetros controlan el comportamiento de las variables. Modelizar sistemas de la naturaleza, que son sistemas reales complejos, implica normalmente el uso de muchos parámetros y variables de entrada[1]. Al modelizar y simular el flujo y desborde de ríos, se ingresan los valores de los parámetros de entrada a un simulador computacional, siendo la salida del modelo, hidrogramas de caudal e información sobre el evento de inundación. Aunque los modelos utilizados consideren la mayor cantidad posible de variables involucradas en el proceso, tratando de simular de la manera más 254 certera el fenómeno, existen otros factores que no permiten obtener resultados confiables. Por diversos motivos, los valores de los datos de entrada son imprecisos provocando diferencias entre los resultados dados por el simulador y los medidos en la realidad [2]. Se detallan algunas de estas causas a continuación. Los parámetros de entrada a la simulación son medidos en unas pocas estaciones distribuidas a lo largo del cauce del río, siendo necesario interpolar sus valores en todo el dominio. Algunos datos, como las precipitaciones, cambian dinámicamente durante todo el proceso y otros son magnitudes físicas que arrastran errores de medición o deben medirse de manera indirecta (coeficiente de Manning, altura de albardones, conductancia, etc.). Los datos de salida son la altura del cauce del río, calculada en estaciones a lo largo de su recorrido y en sucesivos intervalos de tiempo. La idea principal de la investigación es minimizar el impacto en los datos de salida provocado por la incertidumbre en los valores de los parámetros de entrada al simulador, para brindar una mejora que ayude a los ingenieros que trabajan con cuencas hidraúlicas a producir alertas a la población ante eventos de desborde del cauce de los ríos. Una manera de abordar el problema de la incertidumbre es mediante una fase de ajuste de parámetros y posteriormente, la fase de verificación de la mejora y de su impacto en la capacidad predictiva del simulador[8]. La etapa de ajuste se realiza para obtener un conjunto de parámetros que minimice las diferencias entre la salida simulada y la real. En esta etapa se aplica la técnica de simulación paramétrica para procesar una gran cantidad de escenarios (cada configuración de parámetros del sistema simulado) y calcular una medida del ajuste para cada uno. El método presentado es propio del mundo de las altas prestaciones. La sintonización es posible con la ejecución de una enorme cantidad de escenarios, consumiendo un elevado tiempo de ejecución y requiriendo el uso de técnicas de cómputo paralelo a nivel básico para lanzar la mayor cantidad de ejecuciones posibles de manera simultánea. La presentación está organizada de la siguiente manera: En el capítulo 2 se presentan las características del simulador utilizado y el modelo del Río Paraná con el cual se lleva adelante este trabajo. En el capítulo 3, se presenta la metodología propuesta para el método de sintonización del simulador implementada sobre una muestra acotada de escenarios. En los capítulos 4 y 5 se detallan las experiencias realizadas con su análisis y en el capítulo 6 las conclusiones y trabajo futuro. 255 2 El simulador EZEIZA Se seleccionó un programa de simulación utilizado actualmente para brindar alertas ante posibles eventos de inundaciones, el cual es un simulador del cauce y flujo de agua en ríos. El software seleccionado es Ezeiza [7] cuyas características se brindan a continuación. 2.1 Modelo computacional para cauce de ríos: Ezeiza El software de simulación utilizado, Ezeiza V.6, fue desarrollado en el Laboratorio de Hidraúlica Computacional del Instituto Nacional del Agua (INA)4. Ezeiza es un modelo computacional para el cálculo de la traslación de ondas en ríos y canales que comenzó a desarrollarse en la década del ’70. Se basa en un análisis unidimensional, expresado matemáticamente mediante las ecuaciones de Saint Venant(1891) y resolviendo las mismas mediante técnicas numéricas [4]. Como condiciones iniciales, deben proveerse las distribuciones de nivel y caudal sobre todo el sistema. Su versión más actual permite el tratamiento de una red de flujo arbitraria. Actualmente, es usado con un modelo hidrodinámico del Río Paraná y se utiliza como herramienta adicional para el pronóstico de crecidas y bajantes de la Cuenca del Plata, tarea que está llevando a cabo el Servicio de Información y Alerta Hidrológico (SIyAH) del INA. Cabe destacar que el Río Paraná recorre una de las áreas más pobladas e industrializadas de Sudamérica en el tramo modelado, requiriendo implementar tecnologías que mejoren constantemente los pronósticos. La validación del modelo fue efectuada por los ingenieros del INA. En el informe de la validación y calibración del simulador, realizado por el Ing. Menéndez en 1996, ya se expresaba la necesidad de mejorar la precisión de los resultados simulados. Este análisis se retoma en un exhaustivo estudio de rendimiento realizado en 2011 por el Ing. Latessa para el INA [6]. La elección de este simulador se hizo considerando que: – Exporta los resultados en archivos que pueden ser tratados desde paquetes estadísticos y/o matemáticos. – Permite realizar simulaciones paramétricas modificando los valores de los parámetros en archivos de entrada, situación fundamental para el proceso de sintonización. El estudio de Latessa, tuvo como objetivo mejorar la modelización del río Paraná y brindar más certeza a los datos de entrada, debido a que se ingresaban parámetros medidos décadas atrás. Se mejoró mucho la precisión del modelo, 4 http : //www.ina.gov.ar/lha/index.php?lha = 38 256 como puede verse en dicho informe. Aunque, aun se pueden detectar diferencias entre los valores reales y los datos de salida del simulador en las estaciones de seguimiento. Estas diferencias son las que se intentan minimizar en este trabajo, haciendo un estudio del error de predicción del simulador. 2.2 Características del modelo del río Paraná El modelo hidrodinámico del Río Paraná y Paraguay, que corre sobre Ezeiza, fue diseñado por el INA para simular su comportamiento en los tramos que van desde la represa de Yacyretá (Corrientes) hasta Villa Constitución (Santa Fe). Los datos de la red de cálculo pueden verse en la Tabla 1. Tabla 1. Red de Cálculo: Modelo del Paraná Filamento Curso Long.(Km) Secciones Bordes Bordes Ag.Arr. Ag.Aba. 1 Paraná 1.083 76 Caudal Q Nivel H 2 Paraguay 376 77 Nivel H ——Los parámetros críticos del modelo son el coeficiente de Manning y el nivel de los albardones. El coeficiente de fricción de Manning representa una cuantificación de la resistencia hidraúlica y se determina en función de los factores que determinan la rugosidad del cauce. En el trabajo se diferencia entre el coeficiente de Manning en el cauce principal y en la planicie, donde el nivel de resistencia en la dirección de escurrimiento es mucho mayor. Los albardones son formaciones naturales generadas por los sólidos depositados en las márgenes del cauce principal durante las crecidas. Ambos parámetros son considerados críticos en la simulación y contienen un considerable grado de incertidumbre en sus valores. Esta decisión se toma siguiendo las recomendaciones de los ingenieros del INA. La salida que provee el simulador con el modelo del Paraná es un conjunto de archivos con los datos simulados (hidrogramas de alturas y caudales) para 15 estaciones de seguimiento ubicadas a lo largo del cauce. La Figura 1 muestra un esquema de la organización de la entrada y salida al simulador Ezeiza. 3 Metodología El trabajo que se presenta está enfocado en tratar el problema de la incertidumbre de los parámetros de entrada mediante la implementación de un método 257 Fig. 1. Esquema de Entradas y Salidas al Simulador Ezeiza de ajuste de parámetros. Se busca encontrar el mejor escenario, o sea el que minimiza las diferencias entre datos reales (datos observados), y los resultados del simulador (datos simulados). La idea principal del proceso de sintonización aplicado al simulador de cuencas de ríos, se puede ver en la Figura 2. Fig. 2. Esquema del Proceso de Sintonización La fase de ajuste se hace mediante la implementación de una experimentación paramétrica. Este proceso consiste en lanzar tantas simulaciones como combinaciones posibles de los valores de los parámetros que utiliza el simulador, siendo el objetivo alcanzar el mejor escenario. En nuestro modelo, el dominio del río Paraná está dividido en 76 secciones y cada una de ella está divida en subsecciones. Se puede encontrar entre 3 y 11 subsecciones, según el ancho de la planicie de inundación en cada sección. El coeficiente de Manning se considera en cada sección, y en cada una de sus subsecciones, en las cuales es tratado como si fuese un parámetro diferente a los efectos de realizar la simulación paramétrica. Este proceso requiere establecer un índice de similaridad, el cual constituye una métrica para medir la diferencia entre los datos reales y los simulados. La 258 simulación paramétrica permitirá calcular este índice para cada escenario y encontrar el mejor índice de similaridad entre el simulador y el sistema real. La experimentación se realiza con los datos provistos por el INA. Estos son: – Los datos reales: las alturas diarias del río Paraná en diferentes períodos, que van desde el año 1994 al 2011, y en cada estación de seguimiento a lo largo del cauce considerado. – Los datos del modelo: condiciones iniciales en todos los puntos del dominio, series temporales de alturas y caudales correspondientes a las condiciones de borde, la información sobre los parámetros en cada sección y los datos de la geometría del sistema real. Estos datos fueron recibidos del INA, ya mejorados, luego del estudio de rendimiento del 2011 [6]. 3.1 Generación de Escenarios El simulador Ezeiza permite modificar los valores de los coeficientes de Manning en todas las secciones y subsecciones en que se divide cada sección, y las alturas de los albardones en sus archivos de entrada, haciendo muy fácil llevar adelante la experimentación paramétrica. La cantidad de escenarios posible está determinada por la cardinalidad de cada uno de los N parámetros considerados, ésta se denomina Ci, donde i identifica cada uno de los parámetros. Cada parámetro debe tener asociado un rango de valores propio de su dominio más el paso del barrido con el que se recorrerá dicho intervalo (Incrementoi). Para cada parámetro i se puede representar su intervalo e incremento asociados como la tupla: < [Cotainf , Cotasup], Incrementoi > A continuación, se muestra la relación entre estos parámetros, propios de la experimentación paramétrica [3]. El intervalo de barrido está acotado por Cotainf y Cotasup #Escenarios = N∏ i=1 Cidonde Ci = ((Cotasup − Cotainf ) + Incrementoi)/Incrementoi (1) En este modelo se cuenta con 76 secciones con sus valores de Manning. En cada sección se combinan los valores de Manning para planicie y para cauce. Se verá después que en planicie, Manning puede tomar uno de 6 valores diferentes y en cauce, uno de 9 valores. Al cálculo debemos agregar 57, de las 76 secciones que registran su valor de albardones. De este análisis resultan 76 x 6 x 9 x 57 = 233928 escenarios. 259 La medida tomada como índice de similaridad es el error relativo de los valores simulados respecto a los valores reales. Se obtiene un índice por cada combinación de parámetros, el que tenga el menor valor se corresponde con el mejor escenario de todos. Por otro lado, se calcula el índice de similaridad resultante de correr la simulación con el escenario utilizado por los expertos del INA, con el objetivo de compararlo con el de esta experimentación. Esta fase de ajuste de parámetros se hizo con tres estaciones de prueba, seleccionadas de las 15 estaciones de seguimiento del río Paraná. Para cada una se seleccionó el mejor escenario [9]. Los valores simulados y reales que se comparan son las alturas del río en las estaciones consideradas. Se denonina AkR y A k S a la altura real y simulada, respectivamente, correspondientes a la Estación k. El índice de similaridad, Indicekj , resultante para la Estación j luego de correr la simulación para el Escenario k, surge de la siguiente ecuación: Indicekj = ∣ ∣ ∣AkR −A k S ∣ ∣ ∣ /AkR (2) El mejor escenario, que se denomina como Êscj , es el que obtiene el mínimo índice de similaridad. Se calcula como el mínimo índice de todos los escenarios, medido en la Estación j, o sea: Êsc = Mink(Indice k j ) (3) 3.2 Experimentación En este apartado se presenta un estudio acotado del modelo y su implementación en Ezeiza, con la finalidad de presentar la técnica computacional de sintonización del simulador. La idea es mostrar que se logra el objetivo en tres estaciones de seguimiento demostrando que el método de sintonización es factible y extensible a todo el espacio de parámetros e intervalos. La cantidad de escenarios se limita tomando sólo los valores del coeficiente de Manning como parámetro crítico, en esta etapa no se consideran los albardones. También se acotó la cantidad de Secciones, tomando una muestra representativa de 3 de las 76 posibles, ubicadas en el área de Paraná Alto (Sección 76), Medio (Sección 36) y Bajo (Sección 01). Sobre estas Secciones se efectuó la experimentación paramétrica con los escenarios que se ven en la Tabla 2. La cantidad de escenarios se puede calcular, utilizando la Eq. 1, de la siguiente manera: #Escenarios = 3.2.4.1 = 24 (4) – Cardinalidad Manning Planicie: ( (0.2 - 0.1) + (0.1) / 0.1 = 2 260 Tabla 2. Valores de los parámetros de Manning en cada escenario considerado para la simulación paramétrica. Número de Manning Manning Número de Manning Manning Escenario P lanicie Cauce Escenario P lanicie Cauce 1 0.1 0.01 13 0.2 0.01 2 0.2 0.01 14 0.2 0.02 3 0.1 0.02 15 0.2 0.03 4 0.2 0.02 16 0.2 0.04 5 0.1 0.03 17 0.1 0.01 6 0.2 0.03 18 0.1 0.02 7 0.1 0.04 19 0.1 0.03 8 0.2 0.04 20 0.1 0.04 9 0.1 0.01 21 0.2 0.01 10 0.1 0.02 22 0.2 0.02 11 0.1 0.03 23 0.2 0.03 12 0.1 0.04 24 0.2 0.04 – Cardinalidad Manning Cauce: ( (0.04 – 0.01) + 0.01) / 0.01 = 4 – Cardinalidad Secciones: ( (3 – 1) + 1) / 1 = 3 – Cardinalidad Albardones: 1 En la etapa de ajuste de parámetros, se implementó el programa Ezeiza con los datos de cada una de las 24 configuraciones correspondientes a los escenarios descritos. Para medir los errores de predicción se seleccionaron 3 estaciones (de las 15 de seguimiento): Hernandarias (Est1), Bella Vista (Est2) y Rosario (Est3). Esta simulación se realizó con 2000 pasos de tiempo (2000 días a partir del 01/08/1994) y se obtuvieron mejoras significativas en la predicción en los escenarios que se muestran en la Tabla 3, los cuales se seleccionaron tomando el menor índice de similaridad alcanzado. Los coeficientes de Manning que se muestran corresponden al escenario con mejor índice de similaridad para cada estación considerada. En cada caso se muestran dos índices de similaridad cuya explicación es: – SimulPar: Indice de Similaridad que presenta el mejor Escenario, en la simulación paramétrica, con la Realidad. – EscINA: Indice de Similaridad del Escenario utilizado por el INA, con la Realidad. La Figura 3 representa los datos de la Tabla 3 y visualiza la ganancia obtenida con la elección de los escenarios que proveen la mejor sintonización. De los datos de esta tabla, y del gráfico, se puede afirmar que cada una de las 261 Tabla 3. Escenarios que mejor se ajustan a la predicción en las estaciones seleccionadas. Estación Mejor Manning IndiceSimilaridad Seguimiento Escenario P lanicie Cauce SimulPar EscINA Est1-Hernandarias 5 0.1 0.03 0.0007 0.0526 Est2-Bella Vista 9 0.1 0.01 0.0028 0.0166 Est3-Rosario 18 0.1 0.02 0.0019 0.0684 estaciones consideradas logró mejorar la certeza de sus datos de salida con errores que están por debajo del 0.3%, en cambio en estas estaciones el INA logra su mejor simulación con errores entre 2-7%. Se superó ampliamente la mejora lograda por el INA, luego de su estudio de rendimiento y ajuste de parámetros. Fig. 3. Comparación entre los porcentajes de similaridad logrados en las simulaciones paramétricas y el mejor escenario utilizado por el INA 4 Resultados de la Experimentación Este proceso de sintonización se puede extender a todas las otras estaciones a medida que se amplíe la experimentación paramétrica a todos los escenariosposibles. Esto sucederá al combinar los parámetros de Manning de cauce y planicie de las 76 estaciones en conjunto, y las alturas de los albardones en cada una. Será necesario crear un índice de similaridad que pueda medir los resultados en las 15 estaciones en conjunto. Para llevar adelante esta experimentación se requerirá correr en paralelo todo el proceso. Una simulación completa, para 262 6000 pasos de tiempo, en un procesador Intel(R)-Dual Core de 1.3GHz, y para un juego de parámetros tarda entre 9 y 10 min. Según vimos anteriormente, tenemos 233.928 posibles escenarios, lo que lleva a más de 2.000.000 de minutos de cómputo para hallar el conjunto óptimo de parámetros mediante una búsqueda exhaustiva. Igualmente son tiempos impracticables y las experiencias futuras de este trabajo, necesitarán llegar al conjunto óptimo, o mediante una heurística aproximarse todo lo posible. Esto es una demostración de que se está ante un problema propio del HPC. Por otro lado, el proceso de sintonización se utilizará para intentar mejorar la predicción del simulador en el futuro, por lo cual será necesario efectuar un proceso iterativo que repita la sintonización de los parámetros, aumentando aun más el tiempo de ejecución del proceso. 5 Conclusiones y Trabajo Futuro En este artículo se ha descrito una metodología que se aplicará para mejorar la predicción del simulador Ezeiza. Se utilizó un conjunto de experiencias acotadas para probar la factibilidad del método, el cuál proporcionó muy buenos resultados en las tres estaciones que fueron estudiadas. La batería de experiencias que se puso en práctica permitió tener una idea de la necesidad de recursos de cómputo para la etapa siguiente y mostrar que este problema deberá ser resuelto con HPC. Actualmente, se desarrolla la etapa siguiente mediante el uso de procesamiento paralelo. Se implementa un esquema paralelo Master-Worker con asignación dinámica de cargas, utilizando bibliotecas de pasaje de mensajes. El reparto de cargas se refiere a la distribución de sucesivos escenarios a cada nodo worker hasta que finalice el proceso encontrando el mejor escenario. Esta etapa requerirá diseñar una heurística inteligente para continuar con los objetivos del trabajo. Teniendo en cuenta la importancia de dar alertas hidrológicas sobre crecidas en las cuencas de ríos, el brindar una metodología destinada a mejorar la certeza de los pronósticos de los programas de simulación sería un aporte muy valioso para los expertos que utilizan diariamente estas herramientas computacionales.	﻿modelos computacionales , simulación paramétrica , cómputo de alto rendimiento , técnicas de paralelización	es	31900
56	WebECALEAD diseño de un prototipo web como herramienta de soporte para la Evaluación de Calidad en Educación a Distancia	﻿ Se presenta la descripción del diseño de un prototipo de sistema web  para la evaluación de calidad en Educación a Distancia, denominado  WebECALEAD, que tiene como objetivo facilitar la tarea de evaluación de los  procesos más destacados que intervienen en los modelos educativos a distancia  o híbridos, basado en la propuesta ECALEAD desarrollada por lo autores. El  prototipo contempla las fortalezas y debilidades de ECALEAD, en base a su  aplicación en experiencias educativas concreta. Se presenta aquí una  introducción a la temática y algunos antecedentes, la descripción del prototipo  diseñado, un análisis de las posibilidades que ofrece éste en función del modelo  ECALEAD. Finalmente, se presentan las conclusiones y trabajos futuros.  Keywords: calidad, educación, modelo de evaluación, sistemas web para  evaluación de procesos  1   Introducción  Los actuales escenarios educativos, están fuertemente impregnados de la revolución  tecnológica, y proponen un verdadero desafío para las instituciones de educación  superior, que deben aprovechar el potencial de los recursos tecnológicos para ofrecer  propuestas educativas de calidad. En este sentido, las instituciones destinan, parte de  sus esfuerzos, al diseño e implementación de nuevas formas de organización y gestión  de sus ofertas, incorporando innovación tecnológica para hacer frente a los nuevos  desafíos. Surge entonces la necesidad de establecer mecanismos para asegurar la  calidad, tanto de la oferta pública como privada en el ámbito de la Educación  Superior.  En varios países de América Latina se vienen implementando algunas  políticas en este sentido. Así es que las instituciones de Educación Superior (IES) se  han comprometido a realizar procesos de autoevaluación, someterse a procesos de  acreditación y emplear indicadores de desempeño, entre otras acciones [1][2][3][4].  En este sentido, resulta necesario que las IES fijen claramente los criterios y  estándares de calidad a seguir para alcanzar sus fines.   Para determinar el conjunto de criterios de calidad de las IES, en cualquiera de las  modalidades presencial o a distancia, es fundamental analizar cuál es el contexto  661 particular en el que se desarrollan, cuáles son sus componentes principales, los  aspectos críticos, los actores que formarán parte, y sus características, necesidades y  demandas, entre otros.   En [5] se presentó ECALEAD, un modelo para evaluar la calidad de un sistema /  programa/ proceso educativo a distancia, que integra algunos de esos aspectos  poniendo el foco específicamente en los procesos más importante involucrados a  criterio de sus autores. Este modelo puede ser utilizado también en modalidades  educativas híbridas que combinan aspectos de la modalidad presencial con estrategias  de trabajo y aprendizaje a distancia.  En [6] [7] se presentó un análisis de las bondades y debilidades que ofrece el  modelo ECALEAD derivadas de su aplicación en algunas experiencias educativas  concretas. Este trabajo ha ido evolucionando, y actualmente se está trabajando en el  sistema foco de este trabajo.  A partir de estas consideraciones se presenta ahora la descripción del diseño de un  prototipo de sistema web denominado WebECALEAD, y tiene como objetivo facilitar  y acompañar el proceso de evaluación propuesta por ECALEAD.   2  Breve descripción de los aspectos de evaluación de ECALEAD   El modelo ECALEAD, sigue los lineamientos planteados por García Aretio, y  propone un modelo integrador, de desarrollo y control de la calidad  total, vinculado  al contexto, metas, entradas, procesos, resultados, y mejoras. En ese contexto se  establecen algunos criterios generales como funcionalidad, disponibilidad, eficacia,  eficiencia, información e innovación [8].   ECALEAD es un modelo de evaluación de calidad basado en tres capas que  profundizan en la tarea:   En la Capa 1 se definen los procesos a evaluar y los criterios generales a  considerar en el modelo. Se consideran los criterios mencionados, y se los relaciona  con las metas/objetivos, procesos, recursos, resultados y mejoras involucrados en el  objeto de evaluación, que se muestran en la Fig. 1.      Fig. 1. Criterios a considerar en la capa 1, en función de los componentes vinculados a una  IES, en el marco de este trabajo    En la Capa 2 se proponen algunos indicadores que se relacionan en forma directa  con los criterios y procesos de la primera capa. Para ello, deben determinarse los  recursos y resultados, que se consideren de interés en un sistema/programa/curso a  662 distancia. Al aplicar la capa 2 en una evaluación específica, se deben ajustar estos  indicadores propuestos por el modelo, acorde al contexto. No siempre será necesario  aplicar todos los indicadores propuestos, o tal vez, en algunos casos se requiera  involucrar nuevos indicadores. El modelo aborda un conjunto de indicadores que se  creen pertinentes en una evaluación de calidad de sistemas de EAD, en general.  A modo de ejemplo se detallan algunos de los indicadores correspondientes a uno  de los procesos que ECALEAD propone, vinculándolos con los criterios generales de  la Capa 1. El lector puede observar la descripción completa de procesos, criterios e  indicadores en [9]    El ejemplo que se presenta aquí es el del proceso de Administración y Gestión  que incluye, entre otros aspectos, establecer circuitos de difusión, inscripción,  atención de consultas administrativas, gestión de alumnos y docentes, tales como  mantener, registrar y dar información sobre cursos aprobados, notas, certificaciones  alcanzadas, etc. También se involucran los circuitos para entregar credenciales de  acceso a entornos virtuales de enseñanza y aprendizaje (en caso de haberlos). Algunos  Indicadores a considerar para este proceso son  Vinculados a los criterios de eficacia y eficiencia:   Cantidad de inscripciones (analizando procedencia del alumno) que se reciben, en  relación con la cantidad de recursos humanos que atienden dichas inscripciones y  circuitos disponibles para la inscripción presencial o a distancia.  Cantidad de alumnos que inician el proceso luego de la inscripción, en relación con la  cantidad de inscriptos. Pueden indicar faltas de información administrativa adecuadas.  Cantidad de trámites administrativos iniciados en el día en relación con los  finalizados.  Porcentaje de consultas recibidas a través de los diferentes medios disponibles.  Vinculados al criterio de funcionalidad:   Cantidad de alumnos y docentes que manifiestan satisfacción respecto de los circuitos  administrativos y de consulta disponibles en la institución.  Vinculados al criterio de información e innovación:   Verificación de que existe un informe que reporte los resultados obtenidos a partir de  los indicadores.  Cantidad de mejoras y cambios implementados en vinculación con el plan de mejoras  determinado por la institución.  Los indicadores mencionados constituyen un conjunto de ejemplo, que no es  exhaustivo, pero permite orientar el tipo de trabajo a realizar en la capa 2. Como ya se  explicó, la definición concreta de indicadores es establecida acorde al objeto de  evaluación y su contexto [10].  En la Capa 3 se propone tomar cada uno de los indicadores para los procesos y  criterios en cuestión, y analizarlos de manera tal, de concretar la evaluación para el  objeto particular a considerar (curso, sistema, proyecto de EAD). Esta capa es la más  específica y debe ajustarse al contexto particular. Como podrá observarse en el  desarrollo que se ha realizado en la capa 2, existen algunos indicadores que son  cuantitativos mientras que hay otros más cualitativos. La definición de escalas para la  determinación de calidad, dependerá de esta característica, en cada caso. Es en esta  capa donde se determinan las medidas a considerar con sus escalas específicas.  663 3  Análisis de fortalezas y debilidades del modelo propuesto  A partir de la aplicación de ECALEAD a experiencias educativas concretas, se ha  observado que la flexibilidad del modelo obliga a revisar detalladamente con qué  procesos, criterios e indicadores se abordará la evaluación para que puedan ser  accesibles y/o adecuados acorde al objeto de estudio particular. Por otra parte,  si la  evaluación es realizada por personal más vinculado a los niveles organizativos o de  gestión de una institución, se contemplará la posibilidad de considerar mayor cantidad  de procesos en detalle, como el de políticas y normativas, el vinculado con lo  económico y financiero, entre otros. Sin embargo, si los evaluadores son docentes de  un curso particular, es probable que no puedan considerarse políticas y normativas  propias del curso sino las propias de la institución que lo gestione/organice. En esos  casos, es probable que el docente no tenga acceso a información para llegar a analizar  dichos procesos, y sólo se trabajará con los más relacionados con el diseño del curso  en sí. Es por ello, que la aplicación del modelo deberá ajustarse acorde a las  posibilidades de cada contexto específico, y de quiénes llevarán a cabo el proceso de  evaluación.  De acuerdo a lo expresado en [7], se detallan algunas de las fortalezas y  debilidades encontradas en ECALEAD y que sirven como punto de partida para el  presente trabajo:  Fortalezas  - Flexibilidad para adecuar el modelo acorde a las necesidades del contexto y objeto a  evaluar  - Gradualidad en las decisiones. Esto permite dividir la tarea, ordenarla y concentrarse  en cada momento en los aspectos propios de cada capa.   - Especificidad en procesos educativos mediados por tecnología digital. El  modelo  tiene en cuenta aspectos directamente relacionados con el ámbito educativo, y  particularmente, aborda el análisis de modalidades educativas híbridas o a distancia.   - Pertinencia para la evaluación de calidad. La aplicación del modelo completo  permite dar cuenta de una variedad de cuestiones que hacen a la calidad del objeto  evaluado.  Debilidades  -Variedad de indicadores (cualitativos y cuantitativos), lo cual puede dificultar la  definición de medidas para determinar calidad, si el evaluador desconoce cómo medir  cada uno.  - Falta de definición del modelo respecto de qué medidas es conveniente adoptar en  cada caso. Sólo se presentan un conjunto de indicadores para cada criterio, pero este  conjunto puede no abarcar algunas situaciones importantes para el objeto a evaluar,  que quedan en manos de quienes lleven adelante la evaluación. Esto puede provocar,  omisiones importantes en la evaluación, si no se lo ajusta adecuadamente.  - Cantidad de recursos humanos involucrados. Llevar adelante el modelo completo,  implica un trabajo que involucra a diferentes recursos humanos. Desde quienes  abordan las decisiones para llevar adelante esta evaluación, hasta involucrar a los que  aportan información para realizarlo, de manera adecuada y lo más completa posible.  La calidad de los instrumentos utilizados para recoger información, influirán  notablemente en los resultados de la evaluación.  664 Este análisis  lleva a los autores a elaborar una propuesta superadora que propone  un prototipo de un sistema web que permitirá desarrollar el proceso de evaluación  propuesto por ECALEAD, considerando las fortalezas y debilidades observadas a  partir de la aplicación del modelo a las diferentes experiencias. De esta manera, se  propone acompañar el proceso propuesto  por ECALEAD a partir de un sistema que  ayude en la toma de decisiones, establecimiento de medidas, entre otros aspectos.  4 Características del prototipo  Se presenta aquí un prototipo de un sistema web cuyo objetivo principal es asistir al  evaluador en el análisis de calidad de procesos educativos mediados por tecnología.  Esto conlleva la definición de criterios, indicadores y medidas para cada uno de los  procesos contemplados en ECALEAD, conforme a lo detallado en cada una de las  capas de este modelo de evaluación. Este trabajo gradual que avanza a través de las  capas, de lo general a lo específico, estará directamente afectado por el rol y las  responsabilidades del evaluador como integrante de la institución educativa.   Se ha decidido que sea web, dado que esto permite aprovechar ventajas tales como  la portabilidad,  que facilita su ejecución desde cualquier computadora con conexión a  internet y su funcionalidad  independiente del sistema operativo instalado en el equipo  del evaluador, entre otras.    Por otra parte, se contemplan dos perfiles de trabajo: un perfil de usuario  administrador y otro de usuario evaluador. El usuario administrador se encargará de  definir los objetos de evaluación, los roles de evaluadores, los procesos, criterios e  indicadores que contempla ECALEAD. Este perfil no se aborda en la presentación del  trabajo. En cuánto al usuario evaluador, el prototipo permite, de acuerdo al rol  asignado (docente, directivo, etc.), armar un plan de evaluación, a través de un  esquema que se genera gradualmente, al que van agregando aquellos procesos,  criterios, indicadores y medidas que el evaluador considere de interés, acorde al  contexto y la intención de la evaluación.   El diseño del prototipo presenta dos áreas principales para organizar el plan de  evaluación:  - Área Superior que está orientada a asistir al evaluador respecto de la capa de  evaluación en la que trabaja y las tareas involucradas. Está compuesta por solapas  que al mismo tiempo sirven como elementos de navegación.    - Área Inferior donde se presentan los elementos que el evaluador va  seleccionando a lo largo del proceso. En todo momento podrá ver el avance del  plan de evaluación a través del esquema generado en esta área.  A continuación, se presentan sintéticamente las distintas etapas que el prototipo  propone para el armado del plan de evaluación.   4.1 Plan de evaluación en el prototipo  El plan de evaluación se organiza de modo que el evaluador avanza a través de las  distintas capas que propone ECALEAD, asistiéndolo en la recolección de los datos  665 para una experiencia concreta, y brindando la posibilidad de observar los resultados  finales de la evaluación. Los pasos a seguir pueden sintetizarse como:    Registrarse: el evaluador debe registrarse a través de un formulario y la institución le  asigna un rol específico. El sistema admite los siguientes roles: evaluador docente,  evaluador administrativo y/o evaluador directivo (Fig. 2). De acuerdo a este rol, se le  habilita la evaluación de determinados procesos, criterios e indicadores.      Fig. 2 Ingresa los datos de registración en el sistema     Seleccionar actividad: el evaluador debe seleccionar la actividad que llevará a cabo y  el objeto de la evaluación. Las opciones que se presentan son: a- Iniciar Evaluación,  b- Continuar Evaluación, c- Ver Resultados Evaluación (Fig. 3).  Tanto en la opción de Iniciar Evaluación como en la de Continuar Evaluación se  dispone de información sobre la capa de ECALEAD en la que se está trabajando, y  las tareas involucradas en dicha capa.   En Ver Resultados Evaluación se muestra el plan de evaluación completo,  distinguiendo  la situación particular de cada indicador, es decir, si ha alcanzado o no  la medida de calidad esperada.      Fig.3 Selecciona la actividad a realizar en el sistema    Crear Evaluación: el armado del plan de evaluación comienza por la Capa 1 según  ECALEAD. Se selecciona el objeto de la evaluación (curso, programa, carrera, etc).  El evaluador debe seleccionar los procesos a analizar. Para cada proceso establece la  relación con el conjunto de criterios que analizará. Las relaciones elegidas se agregan  a la tabla de procesos y criterios. Cada entrada en la tabla permite acceder al conjunto  de indicadores correspondientes (Fig.4). Es objetivo del sistema, que en un trabajo  666 futuro permita la incorporación de otros criterios y procesos no contemplados en  ECALEAD, al momento.      Fig. 4 Arma la tabla de procesos y criterios a analizar    Seleccionar los indicadores: en la Capa 2, el evaluador debe seleccionar los  indicadores para cada entrada de la tabla de procesos y criterios generada en la Capa  1.  El sistema permite incorporar nuevos indicadores que no aparecen descriptos y que  se consideren pertinentes. En esta etapa, el evaluador puede optar por una de las  siguientes acciones que siempre estarán presentes como elementos de navegación:   - Continuar que permite avanzar a la siguiente etapa (en este caso a la realización de  tareas correspondientes a la Capa 3)  - Anterior que permite ir a la etapa anterior. En este caso sería revisar las entradas de  la tabla de procesos y criterios. Esto puede conducir a otra selección de Indicadores.  - Salir que permite guardar el trabajo realizado hasta el momento y retomarlo  posteriormente, a través de la opción Ver Resultados Evaluación (Fig. 5).  En todos los casos, el evaluador también utiliza el área superior como herramienta  de navegación para ir a una determinada etapa.      Fig.5 Selecciona los indicadores apropiados, a criterio del evaluador    Seleccionar las medidas: en la Capa 3 se presentan los indicadores elegidos, y se  deben establecer las medidas más adecuadas al contexto y objeto de evaluación. En  los casos de indicadores cuantitativos, se permite el ingreso de valores  correspondientes a la cota inferior y superior del intervalo. En otros casos, se deberán  utilizar escalas cualitativas ya estandarizadas (disponibles en WebECALEAD) o  emplear la opción de incorporar una nueva escala. Luego, el evaluador puede optar  por ir a la etapa de Recolección de Datos (Fig. 6) que se explica a continuación.   667   Fig.6 Ingresa las medidas acordes a cada indicador seleccionado  Recolectar Datos: en esta etapa, se ingresan los valores para cada medida, obtenidos  a partir de los diferentes instrumentos de recogida de datos que se hayan utilizado. En  esta etapa, los valores debieran coincidir con aquellos de la escala seleccionada.  Como último paso puede optar por visualizar el plan de evaluación completo con el  estado de cada proceso o continuar con el armado del plan (Fig. 7).      Fig.7 Ingresa los valores obtenidos para cada indicador analizado  Ver Resultados Evaluación: aquí se observa el plan de evaluación completo. Se da  la posibilidad de elegir un determinado proceso de los que componen el plan y uno de  sus criterios. Esta acción despliega la lista de indicadores asignados a ese criterio, y se  muestra el estado de cada uno a partir del uso de colores de referencia: verde cuando  el indicador obtuvo un valor que cae dentro del rango de valores esperados, amarillo  si el indicador no ha sido evaluado aún o rojo si el indicador se evaluó  completamente, pero no alcanzó los resultados esperados (Fig. 8).     Fig. 8 Visualiza los resultados de la evaluación para un determinado proceso y criterio  668 4.2 Análisis de beneficios del prototipo  En esta sección se presentan los resultados de un primer análisis realizado por los  autores, en relación a las posibilidades que ofrece el prototipo, tomando en cuenta el  estudio previo de fortalezas y debilidades, ya realizado para ECALEAD. En trabajos  futuros se someterá el prototipo a evaluación por medio de juicio de expertos.  Respecto de las fortalezas de ECALEAD el prototipo las considera, y respeta, dado  que:  - adecua el modelo a las necesidades del contexto y objeto a evaluar.  Avanza  gradualmente en la toma de decisiones partiendo de los procesos y llegando a los  valores concretos para cada indicador, lo que permite dividir la tarea, ordenarla y  concentrarse en cada momento en los aspectos propios de cada capa.   - los procesos, criterios e indicadores contemplados posibilitan el análisis de  modalidades educativas híbridas o totalmente a distancia, a diferencia de otros  modelos que son aplicables a la modalidad presencial, y por lo tanto, dejan de lado  cuestiones propias de estas modalidades.   - la aplicación del modelo completo puede dar cuenta de todos los aspectos que  hacen a la calidad del objeto evaluado.  Por otra parte, el diseño del prototipo se focaliza en corregir algunas de las  debilidades de ECALEAD,  logrando:   - definir el conjunto de medidas convenientes a adoptar en cada caso  - adaptar el prototipo a la variedad de indicadores (cualitativos y cuantitativos),  facilitando la definición de medidas adecuadas a cada uno  - permitir la incorporación de nuevos indicadores no contemplados en las listas  sugeridas, de manera tal de incluir situaciones no especificadas por ECALEAD.  Al mismo tiempo se han encontrado funcionalidades beneficiosas que aporta el  prototipo tales como: la visualización del plan completo de evaluación, y su estado de  avance en la obtención de resultados, la posibilidad de integrar instrumentos para la  evaluación a través de los cuales se importen directamente los valores para cada  indicador, y la sistematización general del proceso de evaluación propuesto por  ECALEAD que ofrecerá un resguardo de la información pertinente.  5 Conclusiones y trabajos futuros  Se ha presentado el diseño de un prototipo web basado en el modelo ECALEAD  creado por los autores de este trabajo, que tiene como objetivo facilitar la tarea de  evaluación de los aspectos más destacados que intervienen en los procesos educativos  que incluyen el uso de tecnología digital.   El diseño presentado respeta las fortalezas de ECALEAD y corrige algunas de sus  debilidades, permitiendo completar el plan de evaluación acorde al rol y  responsabilidades del evaluador. Al mismo tiempo agrega las ventajas propias de un  sistema informático web que se vinculan con las posibilidades de acceso y  portabilidad, entre otras.  En cada una de las capas propuestas por ECALEAD, el evaluador puede revisar y  669 corregir algunas  decisiones tomadas en etapas anteriores, actualizando los aspectos  que se trabajan en ellas, es decir el proceso en el plan de evaluación es dinámico y  facilita la toma de decisiones.   La evaluación final muestra el plan de evaluación completo distinguiendo la  situación particular de cada proceso con sus correspondientes criterios e indicadores  analizados, de modo de revisar rápidamente cuales son los aspectos débiles  encontrados en la evaluación, cuáles faltan completar y cuáles son sus fortalezas en  relación al objeto de evaluación.  Como trabajo  a futuro se propone:  - la evaluación del diseño del prototipo presentado por medio de la técnica de juicio  de expertos de manera tal de evolucionar el prototipo a un sistema final   - la implementación del sistema web considerando decisiones de diseño para  respetar criterios de usabilidad  - la incorporación a WebECALEAD de otros procesos y/o criterios no  contemplados en el prototipo, y que pueden ser de utilidad para un evaluador  particular. Esto implicaría la incorporación de nuevos indicadores que se ajusten a  tales procesos y/o criterios  - la creación de instrumentos tales como encuestas, entrevistas, etc. que asistan al  evaluador en la recolección de indicadores cualitativos, permitiendo de esta  manera que los resultados obtenidos puedan incluirse en la propia base de datos  del sistema.  Estos son los primeros pasos en relación al diseño e implementación de  WebCALEAD, se espera en el corto plazo tener los primeros resultados de su  evaluación.	﻿modelo de evaluación , sistemas web para evaluación de procesos , calidad , educación	es	32436
