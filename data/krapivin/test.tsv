	sentences	label	title	language	index
0	Commercial recommender systems use various data mining techniques to make appropriate recommendations to users during online, real-time sessions. Published algorithms focus more on the discrete user ratings instead of binary results, which hampers their predictive capabilities when usage data is sparse. The system proposed in this paper, e-VZpro, is an association mining-based recommender tool designed to overcome these problems through a two-phase approach. In the first phase, batches of customer historical data are analyzed through association mining in order to determine the association rules for the second phase. During the second phase, a scoring algorithm is used to rank the recommendations online for the customer. The second phase differs from the traditional approach and an empirical comparison between the methods used in e-VZpro and other collaborative filtering methods including dependency networks, item-based, and association mining is provided in this paper. This comparison evaluates the algorithms used in each of the above methods using two internal customer datasets and a benchmark dataset. The results of this comparison clearly show that e-VZpro performs well compared to dependency networks and association mining. In general, item-based algorithms with cosine similarity measures have the best performance.	customer relationship management , collaborative filtering , association mining , recommender systems , e-commerce , dependency networks	Enhancing Product Recommender Systems on Sparse Binary Data.	en	0
1	We give a formal semantics for a highly expressive language for representing temporal relationships and events. This language, which we call Versatile Event Logic (VEL), provides a general temporal ontology and semantics encompassing many other representations. The system incorporates a number of features that have not been widely employed in AI formalisms. It has the ability to describe alternative histories using a modal operator. It provides a semantics for individuals that explicitly models their identity through time and across alternative possible histories; and enables one to distinguish between necessary and extensional identity of individuals. In virtue of its treatment of individuals and count nouns, the formalism offers a solution to certain puzzles of identity, which arise when individuals are described in different ways. We propose that VEL can be used as a foundational interlingua for comparing and interfacing different AI languages and illustrate this by considering how Situation Calculus and Event Calculus can be represented within VEL.	semantics , temporal logic	A unifying semantics for time and events.	en	1
2	The work described in this report is motivated by the desire to test the expressive possibilities of action language C+. The Causal Calculator (CCALC) is a system that answers queries about action domains described in a fragment of that language. The Zoo World and the Traffic World have been proposed by Erik Sandewall in his Logic Modelling Workshop--an environment for communicating axiomatizations of action domains of nontrivial size.The Zoo World consists of several cages and the exterior, gates between them, and animals of several species, including humans. Actions in this domain include moving within and between cages, opening and closing gates, and mounting and riding animals. The Traffic World includes vehicles moving continuously between road crossings subject to a number of restrictions, such as speed limits and keeping a fixed safety distance away from other vehicles on the road. We show how to represent the two domains in the input language of CCALC, and how to use CCALC to test these representations.	action languages , knowledge representation , reasoning about actions , commonsense reasoning	Representing the zoo world and the traffic world in the language of the causal calculator.	en	2
3	We describe a logic-based AI architecture based on Brooks' subsumption architecture. In this architecture, we axiomatize different layers of control in First-Order Logic (FOL) and use independent theorem provers to derive each layer's outputs given its inputs. We implement the subsumption of lower layers by higher layers using nonmonotonic reasoning principles. In particular, we use circumscription to make default assumptions in lower layers, and nonmonotonically retract those assumptions when higher layers draw new conclusions. We also give formal semantics to our approach. Finally, we describe layers designed for the task of robot control and a system that we have implemented that uses this architecture for the control of a Nomad 200 mobile robot.Our system combines the virtues of using the represent-and-reason paradigm and the behavioral-decomposition paradigm. It allows multiple goals to be serviced simultaneously and reactively. It also allows high-level tasks and is tolerant to different changes and elaborations of its knowledge in runtime. Finally, it allows us to give more commonsense knowledge to robots. We report on several experiments that empirically show the feasibility of using fully expressive FOL theorem provers for robot control with our architecture and the benefits claimed above.	cognitive robotics , subsumption architecture , logical representations	Logic-based subsumption architecture.	en	3
4	This article proves the conjecture of Thomas that, for every graph G, there is an integer k such that every graph with no minor isomorphic to G has a 2-coloring of either its vertices or its edges where each color induces a graph of tree-widlh at most k. Some generalizations are also proved.	vertex partitions , tree-width , edge partitions , small components	Excluding any graph as a minor allows a low tree-width 2-coloring.	en	4
5	The performance of IP networks depends on a wide variety of dynamic conditions. Traffic shifts, equipment failures, planned maintenance, and topology changes in other parts of the Internet can all degrade performance. To maintain good performance, network operators must continually reconfigure the routing protocols. Operators configure BGP to control how traffic flows to neighboring Autonomous Systems (ASes), as well as how traffic traverses their networks. However, because BGP route selection is distributed, indirectly controlled by configurable policies, and influenced by complex interactions with intradomain routing protocols, operators cannot predict how a particular BGP configuration would behave in practice. To avoid inadvertently degrading network performance, operators need to evaluate the effects of configuration changes before deploying them on a live network. We propose an algorithm that computes the outcome of the BGP route selection process for each router in a single AS, given only a static snapshot of the network state, without simulating the complex details of BGP message passing. We describe a BGP emulator based on this algorithm; the emulator exploits the unique characteristics of routing data to reduce computational overhead. Using data from a large ISP, we show that the emulator correctly computes BGP routing decisions and has a running time that is acceptable for many tasks, such as traffic engineering and capacity planning.	routing , modeling , BGP , traffic engineering	A model of BGP routing for network engineering.	en	5
6	We present a collective approach to learning a Bayesian network from distributed heterogeneous data. In this approach, we first learn a local Bayesian network at each site using the local data. Then each site identifies the observations that are most likely to be evidence of coupling between local and non-local variables and transmits a subset of these observations to a central site. Another Bayesian network is learnt at the central site using the data transmitted from the local site. The local and central Bayesian networks are combined to obtain a collective Bayesian network, which models the entire data. Experimental results and theoretical justification that demonstrate the feasibility of our approach are presented.	distributed data mining , bayesian network , heterogeneous data , web log mining , collective data mining	Collective Mining of Bayesian Networks from Distributed Heterogeneous Data.	en	6
7	Measuring the information retrieval effectiveness of World Wide Web search engines is costly because of human relevance judgments involved. However, both for business enterprises and people it is important to know the most effective Web search engines, since such search engines help their users find higher number of relevant Web pages with less effort. Furthermore, this information can be used for several practical purposes. In this study we introduce automatic Web search engine evaluation method as an efficient and effective assessment tool of such systems. The experiments based on eight Web search engines, 25 queries, and binary user relevance judgments show that our method provides results consistent with human-based evaluations. It is shown that the observed consistencies are statistically significant. This indicates that the new method can be successfully used in the evaluation of Web search engines.	information retrieval , world wide web , search engine , performance	Automatic performance evaluation of web search engines.	en	7
8	We present two algorithms for the problem of finding a minimum transversal in a hypergraph of rank 3, also known as the 3-Hitting Set problem. This problem is a natural extension of the vertex cover problem for ordinary graphs. The first algorithm runs in time O(1.6538n) for a hypergraph with n vertices, and needs polynomial space. The second algorithm uses exponential space and runs in time O(1.6316n).	minimum transversal , exact algorithm , hypergraph , 3-hitting set	Exact algorithms for finding minimum transversals in rank-3 hypergraphs.	en	8
9	A prevailing feature of mobile telephony systems is that the cell where a mobile user is located may be unknown. Therefore, when the system is to establish a call between users, it may need to search, or page, all the cells that it suspects the users are located in, to find the cells where the users currently reside. The search consumes expensive wireless links and so it is desirable to develop search techniques that page as few cells as possible.We consider cellular systems with c cells and m mobile users roaming among the cells. The location of the users is uncertain as given by m probability distribution vectors. Whenever the system needs to find specific users, it conducts a search operation lasting some number of rounds (the delay constraint). In each round, the system may check an arbitrary subset of cells to see which users are located there. In this setting the problem of finding one user with minimum expected number of cells searched is known to be solved optimally in polynomial time.In this paper we address the problem of finding several users with the same optimization goal. This task is motivated by the problem of establishing a conference call between mobile users. We first show that the problem is NP-hard. Then we prove that a natural heuristic is an e/(e - 1 )-approximation solution.	approximation algorithms , NP-hardness , conference call , location management , convex optimization	Establishing wireless conference calls under delay constraints.	en	9
10	This paper introduces a class of linear programming examples that cause the simplex method to cycle and that are the simplest possible examples showing this behaviour. The structure of examples from this class repeats after two iterations. Cycling is shown to occur for both the most negative reduced cost and steepest-edge column selection criteria. In addition it is shown that the expand anti-cycling procedure of Gill et al. is not guaranteed to prevent cycling.	degeneracy , simplex method , expand , cycling	The simplest examples where the simplex method cycles and conditions where expand fails to prevent cycling.	en	10
11	Continuous testing uses excess cycles on a developer's workstation to continuously run regression tests in the background, providing rapid feedback about test failures as source code is edited. It is intended to reduce the time and energy required to keep code well-tested and prevent regression errors from persisting uncaught for long periods of time. This paper reports on a controlled human experiment to evaluate whether students using continuous testing are more successful in completing programming assignments. We also summarize users' subjective impressions and discuss why the results may generalize.The experiment indicates that the tool has a statistically significant effect on success in completing a programming task, but no such effect on time worked. Participants using continuous testing were three times more likely to complete the task before the deadline than those without. Participants using continuous compilation were twice as likely to complete the task, providing empirical support to a common feature in modern development environments. Most participants found continuous testing to be useful and believed that it helped them write better code faster, and 90% would recommend the tool to others. The participants did not find the tool distracting, and intuitively developed ways of incorporating the feedback into their workflow.	continuous testing , continuous compilation , unit testing , test-first development	An experimental evaluation of continuous testing during development.	en	11
12	We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java TreeMap library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.	symbolic execution , coverage , testing object-oriented programs , model checking , red-black trees	Test input generation with java PathFinder.	en	12
13	A multi-mode software system contains several distinct modes of operation and a controller for deciding when to switch between modes. Even when developers rigorously test a multi-mode system before deployment, they cannot foresee and test for every possible usage scenario. As a result, unexpected situations in which the program fails or underperforms (for example, by choosing a non-optimal mode) may arise. This research aims to mitigate such problems by creating a new mode selector that examines the current situation, then chooses a mode that has been successful in the past, in situations like the current one. The technique, called program steering, creates a new mode selector via machine learning from good behavior in testing or in successful operation. Such a strategy, which generalizes the knowledge that a programmer has built into the system, may select an appropriate mode even when the original controller cannot. We have performed experiments on robot control programs written in a month-long programming competition. Augmenting these programs via our program steering technique had a substantial positive effect on their performance in new environments.	program steering , adaptability , mode selection , multi-mode systems	Improving the adaptability of multi-mode systems via program steering.	en	13
14	The use of XML as the de facto data exchange standard has allowed integration of heterogeneous web based software systems regardless of implementation platforms and programming languages. On the other hand, the rich tree-structured data representation, and the expressive XML query languages (such as XPath) make formal specification and verification of software systems that manipulate XML data a challenge. In this paper, we present our initial efforts in automated verification of XML data manipulation operations using the SPIN model checker. We present algorithms for translating (bounded) XML data and XPath expressions to Promela, the input language of SPIN. The techniques presented in this paper constitute the basis of our Web Service Analysis Tool (WSAT) which verifies LTL properties of composite web services.	model checking , MSL , XPath , XML , promela , SPIN , XML schema , web service	Model checking XML manipulating software.	en	14
15	Clustering is a fundamental problem in unsupervised learning, and has been studied widely both as a problem of learning mixture models and as an optimization problem. In this paper, we study clustering with respect to the k-median objective function, a natural formulation of clustering in which we attempt to minimize the average distance to cluster centers. One of the main contributions of this paper is a simple but powerful sampling technique that we call successive sampling that could be of independent interest. We show that our sampling procedure can rapidly identify a small set of points (of size just O(k log \frac{n}{k})) that summarize the input points for the purpose of clustering. Using successive sampling, we develop an algorithm for the k-median problem that runs in O(nk) time for a wide range of values of k and is guaranteed, with high probability, to return a solution with cost at most a constant factor times optimal. We also establish a lower bound of (nk) on any randomized constant-factor approximation algorithm for the k-median problem that succeeds with even a negligible (say \frac{1}{100}) probability. The best previous upper bound for the problem was (nk), where the -notation hides polylogarithmic factors in n and k. The best previous lower bound of (nk) applied only to deterministic k-median algorithms. While we focus our presentation on the k-median objective, all our upper bounds are valid for the k-means objective as well. In this context our algorithm compares favorably to the widely used k-means heuristic, which requires O(nk) time for just one iteration and provides no useful approximation guarantees.	k-median , discrete location theory , approximation algorithms , unsupervised clustering , k-means	Optimal Time Bounds for Approximate Clustering.	en	15
16	We consider the following clustering problem: we have a complete graph on n vertices (items), where each edge (u, v) is labeled either + or  depending on whether u and v have been deemed to be similar or different. The goal is to produce a partition of the vertices (a clustering) that agrees as much as possible with the edge labels. That is, we want a clustering that maximizes the number of + edges within clusters, plus the number of  edges between clusters (equivalently, minimizes the number of disagreements: the number of  edges inside clusters plus the number of + edges between clusters). This formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data, and the goal is to partition the current set of documents in a way that correlates with f as much as possible&semi; it can also be viewed as a kind of agnostic learning problem.An interesting feature of this clustering formulation is that one does not need to specify the number of clusters k as a separate parameter, as in measures such as k-median or min-sum or min-max clustering. Instead, in our formulation, the optimal number of clusters could be any value between 1 and n, depending on the edge labels. We look at approximation algorithms for both minimizing disagreements and for maximizing agreements. For minimizing disagreements, we give a constant factor approximation. For maximizing agreements we give a PTAS, building on ideas of Goldreich, Goldwasser, and Ron (1998) and de la Veg (1996). We also show how to extend some of these results to graphs with edge labels in [1, +1], and give some results for the case of random noise.	document classification , clustering , approximation algorithm	Correlation Clustering.	en	16
17	One of the central problems in information retrieval, data mining, computational biology, statistical analysis, computer vision, geographic analysis, pattern recognition, distributed protocols is the question of classification of data according to some clustering rule. Often the data is noisy and even approximate classification is of extreme importance. The difficulty of such classification stems from the fact that usually the data has many incomparable attributes, and often results in the question of clustering problems in high dimensional spaces. Since they require measuring distance between every pair of data points, standard algorithms for computing the exact clustering solutions use quadratic or nearly quadratic running time&semi; i.e., O(dn2(d)) time where n is the number of data points, d is the dimension of the space and (d) approaches 0 as d grows. In this paper, we show (for three fairly natural clustering rules) that computing an approximate solution can be done much more efficiently. More specifically, for agglomerative clustering (used, for example, in the Alta Vista search engine), for the clustering defined by sparse partitions, and for a clustering based on minimum spanning trees we derive randomized (1 approximation algorithms with running times (d2 n2) where  > 0 depends only on the approximation parameter &epsi; and is independent of the dimension d.	sparse partitions , high dimensional spaces , graph-theoretic clustering	Subquadratic Approximation Algorithms for Clustering Problems in High Dimensional Spaces.	en	17
18	A procedure is presented to untangle unstructured 2D meshes containing inverted elements by node repositioning. The inverted elements may result from node movement in Arbitrary Lagrangian Eulerian (ALE) simulations of continuum mechanics problems with large shear deformation such as fluid flow and metal forming. Meshes with inverted elements may also be created due to the limitations of mesh generation algorithms particularly for nonsimplicial mesh generation. The untangling procedure uses a combination of direct node placement based on geometric computation of the feasible set, and node repositioning driven by numerical optimization of an objective function that achieves its minimum on a valid mesh. It is shown that a combination of the feasible set, based method and the optimization method achieves the best results in untangling complex 2D meshes. Preliminary results are also presented for untangling of 3D unstructured meshes by the same approach.	element validity , arbitrary Lagrangian-Eulerian ALE simulations , mesh untangling , rayleigh taylor simulation , triangles , quadrilaterals	Untangling of 2D meshes in ALE simulations.	en	18
19	"We address a hierarchical generalization of the well-known disk paging problem. In the hierarchical cooperative caching problem, a set of n machines residing in an ultrametric space cooperate with one another to satisfy a sequence of read requests to a collection of (read-only) files. A seminal result in the area of competitive analysis states that LRU (the widely-used deterministic online paging algorithm based on the ""least recently used"" eviction policy) is constant-competitive if it is given a constant-factor blowup in capacity over the offline algorithm. Does such a constant-competitive deterministic algorithm (with a constant-factor blowup in the machine capacities) exist for the hierarchical cooperative caching problem? The main contribution of the present paper is to answer this question in the negative. More specifically, we establish an (log log n) lower bound on the competitive ratio of any online hierarchical cooperative caching algorithm with capacity blowup O((log n)1-), where  denotes an arbitrarily small positive constant."	online computation , hierarchical cooperative caching	Online hierarchical cooperative caching.	en	19
20	This paper is devoted to the proof, applications, and generalisation of a theorem, due to Bird and de Moor, that gave conditions under which a total function can be expressed as a relational fold. The theorem is illustrated with three problems, all dealing with constructing trees with various properties. It is then generalised to give conditions under which the inverse of a partial function can be expressed as a relational hylomorphism. Its proof makes use of Doornbos and Backhouse's theory on well-foundedness and reductivity. Possible applications of the generalised theorem is discussed.	program derivation , program inversion	Theory and applications of inverting functions as folds.	en	20
21	We consider the solution of the linear system real values of . This family of shifted systems arises, for example, in Tikhonov regularization and computations in lattice quantum chromodynamics. For each single shift  this system can be solved using the conjugate gradient method for least squares problems (CGLS). In literature various implementations of the, so-called, multishift CGLS methods have been proposed. These methods are mathematically equivalent to applying the CGLS method to each shifted system separately but they solve all systems simultaneously and require only two matrix-vector products (one by A and one by AT) and two inner products per iteration step. Unfortunately, numerical experiments show that, due to roundoff errors, in some cases these implementations of the multishift CGLS method can only attain an accuracy that depends on the square of condition number of the matrix A. In this paper we will argue that, in the multishift CGLS method, the impact on the attainable accuracy of rounding errors in the Lanczos part of the method is independent of the effect of roundoff errors made in the construction of the iterates. By making suitable design choices for both parts, we derive a new (and efficient) implementation that tries to remove the limitation of previous proposals. A partial roundoff error analysis and various numerical experiments show promising results.	finite precision arithmetic , tikhonov regularization , iterative methods , shifted systems , accuracy	Accurate conjugate gradient methods for families of shifted systems.	en	21
22	A matrix-free algorithm, IRLANB, for the efficient computation of the smallest singular triplets of large and possibly sparse matrices is described. Key characteristics of the approach are its use of Lanczos bidiagonalization, implicit restarting, and harmonic Ritz values. The algorithm also uses a deflation strategy that can be applied directly on Lanczos bidiagonalization. A refinement postprocessing phase is applied to the converged singular vectors. The computational costs of the above techniques are kept small as they make direct use of the bidiagonal form obtained in the course of the Lanczos factorization. Several numerical experiments with the method are presented that illustrate its effectiveness and indicate that it performs well compared to existing codes.	deflation , implicit restarting , harmonic Ritz values , pseudospectrum , refined singular vectors , lanczos bidiagonalization	Computing smallest singular triplets with implicitly restarted Lanczos bidiagonalization.	en	22
23	Linear temporal logic (LTL) has been widely used for specification and verification of reactive systems. Its standard model is sequences of states (or state transitions), and formulas describe sequencing of state transitions. When LTL is used to model real-time systems, a state is extended with a time stamp to record when a state transition takes place. Duration calculus (DC) is another well studied approach for real-time systems development. DC models behaviours of a system by functions from the domain of reals representing time to the system states. This paper extends this time domain to the Cartesian product of the real and the natural numbers. With the extended time domain, we provide the chop modality with a non-overlapping interpretation. This allows some linear temporal operators explicitly dealing with the discrete dimension of time to be derivable from the chop modality in essentially the same way that their continuous-time counterparts are in the classical DC. This provides a nice embedding of some timed LTL (TLTL) modalities into DC to unify the methods from DC and LTL for real-time systems development: Requirements and high level design decisions are interval properties and are therefore specified and reasoned about in DC, while properties of an implementation, as well as the refinement relation between two implementations, are specified and verified compositionally and inductively in LTL. Implementation properties are related to requirement and design properties by rules for lifting LTL formulas to DC formulas.	design , refinement , real-time , specification , verification	Unifying proof methodologies of duration calculus and timed linear temporal logic.	en	23
24	The unit price seat reservation problem is investigated. The seat reservation problem is the problem of assigning seat numbers on-line to requests for reservations in a train traveling through k stations. We are considering the version where all tickets have the same price and where requests are treated fairly, that is, a request which can be fulfilled must be granted.For fair deterministic algorithms, we provide an asymptotically matching upper bound to the existing lower bound which states that all fair algorithms for this problem are -competitive on accommodating sequences, when there are at least three seats.Additionally, we give an asymptotic upper bound of 7/9 for fair randomized algorithms against oblivious adversaries.We also examine concrete on-line algorithms, First-Fit and Random for the special case of two seats. Tight analyses of their performance are given.	accomodating sequences , competitive ratio , seat reservation problem , on-line algorithms	Tight bounds on the competitive ratio on accommodating sequences for the seat reservation problem.	en	24
25	When admission control is used, an on-line scheduler chooses whether or not to complete each individual job successfully by its deadline. An important consideration is at what point in time the scheduler determines if a job request will be satisfied, and thus at what point the scheduler is able to provide notification to the job owner as to the fate of the request. In the loosest model, often seen in real-time systems, such a decision can be deferred up until the job's deadline passes. In the strictest model, more suitable for customer-based applications, a scheduler would be required to give notification at the instant that a job request arrives.Unfortunately there seems to be little existing research which explicitly studies the effect of the notification model on the performance guarantees of a scheduler. We undertake such a study by reexamining a problem from the literature. Specifically, we study the effect of the notification model on the non-preemptive scheduling of a single resource in order to maximize utilization. At first glance, it appears severely more restrictive to compare a scheduler required to give immediate notification to one which need not give any notification. Yet we are able to present alternate algorithms which provide immediate notification, while matching most of the performance guarantees which are possible by schedulers which provide no such notification. In only one case are we able to give evidence that providing immediate notification may be more difficult.	admission control , notification , online scheduling , firm deadlines	Admission control with immediate notification.	en	25
26	This paper studies the problem of electing a small number of representatives (council) out of a (possible large) group of anonymous candidates. The problem arises in scenarios such as multicast where, to avoid feedback implosion, a small subset of the receivers is chosen to provide feedback on network conditions.We present several algorithms for this problem and analyze the expected number of messages and rounds required for their convergence. In particular, we present an algorithm that almost always converges in one round using a small number of messages (for typical council size) when the number of hosts is known. In the case where the number of hosts is unknown (and too large to be polled), our algorithms converge in a small number of rounds that improves previous results by Bolot et al. (1994).	multicast , leader election	Distributed council election.	en	26
27	Recently Victor Shoup noted that there is a gap in the widely believed security result of OAEP against adaptive chosen-ciphertext attacks. Moreover, he showed that, presumably, OAEP cannot be proven secure from the one-wayness of the underlying trapdoor permutation. This paper establishes another result on the security of OAEP. It proves that OAEP offers semantic security against adaptive chosen-ciphertext attacks, in the random oracle model, under the partial-domain one-wayness of the underlying permutation. Therefore, this uses a formally stronger assumption. Nevertheless, since partial-domain one-wayness of the RSA function is equivalent to its (full-domain) onewayness, it follows that the security of RSA-OAEP can actually be proven under the sole RSA assumption, although the reduction is not tight.	provable security , public-key encryption , OAEP , RSA	RSA-OAEP Is Secure under the RSA Assumption.	en	27
28	Most previous work on the recently developed language-modeling approach to information retrieval focuses on document-specific characteristics, and therefore does not take into account the structure of the surrounding corpus. We propose a novel algorithmic framework in which information provided by document-based language models is enhanced by the incorporation of information drawn from clusters of similar documents. Using this framework, we develop a suite of new algorithms. Even the simplest typically outperforms the standard language-modeling approach in precision and recall, and our new interpolation algorithm posts statistically significant improvements for both metrics over all three corpora tested.	clustering , language modeling , interpolation model , cluster-based language models , smoothing , aspect models	Corpus structure, language models, and ad hoc information retrieval.	en	28
29	Enumerating all solutions of a relational algebra equation is a natural and powerful operation which, when added as a query language primitive to the nested relational algebra, yields a query language for nested relational databases, equivalent to the well-known powerset algebra.  We study sparse equations, which are equations with at most polynomially many solutions.  We look at their complexity and compare their expressive power with that of similar notions in the powerset algebra.	sparse expression , parity , equation , fagin's theorem , relational algebra , nested relation	Solving Equations in the Relational Algebra.	en	29
30	We give a new proof showing that it is NP-hard to color a 3-colorable graph using just 4 colors. This result is already known , [S. Khanna, N. Linial, and S. Safra, Combinatorica, 20 (2000), pp. 393--415], but our proof is novel because it does not rely on the PCP theorem, while the known one does. This highlights a qualitative difference between the known hardness result for coloring 3-colorable graphs and the factor $n^{\epsilon}$ hardness for approximating the chromatic number of general graphs, as the latter result is known to imply (some form of) PCP theorem  [M. Bellare, O. Goldreich, and M. Sudan, SIAM J. Comput., 27 (1998), pp. 805--915].Another aspect in which our proof is novel is in its use of the PCP theorem to show that 4-coloring of 3-colorable graphs remains NP-hard even on bounded-degree graphs (this hardness result does not seem to follow from the earlier reduction of Khanna, Linial, and Safra). We point out that such graphs can always be colored using O(1) colors by a simple greedy algorithm, while the best known algorithm for coloring (general) 3-colorable graphs requires $n^{\Omega(1)}$ colors. Our proof technique also shows that there is an $\varepsilon_0 > 0$ such that it is NP-hard to legally 4-color even a $(1-\varepsilon_0)$ fraction of the edges of a 3-colorable graph.	NP-hardness , graph coloring , hardness of approximation , PCP theorem	On the Hardness of 4-Coloring a 3-Colorable Graph.	en	30
31	"Many data analysis tasks can be viewed as search or mining in a multidimensional space (MDS). In such MDSs, dimensions capture potentially important factors for given applications, and cells represent combinations of values for the factors. To systematically analyze data in MDS, an interesting notion, called ""cubegrade was recently introduced by Imielinski et al. [CHECK END OF SENTENCE], which focuses on the notable changes in measures in MDS by comparing a cell (which we refer to as probe cell) with its gradient cells, namely, its ancestors, descendants, and siblings. We call such queries gradient analysis queries (GQs). Since an MDS can contain billions of cells, it is important to answer GQs efficiently. In this study, we focus on developing efficient methods for mining GQs constrained by certain (weakly) antimonotone constraints. Instead of conducting an independent gradient-cell search once per probe cell, which is inefficient due to much repeated work, we propose an efficient algorithm, LiveSet-Driven. This algorithm finds all good gradient-probe cell pairs in one search pass. It utilizes measure-value analysis and dimension-match analysis in a set-oriented manner, to achieve bidirectional pruning between the sets of hopeful probe cells and of hopeful gradient cells. Moreover, it adopts a hypertree structure and an H-cubing method to compress data and to maximize sharing of computation. Our performance study shows that this algorithm is efficient and scalable. In addition to data cubes, we extend our study to another important scenario: mining constrained gradients in transactional databases where each item is associated with some measures such as price. Such transactional databases can be viewed as sparse MDSs where items represent dimensions, although they have significantly different characteristics than data cubes. We outline efficient mining methods for this problem in this paper."	data mining , antimonotonicity , constraint-based pruning , dimension-based pruning , complex measures , iceberg query , gradient analysis , data cube	Mining Constrained Gradients in Large Databases.	en	31
32	In this paper, we describe a tool to verify Erlang programs and show, by means of an industrial case study, how this tool is used. The tool includes a number of components, including a translation component, a state space generation component and a model checking component. To verify properties of the code, the tool first translates the Erlang code into a process algebraic specification. The outcome of the translation is made more efficient by taking advantage of the fact that software written in Erlang builds upon software design patterns such as clientserver behaviours. A labelled transition system is constructed from the specification by use of the CRL toolset. The resulting labelled transition system is model checked against a set of properties formulated in the -calculus using the Caesar/Aldbaran toolset.As a case study we focus on a simplified resource manager modelled on a real implementation in the control software of the AXD 301 ATM switch. Some of the key properties we verified for the program are mutual exclusion and non-starvation. Since the toolset supports only the regular alternation-free -calculus, some ingenuity is needed for checking the liveness property non-starvation. The case study has been refined step by step to provide more functionality, with each step motivated by a corresponding formal verification using model checking .	software verification , formal methods , functional programming , erlang , model checking	Development of a verified Erlang program for resource locking.	en	32
33	It is well known that one can build models of full higher-order dependent-type theory (also called the calculus of constructions) using partial equivalence relations (PERs) and assemblies over a partial combinatory algebra. But the idea of categories of PERs and ERs (total equivalence relations) can be applied to other structures as well. In particular, we can easily define the category of ERs and equivalence-preserving continuous mappings over the standard category Top0 of topological T0-spaces; we call these spaces (a topological space together with an ER) equilogical spaces and the resulting category Equ. We show that this category--in contradistinction to Top0--is a cartesian closed category. The direct proof outlined here uses the equivalence of the category Equ to the category PEqu of PERs over algebraic lattices (a full subcategory of Top0 that is well known to be cartesian closed from domain theory). In another paper with Carboni and Rosolini (cited herein), a more abstract categorical generalization shows why many such categories are cartesian closed. The category Equ obviously contains Top0 as a full subcategory, and it naturally contains many other well known subcategories. In particular, we show why, as a consequence of work of Ershov, Berger, and others, the Kleene-Kreisel hierarchy of countable functionals of finite types can be naturally constructed in Equ from the natural numbers object N by repeated use in Equ of exponentiation and binary products. We also develop for Equ notions of modest sets (a category equivalent to Equ) and assemblies to explain why a model of dependent type theory is obtained. We make some comparisons of this model to other, known models.	realizability , domain theory , type theory , logic , topology	Equilogical spaces.	en	33
34	Two-level languages incorporate binding time information inside types, that is, whether a piece of code is completely known at compile-time, or needs some more inputs and can be evaluated only at run-time. We consider the use of 2-level languages in the framework of partial evaluation, and use a 2-level version of the simply typed lambda calculus with recursion. We give an operational semantics, an equational theory and a denotational semantics, that give an account of the distinction between compilation and execution phases. An adequacy theorem is given to relate the two semantics, showing in particular how they agree on non-termination at compile time. We finally give a more refined model using functor categories.	program generation , partial evaluation , semantics	Two-level languages for program optimization.	en	34
35	We describe a method for constructing models of linear logic based on the category of sets and relations. The resulting categories are non-degenerate in general; in particular they are not compact closed nor do they have biproducts. The construction is simple, lifting the structure of a poset to the new category. The underlying poset thus controls the structure of this category, and different posets give rise to differently-flavoured models. As a result, this technique allows the construction of models for both, intuitionistic or classical linear logic as desired. A number of well-known models, for example coherence spaces and hypercoherences, are instances of this method.	categorical models , linear logic	Poset-valued sets or how to build models for linear logics.	en	35
36	The main contribution of this paper is a formal characterization of recursive object specifications and their existence based on a denotational untyped semantics of the object calculus. Existence is not guaranteed but can be shown employing Pitts' results on relational properties of domains. The semantics can be used to analyse and verify Abadi and Leino's object logic but it also suggests extensions. For example, specifications of methods may not only refer to fields but also to methods of objects in the store. This can be achieved without compromising the existence theorem. An informal logic of predomains is in use intentionally in order to avoid any commitment to a particular syntax of specification logic.	domain theory , program verification , programming logic , object logic , denotational semantics	Semantics and logic of object calculi.	en	36
37	Graph-coloring register allocators eliminate copies by coalescing the source and target nodes of a copy if they do not interfere in the interference graph. Coalescing, however, can be harmful to the colorability of the graph because it tends to yield a graph with nodes of higher degrees. Unlike aggressive coalescing, which coalesces any pair of noninterfering copy-related nodes, conservative coalescing or iterated coalescing perform safe coalescing that preserves the colorability. Unfortunately, these heuristics give up coalescing too early, losing many opportunities for coalescing that would turn out to be safe. Moreover, they ignore the fact that coalescing may even improve the colorability of the graph by reducing the degree of neighbor nodes that are interfering with both the source and target nodes being coalesced. This article proposes a new heuristic called optimistic coalescing which optimistically performs aggressive coalescing, thus exploiting the positive impact of coalescing aggressively, but when a coalesced node is to be spilled, it is split back into separate nodes. Since there is a better chance of coloring one of those splits, we can reduce the overall spill amount.	noncopy coalescing , graph coloring , copy coalescing , register allocation	Optimistic register coalescing.	en	37
38	We explore the computational power of networks of small resource-limited mobile agents. We define two new models of computation based on pairwise interactions of finite-state agents in populations of finite but unbounded size. With a fairness condition on interactions, we define the concept of stable computation of a function or predicate, and give protocols that stably compute functions in a class including Boolean combinations of threshold-k, parity, majority, and simple arithmetic. We prove that all stably computable predicates are in NL. With uniform random sampling of pairs to interact, we define the model of conjugating automata and show that any counter machine with O(1) counters of capacity O(n) can be simulated with high probability by a protocol in a population of size n. We prove that all predicates computable with high probability in this model are in P  RL. Several open problems and promising future directions are discussed.	intermittent communication , sensor net , stable computation , finite-state agent , diffuse computation , mobile agent	Computation in networks of passively mobile finite-state sensors.	en	38
39	Group key exchange protocols allow a group of servers communicating over an asynchronous network of point-to-point links to establish a common key, such that an adversary which fully controls the network links (but not the group members) cannot learn the key. Currently known group key exchange protocols rely on the assumption that all group members participate in the protocol and if a single server crashes, then no server may terminate the protocol. In this paper, we propose the first purely asynchronous group key exchange protocol that tolerates a minority of servers to crash. Our solution uses a constant number of rounds, which makes it suitable for use in practice. Furthermore, we also investigate how to provide forward secrecy with respect to an adversary that may break into some servers and observe their internal state. We show that any group key exchange protocol among n servers that tolerates tc > 0 servers to crash can only provide forward secrecy if the adversary breaks into less than n - 2tc servers, and propose a group key exchange protocol that achieves this bound.	provable security , universal composability , group key exchange , group communication	Asynchronous group key exchange with failures.	en	39
40	We describe a method for computing the likelihood that a completion joining two contour fragments passes through any given position and orientation in the image plane. Like computations in primary visual cortex (and unlike all previous models of contour completion), the output of our computation is invariant under rotations and translations of the input pattern. This is achieved by representing the input, output, and intermediate states of the computation in a basis of shiftable-twistable functions.	visual cortex , fokker-planck equation , boundary completion , euclidean invariant computation , shiftable basis	Euclidean Group Invariant Computation of Stochastic Completion Fields Using Shiftable-Twistable Functions.	en	40
41	A metering scheme is a method by which an audit agency is able to measure the interaction between servers and clients during a certain number of time frames. Naor and Pinkas (Vol. 1403 of LNCS, pp. 576590) proposed metering schemes where any server is able to compute a proof (i.e., a value to be shown to the audit agency at the end of each time frame), if and only if it has been visited by a number of clients larger than or equal to some threshold h during the time frame. Masucci and Stinson (Vol. 1895 of LNCS, pp. 7287) showed how to construct a metering scheme realizing any access structure, where the access structure is the family of all subsets of clients which enable a server to compute its proof. They also provided lower bounds on the communication complexity of metering schemes. In this paper we describe a linear algebraic approach to design metering schemes realizing any access structure. Namely, given any access structure, we present a method to construct a metering scheme realizing it from any linear secret sharing scheme with the same access structure. Besides, we prove some properties about the relationship between metering schemes and secret sharing schemes. These properties provide some new bounds on the information distributed to clients and servers in a metering scheme. According to these bounds, the optimality of the metering schemes obtained by our method relies upon the optimality of the linear secret sharing schemes for the given access structure.	distributed audit , metering , secret sharing , cryptography , entropy	A Linear Algebraic Approach to Metering Schemes.	en	41
42	Motivated by formal models recently proposed in the context of XML, we study automata and logics on strings over infinite alphabets. These are conservative extensions of classical automata and logics defining the regular languages on finite alphabets. Specifically, we consider register and pebble automata, and extensions of first-order logic and monadic second-order logic. For each type of automaton we consider one-way and two-way variants, as well as deterministic, nondeterministic, and alternating control. We investigate the expressiveness and complexity of the automata and their connection to the logics, as well as standard decision problems. Some of our results answer open questions of Kaminski and Francez on register automata.	automata , expressiveness , first-order logic , monadic second-order logic , infinite alphabets , XML , registers , pebbles	Finite state machines for strings over infinite alphabets.	en	42
43	A metalogical framework is a logic with an associated methodology that is used to represent other logics and to reason about their metalogical properties. We propose that logical frameworks can be good metalogical frameworks when their theories always have initial models and they support reflective and parameterized reasoning.We develop this thesis both abstractly and concretely. Abstractly, we formalize our proposal as a set of requirements and explain how any logic satisfying these requirements can be used for metalogical reasoning. Concretely, we present membership equational logic as a particular metalogic that satisfies these requirements. Using membership equational logic, and its realization in the Maude system, we show how reflection can be used for different, nontrivial kinds of formal metatheoretic reasoning. In particular, one can prove metatheorems that relate theories or establish properties of parameterized classes of theories.	rewriting logic , metalogics , reflection , membership equational logic	Reflective metalogical frameworks.	en	43
44	We analyze preemptive on-line scheduling against randomized adversaries, with the goal to finish an unknown distinguished target job. Our motivation comes from clinical gene search projects, but the subject leads to general theoretical questions of independent interest, including some natural but unusual probabilistic models. We study problem versions with known and unknown processing times of jobs and target probabilities, and models where the on-line player gets some randomized extra information about the target. For some versions we get optimal competitive ratios, expressed in terms of given parameters of instances.	searching , bayesian models , on-line algorithms , randomized adversary	Scheduling Search Procedures.	en	44
45	We present here an implementation relation intended to formalise the notion that a system built of communicating processes is an acceptable implementation of another base, or target, system in the event that the two systems have different interfaces. Such a treatment has clear applicability in the software development process, where (the interface of) an implementation component may be expressed at a different level of abstraction to (the interface of) the relevant specification component.Technically, processes are formalised using Hoare's CSP language, with its standard failures-divergences model. The implementation relation is formulated in terms of failures and divergences of the implementation and target processes. Interface difference is modelled by endowing the implementation relation with parameters called extraction patterns. These are intended to interpret implementation behaviour as target behaviour, and suitably constrain the former in connection to well-formedness and deadlock properties.We extend the results of our previous work and replace implementation relations previously presented by a single, improved scheme. We also remove all the restrictions previously placed upon target processes. Two basic kinds of results are obtained: realisability and compositionality. The latter means that a target composed of several connected systems may be implemented by connecting their respective implementations. The former means that, if target and implementation in fact have the same interface, then the implementation relation they should satisfy collapses into standard implementation pre-order.We also show how to represent processes and extraction patterns in a manner amenable to computer implementation, and detail a graph-theoretic restatement of the conditions defining the implementation relation, from which algorithms for their automatic verification are easily derived.	communicating sequential processes , theory of parallel and distributed computation , refinement , behaviour abstraction , compositionality , verification	Relating communicating processes with different interfaces.	en	45
46	An information-theoretic model for steganography with a passive adversary is proposed. The adversary's task of distinguishing between an innocent cover message C and a modified message S containing hidden information is interpreted as a hypothesis testing problem. The security of a steganographic system is quantified in terms of the relative entropy (or discrimination) between the distributions of C and S, which yields bounds on the detection capability of any adversary. It is shown that secure steganographic schemes exist in this model provided the covertext distribution satisfies certain conditions. A universal stegosystem is presented in this model that needs no knowledge of the covertext distribution, except that it is generated from independently repeated experiments.	covert channel , universal data compression , perfect security , information hiding , one-time pad , subliminal channel	An information-theoretic model for steganography.	en	46
47	The implicit characterizations of the polynomial-time computable functions FP given by Bellantoni-Cook and Leivant suggest that this class is the complexity-theoretic analog of the primitive recursive functions. Hence, it is natural to add minimization operators to these characterizations and investigate the resulting class of partial functions as a candidate for the analog of the partial recursive functions. We do so in this paper for Cobham's definition of FP by bounded recursion and for Bellantoni-Cook's safe recursion and prove that the resulting classes capture exactly NPMV, the nondeterministic polynomial-time computable partial multifunctions. We also consider the relationship between our schemes and a notion of nondeterministic recursion defined by Leivant and show that the latter characterizes the total functions of NPMV. We view these results as giving evidence that NPMV is the appropriate analog of partial recursive. This view is reinforced by earlier results of Spreen and Stahl who show that for many of the relationships between partial recursive functions and r.e. sets, analogous relationships hold between NPMV and NP sets. Furthermore, since NPMV is obtained from FP in the same way as the recursive functions are obtained from the primitive recursive functions (when defined via function schemes), this also gives further evidence that FP is properly seen as playing the role of primitive recursion.	implicit computational complexity , safe recursion , minimization , non-deterministic partial multifunctions	Minimization and NP multifunctions.	en	47
48	We give a realizability model of Girard-Scedrov-Scott's Bounded Linear Logic (BLL). This gives a new proof that all numerical functions representable in that system are polytime. Our analysis naturally justifies the design of the BLL syntax and suggests further extensions.	complexity lambda calculus , finite model theory , linear logic	Realizability models for BLL-like languages.	en	48
49	A space-efficient algorithm is one in which the output is given in the same location as the input and only a small amount of additional memory is used by the algorithm. We describe four space-efficient algorithms for computing the convex hull of a planar point set.	convex hulls , in-place , in situ , computational geometry	Space-efficient planar convex hull algorithms.	en	49
50	This work stresses the fact that all current proposals for electronic voting schemes disclose the final tally of the votes. In certain situations, like jury voting, this may be undesirable. We present a robust and universally verifiable membership testing scheme (MTS) that allows, among other things, a collection of voters to cast votes and determine whether their tally belongs to some pre-specified small set (e.g., exceeds a given threshold)--our scheme discloses no additional information than that implied from the knowledge of such membership. We discuss several extensions of our basic MTS. All the constructions presented combine features of two parallel lines of research concerning electronic voting schemes, those based on MIX-networks and in homomorphic encryption.	election scheme , MIX-networks , majority voting , membership testing	Electronic jury voting protocols.	en	50
51	Sensing devices can be deployed to form a network for monitoring a region of interest. This chapter investigates the detection of a target in the region being monitored by using collaborative target detection algorithms among the sensors. The objective is to develop a low cost sensor deployment strategy to meet a performance criteria. A path exposure metric is proposed to measure the goodness of deployment. Exposure can be defined and efficiently computed for various target activities, targets traveling at variable speed and in the presence of obstacles in the region.Using exposure to evaluate the detection performance, the problem of random sensor deployment is formulated. The problem defines cost functions that take into account the cost of single sensors and the cost of deployment. A sequential sensor deployment approach is then developed. The chapter illustrates that the overall cost of deployment can be minimized to achieve a desired detection performance by appropriately choosing the number of sensors deployed in each step of the sequential deployment strategy.	exposure , sensor networks , deployment , collaborative target detection , value fusion	Detecting unauthorized activities using a sensor network.	en	51
52	"We present a construction of symmetry plane-groups for quasiperiodic point-sets named beta-lattices. The framework is issued from beta-integers counting systems. Beta-lattices are vector superpositions of beta-integers. When  > 1 is a quadratic Pisot-Vijayaraghavan algebraic unit, the set of beta-integers can be equipped with an abelian group structure and an internal multiplicative law. When we show that these arithmetic and algebraic structures lead to freely generated symmetry plane-groups for beta-lattices. These plane-groups are based on repetitions of discrete adapted rotations and translations we shall refer to as ""beta-rotations"" and ""beta-translations"". Hence beta-lattices, endowed with beta-rotations and beta-translations, can be viewed like lattices. The quasiperiodic function S(n), defined on the set of beta-integers as counting the number of small tiles between the origin and the nth beta-integer, plays a central part in these new group structures. In particular, this function behaves asymptotically like a linear function. As an interesting consequence, beta-lattices and their symmetries behave asymptotically like lattices and lattice symmetries, respectively."	plane groups , tilings , beta-lattices , quasicrystals , pisot numbers	Symmetry groups for beta-lattices.	en	52
53	Existing data mining algorithms on graphs look for nodes satisfying specific properties, such as specific notions of structural similarity or specific measures of link-based importance. While such analyses for predetermined properties can be effective in well-understood domains, sometimes identifying an appropriate property for analysis can be a challenge, and focusing on a single property may neglect other important aspects of the data. In this paper, we develop a foundation for mining the properties themselves. We present a theoretical framework defining the space of graph properties, a variety of mining queries enabled by the framework, techniques to handle the enormous size of the query space, and an experimental system called F-Miner that demonstrates the utility and feasibility of property mining.	data mining , graph mining	Mining the space of graph properties.	en	53
54	This paper presents a methodology for using simulated execution to assist a theorem prover in verifying safety properties of distributed systems. Execution-based techniques such as testing can increase confidence in an implementation, provide intuition about behavior, and detect simple errors quickly. They cannot by themselves demonstrate correctness. However, they can aid theorem provers by suggesting necessary lemmas and providing tactics to structure proofs. This paper describes the use of these techniques in a machine-checked proof of correctness of the Paxos algorithm for distributed consensus .	dynamic analysis , static analysis , theorem proving , invariant detection	Using simulated execution in verifying distributed algorithms.	en	54
55	We study combinatorial optimization problems in which a set of distributed agents must achieve a global objective using only local information. Papadimitriou and Yannakakis [Proceedings of the 25th ACM Symposium on Theory of Computing, 1993, pp. 121--129] initiated the study of such problems in a framework where distributed decision-makers must generate feasible solutions to positive linear programs with information only about local constraints. We extend their model by allowing these distributed decision-makers to perform local communication to acquire information over time and then explore the tradeoff between the amount of communication and the quality of the solution to the linear program that the decision-makers can obtain.Our main result is a distributed algorithm that obtains a $(1 approximation to the optimal linear programming solution while using only a polylogarithmic number of rounds of local communication. This algorithm offers a significant improvement over the logarithmic approximation ratio previously obtained by Awerbuch and Azar [Proceedings of the 35th Annual IEEE Symposium on  Foundations of Computer Science, 1994, pp. 240--249] for this problem while providing a comparable running time. Our results apply directly to the application of network flow control, an application in which distributed routers must quickly choose how to allocate bandwidth to connections using only local information to achieve global objectives. The sequential version of our algorithm is faster and considerably simpler than the best known approximation algorithms capable of achieving a $(1 approximation ratio for positive linear programming.	primal-dual , linear programming , approximation algorithm , flow control	Fast, Distributed Approximation Algorithms for Positive Linear Programming with Applications to Flow Control.	en	55
56	Let M be a $2\times 2$ matrix of Laurent polynomials with real coefficients and symmetry. In this paper, we obtain a necessary and sufficient condition for the existence of four Laurent polynomials (or finite-impulse-response filters) u1, u2, v1, v2 with real coefficients and symmetry such that   $$ \left[ \begin{matrix} u_1(z) &v_1(z)\\ u_2(z) &v_2(z) \end{matrix}\right] \left[ \begin{matrix} u_1(1/z) &u_2(1/z)\\ v_1(1/z) &v_2(1/z)\end{matrix}\right]=M(z) \qquad \forall\; z\in \CC \bs \{0 \} $$   and [Su1](z)[Sv2](z)=[Su2](z)[Sv1](z), where [Sp](z)=p(z)/p(1/z) for a nonzero Laurent polynomial p. Our criterion can be easily checked and a step-by-step algorithm will be given to construct the symmetric filters u1, u2, v1, v2. As an application of this result to symmetric framelet filter banks, we present a necessary and sufficient condition for the construction of a symmetric multiresolution analysis tight wavelet frame with two compactly supported generators derived from a given symmetric refinable function. Once such a necessary and sufficient condition is satisfied, an algorithm will be used to construct a symmetric framelet filter bank with two high-pass filters which is of interest in applications such as signal denoising and image processing. As an illustration of our results and algorithms in this paper, we give several examples of symmetric framelet filter banks with two high-pass filters which have good vanishing moments and are derived from various symmetric low-pass filters including some B-spline filters.	refinable functions , low-pass and high-pass filters , framelet filter banks , tight wavelet frames , matrix splitting , symmetry	Splitting a Matrix of Laurent Polynomials with Symmetry and its Application to Symmetric Framelet Filter Banks.	en	56
57	We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows overcoming these difficulties. Our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query-mapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows us to perform a correct analysis of such computations automatically, by extending previous work on a constraint-based approach to termination. Finally, we discuss possible extensions of the technique, including incorporating general term orderings.	numerical computation , termination analysis	Inference of termination conditions for numerical loops in Prolog.	en	57
58	In this paper we present efficient symbolic techniques for probabilistic model checking. These have been implemented in PRISM, a tool for the analysis of probabilistic models such as discrete-time Markov chains, continuous-time Markov chains and Markov decision processes using specifications in the probabilistic temporal logics PCTL and CSL. Motivated by the success of model checkers such as SMV which use BDDs (binary decision diagrams), we have developed an implementation of PCTL and CSL model checking based on MTBDDs (multi-terminal BDDs) and BDDs. Existing work in this direction has been hindered by the generally poor performance of MTBDD-based numerical computation, which is often substantially slower than explicit methods using sparse matrices. The focus of this paper is a novel hybrid technique which combines aspects of symbolic and explicit approaches to overcome these performance problems. For typical examples, we achieve a dramatic improvement over the purely symbolic approach. In addition, thanks to the compact model representation using MTBDDs, we can verify systems an order of magnitude larger than with sparse matrices, while almost matching or even beating them for speed.	probabilistic model checking , symbolic model checking , binary decision diagrams	Probabilistic symbolic model checking with PRISM: a hybrid approach.	en	58
59	The task of finding a set of test sequences that provides good coverage of industrial circuits is infeasible because of the size of the circuits. For small critical subcircuits of the design, however, designers can create a set of test sequences that achieve good coverage. These sequences cannot be used on the full design because the inputs to the subcircuit may not be accessible. In this work we present an efficient test generation algorithm that receives a test sequence created for the subcircuit and finds a test sequence for the full design that reproduces the given sequence on the subcircuit. The algorithm uses a new technique called dynamic transition relations to increase its efficiency .The most common and most expensive step in our algorithm is the computation of the set of predecessors of a set of states. To make this computation more efficient we exploit a partitioning of the transition relation into a set of simpler relations. At every step we use only those that are necessary, resulting in a smaller relation than the original one. A different relation is used for each step, hence the name dynamic transition relations. The same idea can be used to improve symbolic model checking for the temporal logic CTL.We have implemented the new method in SMV and run it on several large circuits. Our experiments indicate that the new method can provide gains of up to two orders of magnitude in time and space during verification. These results show that dynamic transition relations can make it possible to verify circuits that were previously unmanageable due to their size and complexity .	symbolic model checking , binary decision diagrams , test sequence generation	Test sequence generation and model checking using dynamic transition relations.	en	59
60	Group key agreement is a fundamental building block for secure peer group communication systems. Several group key management techniques were proposed in the last decade, all assuming the existence of an underlying group communication infrastructure to provide reliable and ordered message delivery as well as group membership information. Despite analysis, implementation, and deployment of some of these techniques, the actual costs associated with group key management have been poorly understood so far. This resulted in an undesirable tendency: on the one hand, adopting suboptimal security for reliable group communication, while, on the other hand, constructing excessively costly group key management protocols.This paper presents a thorough performance evaluation of five notable distributed key management techniques (for collaborative peer groups) integrated with a reliable group communication system. An in-depth comparison and analysis of the five techniques is presented based on experimental results obtained in actual local- and wide-area networks. The extensive performance measurement experiments conducted for all methods offer insights into their scalability and practicality. Furthermore, our analysis of the experimental results highlights several observations that are not obvious from the theoretical analysis.	peer groups , Group Key Management , secure communication , group communication	On the performance of group key agreement protocols.	en	60
61	All Internet routers contain buffers to hold packets during times of congestion. Today, the size of the buffers is determined by the dynamics of TCP's congestion control algorithm. In particular, the goal is to make sure that when a link is congested, it is busy 100% of the time; which is equivalent to making sure its buffer never goes empty. A widely used rule-of-thumb states that each link needs a buffer of size is the average round-trip time of a flow passing across the link, and C is the data rate of the link. For example, a 10Gb/s router linecard needs approximately 250ms x 2.5Gbits of buffers; and the amount of buffering grows linearly with the line-rate. Such large buffers are challenging for router manufacturers, who must use large, slow, off-chip DRAMs. And queueing delays can be long, have high variance, and may destabilize the congestion control algorithms. In this paper we argue that the rule-of-thumb now outdated and incorrect for backbone routers. This is because of the large number of flows (TCP connections) multiplexed together on a single backbone link. Using theory, simulation and experiments on a network of real routers, we show that a link with n flows requires no more than long-lived or short-lived TCP flows. The consequences on router design are enormous: A 2.5Gb/s link carrying 10,000 flows could reduce its buffers by 99% with negligible difference in throughput; and a 10Gb/s link carrying 50,000 flows requires only 10Mbits of buffering, which can easily be implemented using fast, on-chip SRAM.	buffer size , internet router , TCP , bandwidth delay product	Sizing router buffers.	en	61
62	Currently the Internet has only one level of name resolution, DNS, which converts user-level domain names into IP addresses. In this paper we borrow liberally from the literature to argue that there should be three levels of name resolution: from user-level descriptors to service identifiers; from service identifiers to endpoint identifiers; and from endpoint identifiers to IP addresses. These additional levels of naming and resolution (1) allow services and data to be first class Internet objects (in that they can be directly and persistently named), (2) seamlessly accommodate mobility and multi-homing and (3) integrate middleboxes (such as NATs and firewalls) into the Internet architecture. We further argue that flat names are a natural choice for the service and endpoint identifiers. Hence, this architecture requires scalable resolution of flat names, a capability that distributed hash tables (DHTs) can provide.	naming , internet architecture , middleboxes , global identifiers , name resolution , distributed hash tables	A layered naming architecture for the internet.	en	62
63	This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.	gaussian processes , inverse kinematics , motion style , non-linear dimensionality reduction , character animation , style interpolation , machine learning	Style-based inverse kinematics.	en	63
64	In this paper we show that in sorting-based applications of parametric search, Quicksort can replace the parallel sorting algorithms that are usually advocated. Because of the simplicity of Quicksort, this may lead to applications of parametric search that are not only efficient in theory, but in practice as well. Also, we argue that Cole's optimization of certain parametric-search algorithms may be unnecessary under realistic assumptions about the input. Furthermore, we present a generic, flexible, and easy-to-use framework that greatly simplifies the implementation of algorithms based on parametric search. We use our framework to implement an algorithm that solves the Frchet-distance problem. The implementation based on parametric search is faster than the binary-search approach that is often suggested as a practical replacement for the parametric-search technique.	implementation , parametric search	Parametric search made practical.	en	64
65	We study the problem of providing end-to-end delay guarantees in connection-oriented networks. In this environment, multiple-hop sessions coexist and interfere with one another.Parekh and Gallager showed that the Weighted Fair Queueing (WFQ) scheduling discipline provides a worst-case delay guarantee comparable to (1/i)  Ki for a session with rate i and Ki hops. Such delays can occur since a session-i packet can wait for time 1/i at every hop.We describe a randomized work-conserving scheme that guarantees, with high probability, an additive delay bound of approximately 1/i + Ki. This bound is smaller than the multiplicative bound (1/i)  Ki of WFQ, especially when the hop count Ki is large. We call our scheme COORDINATED-EARLIEST-DEADLINE-FIRST (CEDF) since it uses an earliest-deadline-first approach in which simple coordination is applied to the deadlines for consecutive hops of a session. The key to the bound is that once a packet has passed through its first server, it can pass through all its subsequent servers quickly.We conduct simulations to compare the delays actually produced by the two scheduling disciplines. In many cases, these actual delays are comparable to their analytical worst-case bounds, implying that CEDF outperforms WFQ.	packet routing , earliest deadline first , scheduling , weighted fair queueing , delay bounds	Minimizing end-to-end delay in high-speed networks with a simple coordinated schedule.	en	65
66	We show that a boolean valued function over n variables, where each variable ranges in an arbitrary probability space, can be tested for the property of depending on only J of them using a number of queries that depends only polynomially on J and the approximation parameter . We present several tests that require a number of queries that is polynomial in J and linear in -1. We showa non-adaptive tests that has one-sided error, an adaptive version of it that requires fewer queries, and a non-adaptive two-sided version of the test that requires the least number of queries. We also show a two-sided non-adaptive test that applies to functions over n boolean variables, and has a more compact analysis.We then provide a lower bound of (J) on the number of queries required for the nonadaptive testing of the above property; a lower bound of (log(J + 1)) for adaptive algorithms naturally follows from this. In establishing this lower bound we also prove a result about random walks on the group Z2q that may be interesting in its own right. We show that for some the distributions of the random walk at times t and t are close to each other, independently of the step distribution of the walk.We also discuss related questions. In particular, when given in advance a known J-junta function h, we show how to test a function f for the property of being identical to h up to a permutation of the variables, in a number of queries that is polynomial in J and -1.	discrete Fourier analysis , property testing , juntas , boolean functions	Testing juntas.	en	66
67	An algorithm is presented which generates a triangular mesh to approximate an iso-surface. It starts with a triangulation of a sphere and next applies a series of deformations to this triangulation to transform it into the required surface. These deformations leave the topology invariant, so the final iso-surface should be homeomorphic with a sphere. The algorithm is adaptive in the sense that the lengths of the sides of the triangles in the mesh vary with the local curvature of the underlying surface. A quantitative analysis of the accuracy of the algorithm is given along with an empirical comparison with earlier algorithms.	implicit surfaces , iso-surfaces , polygonization	Shrinkwrap: An efficient adaptive algorithm for triangulating an iso-surface.	en	67
68	Programs in embedded languages contain invariants that are not automatically detected or enforced by their host language. We show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language. We demonstrate the effectiveness of this technique in improving the results of a value flow analysis.	value flow analysis , embedded languages , macros , partial evaluation	Improving the static analysis of embedded languages via partial evaluation.	en	68
69	Open Shortest Path First (OSPF) is one of the most commonly used intra-domain internet routing protocol. Traffic flow is routed along shortest paths, splitting flow evenly at nodes where several outgoing links are on shortest paths to the destination. The weights of the links, and thereby the shortest path routes, can be changed by the network operator. The weights could be set proportional to the physical lengths of the links, but often the main goal is to avoid congestion, i.e. overloading of links, and the standard heuristic recommended by Cisco (a major router vendor) is to make the weight of a link inversely proportional to its capacity.We study the problem of optimizing OSPF weights for a given a set of projected demands so as to avoid congestion. We show this problem is NP-hard, even for approximation, and propose a local search heuristic to solve it. We also provide worst-case results about the performance of OSPF routing vs. an optimal multi-commodity flow routing. Our numerical experiments compare the results obtained with our local search heuristic to the optimal multi-commodity flow routing, as well as simple and commonly used heuristics for setting the weights. Experiments were done with a proposed next-generation AT&T WorldNet backbone as well as synthetic internetworks.	shortest path routing , local search , traffic engineering	Increasing Internet Capacity Using Local Search.	en	69
70	A variant of Reiter's default logic is proposed as a logic for reasoning with (defeasible) observations. Traditionally, default rules are assumed to represent generic information and the facts are assumed to represent specific information about the situation, but in this paper, the specific information derives from defeasible observations represented by (normal free) default rules, and the facts represent (hard) background knowledge. Whenever the evidence underlying some observation is more refined than the evidence underlying another observation, this is modelled by means of a priority between the default rules representing the observations. We thus arrive at an interpretation of prioritized normal free default logic as an observation logic, and we propose a semantics for this observation logic. Finally, we discuss how the proposed observation logic relates to the multiple extension problem and the problem of sensor fusion.	defeasible observations , nonmonotonic logic , multiple extension problem , prioritized default logic , sensor fusion , free default logic	A nonmonotonic observation logic.	en	70
71	We initiate a study of bounded clock synchronization under a more severe fault model than that proposed by Lamport and Melliar-Smith [1985]. Realistic aspects of the problem of synchronizing clocks in the presence of faults are considered. One aspect is that clock synchronization is an on-going task, thus the assumption that some of the processors never fail is too optimistic. To cope with this reality, we suggest self-stabilizing protocols that stabilize in any (long enough) period in which less than a third of the processors are faulty. Another aspect is that the clock value of each processor is bounded. A single transient fault may cause the clock to reach the upper bound. Therefore, we suggest a bounded clock that wraps around when appropriate.We present two randomized self-stabilizing protocols for synchronizing bounded clocks in the presence of Byzantine processor failures. The first protocol assumes that processors have a common pulse, while the second protocol does not. A new type of distributed counter based on the Chinese remainder theorem is used as part of the first protocol.	clock synchronization , self-stabilization , byzantine failures	Self-stabilizing clock synchronization in the presence of Byzantine faults.	en	71
72	Extension languages enable users to expand the functionality of an application without touching its source code. Commonly, these languages are dynamically typed languages, such as Lisp, Python, or domain-specific languages, which support runtime plugins via dynamic loading of components. We show that Haskell can be comfortably used as a statically typed extension language for both Haskell and foreign-language applications supported by the Haskell FFI, and that it can perform type-safe dynamic loading of plugins using dynamic types. Moreover, we discuss how plugin support is especially useful to applications where Haskell is used as an embedded domain-specific language (EDSL). We explain how to realise type-safe plugins using dynamic types, runtime compilation, and dynamic linking, exploiting infrastructure provided by the Glasgow Haskell Compiler. We demonstrate the practicability of our approach with several applications that serve as running examples.	plugins , functional programming , dynamic loading , dynamic typing , extension languages , staged type inference	Plugging Haskell in.	en	72
73	In reactive systems, execution is driven by external events to which the system should respond with appropriate actions. Such events can be simple, but systems are often supposed to react to sophisticated situations involving a number of simpler events occurring in accordance with some pattern. A systematic approach to handle this type of systems is to separate the mechanism for detecting composite events from the rest of the application logic.In this paper, we present an event algebra for composite event detection. We show a number of algebraic laws that facilitate formal reasoning, and justify the algebra semantics by showing to what extent the operators comply with intuition. Finally, we present an implementation of the algebra, and identify a large subset of expressions for which detection can be performed with bounded resources.	event algebra , reactive systems , resource-efficiency , event detection	An event detection algebra for reactive systems.	en	73
74	High-level programming languages offer significant expressivity but provide little or no guarantees about resource use. Resource-bounded languages --- such as hardware-description languages --- provide strong guarantees about the runtime behavior of computations but often lack mechanisms that allow programmers to write more structured, modular, and reusable programs. To overcome this basic tension in language design, recent work advocated the use of Resource-aware Programming (RAP) languages, which take into account the natural distinction between the development platform and the deployment platform for resource-constrained software.This paper investigates the use of RAP languages for the generation of combinatorial circuits. The key challenge that we encounter is that the RAP approach does not safely admit a mechanism to express a posteriori (post-generation) optimizations. The paper proposes and studies the use of abstract interpretation to overcome this problem. The approach is illustrated using an in-depth analysis of the Fast Fourier Transform (FFT). The generated computations are comparable to those generated by FFTW.	multi-stage programming , abstract interpretation	A methodology for generating verified combinatorial circuits.	en	74
75	Pervasive computing lets users continuously and consistently access an application on heterogeneous devices. However, delivering complex applications on resource-constrained mobile devices such as cell phones is challenging. Application- or system-based adaptations attempt to address the problem, but often at the cost of considerable degradation to application fidelity. The solution is to dynamically partition the application and offload part of the application execution data to a powerful nearby surrogate. This allows delivery of the application in a pervasive computing environment without significant fidelity degradation or expensive application rewriting. Runtime offloading must adapt to different application execution patterns and resource fluctuations in the pervasive computing environment. This offloading inference engine adaptively solves two key decision-making problems in runtime offloading: timely triggering of offloading and efficient partitioning of applications. Both trace-driven simulations and prototype experiments confirm the effectiveness of this adaptive offloading system.	adaptive offloading , application execution , pervasive computing , dynamic partitioning , mobile device	Adaptive Offloading for Pervasive Computing.	en	75
76	"After [15], [31], [19], [8], [25], [5], minimum cut/maximum flow algorithms on graphs emerged as an increasingly useful tool for exact or approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-Tarjan style ""push-relabel methods and algorithms based on Ford-Fulkerson style ""augmenting paths. We benchmark these algorithms on a number of typical graphs in the contexts of image restoration, stereo, and segmentation. In many cases, our new algorithm works several times faster than any of the other methods, making near real--time performance possible. An implementation of our max-flow/min-cut algorithm is available upon request for research purposes."	maximum flow , segmentation , minimum cut , multicamera scene reconstruction , image restoration , stereo , graph algorithms , Index Terms- Energy minimization	An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision.	en	76
77	A fundamental question of complexity theory is the direct product question. A famous example is Yao's XOR-lemma, in which one assumes that some function f is hard on average for small circuits (meaning that every circuit of some fixed size s which attempts to compute f is wrong on a non-negligible fraction of the inputs) and concludes that every circuit of size s' only has a small advantage over guessing randomly when computing ... f(xk) on independently chosen x1,...,xk. All known proofs of this lemma have the property that s' < s. In words, the circuit which attempts to compute fk is smaller than the circuit which attempts to compute f on a single input! This paper addresses the issue of proving strong direct product assertions, that is, ones in which s'  ks and is in particular larger than s. We study the question of proving strong direct product question for decision trees and communication protocols.	product theorems , average case complexity , hardness amplification , XOR-lemma	Towards proving strong direct product theorems.	en	77
78	"The ability to cooperate on common tasks in a distributed setting is key to solving a broad range of computation problems ranging from distributed search such as SETI to distributed simulation and multi-agent collaboration. Do-All, an abstraction of such cooperative activity, is the problem of performing N tasks in a distributed system of P failure-prone processors. Many distributed and parallel algorithms have been developed for this problem and several algorithm simulations have been developed by iterating Do-All algorithms. The efficiency of the solutions for Do-All is measured in terms of work complexity where all processing steps taken by all processors are counted. Work is ideally expressed as a function of N, P, and f, the number of processor crashes. However the known lower bounds and the upper bounds for extant algorithms do not adequately show how work depends on f. We present the first non-trivial lower bounds for Do-All that capture the dependence of work on N, P and f. For the model of computation where processors are able to make perfect load-balancing decisions locally, we also present matching upper bounds. We define the r-iterative Do-All problem that abstracts facts the repeated use of Do-All such as found in typical algorithm simulations. Our f-sensitive analysis enables us to derive tight bounds for r-iterative Do-All work (that are stronger than the r-fold work complexity of a single Do-All). Our approach that models perfect load-balancing allows for the analysis of specific algorithms to be divided into two parts: (i) the analysis of the cost of tolerating failures while performing work under ""free"" load-balancing, and (ii) the analysis of the cost of implementing load-balancing. We demonstrate the utility and generality of this approach by improving the analysis of two known efficient algorithms. We give an improved analysis of an efficient message-passing algorithm. We also derive a tight and complete analysis of the best known Do-All algorithm for the synchronous shared-memory model. Finally we present a new upper bound on simulations of synchronous shared-memory algorithms on crash-prone processors."	fault , work complexity , lower bounds , tolerence , distributed algorithms	The complexity of synchronous iterative Do-All with crashes.	en	78
79	This paper studies nested simulation and nested trace semantics over the language BCCSP, a basic formalism to express finite process behaviour. It is shown that none of these semantics affords finite (in)equational axiomatizations over BCCSP. In particular, for each of the nested semantics studied in this paper, the collection of sound, closed (in)equations over a singleton action set is not finitely based.	possible futures , nested simulation , nested trace semantics , concurrency , non-finitely based algebras , hennessy-milner logic , complete axiomatizations , equational logic , BCCSP , process algebra	Nested semantics over finite trees are equationally hard.	en	79
80	Order-sorted logic iucludes many and partially ordered sorts as a sort-hierarchy. In the field of knowledge representation and reasoning, it is useful to develop reasoning systems fbr terminological knowledge, together with assertional knowledge. However, the expression of sort-hierarchies cannot sufficiently capture the lexical diversity of terminological knowledge. In addition to sorts, various kinds of symbols: constants, functions and predicates are semantically and hierarchically associated with each other. This is because natural language words identifying these symbols can be employed in the description of terminological knowledge. In this paper, we present a label-based language for consistently handling the variety of hierarchical relationships among symbol names. For this language we develop a sorted resolution system whose reasoning power is enhanced by adding hierarchical inference rules with labeled substitutions.	terminological knowledge , label-based expressions , order-sorted logic , knowledge representation , resolution system	Resolution for label-based formulas in hierarchical representation.	en	80
81	Finding a good embedding of a unit disk graph given by its connectivity information is a problem of practical importance in a variety of fields. In wireless ad hoc and sensor networks, such an embedding can be used to obtain virtual coordinates. In this paper, we prove a non-approximability result for the problem of embedding a given unit disk graph. Particularly, we show that if non-neighboring nodes are not allowed to be closer to each other than distance 1, then two neighbors can be as far apart as 3/2 - , where  goes to 0 as n goes to infinity, unless P=NP. We further show that finding a realization of a d-quasi unit disk graph with d  1/2 is NP-hard.	sensor networks , unit disk graph , ad hoc networks , embedding , virtual coordinates	Unit disk graph approximation.	en	81
82	The checkability designed into the LSL (Larch shared language) is described, and two tools that help perform the checking are discussed. LP (the Larch power) is the principal debugging tool. Its design and development have been motivated primarily by work on LSL, but it also has other uses (e.g. reasoning about circuits and concurrent algorithms). Because of these other uses, and because they also tend to use LP to analyze Larch interface specifications, the authors have tried not to make LP too LSL-specific. Instead, they have chosen to build a second tool, LSLC (the LSL checker), to serve as a front-end to LP. LSLC checks the syntax and static semantics of LSL specifications and generates LP proof obligations from their claims. These proof obligations fall into three categories: consistency (that a specification does not contradict itself), theory containment (that a specification has intended consequences), and relative completeness (that a set of operators is adequately defined). An extended example illustrating how LP is used to debug LSL specifications is presented.	concurrent algorithms , larch shared language specifications , inference mechanisms , static semantics , theory containment , design , program debugging , debugging , larch power , checkability , development , parallel programming , formal specification , consistency	Debugging Larch Shared Language Specifications.	en	82
83	Recent years have seen the growing popularity of multi-rate wireless network devices (e.g., 802.11a cards) that can exploit variations in channel conditions and improve overall network throughput. Concurrently, rate adaptation schemes have been developed that selectively increase data transmissions on a link when it offers good channel quality. In this paper, we propose a Medium Access Diversity (MAD) scheme that leverages the benefits of rate adaptation schemes by aggressively exploiting multiuser diversity. The basic mechanism of MAD is to obtain instantaneous channel condition information from multiple receivers and selectively transmit data to a receiver that improves the overall throughput of the network, while maintaining temporal fairness among multiple data flows. We identify and address the challenges in the design and implementation of MAD's three phases: channel probing, data transmission, and receiver scheduling. We also use analytical models to examine the tradeoff between network performance improvement and overhead of channel probing, and derive an asymptotic performance bound for the receiver scheduling algorithms used by MAD. Results from the analysis and the extensive simulations demonstrate that, on average, MAD can improve the overall throughput of IEEE 802.11 wireless LANs by 50% as compared with the best existing rate adaptation scheme.	medium access , multiuser diversity , scheduling , wireless LAN	Exploiting medium access diversity in rate adaptive wireless LANs.	en	83
84	This paper addresses the problems of counting proof-trees (as introduced by Venkateswaran and Tompa) and counting proof-circuits, a related but seemingly more natural question.  These problems lead to a common generalization of straight-line programs which we call polynomial replacement systems {PRSs}.  We contribute a classification of these systems and we investigate their complexity.  Diverse problems falling within the scope of this study include, for example, counting proof-circuits and evaluating $\{\cup,+\}$-circuits over the natural numbers.  A number of complexity results are obtained, including a proof that counting proof-circuits is $\numP$-complete.	counting classes , arithmetic circuit , computational complexity	Arithmetic Circuits and Polynomial Replacement Systems.	en	84
85	In this paper we consider the problem of testing bipartiteness of general graphs.  The problem has previously been studied in two models, one most suitable for dense graphs and one most suitable for bounded-degree graphs. Roughly speaking, dense graphs can be tested for bipartiteness with constant complexity, while the complexity of testing bounded-degree graphs is $\tilde{\Theta}(\sqrt{n})$, where $n$ is the number of vertices in the graph (and $\tilde{\Theta}(f(n))$ means $\Theta(f(n)\cdot{\rm polylog}(f(n)))$). Thus there is a large gap between the complexity of testing in the two cases.In this work we bridge the gap described above. In particular, we study the problem of testing bipartiteness in a model that is suitable for all densities. We present an algorithm whose complexity is $\tilde{O}(\min(\sqrt{n},n^2/m))$, where $m$ is the number of edges in the graph, and we match it with an almost tight lower bound.	bipartiteness , property testing , randomized algorithms	Tight Bounds for Testing Bipartiteness in General Graphs.	en	85
86	Sparse Gaussian elimination with partial pivoting computes the factorization of a sparse matrix A, where the row ordering P is selected during factorization using standard partial pivoting with row interchanges. The goal is to select a column preordering, Q, based solely on the nonzero pattern of A, that limits the worst-case number of nonzeros in the factorization. The fill-in also depends on P, but Q is selected to reduce an upper bound on the fill-in for any subsequent choice of P. The choice of Q can have a dramatic impact on the number of nonzeros in L and U. One scheme for determining a good column ordering for A is to compute a symmetric ordering that reduces fill-in in the Cholesky factorization of ATA. A conventional minimum degree ordering algorithm would require the sparsity structure of ATA to be computed, which can be expensive both in terms of space and time since ATA may be much denser than A. An alternative is to compute Q directly from the sparsity structure of A; this strategy is used by MATLAB's COLMMD preordering algorithm. A new ordering algorithm, COLAMD, is presented. It is based on the same strategy but uses a better ordering heuristic. COLAMD is faster and computes better orderings, with fewer nonzeros in the factors of the matrix.	sparse nonsymmetric matrices , ordering methods , linear equations	A column approximate minimum degree ordering algorithm.	en	86
87	Given an undirected graph with edge costs and a subset ofk=3 nodes calledterminals, a multiway, ork-way, cut is a subset of the edges whose removal disconnects each terminal from the others. The multiway cut problem is to find a minimum-cost multiway cut. This problem is Max-SNP hard. Recently, Calinescu et al. (Calinescu, G., H. Karloff, Y. Rabani. 2000. An improved approximation algorithm for Multiway Cut.J. Comput. System Sci.60(3) 564--574) gave a novel geometric relaxation of the problem and a rounding scheme that produced a (3/2-1/ k)-approximation algorithm.In this paper, we study their geometric relaxation. In particular, we study the worst-case ratio between the value of the relaxation and the value of the minimum multicut (the so-called integrality gap of the relaxation). Fork=3, we show the integrality gap is 12/11, giving tight upper and lower bounds. That is, we exhibit a family of graphs with integrality gaps arbitrarily close to 12/11 and give an algorithm that finds a cut of value 12/11 times the relaxation value. Our lower bound shows that this is the best possible performance guarantee for any algorithm based purely on the value of the relaxation. Our upper bound meets the lower bound and improves the factor of 7/6 shown by Calinescu et al.For allk, we show that there exists a rounding scheme with performance ratio equal to the integrality gap, and we give explicit constructions of polynomial-time rounding schemes that lead to improved upper bounds. Fork=4 and 5, our best upper bounds are based on computer-constructed rounding schemes (with computer proofs of correctness). For generalk we give an algorithm with performance ratio 1.3438-e k .Our results were discovered with the help of computational experiments that we also describe here.	multiway cut , approximation algorithm	Rounding Algorithms for a Geometric Embedding of Minimum Multiway Cut.	en	87
88	We consider the following integer feasibility problem: Given positive integer numbersa0,a1, ... , a n , with gcd( a1,..., a n does there exist a vectorx?Z n =0satisfyinga x= a0? We prove that if the coefficientsa1,..., a nhave a certain decomposable structure, then the Frobenius number associated witha1,..., a n , i.e., the largest value ofa0 for whicha x= a0 does not have a nonnegative integer solution, is close to a known upper bound. In the instances we consider, we takea0 to be the Frobenius number. Furthermore, we show that the decomposable structure ofa1,..., a nmakes the solution of a lattice reformulation of our problem almost trivial, since the number of lattice hyperplanes that intersect the polytope resulting from the reformulation in the direction of the last coordinate is going to be very small. For branch-and-bound such instances are difficult to solve, since they are infeasible and have large values ofa0/ a i , 1= i= n. We illustrate our results by some computational examples.	lattice basis reduction , frobenius number , branching on hyperplanes	Hard Equality Constrained Integer Knapsacks.	en	88
89	In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.	energy minimization , part-based object recognition , statistical models	Pictorial Structures for Object Recognition.	en	89
90	We study here a natural situation when constraint programming can be entirely reduced to rule-based programming. To this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first-order formulas. Then we consider constraint satisfaction problems that are based on predefined, explicitly given constraints. To solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules, combined with labeling. We consider two types of rule here. The first type, that we call equality rules, leads to a new notion of local consistency, called rule consistency that turns out to be weaker than arc consistency for constraints of arbitrary arity (called hyper-arc consistency in Marriott & Stuckey (1998)). For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints. The second type of rules, that we call membership rules, yields a rule-based characterization of arc consistency. To show feasibility of this rule-based approach to constraint programming, we show how both types of rules can be automatically generated, as CHR rules of Frhwirth (1995). This yields an implementation of this approach to programming by means of constraint logic programming. We illustrate the usefulness of this approach to constraint programming by discussing various examples, including Boolean constraints, two typical examples of many valued logics, constraints dealing with Waltz's language for describing polyhedral scenes, and Allen's qualitative approach to temporal logic.	constraint programming , rule-based programming , finite domain	Constraint programming viewed as rule-based programming.	en	90
91	We present a prescriptive type system with parametric polymorphism and subtyping for constraint logic programs. The aim of this type system is to detect programming errors statically. It introduces a type discipline for constraint logic programs and modules, while maintaining the capabilities of performing the usual coercions between constraint domains, and of typing meta-programming predicates, thanks to the exibility of subtyping. The property of subject reduction expresses the consistency of a prescriptive type system w.r.t. the execution model: if a program is well-typed, then all derivations starting from a well-typed goal are again well-typed. That property is proved w.r.t. the abstract execution model of constraint programming which proceeds by accumulation of constraints only, and w.r.t. an enriched execution model with type constraints for substitutions. We describe our implementation of the system for type checking and type inference. We report our experimental results on type checking ISO-Prolog, the (constraint) libraries of Sicstus Prolog and other Prolog programs.	metaprogramming , type systems , prolog , constraint logic programming , subtyping	Typing constraint logic programs.	en	91
92	In existing simulation proof techniques, a single step in a lower-level specification may be simulated by an extended execution fragment in a higher-level one. As a result, it is cumbersome to mechanize these techniques using general-purpose theorem provers. Moreover, it is undecidable whether a given relation is a simulation, even if tautology checking is decidable for the underlying specification logic. This article studies various types of <i>normed simulations</i>. In a normed simulation, each step in a lower-level specification can be simulated by at most one step in the higher-level one, for any related pair of states. In earlier work we demonstrated that normed simulations are quite useful as a vehicle for the formalization of refinement proofs via theorem provers. Here we show that normed simulations also have pleasant theoretical properties: (1) under some reasonable assumptions, it is decidable whether a given relation is a normed forward simulation, provided tautology checking is decidable for the underlying logic; (2) at the semantic level, normed forward and backward simulations together form a complete proof method for establishing behavior inclusion, provided that the higher-level specification has finite invisible nondeterminism.	automata , computer-aided verification , prophecy variables , forward simulations , normed simulations , refinement mappings , backward simulations , history variables	A theory of normed simulations.	en	92
93	In this paper, we analyze the node spatial distribution of mobile wireless ad hoc networks. Characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks, such as connectivity, average route length, and network capacity. In particular, we have investigated under what conditions the node spatial distribution resulting after a large number of mobility steps resembles the uniform distribution. This is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption. In order to test this hypothesis, we performed extensive simulations using two well-known mobility models: the random waypoint model, which resembles intentional movement, and a Brownian-like model, which resembles nonintentional movement. Our analysis has shown that in Brownian-like motion the uniformity assumption does hold, and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters. For extreme values of these parameters, the uniformity assumption is impaired.	mobility modeling , mobile ad hoc networks , random waypoint model , node spatial distribution	A statistical analysis of the long-run node spatial distribution in mobile ad hoc networks.	en	93
94	The proliferation of new data-intensive applications in asymmetric communication environments has led to an increasing interest in the development of push-based techniques, in which the information is broadcast to a large population of clients in order to achieve the most efficient use of the limited server and communication resources. It is important to note that quite often the data that is broadcast is time-critical in nature. Most of the related current research focuses on a pure push-based approach (Broadcast Disks model), where the transmission of data is done without allowing explicit requests from the users. More recently, some bidirectional models incorporating a low-capacity uplink channel have been proposed in order to increase the functionality of the Broadcast Disks model. However, the impact of integration of the uplink channel has been investigated using only static client profiles or ignoring the existence of time-sensitive data. None of the existing models integrates all the characteristics needed to perform effectively in a real-world, dynamic time-critical asymmetric communication environment. In this paper we present an adaptive data dissemination model and the associated on-line scheduling algorithms. These improve the functionality and performance of bidirectional broadcast models, maximizing the total number of satisfied users in asymmetric communication environments with dynamic client profiles and time requirements (e.g., mobile systems). This is achieved by means of dynamic adaptation of the broadcast program to the needs of the users, taking into account the bandwidth constraints inherent in asymmetric communication environments and the deadline requirements of the user requests. Performance is evaluated by simulation of a real-time asymmetric communication environment	asymmetric communication , time-critical data , push-based techniques , scheduling , broadcast data dissemination	Adaptive dissemination of data in time-critical asymmetric communication environments.	en	94
95	A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, there is no wait-free implementation of X by Y. We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that    atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest&set and fetch&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.	wait-free synchronization , linearization	Wait-free synchronization.	en	95
96	We introduce a short signature scheme based on the Computational DiffieHellman assumption on certain elliptic and hyperelliptic curves. For standard security parameters, the signature length is about half that of a DSA signature with a similar level of security. Our short signature scheme is designed for systems where signatures are typed in by a human or are sent over a low-bandwidth channel. We survey a number of properties of our signature scheme such as signature aggregation and batch verification.	short signatures , digital signatures , bilinear maps , elliptic curves , pairings	Short Signatures from the Weil Pairing.	en	96
97	"We propose a formal framework for the security analysis of on-demand source routing protocols for wireless ad hoc networks. Our approach is based on the well-known simulation paradigm that has been proposed to prove the security of cryptographic protocols. Our main contribution is the application of the simulation-based approach in the context of ad hoc routing. This involves a precise definition of a real-world model, which describes the real operation of the protocol, and an ideal-world model, which captures what the protocol wants to achieve in terms of security. Both models take into account the peculiarities of wireless communications and ad hoc routing. Then, we give a formal definition of routing security in terms of indistinguishability of the two models from the point of view of honest parties. We demonstrate the usefulness of our approach by analyzing two ""secure"" ad hoc routing protocols, SRP and Ariadne. This analysis leads to the discovery of as yet unknown attacks against both protocols. Finally, we propose a new ad hoc routing protocol and prove it to be secure in our model."	ad hoc networks , provable security , routing protocols , on-demand source routing , simulatability	Towards provable security for ad hoc routing protocols.	en	97
98	Anonymity networks have long relied on diversity of node location for protection against attacks---typically an adversary who can observe a larger fraction of the network can launch a more effective attack. We investigate the diversity of two deployed anonymity networks, Mixmaster and Tor, with respect to an adversary who controls a single Internet administrative domain. Specifically, we implement a variant of a recently proposed technique that passively estimates the set of administrative domains (also known as autonomous systems, or ASes) between two arbitrary end-hosts without having access to either end of the path. Using this technique, we analyze the AS-level paths that are likely to be used in these anonymity networks. We find several cases in each network where multiple nodes are in the same administrative domain. Further, many paths between nodes, and between nodes and popular endpoints, traverse the same domain.	interdomain routing , mix networks , anonymity	Location diversity in anonymity networks.	en	98
99	In 1998, Asperti and Mairson proved that the cost of reducing a -term using an optimal -reducer (a la Lvy) cannot be bound by any elementary function in the number of shared-beta steps. We prove in this paper that an analogous result holds for Lamping's abstract algorithm. That is, there is no elementary function in the number of shared beta steps bounding the number of duplication steps of the optimal reducer. This theorem vindicates the oracle of Lamping's algorithm as the culprit for the negative result of Asperti and Mairson. The result is obtained using as a technical tool Elementary Affine Logic.	complexity , elementary affine logic , graph rewriting , optimal reduction	(Optimal) duplication is not elementary recursive.	en	99
100	We show that the class of monotone 2<sup>O(log<i>n</i>)</sup>-term DNF formulae can be PAC learned in polynomial time under the uniform distribution from random examples only. This is an exponential improvement over the best previous polynomial-time algorithms in this model, which could learn monotone o(log<sup>2</sup> <i>n</i>)-term DNF. We also show that various classes of small constant-depth circuits which compute monotone functions are PAC learnable in polynomial time under the uniform distribution. All of our results extend to learning under any constant-bounded product distribution.	computational learning theory , DNF formula , PAC learning , monotone boolean function	On learning monotone DNF under product distributions.	en	100
101	Dynamic detection of likely invariants is a program analysis that generalizes over observed values to hypothesize program properties. The reported program properties are a set of likely invariants over the program, also known as an operational abstraction. Operational abstractions are useful in testing, verification, bug detection, refactoring, comparing behavior, and many other tasks. Previous techniques for dynamic invariant detection scale poorly or report too few properties. Incremental algorithms are attractive because they process each observed value only once and thus scale well with data sizes. Previous incremental algorithms only checked and reported a small number of properties. This paper takes steps toward correcting this problem. The paper presents two new incremental algorithms for invariant detection and compares them analytically and experimentally to two existing algorithms. Furthermore, the paper presents four optimizations and shows how to implement them in the context of incremental algorithms. The result is more scalable invariant detection that does not sacrifice functionality.	reversing optimizations , batch algorithm , incremental algorithm , dynamic invariant detection	Efficient incremental algorithms for dynamic detection of likely invariants.	en	101
102	We show that a number of recent definitions and constructions of fuzzy extractors are not adequate for multiple uses of the same fuzzy secret---a major shortcoming in the case of biometric applications. We propose two particularly stringent security models that specifically address the case of fuzzy secret reuse, respectively from an outsider and an insider perspective, in what we call a chosen perturbation attack. We characterize the conditions that fuzzy extractors need to satisfy to be secure, and present generic constructions from ordinary building blocks. As an illustration, we demonstrate how to use a biometric secret in a remote fuzzy authentication protocol that does not require any storage on the client's side.	fuzzy extractor , chosen perturbation security , biometric keying , zero storage biometric authentication	Reusable cryptographic fuzzy extractors.	en	102
103	A forward-secure encryption scheme protects secret keys from exposure by evolving the keys with time. Forward security has several unique requirements in hierarchical identity-based encryption (HIBE) scheme: (1) users join dynamically; (2) encryption is joining-time-oblivious; (3) users evolve secret keys autonomously. We present a scalable forward-secure HIBE (fs-HIBE) scheme satisfying the above properties. We also show how our fs-HIBE scheme can be used to construct a forward-secure public-key broadcast encryption scheme, which protects the secrecy of prior transmissions in the broadcast encryption setting. We further generalize fs-HIBE into a collusion-resistant multiple hierarchical ID-based encryption scheme, which can be used for secure communications with entities having multiple roles in role-based access control. The security of our schemes is based on the bilinear Diffie-Hellman assumption in the random oracle model.	broadcast encryption , forward security , ID-Based encryption	ID-based encryption for complex hierarchies with applications to forward security and broadcast encryption.	en	103
104	A general model of software development environments that consists of structures, mechanisms, and policies is presented. The advantage of this model is that it distinguishes intuitively those aspects of an environment that are useful in comparing and contrasting software development environments. Four classes of environments-the individual, the family, the city. and the state-are characterized by means of a sociological metaphor based on scale. The utility of the taxonomy is that it delineates the important classes of interactions among software developers and exposes the ways in which current software development environments inadequately support the development of large systems. The generality of the model is demonstrated by its application to a previously published taxonomy that categorizes environments according to how they relate to language-centered, structure-oriented, toolkit, and method-based environments.	language centered environments , toolkit environments , method-based environments , software development environments , structure-oriented environments , programming environments , sociological metaphor	Models of Software Development Environments.	en	104
105	Computing the solution of the initial value problem in ordinary differential equations (ODEs) may be only part of a larger task. One such task is finding where an algebraic function of the solution (an event function) has a root (an event occurs). This is a task which is difficult both in theory and in software practice. For certain useful kinds of event functions, it is possible to avoid two fundamental difficulties. It is described how to achieve the reliable solutions of such problems in a way that allows the capability to be grafted onto popular codes for the initial value problem.	dense output , root finding , event location	Reliable solution of special event location problems for ODEs.	en	105
106	We introduce and analyze a new mixed finite element method for the numerical approximation of stationary incompressible magneto-hydrodynamics (MHD) problems in polygonal and polyhedral domains. The method is based on standard inf-sup stable elements for the discretization of the hydrodynamic unknowns and on nodal elements for the discretization of the magnetic variables. In order to achieve convergence in non-convex domains, the magnetic bilinear form is suitably modified using the weighted regularization technique recently developed in [Numer. Math. 93 (2002) 239]. We first discuss the well-posedness of this approach and establish a novel existence and uniqueness result for non-linear MHD problems with small data. We then derive quasi-optimal error bounds for the proposed finite element method and show the convergence of the approximate solutions in non-convex domains. The theoretical results are confirmed in a series of numerical experiments for a linear two-dimensional Oseen-type MHD problem, demonstrating that weighted regularization is indispensable for the resolution of the strongest magnetic singularities.	weighted regularization , mixed methods , incompressible magneto-hydrodynamics	Mixed finite element approximation of incompressible MHD problems based on weighted regularization.	en	106
107	We present an I/O-efficient dynamic data structure for point location in a general planar subdivision. Our structure uses O(<i>N/B</i>) disk blocks of size <i>B</i> to store a subdivision of size <i>N</i>. Queries can be answered in O(log<inf><i>B</i></inf><sup>2</sup><i>N</i>) I/Os in the worst-case, and insertions and deletions can be performed in O(log<inf><i>B</i></inf><sup>2</sup><i>N</i>) and O(log<inf><i>B</i></inf><i>N</i>) I/Os amortized, respectively. Part of our data structure is based on an external version of the so-called logarithmic method that allows for efficient dynamization of static external-memory data structures with certain characteristics. Another important part of our structure is an external data structure for vertical ray-shooting among line segments in the plane with endpoints on <i>B</i> lines, developed using an external version of dynamic fractional cascading. We believe that these methods could prove helpful in the development of other dynamic external memory data structures.	dynamic data structures , external memory , point location , I/O efficient	I/O-efficient dynamic planar point location.	en	107
108	We develop upper and lower bound arguments for counting acceptance modes of communication protocols. A number of separation results for counting communication complexity classes is established. This extends the investigation of the complexity of communication between two processors in terms of complexity classes initiated by Babai et al. (Proceedings of the 27th IEEE FOCS, 1986, pp. 337-347) and continued in several papers (e.g., J. Comput. System Sci. 41 (1990) 402; 49 (1994) 247; Proceedings of the 36th IEEE FOCS, 1995, pp. 6-15). In particular, it will be shown that for all pairs of distinct primes <i>p</i> and <i>q</i> the communication complexity classes MOD<inf><i>p</i></inf><i>P</i><sup>cc</sup> and MOD<inf><i>q</i></inf><i>P</i><sup>cc</sup> are incomparable with regard to inclusion. The same is true for PP<sup>cc</sup> and MOD<inf><i>m</i></inf><i>P</i><sup>cc</sup>, for any number <i>m</i>2. Moreover, non-determinism and modularity are incomparable to a large extend. On the other hand, if <i>m</i>=<i>p</i><inf>1</inf><i>l</i><inf>1</inf>'...' <i>p</i><inf>r</inf><i>l</i><inf>r</inf> is the prime decomposition of <i>m</i> 2, then the complexity classes MOD<inf><i>m</i></inf><i>P</i><sup>cc</sup> and MOD<inf><i>(m)</i></inf><i>P</i><sup>cc</sup> coincide, where <i>(m)</i>=<i>p</i><inf>1</inf>'...'<i>p</i><inf>r</inf>. The results are obtained by characterizing the modular and probabilistic communication complexity in terms of the minimum rank of matrices ranging over certain equivalence classes. Methods from algebra and analytic geometry are used. This paper is the completely revised and strongly extended version of the conference paper Damm et al. (Proc. 9th Ann. STACS, pp. 281-291) where a subset of the results was presented.	modularity , protocol , upper bound , probabilism , non-determinism , lower bound , communication complexity class	On relations between counting communication complexity classes.	en	108
109	In this paper, we propose an extension to the personal communication services (PCS) location management protocol which uses dynamically overlapped registration areas. The scheme is based on monitoring the aggregate mobility and call pattern of the users during each reconfiguration period and adapting to the mobility and call patterns by either expanding or shrinking registration areas at the end of each reconfiguration period. We analytically characterize the trade-off resulting from the inclusion or exclusion of a cell in a registration area in terms of expected change in aggregate database access cost and signaling overhead. This characterization is used to guide the registration area adaption in a manner in which the signaling and database access load on any given location register (LR) does not exceed a specified limit. Our simulation results show that it is useful to dynamically adapt the registration areas to the aggregate mobility and call patterns of the mobile units when the mobility pattern exhibits locality. For such mobility and call patterns, the proposed scheme can greatly reduce the average signaling and database access load on LRs. Further, the cost of adapting the registration areas is shown to be low in terms of memory and communication requirements.	distributed location management , cellular network , adaptive protocol , distributed load balancing , dynamic optimization , mobile communication network , distributed algorithm	Dynamically adapting registration areas to user mobility and call patterns for efficient location management in PCS networks.	en	109
110	This paper explores the use of compiler optimizations which optimize the layout of instructions in memory. The target is to enable the code to make better use of the underlying hardware resources regardless of the specific details of the processor/architecture in order to increase fetch performance. The Software Trace Cache (STC) is a code layout algorithm with a broader target than previous layout optimizations. We target not only an improvement in the instruction cache hit rate, but also an increase in the effective fetch width of the fetch engine. The STC algorithm organizes basic blocks into chains trying to make sequentially executed basic blocks reside in consecutive memory positions, then maps the basic block chains in memory to minimize conflict misses in the important sections of the program. We evaluate and analyze in detail the impact of the STC, and code layout optimizations in general, on the three main aspects of fetch performance: the instruction cache hit rate, the effective fetch width, and the branch prediction accuracy. Our results show that layout optimized codes have some special characteristics that make them more amenable for high-performance instruction They have a very high rate of not-taken branches and execute long chains of sequential instructions; also, they make very effective use of instruction cache lines, mapping only useful instructions which will execute close in time, increasing both spatial and temporal locality.	compiler optimizations , trace cache , instruction fetch , Index Terms- Pipeline processors , branch prediction	Software Trace Cache.	en	110
111	"We solve an open problem concerning the mixing time of symmetric random walk on the n-dimensional cube truncated by a hyperplane, showing that it is polynomial in n.  As a consequence, we obtain a fully polynomial randomized approximation scheme for counting the feasible solutions of a 0-1 knapsack problem. The results extend to the case of any fixed number of hyperplanes. The key ingredient in our analysis is a combinatorial construction we call a ""balanced almost uniform permutation,"" which is of independent interest."	random permutations , knapsack problem , balanced permutations , random sampling , hypercubes , markov chains , random walks	Random Walks on Truncated Cubes and Sampling 0-1 Knapsack Solutions.	en	111
112	"This paper proposes a framework for detecting global state predicates in systems of processes with approximately-synchronized real-time clocks. Timestamps from these clocks are used to define two orderings on events: ""definitely occurred before"" and ""possibly occurred before"". These orderings lead naturally to definitions of 3 distinct detection modalities, i.e., 3 meanings of ""predicate  held during a computation"", namely: Poss <sup><i>db</i></sup>  ("" possibly held""), definitely held""), and Inst  ("" definitely held in a specific global state""). This paper defines these modalities and gives efficient algorithms for detecting them. The algorithms are based on algorithms of Garg and Waldecker, Alagar and Venkatesan, Cooper and Marzullo, and Fromentin and Raynal. Complexity analysis shows that under reasonable assumptions, these real-time-clock-based detection algorithms are less expensive than detection algorithms based on Lamport's happened-before ordering. Sample applications are given to illustrate the benefits of this approach."	real-time monitoring , global predicate detection , distributed debugging , consistent global states	Detecting global predicates in distributed systems with clocks.	en	112
113	A superstabilizing protocol is a protocol that (i) is self-stabilizing, meaning that it can recover from an arbitrarily severe transient fault; and (ii) can recover from a local transient fault while satisfying a passage predicate during recovery. This paper investigates the possibility of superstabilizing protocols for mutual exclusion in a ring of processors, where a local fault consists of any transient fault at a single processor; the passage predicate specifies that there be at most one token in the ring, with the single exception of a spurious token colocated with the transient fault. The first result of the paper is an impossibility theorem for a class of superstabilizing mutual exclusion protocols. Two unidirectional protocols are then presented to show that conditions for impossibility can independently be relaxed so that superstabilization is possible using either additional time or communication registers. A bidirectional protocol subsequently demonstrates that superstabilization in <i>O</i>(1) time is possible. All three superstabilizing protocols are optimal with respect to the number of communication registers used.	mutual exclusion , fault-containment , self-stabilization	Superstabilizing mutual exclusion.	en	113
114	A secure reliable multicast protocol enables a process to send a message to a group of recipients such that all correct destinations receive the same message, despite the malicious efforts of fewer than a third of the total number of processes, including the sender. This has been shown to be a useful tool in building secure distributed services, albeit with a cost that typically grows linearly with the size of the system. For very large networks, for which this is prohibitive, we present two approaches for reducing the cost: First, we show a protocol whose cost is on the order of the number of tolerated failures. Secondly, we show how relaxing the consistency requirement to a probabilistic guarantee can reduce the associated cost, effectively to a constant.	wide area network , probabilistic algorithms , secure multicast	Secure reliable multicast protocols in a WAN.	en	114
115	"A useless checkpoint is a local checkpoint that cannot be part of a consistent global checkpoint. This paper addresses the following problem. Given a set of processes that take (basic) local checkpoints in an independent and unknown way, the problem is to design communication-induced checkpointing protocols that direct processes to take additional local (forced) checkpoints to ensure no local checkpoint is useless. The paper first proves two properties related to integer timestamps which are associated with each local checkpoint. The first property is a necessary and sufficient condition that these timestamps must satisfy for no checkpoint to be useless. The second property provides an easy timestamp-based determination of consistent global checkpoints. Then, a general communication-induced checkpointing protocol is proposed. This protocol, derived from the two previous properties, actually defines a family of timestamp-based communication-induced checkpointing protocols. It is shown that several existing checkpointing protocols for the same problem are particular instances of the general protocol. The design of this general protocol is motivated by the use of communication-induced checkpointing protocols in ""consistent global checkpoint""-based distributed applications such as the detection of stable or unstable properties and the determination of distributed breakpoints."	fault-tolerance , checkpointing protocols , asynchronous distributed system	Communication-based prevention of useless checkpoints in distributed computations.	en	115
116	The complexity of designing protocols has led to compositional techniques for designing and verifying protocols. We propose a technique based on the notion of parallel composition of protocols. We view a composite protocol as an interleaved execution of the component protocols subject to a set of constraints. Using the constraints as building blocks, we define several constraint-based structures with each structure combining the properties of the component protocols in a different way. For instance, the component protocols of a multifunction protocol can be structured so that the composite protocol performs all the individual functions concurrently or performs only one of them depending on the order of initiation of the component protocols. We provide inference rules to infer safety and liveness properties of the composite protocol. Some properties are derived from those of the component protocols while others are derived from the structuring mechanism (the set of constraints) used to combine the component protocols.	multifunction protocols , safety and liveness properties , protocol composition , distributed protocols	Constraint-based structuring of network protocols.	en	116
117	Quorum systems are well-known tools for ensuring the consistency and availability of replicated data despite the benign failure of data repositories. In this paper we consider the arbitrary (Byzantine) failure of data repositories and present the first study of quorum system requirements and constructions that ensure data availability and consistency despite these failures. We also consider the load associated with our quorum systems, i.e., the minimal access probability of the busiest server. For services subject to arbitrary failures, we demonstrate quorum systems over <i>n</i> servers with a load of <i>O</i>(1/<i>n</i>), thus meeting the lower bound on load for benignly fault-tolerant quorum systems. We explore several variations of our quorum systems and extend our constructions to cope with arbitrary client failures.	byzantine failures , quorum systems , replication , fault tolerance	Byzantine quorum systems.	en	117
118	We tackle a natural problem from distributed computing, involving time-stamps. Let <i>p</i>={<i>p</i><inf>1</inf>,<i>p</i><inf>2</inf>, ...,<i>p</i><inf>N</inf>} be a set of computing agents or processes which synchronize with each other from time to time and exchange information about themselves and others. The gossip problem is the following: Whenever a set <i>P</i>  <i>P</i> meets, the processes in <i>P</i> must decide amongst themselves which of them has the latest information, direct or indirect, about each agent <i>p</i> in the system. We propose an algorithm to solve this problem which is finite-state and local. Formally, this means that our algorithm can be implemented as an asynchronous automation.	asynchronous automata , synchronous communication , bounded time-stamps , distributed algorithms	Keeping track of the latest gossip in a distributed system.	en	118
119	This paper presents a protocol for leader election in complete networks with a sense of direction. Sense of direction provides nodes the capability of distinguishing between their incident links according to a global scheme. We propose a protocol for leader election which requires <i>O(N)</i> messages and <i>O</i>(log <i>N</i>) time. The protocol is message optimal and the time complexity is a significant improvement over currently known protocols for this problem.	complete networks , leader election , distributed algorithm , message complexity	Efficient leader election using sense of direction.	en	119
120	A naming protocol assigns unique names (keys) to every process out of a set of communicating processes. We construct a randomized wait-free naming protocol using wait-free atomic read/write registers (shared variables) as process intercommunication primitives. Each process has its own private register and can read all others. The addresses/names each one uses for the others are possibly different: Processes <i>p</i> and <i>q</i> address the register of process <i>r</i> in a way not known to each other. For <i>n</i> processes and  > 0, the protocol uses a name space of size (1 + )<i>n</i> and <i>O</i>(<i>n</i> log <i>n</i> log log <i>n</i>) running time (read/writes to shared bits) with probability at least 1-<i>o</i>(1), and <i>O</i>(<i>n</i>log<sup>2</sup><i>n</i>) overall expected running time. The protocol is based on the wait-free implementation of a novel -Test&SetOnce object that randomly and fast selects a winner from a set of <i>q</i> contenders with probability at least  in the face of the strongest possible adaptive adversary.	symmetry breaking , atomicity , asynchronous distributed protocols , test-and-set objects , wait-free read/write registers , randomized algorithms , fault-tolerance , shared memory , adaptive adversary , naming problem , unique process ID	Randomized naming using wait-free shared variables.	en	120
121	We present lower bounds on the amount of communication that matrix multiplication algorithms must perform on a distributed-memory parallel computer. We denote the number of processors by <i>P</i> and the dimension of square matrices by <i>n</i>. We show that the most widely used class of algorithms, the so-called two-dimensional (2D) algorithms, are optimal, in the sense that in any algorithm that only uses <i>O</i>(<i>n</i><sup>2</sup>/<i>P</i>) words of memory per processor, at least one processor must send or receive (<i>n</i><sup>2</sup>/<i>P</i><sup>1/2</sup>) words. We also show that algorithms from another class, the so-called three-dimensional (3D) algorithms, are also optimal. These algorithms use replication to reduce communication. We show that in any algorithm that uses <i>O</i>(<i>n</i><sup>2</sup>/<i>P</i><sup>2/3</sup>) words of memory per processor, at least one processor must send or receive (<i>n</i><sup>2</sup>/<i>P</i><sup>2/3</sup>) words. Furthermore, we show a continuous tradeoff between the size of local memories and the amount of communication that must be performed. The 2D and 3D bounds are essentially instantiations of this tradeoff. We also show that if the input is distributed across the local memories of multiple nodes without replication, then (<i>n</i><sup>2</sup>) words must cross any bisection cut of the machine. All our bounds apply only to conventional (<i>n</i><sup>3</sup>) algorithms. They do not apply to Strassen's algorithm or other <i>o</i>(<i>n</i><sup>3</sup>) algorithms.	communication , lower bounds , distributed memory , matrix multiplication	Communication lower bounds for distributed-memory matrix multiplication.	en	121
122	We study a generalization of covering problems called partial covering. Here we wish to cover only a desired number of elements, rather than covering all elements as in standard covering problems. For example, in <i>k</i>-partial set cover, we wish to choose a minimum number of sets to cover at least <i>k</i> elements. For <i>k</i>-partial set cover, if each element occurs in at most <i>f</i> sets, then we derive a primal-dual <i>f</i>-approximation algorithm (thus implying a 2-approximation for <i>k</i>-partial vertex cover) in polynomial time. Without making any assumption about the number of sets an element is in, for instances where each set has cardinality at most three, we obtain an approximation of 4/3. We also present better-than-2-approximation algorithms for <i>k</i>-partial vertex cover on bounded degree graphs, and for vertex cover on expanders of bounded average degree. We obtain a polynomial-time approximation scheme for <i>k</i>-partial vertex cover on planar graphs, and for covering <i>k</i> points in <i>R<sup>d</sup></i> by disks.	vertex cover , approximation algorithms , primal-dual methods , set cover , partial covering , randomized rounding	Approximation algorithms for partial covering problems.	en	122
123	A minimally unsatisfiable subformula (MUS) is a subset of clauses of a given CNF formula which is unsatisfiable but becomes satisfiable as soon as any of its clauses is removed. The selection of a MUS is of great relevance in many practical applications. This expecially holds when the propositional formula encoding the application is required to have a well-defined satisfiability property (either to be satisfiable or to be unsatisfiable). While selection of a MUS is a hard problem in general, we show classes of formulae where this problem can be solved efficiently. This is done by using a variant of Farkas' lemma and solving a linear programming problem. Successful results on real-world contradiction detection problems are presented.	UnSatisfiability , MUS selection , infeasibility analysis	On Exact Selection of Minimally Unsatisfiable Subformulae.	en	123
124	The <i>crossing number</i> cr(<i>G</i>) of a graph <i>G</i> is the minimum possible number of edge crossings in a drawing of <i>G</i> in the plane, while the <i>pair-crossing number</i> pcr(<i>G</i>) is the smallest number of pairs of edges that cross in a drawing of <i>G</i> in the plane. While cr(<i>G</i>)  pcr(<i>G</i>) holds trivially, it is not known whether a strict inequality can ever occur (this question was raised by Mohar and Pach and Tth). We aim at bounding cr(<i>G</i>) in terms of pcr(<i>G</i>). Using the methods of Leighton and Rao, Bhatt and Leighton, and Even, Guha and Schieber, we prove that One of the main steps is an analogy of the well-known lower bound cr(<i>G</i>) = (<i>b</i>(<i>G</i>)<sup>2</sup>) - <i>O</i>(ssqd(<i>G</i>)), where <i>b(G)</i> is the <i>bisection width</i> of <i>G</i>, that is, the smallest number of edges that have to be removed so that no component of the resulting graph has more than 2/3 <i>n</i> vertices. We show that We also prove by similar methods that a graph <i>G</i> with crossing number log<sup>2</sup> <i>n</i> has a nonplanar subgraph on at most <i>O</i>(<i>nm</i> log<sup>2</sup> <i>n</i>/<i>k</i>) vertices, where <i>m</i> is the number of edges,  is the maximum degree in <i>G</i>, and <i>C</i> is a suitable sufficiently large constant.	expansion , crossing number , graph drawing , pair-crossing number	Crossing number, pair-crossing number, and expansion.	en	124
125	We present source tracing as a new viable approach to routing in ad hoc networks in which routers communicate the second-to-last hop and distance in preferred paths to destinations. We introduce a table-driven protocol (BEST) in which routers maintain routing information for all destinations, and an on-demand routing protocol (DST) in which routers maintain routing information for only those destinations to whom they need to forward data. Simulation experiments are used to compare these protocols with DSR, which has been shown to incur less control overhead that other on-demand routing protocols. The simulations show that DST requires far less control packets to achieve comparable or better average delays and percentage of packet delivered than DSR, and that BEST achieves comparable results to DSR while maintaining routing information for all destinations.	ad hoc networks , on-demand routing , wireless routing	Scenario-based comparison of source-tracing and dynamic source routing protocols for ad hoc networks.	en	125
126	The Web is a source of valuable information, but the process of collecting, organizing, and effectively utilizing the resources it contains is difficult. We describe CorpusBuilder, an approach for automatically generating Web search queries for collecting documents matching a minority concept. The concept used for this paper is that of text documents belonging to a minority natural language on the Web. Individual documents are automatically labeled as relevant or nonrelevant using a language filter, and the feedback is used to learn what query lengths and inclusion/exclusion term-selection methods are helpful for finding previously unseen documents in the target language. Our system learns to select good query terms using a variety of term scoring methods. Using <i>odds ratio</i> scores calculated over the documents acquired was one of the most consistently accurate query-generation methods. To reduce the number of estimated parameters, we parameterize the query length using a Gamma distribution and present empirical results with learning methods that vary the time horizon used when learning from the results of past queries. We find that our system performs well whether we initialize it with a whole document or with a handful of words elicited from a user. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes well across several languages regardless of the initial conditions.	online learning , query generation , web mining , corpus construction	Building Minority Language Corpora by Learning to Generate Web Search Queries.	en	126
127	The authors describe a novel maximum-entropy (maxent) approach for generating online recommendations as a user navigates through a collection of documents. They show how to handle high-dimensional sparse data and represent it as a collection of ordered sequences of document requests. This representation and the maxent approach have several advantages: (1) you can naturally model long-term interactions and dependencies in the data sequences; (2) you can query the model quickly once it is learned, which makes the method applicable to high-volume Web servers; and (3) you obtain empirically high-quality recommendations. Although maxent learning is computationally infeasible if implemented in the straightforward way, the authors explored data clustering and several algorithmic techniques to make learning practical even in high dimensions. They present several methods for combining the predictions of maxent models learned in different clusters. They conducted offline tests using over six months' worth of data from ResearchIndex, a popular online repository of over 470,000 computer science documents. They show that their maxent algorithm is one of the most accurate recommenders, as compared to such techniques as correlation, a mixture of Markov models, a mixture of multinomial models, individual similarity-based recommenders currently available on ResearchIndex, and even various combinations of current ResearchIndex recommenders.	mixture models , recommender systems , sequence modeling , maximum entropy model	Collaborative Filtering with Maximum Entropy.	en	127
128	Data mining is under attack from privacy advocates because of a misunderstanding about what it actually is and a valid concern about how it's generally done. This article shows how technology from the security community can change data mining for the better, providing all its benefits while still maintaining privacy.	privacy , data mining	Privacy-Preserving Data Mining.	en	128
129	A data warehouse stores materialized views of data from one or more sources, with the purpose of efficiently implementing decision-support or OLAP queries. One of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse. The goal is to select an appropriate set of views that minimizes total query response time and the cost of maintaining the selected views, given a limited amount of resource, e.g., materialization time, storage space, etc. In this article, we have developed a theoretical framework for the general problem of selection of views in a data warehouse. We present polynomial-time heuristics for a selection of views to optimize total query response time under a disk-space constraint, for some important special cases of the general data warehouse scenario, viz.: 1) an AND view graph, where each query/view has a unique evaluation, e.g., when a multiple-query optimizer can be used to general a global evaluation plan for the queries, and 2) an OR view graph, in which any view can be computed from any one of its related views, e.g., data cubes. We present proofs showing that the algorithms are guaranteed to provide a solution that is fairly close to (within a constant factor ratio of) the optimal solution. We extend our heuristic to the general AND-OR view graphs. Finally, we address in detail the view-selection problem under the maintenance cost constraint and present provably competitive heuristics.	Index Terms- Views , materialization , view selection , data warehouse	Selection of Views to Materialize in a Data Warehouse.	en	129
130	We introduce efficient learning equilibrium (ELE), a normative approach to learning in noncooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms must arrive at a desired value after polynomial time, and a deviation from the prescribed ELE becomes irrational after polynomial time. We prove the existence of an ELE (where the desired value is the expected payoff in a Nash equilibrium) and of a Pareto-ELE (where the objective is the maximization of social surplus) in repeated games with perfect monitoring. We also show that an ELE does not always exist in the imperfect monitoring case. Finally, we discuss the extension of these results to general-sum stochastic games.	stochastic games , repeated games , multi-agent learning , learning equilibrium , efficiency , ex-post equilibrium	Efficient learning equilibrium.	en	130
131	"In the framework of self-stabilizing systems, the convergence proof is generally done by exhibiting a measure that strictly decreases until a legitimate configuration is reached. The discovery of such a measure is very specific and requires a deep understanding of the studied transition system. In contrast we propose here a simple method for proving convergence, which regards self-stabilizing systems as string rewrite systems, and adapts a procedure initially designed by Dershowitz for proving termination of string rewrite systems. In order to make the method terminate more often, we also propose an adapted procedure that manipulates ""schemes"", i.e. regular sets of words, and incorporates a process of scheme generalization. The interest of the method is illustrated on several nontrivial examples."	self-stabilization , rewriting systems	Proving convergence of self-stabilizing systems using first-order rewriting and regular languages.	en	131
132	A locally decodable code (LDC) encodes n-bit strings x in m-bit codewords C(x) in such a way that one can recover any bit xi from a corrupted codeword by querying only a few bits of that word. We use a quantum argument to prove that LDCs with 2 classical queries require exponential 2(n). Previously, this was known only for linear codes (Goldreich et al., in: Proceedings of 17th IEEE Conference on Computation Complexity, 2002, pp. 175-183). The proof proceeds by showing that a 2-query LDC can be decoded with a single quantum query, when defined in an appropriate sense. It goes on to establish an exponential lower bound on any 'l-query locally quantum-decodable code'. We extend our lower bounds to non-binary alphabets and also somewhat improve the polynomial lower bounds by Katz and Trevisan for LDCs with more than 2 queries. Furthermore, we show that q quantum queries allow more succinct LDCs than the best known LDCs with q classical queries. Finally, we give new classical lower bounds and quantum upper bounds for the setting of private information retrieval. In particular, we exhibit a quantum 2-server private information retrieval (PIR) scheme with O(n3/10) qubits of communication, beating the O(n1/3) bits of communication of the best known classical 2-server PIR.	locally decodable codes , private information retrieval , quantum computing	Exponential lower bound for 2-query locally decodable codes via a quantum argument.	en	132
133	In this paper, we show that any n point metric space can be embedded into a distribution over dominating tree metrics such that the expected stretch of any edge is O(log n). This improves upon the result of Bartal who gave a bound of O(log n log log n). Moreover, our result is existentially tight; there exist metric spaces where any tree embedding must have distortion (log n)-distortion. This problem lies at the heart of numerous approximation and online algorithms including ones for group Steiner tree, metric labeling, buy-at-bulk network design and metrical task system. Our result improves the performance guarantees for all of these problems.	tree metrics , embeddings , metrics	A tight bound on approximating arbitrary metrics by tree metrics.	en	133
134	We present solutions to statically load-balance scatter operations in parallel codes run on grids. Our load-balancing strategy is based on the modification of the data distributions used in scatter operations. We study the replacement of scatter operations with parameterized scatters, allowing custom distributions of data. The paper presents: (1) a general algorithm which finds an optimal distribution of data across processors; (2) a quicker guaranteed heuristic relying on hypotheses on communications and computations; (3) a policy on the ordering of the processors. Experimental results with an MPI scientific code illustrate the benefits obtained from our load-balancing.	heterogeneous computing , grid computing , parallel programming , load-balancing , scatter operation	Load-balancing scatter operations for grid computing.	en	134
135	"The dynamic behavior of a network in which information is changing continuously over time requires robust and efficient mechanisms for keeping nodes updated about new information. Gossip protocols are mechanisms for this task in which nodes communicate with one another according to some underlying deterministic or randomized algorithm, exchanging information in each communication step. In a variety of contexts, the use of randomization to propagate information has been found to provide better reliability and scalability than more regimented deterministic approaches.In many settings, such as a cluster of distributed computing hosts, new information is generated at individual nodes, and is most ""interesting"" to nodes that are nearby. Thus, we propose distance-based propagation bounds as a performance measure for gossip mechanisms: a node at distance d from the origin of a new piece of information should be able to learn about this information with a delay that grows slowly with d, and is independent of the size of the network.For nodes arranged with uniform density in Euclidean space, we present natural gossip mechanisms, called spatial gossip, that satisfy such a guarantee: new information is spread to nodes at distance d, with high probability, in O(log1 steps. Such a bound combines the desirable qualitative features of uniform gossip, in which information is spread with a delay that is logarithmic in the full network size, and deterministic flooding, in which information is spread with a delay that is linear in the distance and independent of the network size. Our mechanisms and their analysis resolve a conjecture of Demers et al. [1987].We further show an application of our gossip mechanisms to a basic resource location problem, in which nodes seek to rapidly learn the location of the nearest copy of a resource in a network. This problem, which is of considerable practical importance, can be solved by a very simple protocol using Spatial Gossip, whereas we can show that no protocol built on top of uniform gossip can inform nodes of their approximately nearest resource within poly-logarithmic time. The analysis relies on an additional useful property of spatial gossip, namely that information travels from its source to sinks along short paths not visiting points of the network far from the two nodes."	resource location , decentralized algorithm , gossip	Spatial gossip and resource location protocols.	en	135
136	"We consider the problem of approximating a given m  n matrix A by another matrix of specified rank k, which is smaller than m and n. The Singular Value Decomposition (SVD) can be used to find the ""best"" such approximation. However, it takes time polynomial in m, n which is prohibitive for some modern applications. In this article, we develop an algorithm that is qualitatively faster, provided we may sample the entries of the matrix in accordance with a natural probability distribution. In many applications, such sampling can be done efficiently. Our main result is a randomized algorithm to find the description of a matrix D* of rank at most k so that holds with probability at least 1   (where &verbar;&verbar;F is the Frobenius norm). The algorithm takes time polynomial in k,1/&epsi;, log(1/) only and is independent of m and n. In particular, this implies that in constant time, it can be determined if a given matrix of arbitrary size has a good low-rank approximation."	matrix algorithms , sampling , low-rank approximation	Fast monte-carlo algorithms for finding low-rank approximations.	en	136
137	In this paper we study the following question posed by H. S. Wilf: what is, asymptotically as $n\rightarrow \infty$, the probability that a randomly chosen part size in a random composition  of an integer n has multiplicity m? More specifically, given positive integers n and m, suppose that a composition $\lambda$ of n is selected uniformly at random and then, out of the set of part sizes in $\lambda$, a part size j is chosen  uniformly at random. Let $\P(A_n^{(m)})$ be the probability that j has multiplicity m. We show that for fixed m, $\P(A_n^{(m)}$) goes to 0 at the rate $1/\ln n$. A more careful analysis uncovers an unexpected result: $(\ln n)\P(A_n^{(m)})$ does not have a limit but instead oscillates around the value $1/m$ as $n\to\infty$.This work is a counterpart of a recent paper of Corteel, Pittel, Savage, and Wilf, who studied the same problem in the case of partitions rather than compositions.	compositions of an integer , geometric random variables , random compositions	On the Multiplicity of Parts in a Random Composition of a Large Integer.	en	137
138	In the 0-extension problem, we are given a weighted graph with some nodes marked as terminals and a semimetric on the set of terminals. Our goal is to assign the rest of the nodes to terminals so as to minimize the sum, over all edges, of the product of the edge's weight and the distance between the terminals to which its endpoints are assigned. This problem generalizes the multiway cut problem of Dahlhaus et al. [SIAM J. Comput.}, 23 (1994), pp. 864--894] and is closely related to the metric labeling problem introduced by Kleinberg and Tardos [Proceedings of the 40th IEEE Annual Symposium on Foundations of Computer Science, New York, 1999, pp. 14--23].We present approximation algorithms for {\sc 0-Extension}. In arbitrary graphs, we present a O(log k)-approximation algorithm, k being the number of terminals. We also give O(1)-approximation guarantees for weighted planar graphs. Our results are based on a natural metric relaxation of the problem previously considered by Karzanov [European J. Combin., 19 (1998), pp. 71--101]. It is similar in flavor to the linear programming relaxation of Garg, Vazirani, and Yannakakis [SIAM J. Comput.}, 25 (1996), pp. 235--251] for the multicut problem, and similar to relaxations for other graph partitioning problems. We prove that the integrality ratio of the metric relaxation is at least $c \sqrt{\lg k}$ for a positive c for infinitely many k. Our results improve some of the results of Kleinberg and Tardos, and they further our understanding on how to use metric relaxations.	graph partitioning , linear programming relaxation , approximation algorithm , metric space	Approximation Algorithms for the 0-Extension Problem.	en	138
139	We show how to use numerical continuation to compute the intersection $C=A\cap B$ of two algebraic sets A and B, where A, B, and C are numerically represented by witness sets. En route to this result, we first show how to find the irreducible decomposition of a system of polynomials restricted to an algebraic set. The intersection of components A and B then follows by considering the decomposition of the diagonal system of equations u restricted to {u,v}$\in$ A $\times$ B. An offshoot of this new approach is that one can solve a large system of equations by finding the solution components of its subsystems and then intersecting these.  It also allows one to find the intersection of two components of the two polynomial systems, which is not possible with any previous numerical continuation approach.	generic points , homotopy continuation , numerical algebraic geometry , irreducible components , polynomial system , components of solutions , embedding	Homotopies for Intersecting Solution Components of Polynomial Systems.	en	139
140	"Two different ways of defining ad-hoc polymorphic operations commonly occur in programming languages. With the first form polymorphic operations are defined inductively on the structure of types while with the second form polymorphic operations are defined for specific sets of types.In intensional type analysis operations are defined by induction on the structure of types. Therefore no new cases are necessary for user-defined types, because these types are eQuivalent to their underlying structure. However, intensional type analysis is ""closed"" to extension, as the behavior of the operations cannot be differentiated for the new types, thus destroying the distinctions that these types are designed to express.Haskell type classes on the other hand define polymorphic operations for sets of types. Operations defined by class instances are considered ""open""---the programmer can add instances for new types without modifying existing code. However, the operations must be extended with specialized code for each new type, and it may be tedious or even impossible to add extensions that apply to a large universe of new types.Both approaches have their benefits, so it is important to let programmers decide which is most appropriate for their needs. In this paper, we define a language that supports both forms of ad-hoc polymorphism, using the same basic constructs."	ad-hoc polymorphism , reflexivity , intensional type analysis , generativity	An open and shut typecase.	en	140
141	Haskell's type classes allow ad-hoc overloading, or type-indexing, of functions. A natural generalisation is to allow type-indexing of data types as well. It turns out that this idea directly supports a powerful form of abstraction called associated types, which are available in C++ using traits classes. Associated types are useful in many applications, especially for self-optimising libraries that adapt their data representations and algorithms in a type-directed manner.In this paper, we introduce and motivate associated types as a rather natural generalisation of Haskell's existing type classes. Formally, we present a type system that includes a type-directed translation into an explicitly typed target language akin to System F; the existence of this translation ensures that the addition of associated data types to an existing Haskell compiler only requires changes to the front end.	self-optimising libraries , type-directed translation , type-indexed types , type classes , associated types	Associated types with class.	en	141
142	Ergodicity and throughput bound characterization are addressed for a subclass of timed and stochastic Petri nets, interleaving qualitative and quantitative theories. The nets considered represent an extension of the well-known subclass of marked graphs, defined as having a unique consistent firing count vector, independently of the stochastic interpretation of the net model. In particular, persistent and mono-T-semiflow net subclasses are considered. Upper and lower throughput bounds are computed using linear programming problems defined on the incidence matrix of the underlying net. The bounds proposed depend on the initial marking and the mean values of the delays but not on the probability distributions (thus including both the deterministic and the stochastic cases). From a different perspective, the considered subclasses of synchronized queuing networks; thus, the proposed bounds can be applied to these networks.	linear programming , unique consistent firing count vector , persistent nets , synchronized queuing networks , petri nets , mono-T-semiflow net subclasses , incidence matrix , ergodicity , throughput bounds , marked graphs	Ergodicity and Throughput Bounds of Petri Nets with Unique Consistent Firing Count Vector.	en	142
143	A multiple resolution algorithm is presented for segmenting images into regions with differing statistical behavior. In addition, an algorithm is developed for determining the number of statistically distinct regions in an image and estimating the parameters of those regions. Both algorithms use a causal Gaussian autoregressive model to describe the mean, variance, and spatial correlation of the image textures. Together, the algorithms can be used to perform unsupervised texture segmentation. The multiple resolution segmentation algorithm first segments images at coarse resolution and then progresses to finer resolutions until individual pixels are classified. This method results in accurate segmentations and requires significantly less computation than some previously known methods. The field containing the classification of each pixel in the image is modeled as a Markov random field. Segmentation at each resolution is then performed by maximizing the a posteriori probability of this field subject to the resolution constraint. At each resolution, the a posteriori probability is maximized by a deterministic greedy algorithm which iteratively chooses the classification of individual pixels or pixel blocks. The unsupervised parameter estimation algorithm determines both the number of textures and their parameters by minimizing a global criterion based on the AIC information criterion. Clusters corresponding to the individual textures are formed by alternately estimating the cluster parameters and repartitioning the data into those clusters. Concurrently, the number of distinct textures is estimated by combining clusters until a minimum of the criterion is reached.	statistical behavior , classification , variance , picture processing , posteriori probability , deterministic greedy algorithm , markov random field , statistics , textured images , pattern recognition , probability , parameter estimation , unsupervised texture segmentation , spatial correlation , multiple resolution segmentation , causal Gaussian autoregressive model , coarse resolution , AIC information criterion	Multiple Resolution Segmentation of Textured Images.	en	143
144	Model-based recognition and motion tracking depend upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. The author extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The Levenberg-Marquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved, parameterized objects.	motion tracking , curve fitting , arbitrary curved surfaces , 2D image matching , levenberg-marquardt method , picture processing , 3D model , model based pattern recognition , pattern recognition	Fitting Parameterized Three-Dimensional Models to Images.	en	144
145	We present an interactive modeling and animation system that facilitates the integration of a variety of simulation and animation paradigms. This system permits the modeling of diverse objects that change in shape, appearance, and behaviour over time. Our system thus extends modeling tools to include animation controls. Changes can be effected by various methods of control, including scripted, gestural, and behavioral specification. The system is an extensible testbed that supports research in the interaction of disparate control methods embodied in controller objects. This paper discusses some of the issues involved in modeling such interactions and the mechanisms implemented to provide solutions to some of these issues.The system's object-oriented architecture uses delegation hierarchies to let objects change all of their attributes dynamically. Objects include displayable objects, controllers, cameras, lights, renderers, and user interfaces. Techniques used to obtain interactive performance include the use of data-dependency networks, lazy evaluation, and extensive caching to exploit inter- and intra-frame coherency.	delegation , real-time animation , user interaction , object-oriented design , interactive illustrations , electronic books	An object-oriented framework for the integration of interactive animation techniques.	en	145
146	"This paper introduces new techniques for interactive piecewise flattening of parametric 3-D surfaces, leading to a non-distorted, hence realistic, texture mapping. Cuts are allowed on the mapped texture and we make a compromise between discontinuities and distortions. These techniques are based on results from differential geometry, more precisely on the notion of ""geodesic curvature"": isoparametric curves of the surface are mapped, in a constructive way, onto curves in the texture plane with preservation of geodesic curvature at each point. As an application, we give a concrete example which is a first step towards an efficient and robust CAD tool for shoe modeling."	differential geometry , geodesic curvature , piecewise surface flattening , non distorted texture mapping	Piecewise surface flattening for non-distorted texture mapping.	en	146
147	A multiorder routing strategy is developed which is loop-free even in the presence of link/node failures. Unlike most conventional methods in which the same routing strategy is applied indiscriminately to all nodes in the network, nodes under this proposal may adopt different routing strategies according to the network structure. Formulas are developed to determine the minimal order of routing strategy for each node to eliminate looping completely. A systematic procedure for striking a compromise between the operational overhead and network adaptability is proposed. Several illustrative examples are presented.	minimal order loop free routing strategy , network adaptability , packet switching , operational overhead , computer networks , multiorder routing strategy	Minimal Order Loop-Free Routing Strategy.	en	147
148	In constructive solid geometry, geometric solids are represented as trees whose leaves are labeled by primitive solids and whose internal nodes are labeled by set-theoretic operations. A bounding function in this context is an upper or lower estimate on the extent of the constituent sets; such bounds are commonly used to speed up algorithms based on such trees. We introduce the class of totally consistent bounding functions, which have the desirable properties of allowing surprisingly good bounds to be built quickly. Both outer and inner bounds can be refined using a set of rewrite rules, for which we give some complexity and convergence results. We have implemented the refinement rules for outer bounds within a solid modeling system, where they have proved especially useful for intersection testing in three and four dimensions. Our implementations have used boxes as bounds, but different classes (shapes) of bounds are also explored. The rewrite rules are also applicable to relatively slow, exact operations, which we explore for their theoretical insight, and to general Boolean algebras. Results concerning the relationship between these bounds and active zones are also noted.	representation simplification , robotics , collision detection , constructive solid geometry , solid modeling , boolean algebra , interference detection	Refinement methods for geometric bounds in constructive solid geometry.	en	148
149	In this paper three problems for a connectionist account of language are considered:1. What is the nature of linguistic representations?2. How can complex structural relationships such as constituent structure be represented?3. How can the apparently open-ended nature of language be accommodated by a fixed-resource system?Using a prediction task, a simple recurrent network (SRN) is trained on multiclausal sentences which contain multiply-embedded relative clauses. Principal component analysis of the hidden unit activation patterns reveals that the network solves the task by developing complex distributed representations which encode the relevant grammatical relations and hierarchical constituent structure. Differences between the SRN state representations and the more traditional pushdown store are discussed in the final section.	grammatical structure , simple recurrent networks , distributed representations	Distributed Representations, Simple Recurrent Networks, And Grammatical Structure.	en	149
150	The authors present embeddings of complete binary trees into butterfly networks with or without wrap-around connections. Let m be an even integer and q=m+(log m)-1. The authors show how to embed a 2/sup q+1/-1-node complete binary tree T(q) into a (m+1)2/sup m+1/-node wrap-around butterfly B/sub w/(m+1) with a dilation of 4, and how to embed T(q) into a (m+2)2/sup m+2/-node wrap-around butterfly B/sub w/(m+2) with an optimal dilation of 2. They also present an embedding of a wrap-around butterfly B/sub w/(m) into a (m+1)2/sup m/-node no-wrap-around butterfly B(m) with a dilation of 3. Using this embedding it is shown that T(q) can be embedded into a no-wrap butterfly B(m+1) (resp. B(m+2)) with a dilation of 8 (resp. 5).	multiprocessor interconnection networks , butterfly networks , trees mathematics , wrap-around connections , embeddings , complete binary trees	Embedding Complete Binary Trees Into Butterfly Networks.	en	150
151	A packaging system that allows diverse software components to be easily interconnected within heterogeneous programming environments is described. Interface software and stubs are generated for programmers automatically once the programmers express their application's geometry in a few simple rules and module interconnection language attributes. By generating custom interface code for each application, based on analysis and extraction of interfacing requirements, the system is able to produce executables whose run-time performance is comparable to manually integrated applications. The system is implemented within the Unix environment.	heterogeneous execution environments , packaging system , module interconnection language attributes , configuration management , interfacing requirements , diverse software components , user interfaces , heterogeneous programming environments , custom interface code , geometry , programming environments , unix environment , automatic programming	A Packaging System for Heterogeneous Execution Environments.	en	151
152	Program slicing is applied to the software maintenance problem by extending the notion of a program slice (that originally required both a variable and line number) to a decomposition slice, one that captures all computation on a given variable, i.e., is independent of line numbers. Using the lattice of single variable decomposition slices ordered by set inclusion, it is shown how a slice-based decomposition for programs can be formed. One can then delineate the effects of a proposed change by isolating those effects in a single component of the decomposition. This gives maintainers a straightforward technique for determining those statements and variables which may be modified in a component and those which may not. Using the decomposition, a set of principles to prohibit changes which will interfere with unmodified components is provided. These semantically consistent changes can then be merged back into the original program in linear time.	set inclusion , linear time , program testing , software maintenance , line number , software maintenance problem , semantically consistent changes , program slice , program slicing , slice-based decomposition , single variable decomposition slices , unmodified components	Using Program Slicing in Software Maintenance.	en	152
153	An adaptive program is one that changes its behavior base on the current state of its environment. This notion of adaptivity is formalized, and a logic for reasoning about adaptive programs is presented. The logic includes several composition operators that can be used to define an adaptive program in terms of given constituent programs; programs resulting from these compositions retain the adaptive properties of their constituent programs. The authors begin by discussing adaptive sequential programs, then extend the discussion to adaptive distributed programs. The relationship between adaptivity and self-stabilization is discussed. A case study for constructing an adaptive distributed program where a token is circulated in a ring of processes is presented.	adaptivity , token ring networks , adaptive distributed programs , formal logic , adaptive systems , composition operators , self-stabilization , programming theory , parallel programming , adaptive sequential programs , constituent programs	Adaptive Programming.	en	153
154	The efficiency of the basic operations of a NUMA (nonuniform memory access) multiprocessor determines the parallel processing performance on a NUMA multiprocessor. The authors present several analytical models for predicting and evaluating the overhead of interprocessor communication, process scheduling, process synchronization, and remote memory access, where network contention and memory contention are considered. Performance measurements to support the models and analyses through several numerical examples have been done on the BBN GP1000, a NUMA shared-memory multiprocessor. Analytical and experimental results give a comprehensive understanding of the various effects, which are important for the effective use of NUMA shared-memory multiprocessor. The results presented can be used to determine optimal strategies in developing an efficient programming environment for a NUMA system.	analytical models , programming environment , process synchronization , BBN GP1000 , parallel processing performance , nonuniform memory access , multiprocessing systems , performance evaluation , process scheduling , network contention , parallel processing , scheduling , NUMA shared-memory multiprocessor , interprocessor communication , remote memory access , memory contention , optimal strategies	Performance Prediction and Evaluation of Parallel Processing on a NUMA Multiprocessor.	en	154
155	Previous work on superimposed coding has been characterized by two aspects. First, it is generally assumed that signatures are generated from logical text blocks of the same size; that is, each block contains the same number of unique terms after stopword and duplicate removal. We call this approach the fixed-size block (FSB) method, since each text block has the same size, as measured by the number of unique terms contained in it. Second, with only a few exceptions [6,7,8,9,17], most previous work has assumed that each term in the text contributes the same number of ones to the signature (i.e., the weight of the term signatures is fixed). The main objective of this paper is to derive an optimal weight assignment that assigns weights to document terms according to  their occurrence and query frequencies in order to minimize the false-drop probability. The optimal scheme can account for both uniform and nonuniform occurence and query frequencies, and the signature generation method is still based on hashing rather than on table lookup. Furthermore, a new way of generating signatures, the fixed-weight block (FWB) method, is introduced. FWB controls the weight of every signature to a constant, whereas in FSB, only the expected signature weight is constant. We have shown that FWB has a lower false-drop probability than that of the FSB method, but its storage overhead is slightly  higher. Other advantages of FWB are that the optimal weight assignment can be obtained analytically without making unrealistic assumptions and that the formula for computing the term signature weights is simple and efficient.	signature file , optimization , access method , coding methods , text retrieval , information retrieval , document retrieval , superimposed coding	Optimal weight assignment for signature generation.	en	155
156	Two of the more important concurrent logic programming languages with nonflat guards are GHC and Parlog. They balance the requirements of having clean semantics and providing good control facilities rather differently, and their respective merits are compared and contrasted. Since concurrent logic programming would benefit from both, but neither language is able to express all the programs expressible in the other language, a lingua franca of these languages is defined and justified. A method is given for translating GHC and Parlog to and from it. The method preserves the arities and execution conditions of each clause. It enables a lingua franca implementation to support both languages transparently, and to provide a simple concurrent logic programming language suitable for programming in its own right.	parallel languages , nonflat guards , GHC , language translation , concurrent logic programming languages , execution conditions , logic programming , lingua franca , clean semantics , parlog , parallel programming , control facilities	A Lingua Franca for Concurrent Logic Programming.	en	156
157	A single method for normalizing the control-flow of programs to facilitate program transformations, program analysis, and automatic parallelization is presented. While previous methods result in programs whose control flowgraphs are reducible, programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods. In particular, all control-flow cycles are normalized into single-entry, single-exit while loops and all GOTOs are eliminated. Furthermore, the method avoids problems of code replication that are characteristic of node-splitting techniques. This restructuring obviates the control dependence graph, since afterwards control dependence relations are manifest in the syntax tree of the program. Transformations that effect this normalization are presented, and the complexity of the method is studied.	node-splitting techniques , parallel algorithms , automatic parallelization , control dependence relations , control flowgraphs , complexity , graph theory , control-flow cycles , computational complexity , syntax tree , GOTOs , structured programming , control-flow normalization algorithm	A Control-Flow Normalization Algorithm and its Complexity.	en	157
158	In shared-memory parallel programs that use explicit synchronization, race conditions result when accesses to shared memory are not properly synchronized. Race conditions are often considered to be manifestations of bugs, since their presence can cause the program to behave unexpectedly.  Unfortunately, there has been little agreement in the literature as to precisely what constitutes a race condition. Two different notions have been implicitly considered: one pertaining to programs intended to be deterministic (which we call general races) and the other to nondeterministic programs containing critical sections (which we call data races). However, the differences between general races and data races have not yet been recognized. This    paper examines these differences by characterizing races using a formal model and exploring their properties. We show that two variations of each type of race exist: feasible general races and data races capture the intuitive notions desired for debugging and apparent races capture less accurate notions implicitly assumed by most dynamic race detection methods. We also show that locating feasible races is an NP-hard problem, implying that only the apparent races, which are approximations to feasible races, can be detected in practice. The complexity of dynamically locating apparent races depends on the type of synchronization used by the program. Apparent races can be exhaustively located efficiently only for weak types of   synchronization that are incapable of implementing mutual exclusion. This result has important implications since we argue that debugging general races requires exhaustive race detection and is inherently harder than debugging data races (which requires only partial race detection). Programs containing data races can therefore be efficiently debugged by locating certain easily identifiable races. In contrast, programs containing general races require more complex debugging techniques.	race conditions , critical sections , nondeterminacy , parallel programs , debugging , data races	What are race conditions?.	en	158
159	Qualitative models arising in artificial intelligence domain often concern real systems that are difficult to represent with traditional means. However, some promise for dealing with such systems is offered by research in simulation methodology. Such research produces models that combine both continuous and discrete-event formalisms. Nevertheless, the aims and approaches of the AI and the simulation communities remain rather mutually ill understood. Consequently, there is a need to bridge theory and methodology in order to have a uniform language when either analyzing or reasoning about physical systems. This article introduces a methodology and formalism for developing multiple, cooperative models of physical systems of the type studied in qualitative physics. The formalism combines discrete-event and continuous models and offers an approach to building intelligent machines capable of physical modeling and reasoning.	combined simulation , abstraction levels , qualitative simulation , homomorphism , systems theory , multimodeling	A multimodel methodology for qualitative model engineering.	en	159
160	A relational database workload analyzer (REDWAR) is developed to characterize the workload in a DB2 environment. This is applied to study a production DB2 system where a structured query language (SQL) trace for a two-hour interval and an image copy of the database catalog were obtained. The results of the workload study are summarized. The structure and complexity of SQL statements, the makeup and run-time behavior of transactions/queries, and the composition of relations and views are discussed. The results obtained provide the important information needed to build a benchmark workload to evaluate the alternative design tradeoffs of database systems.	run-time behavior , SQL , relational databases , DP management , REDWAR , database theory , views , database catalog , structured query language , relational database workload analyzer , benchmark workload , query languages , systems analysis , DB2 environment , design tradeoffs	On Workload Characterization of Relational Database Environments.	en	160
161	A mechanism for modeling timing, precedence, and data-consistency constraints on concurrently executing processes is presented. The model allows durations and intervals between events to be specified. An algorithm is provided to detect schedules which may be unsafe with respect to the constraints. This work, motivated by the design and validation of autonomous error-recovery strategies on the Galileo spacecraft, appears to be applicable to a variety of asynchronous real-time systems.	data-consistency constraints , precedence , real-time systems , concurrently executing processes , asynchronous real-time systems , galileo spacecraft , scheduling , modeling timing , unsafe error recovery schedules , fault tolerant computing , aerospace computing	Detecting Unsafe Error Recovery Schedules.	en	161
162	A suite of computer programs for the evaluation of Bessel functions and modified Bessel functions of orders zero and one for a vector of real arguments is described. Distinguishing characteristics of these programs are that (a) they are portable across a wide range of machines, and (b) they are vectorized in the case when multiple function evaluations are to be performed. The performance of the new programs are compared with software from the FNLIB collection of Fullerton on which the new software is based.	special function , mathematical software , modified Bessel function , hyperbolic Bessel function , vectorized software , order zero and one , portable software , bessel function	vectorized software for Bessel function evaluation.	en	162
163	The Continuous Media File System, CMFS, supports real-time storage and retrieval of continuous media data (digital audio and video) on disk. CMFS clients read or write files in sessions, each with a guaranteed minimum data rate. Multiple sessions, perhaps with different rates, and non-real-time access can proceed concurrently. CMFS addresses several interrelated design issues; real-time semantics fo sessions, disk layout, an acceptance test for new sessions, and disk scheduling policy. We use simulation to compare different design choices.	disk scheduling , multimedia	A file system for continuous media.	en	163
164	The method of temporal differences (TD) is one way of making consistent predictions about the future. This paper uses some analysis of Watkins (1989) to extend a convergence theorem due to Sutton (1988) from the case which only uses information from adjacent time steps to that involving information from arbitrary ones.It also considers how this version of TD behaves in the face of linearly dependent representations for statesdemonstrating that it still converges, but to a different answer from the least mean squares algorithm. Finally it adapts Watkins' theorem that \cal Q-learning, his closely related prediction and action learning method, converges with probability one, to demonstrate this strong form of convergence for a slightly modified version of TD.	temporal differences , reinforcement learning , asynchronous dynamic programming	The Convergence of TD(MYAMPERSANDlambda;) for General MYAMPERSANDlambda;.	en	164
165	Distributed client/server models are becoming increasingly prevalent in multimedia systems and advanced user interface design. A multimedia application, for example, may play and record audio, use speech recognition input, and use a window system for graphical I/O. The software architecture of such a system can be simplified if the application communicates to multiple servers (e.g., audio servers, recognition servers) that each manage different types of input and output. This paper describes tools for rapidly prototyping distributed asynchronous servers and applications, with an emphasis on supporting highly interactive user interfaces, temporal media, and multi-modal I/O. The Socket Manager handles low-level connection management and device I/O by  supporting a callback mechanism for connection initiation, shutdown, and for reading incoming data. The Byte Stream Manager consists of an RPC compiler and run-time library that supports synchronous and asynchronous calls, with both a programmatic interface and a telnet interface that allows the server to act as a command interpreter. This paper details the tools developed for building asynchronous servers, several audio and speech servers built using these tools, and applications that exploit the features provided by the servers.	speech and studio applications , remote procedure call , asynchronous message passing , speech recognition and synthesis , audio servers , distributed client-server architecture	Tools for building asynchronous servers to support speech and audio applications.	en	165
166	Graphical user interfaces (GUI) provide intuitive and easy means for users to communicate with computers. However, construction of GUI software requires complex programming that is far from being intuitive. Because of the semantic gap between the textual application program and its graphical interface, the programmer himself must conceptually maintain the correspondence between the textual programming and the graphical image of the resulting interface. Instead, we propose a programming environment based on the programming by visual example (PBVE) scheme, which allows the GUI designers to program visual interfaces for their applications by drawing the example visualization of application data with a direct  manipulation interface. Our system, TRIP3, realizes this with (1) the bi-directional translation model between the (abstract) application data and the pictorial data of the GUI, and (2) the ability to generate mapping rules for the translation from example application data and its corresponding example visualization. The latter is made possible by the use of generalization of visual examples, where the system is able to automatically generate generalized mapping rules from a given set of examples.	constraints , graphical user interface , direct manipulation , visualization , layouts , programming by example	Declarative programming of graphical interfaces by visual examples.	en	166
167	We present a simple randomized algorithm which solves linear programs with n constraints and d variables in expected O(nde(d ln(n+1))1/4) time in the unit cost model (where we count the number of arithmetic operations on the numbers in the input). The expectation is over the internal randomizations performed by the algorithm, and holds for any input. The algorithm is presented in an abstract framework, which facilitates its application to several other related problems. The algorithm has been presented in a previous work by the authors [ShW], but its analysis and the subexponential complexity bound are new.	randomized incremental algorithms , linear programming , combinatorial optimization , computational geometry	A subexponential bound for linear programming.	en	167
168	The Virtual Reality user interface style allows the user to manipulate virtual objects in a 3D environment using 3D input devices. This style is best suited to application areas where traditional two dimensional styles fall short, but the current programming effort required to produce a VR application is somewhat large. We have built a toolkit called MR, which facilitates the development of VR applications. The toolkit provides support for distributed computing, head-mounted displays, room geometry, performance monitoring, hand input devices, and sound feedback. In this paper, the architecture of the toolkit is outlined, the programmer's view is described, and two simple applications are described.	user interface software , virtual reality , interactive 3D graphics	The decoupled simulation model for virtual reality systems.	en	168
169	In recent years, the computer science community has realized the advantages of GUIs (Graphical User Interfaces). Because high-quality GUIs are difficult to build, support tools such as UIMSs, UI Toolkits, and Interface Builders have been developed. Although these tools are powerful, they typically make two assumptions: first, that the programmer has some familiarity with the GUI model, and second, that he is willing to invest several weeks becoming proficient with the tool. These tools typically operate only on specific platforms, such as DOS, the Macintosh, or UNIX/X-windows. The existing tools are beyond the reach of most undergraduate computer science majors, or professional programmers who wish to quickly build GUIs without investing the time to become specialists in  GUI design. For this class of users, we developed SUIT, the Simple User Iinterface Toolkit. SUIT is an attempt to distill the fundamental components of an interface builder and GUI toolkit, and to explain those concepts with the tool itself, all in a short period of time. We have measured that college juniors with no previous GUI programming experience can use SUIT productively after less than three hours. SUIT is a C subroutine library which provides an external control UIMS, an interactive layout editor, and a set of standard widgets, such as sliders, buttons, and check boxes. SUIT-based applications run transparently across the Macintosh, DOS, and UNIX/X platforms. SUIT has been exported  to hundreds of external sites on the internet. This paper describes SUIT's architecture, the design decisions we made during its development, and the lessons we learned from extensive observations of over 120 users.	portability , software tools , graphical user interface , learnability , user interface toolkit , export , UIMS , pedagogy , rapid prototyping , GUI	Lessons learned from SUIT, the simple user interface toolkit.	en	169
170	Lexical ambiguity is a pervasive problem in natural language processing. However, little quantitative information is available about the extent of the problem or about the impact that it has on information retrieval systems. We report on an analysis of lexical ambiguity in information retrieval test collections and on experiments to determine the utility of word meanings for separating relevant from nonrelevant documents. The experiments show that there is considerable ambiguity even in a specialized database. Word senses provide a significant separation between relevant and nonrelevant documents, but several factors contribute to determining whether disambiguation will make an improvement in performance. For example, resolving lexical ambiguity was found to have little impact on retrieval effectiveness for documents that have many words in common with the query. Other uses of word sense disambiguation in an information retrieval context are discussed.	document retrieval , disambiguation , word senses , semantically based search	Lexical ambiguity and information retrieval.	en	170
171	A simulation-based method for obtaining numerical estimates of the reliability of N-version, real-time software is proposed. An extended stochastic Petri net is used to represent the synchronization structure of N versions of the software, where dependencies among versions are modeled through correlated sampling of module execution times. The distributions of execution times are derived from automatically generated test cases that are based on mutation testing. Since these test cases are designed to reveal software faults, the associated execution times and reliability estimates are likely to be conservative. Experimental results using specifications for NASA's planetary lander control software suggest that mutation-based testing could hold greater potential for enhancing reliability than the desirable but perhaps unachievable goal of independence among N versions. Nevertheless, some support for N-version enhancement of high-quality, mutation-tested code is also offered. Mutation analysis could also be valuable in the design of fault-tolerant software systems.	software reliability , software faults , NASA , synchronization structure , dependencies , stochastic Petri net , planetary lander control software , simulation , mutation analysis , module execution times , mutation-tested code , fault-tolerant software systems , computational complexity , numerical estimates , real-time software reliability , petri nets , correlated sampling , fault tolerant computing , mutation testing	Estimation and Enhancement of Real-Time Software Reliability Through Mutation Analysis.	en	171
172	The benefits of using a synchronous data-flow language for programming critical real-time systems are investigated. These benefits concern ergonomy (since the dataflow approach meets traditional description tools used in this domain) and ability to support formal design and verification methods. It is shown, using a simple example, how the language LUSTRE and its associated verification tool LESAR, can be used to design a program, to specify its critical properties, and to verify these properties. As the language LUSTRE and its uses have already been discussed in several papers, emphasis is put on program verification.	parallel languages , dataflow approach , real-time systems , formal design , program verification , critical real-time systems , verification tool LESAR , verification methods , parallel programming , synchronous data-flow language , traditional description tools , ergonomy , data-flow language LUSTRE , critical properties	Programming and Verifying Real-Time Systems by Means of the Synchronous Data-Flow Language LUSTRE.	en	172
173	A procedure is given for recognizing sets of inference rules that generate polynomial time decidable inference relations. The procedure can automatically recognize the tractability of the inference rules underlying congruence closure. The recognition of tractability for that particular rule set constitutes mechanical verification of a theorem originally proved independently by Kozen and Shostak. The procedure is algorithmic, rather than heuristic, and the class of automatically recognizable tractable rule sets can be precisely characterized. A series of examples of rule sets whose tractability is nontrivial, yet machine recognizable, is also given. The technical framework developed here is viewed as a first step toward a general theory of tractable inference relations.	inference rules , machine inference , proof systems , proof theory , mechanical verification , automated reasoning , polynomial-time algorithm , theorem proving	Automatic recognition of tractability in inference relations.	en	173
174	One of the most fundamental operations when simulating a stochastic discrete-event dynamic system is the generation of a nonuniform discrete random variate. The simplest form of this operation can be stated as follows: Generate a random variable X that is distributed over the integers 1,2,,n such that are fixed nonnegative numbers. The well-known alias algorithm is available to accomplish this task in O(1) time. A more difficult problem is to generate variates for X when the ai's are changing with time. We  present three rejection-based algorithms for this task, and for each algorithm we characterize the performance in terms of acceptance probability and the expected effort to generate a variate. We show that, under fairly unrestrictive conditions, the long-run expected effort is O(1). Applications to Markovian queuing networks are discussed. We also compare the three algorithms with competing schemes appearing in the literature.	randomized algorithms , queuing networks	Fast algorithms for generating discrete random variates with changing distributions.	en	174
175	ESP is a language for modeling rule-based software processes that take place in a distributed software development environment. It is based on PoliS, an abstract coordination model that relies on Multiple Tuple Spaces, i.e., collections of tuples a la Linda. PoliS extends Linda aiming at the specification and coordination of logically distributed systems. ESP (Extended Shared Prolog) combines the PoliS mechanisms to deal with concurrency and distribution, with the logic-programming language Prolog, to deal with rules and deduction. Such a combination of a coordination model and a logic language provides a powerful framework in which experiments about rule-based software process programming can be performed and evaluated.	rule-based programming , concurrency , multiuser programming environment , logic programming , software process modeling , software process	Coordinating rule-based software processes with ESP.	en	175
176	Mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. This paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram, which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems. Preliminary performance improvements of over 300% are reported. This method has the additional advantages of being easier to implement than interpretive systems, being simpler to port across a wide range of hardware and software platforms, and using the same compiler and run-time support system that is used during development and/or deployment.	program schemata , fault-based testing , mutation analysis , software testing	Mutation analysis using mutant schemata.	en	176
177	We discuss fundamental limitations of or-parallel execution models of nondeterministic programming languages. Or-parallelism corresponds to the execution of different nondeterministic computational paths in parallel. A natural way to represent the state of (parallel) execution of a nondeterministic program is by means of an or-parallel tree. We identify three important criteria that underlie the design of or-parallel implementations based on the or-parallel tree: constant-time access to variables, constant-time task creation, and constant-time task switching, where the term constant-time means that the time for these operations is independent of the number of nodes in the or-parallel tree, as well as the size of each node. We prove that all three criteria cannot be  simultaneously satisfied by any or-parallel execution model based on a finite number of processors but unbounded memory. We discuss in detail the application of our result to the class of logic programming languages and show how our result can serve as a useful way to categorize the various or-parallel methods proposed in this field. We also discuss the suitability of different or-parallel implemenation strategies for different parallel architectures.	or-parallel execution models	Analysis of Or-parallel execution models.	en	177
178	We propose an object-oriented data model that generalizes the relational, hierarchical, and network models. A database scheme in this model is a directed graph, whose leaves represent data and whose internal nodes represent connections among the data. Instances are constructed from objects, which have separate names and values. We define a logic for the model, and describe a nonprocedural query language that is based on the logic. We also describe an algebraic query language and show that it is equivalent to the logical language.	relational database , algebra , database schema , logic , tuple calculus	The logical data model.	en	178
179	||MAPLE|| (speak: parallel Maple) is a portable system for parallel symbolic computation. The system is built as an interface between the parallel declarative programming language Strand and the sequential computer algebra system Maple, thus providing the elegance of Strand and the power of the existing sequential algorithms in Maple. The implementation of different parallel programming paradigms shows that it is fairly easy to parallelize even complex algebraic algorithms using this system. Sample applications (among them algorithms solving multivariate nonlinear equation systems) are implemented on various parallel architectures. For example a straightforward parallelization of the complex and important problem of real root isolation has been parallelized using a generic  Strand program of fewer than 20 lines of code and a slight modification of 5 lines in the original sequential Maple source. Even with such a simple modification we gained a speed-up of 5 times, that is better than those reported by others in the literature.	logic programming , computer algebra systems	Parallelizing algorithms for symbolic computation using MAPLE.	en	179
180	This paper describes a set of interfaces for numerical subroutines. Typing a short (often one-line) description allows one to solve problems in application domains including least-squares data fitting, differential equations, minimization, root finding, and integration. Our approach of template-driven programming makes it easy to build such an interface: a simple one takes a few hours to construct, while a few days suffice to build the most complex program we describe.	maple , fortran , unix shell	Template-driven interfaces for numerical subroutines.	en	180
181	In the past, nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values. In such domains, the examples can be treated as points and distance metrics can use standard definitions. In symbolic domains, a more sophisticated treatment of the feature space is required. We introduce a nearest neighbor algorithm for learning in domains with symbolic features. Our algorithm calculates distance tables that allow it to produce real-valued distances between instances, and attaches weights to the instances to further modify the structure of feature space. We show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text. Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains. In addition, our algorithm has advantages in training speed, simplicity, and perspicuity. We conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here.	text pronunciation , protein structure , nearest neighbor , exemplar-based learning , instance-based learning	A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features.	en	181
182	A visual execution model for Ada tasking can help programmers attain a deeper understanding of the tasking semantics. It can illustrate subtleties in semantic definitions that are not apparent in natural language design. We describe a contour model of Ada tasking that depicts asynchronous tasks (threads of control), relationships between the environments in which tasks execute, and the manner in which tasks interact. The use of this high-level execution model makes it possible to see what happens during execution of a program. The paper provides an introduction to the contour model of Ada tasking and demonstrates its use.	contour model , visual execution model	A visual execution model for Ada tasking.	en	182
183	Software processes are complex entities that may last for long periods of time and are carried out through the interaction of humans and computerized tools. They need to continuously evolve in order to cope with different kinds of changes or customizations both in the organization and in the technologies used to support software production activities.In recent years, many software process support technologies have been developed, and have currently been further extended and used in trial projects. Moreover, some research prototypes have generated commercial products, that are marketed and currently used in industrial organizations. Despite these significant efforts and results, however, there is still little conceptual characterization and assessment of the properties of software processes and related support environments. It is difficult to compare and assess existing approaches. Even a common characterization of the problems to be addressed seems to be problematic and difficult to achieve. This is particularly true when we consider the process evolution problem, for which it does not seem that a common view of the issue has been established yet.This paper aims at proposing a conceptual framework to describe and assess flexible and evolving software processes. It is based on the assumption that a software process is composed of two main components: a software production process to carry out software production activities, and a software meta-process to improve and evolve the whole software process.The general requirements and properties of the process domain are first discussed, and the meta-process concept is introduced. Then, we discuss several process related concepts and, in particular, the relationship between the meta-process and the rest of the software process. Methods and technologies needed to support the meta-process are highlighted and discussed. Finally, we apply the resulting framework to an example, in order to show the potential and expected benefits of the proposed approach.	software process , process evolution and improvement , process modeling , metaprocess	A conceptual framework for evolving software processes.	en	183
184	Improvement of message latency and network utilization in torus interconnection networks by increasing adaptivity in wormhole routing algorithms is studied. A recently proposed partially adaptive algorithm and four new fully-adaptive routing algorithms are compared with the well-known e-cube algorithm for uniform, hotspot, and local traffic patterns. Our simulations indicate that the partially adaptive north-last algorithm, which causes unbalanced traffic in the network, performs worse than the nonadaptive e-cube routing algorithm for all three traffic patterns. Another result of our study is that the performance does not necessarily improve with full-adaptivity. In particular, a commonly discussed fully-adaptive routing algorithm, which uses 2n virtual channels  per physical channel of a k-ary n-cube, performs  worse than e-cube for uniform and hotspot traffic patterns. The other three fully-adaptive algorithms, which give priority to messages based on distances traveled, perform much better than the e-cube and partially-adaptive algorithms for all three traffic patterns. One of the conclusions of this study is that adaptivity, full or partial, is not necessarily a benefit in wormhole routing.	k-ary n-cubes , deadlocks , wormhole routing , message routing , multicomputer networks , adaptive routing , store-and-forward routing	A comparison of adaptive wormhole routing algorithms.	en	184
185	An X multiplexor allows a single X Window System client to be displayed and interacted with on several X servers simultaneously. Such a service is necessary for the construction of a computer-supported cooperative work (CSCW) environment such as JVTOS (Joint Viewing and Tele-Operation Service) which is being implemented within RACE II project CIO1. This paper describes several existing X multiplexors and evaluates their usefulness for JVTOS.	computer-supported cooperative work , x protocol multiplexer , distributed systems , joint viewing and tele-operation service , computer conferencing , application sharing , X Window system	A survey of X protocol multiplexors.	en	185
186	We consider a class of random walks on a lattice, introduced by Gessel and Zeilberger, for which the reflection principle can be used to count the number of k-step walks between two points which stay within a chamber of a Weyl group. We prove three independent results about such reflectable walks: first, a classification of all such walks&semi; second, many determinant formulas for walk numbers and their generating functions&semi; third, an equality between the walk numbers and the multiplicities of irreducibles in the kth tensor power of certain Lie group representations associated to the walk types. Our results apply to the defining representations of the classical groups, as well as some spin representations of the orthogonal groups.	weyl group , hyperbolic Bessel function , random walk , representation of Lie group , tensor power	Random Walks in Weyl Chambers and the Decomposition of Tensor Powers.	en	186
187	The theoretical background for the design of deadlock-free adaptive routing algorithmsfor wormhole networks is developed. The author proposes some basic definitions and twotheorems. These create the conditions to verify that an adaptive algorithm isdeadlock-free, even when there are cycles in the channel dependency graph. Two designmethodologies are also proposed. The first supplies algorithms with a high degree offreedom, without increasing the number of physical channels. The second methodology isintended for the design of fault-tolerant algorithms. Some examples are given to show theapplication of the methodologies. Simulations show the performance improvement thatcan be achieved by designing the routing algorithms with the new theory.	message passing , concurrency control , performance , index termsdeadlock-free adaptive routing , wormhole networks , telecommunication network routing , channeldependency graph , fault-tolerant algorithms , graph theory , routing algorithms , adaptive algorithm , fault tolerant computing	A New Theory of Deadlock-Free Adaptive Routing in Wormhole Networks.	en	187
188	Examines how the performance of a memoryless vector quantizer changes as a function of its training set size. Specifically, the authors study how well the training set distortion predicts test distortion when the training set is a randomly drawn subset of blocks from the test or training image(s). Using the Vapnik-Chervonenkis (VC) dimension, the authors derive formal bounds for the difference of test and training distortion of vector quantizer codebooks. The authors then describe extensive empirical simulations that test these bounds for a variety of codebook sizes and vector dimensions, and give practical suggestions for determining the training set size necessary to achieve good generalization from a codebook. The authors conclude that, by using training sets comprising only a small fraction of the available data, one can produce results that are close to the results obtainable when all available data are used.	formal bounds , vapnik-chervonenkis dimension , training image , training set distortion , empirical simulations , learning systems , statistics , small training sets , memoryless vector quantizer , vector quantisation , image coding , test distortion , vector quantizer codebooks	Theory and Practice of Vector Quantizers Trained on Small Training Sets.	en	188
189	Three recent trends in database research are object-oriented and deductive databases and graph-based user interfaces. We draw these trends together in a data model we call the Hypernode Model. The single data structure of this model is the hypernode, a graph whose nodes can themselves be graphs. Hypernodes are typed, and types, too, are nested graphs. We give the theoretical foundations of hypernodes and types, and we show that type checking is tractable. We show also how conventional type-forming operators can be simulated by our graph types, including cyclic types. The Hypernode Model comes equipped with a rule-based query language called Hyperlog, which is complete with respect to computation and update. We define the operational semantics of Hyperlog and show  that the evaluation can be performed efficiently. We discuss also the use of Hyperlog for supporting database browsing, an essential feature of Hypertext databases. We compare our work with other graph-based data modelsunlike previous graph-based models, the Hypernode Model provides inherent support for data abstraction via its nesting of graphs. Finally, we briefly discuss the implementation of a DBMS based on the Hypernode Model.	object store , rule-based query and update language , complex object , nested graph , types	A nested-graph model for the representation and manipulation of complex objects.	en	189
190	We introduce a temporal logic for the specification of real-time systems. Our logic, TPTL, employs a novel quantifier construct for referencing time: the freeze quantifier binds a variable to the time of the local temporal context. TPTL is both a natural language for specification and a suitable formalism for verification. We present a tableau-based decision procedure and a model-checking algorithm for TPTL. Several generalizations of TPTL are shown to be highly undecidable.	discrete time , dense time , real-time requirements , linear-time temporal logic , model checking , EXPSPACE-completeness	A really temporal logic.	en	190
191	We present a construction of a single-writer, multiple-reader atomic register from single-writer, single-reader atomic registers. The complexity of our construction is asymptotically optimal; O(M2 shared single-writer, single-reader safe bits are required to construct a single-writer, M-reader, N-bit atomic register.	wait-free synchronization , atomic register , linearizability	The elusive atomic register.	en	191
192	For any fixed dimension ) time, Information Processing Letters, v.22 n.1, p.21-24, January 2, 1986	parallel computation , probabilistic computation , linear programming , computational geometry , multidimensional search	Parallel linear programming in fixed dimension almost surely in constant time.	en	192
193	Strongly-typed languages present programmers with compile-time feedback about the type correctness of programs. Errors during polymorphic type checking take the form of a unification failure for two types. Finding the source of the type error in the code is often difficult because the error may occur far from the spot where the inconsistency is detected. As functional languages use more and more complex type systems, the difficulty of interpreting and locating these errors will increase. To locate the source of type errors the programmer must unravel the long chain of deductions and type instantiations made during type reconstruction. This paper describes an approach that maintains the deductive steps of type inference and the reasons for type instantiations. The approach could be  used in an interactive system to guide the programmer to the source of a type error or to explain why the compiler assigned a particular type to an expression.	polymorphic type reconstruction , type errors	Explaining type errors in polymorphic languages.	en	193
194	The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, program integration, and software maintenance. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. An interprocedural slice is a slice of an entire program, where the slice crosses the boundaries of procedure calls. Weiser's original interprocedural-slicing algorithm produces imprecise slices that are executable programs. A recent algorithm developed by Horwitz, Reps, and Binkley produces more precise (smaller) slices by more accurately identifying those statements that might affect the values of x at point p. These slices, however, are not executable. An extension to their algorithm that produces more precise executable interprocedural slices is described together with a proof of correctness for the new algorithm.	program slicing , program dependence graph , control dependence , data dependence	executable interprocedural slices.	en	194
195	We show how precise groundness information can be extracted from logic programs. The idea is to use abstract interpretation with Boolean functions as approximations to groundness dependencies between variables. This idea is not new, and different classes of Boolean functions have been used. We argue, however, that one class, the positive functions, is more suitable than others. Positive Boolean functions have a certain property which we (inspired by A. Langen) call condensation. This property allows for rapid computation of groundness information.	propositional logic , condensation , abstract interpretation , groundness analysis	Precise and efficient groundness analysis for logic programs.	en	195
196	Many applications of constraint logic programming (CLP) languages require not only testing if a set of constraints is satisfiable, but also finding the optimal solution which satisfies them. Unfortunately, the standard declarative semantics for CLP languages does not consider optimization but only constraint satisfaction. Here we give a model theoretic semantics for optimization, which is a simple extension of the standard semantics, and a corresponding operational semantics, which may be efficiently implemented.	semantics , constraint logic programming	Semantics of constraint logic programs with optimization.	en	196
197	This article develops a dynamic generalization of the nonuniform rational B-spline (NURBS) model. NURBS have become a defacto standard in commercial modeling systems because of their power to represent free-form shapes as well as common analytic shapes. To date, however, they have been viewed as purely geometric primitives that require the user to manually adjust multiple control points and associated weights in order to design shapes. Dynamic NURBS, or D-NURBS, are physics-based models that incorporate mass distributions, internal deformation energies, and other physical quantities into the popular NURBS geometric substrate. Using D-NURBS, a modeler can interactively sculpt curves and surfaces and design complex shapes to required specifications not only in the traditional indirect  fashion, by adjusting control points and weights, but also through direct physical manipulation, by applying simulated forces and local and global shape constraints. D-NURBS move and deform in a physically intuitive manner in response to the user's direct manipulations. Their dynamic behavior results from the numerical integration of a set of nonlinear differential equations that automatically evolve the control points and weights in response to the applied forces and constraints. To derive these equations, we employ Lagrangian mechanics and a finite-element-like discretization. Our approach supports the trimming of D-NURBS surfaces using D-NURBS curves. We demonstrate D-NURBS models and constraints in applications including the rounding of solids, optimal surface fitting to unstructured data, surface design from cross sections, and free-form deformation. We also introduce a new technique for 2D shape metamorphosis using constrained D-NURBS surfaces.	finite elements , constraints , optimal curve and surface fitting , shape metamorphosis , cross-sectional shape design , NURBS , trimming , free-form deformation , dynamics , CAGD , deformable models , solid rounding	Dynamic NURBS with geometric constraints for interactive sculpting.	en	197
198	3D shape modeling has received enormous attention in computer graphics and computer vision over the past decade. Several shape modeling techniques have been proposed in literature, some are local (distributed parameter) while others are global (lumped parameter) in terms of the parameters required to describe the shape. Hybrid models that combine both ends of this parameter spectrum have been in vogue only recently. However, they do not allow a smooth transition between the two extremes of this parameter spectrum. We introduce a new shape-modeling scheme that can transform smoothly from local to global models or vice versa. The modeling scheme utilizes a hybrid primitive called the deformable superquadric constructed in an orthonormal wavelet basis. The multiresolution wavelet basis provides the power to continuously transform from local to global shape deformations and thereby allow for a continuum of shape modelsfrom those with local to those with global shape descriptive powerto be created. The multiresolution wavelet basis allows us to generate fractal surfaces of arbitrary order that can be useful in describing natural detail. We embed these multiresolution shape models in a probabilistic framework and use them for recovery of anatomical structures in the human brain from MRI data. A salient feature of our modeling scheme is that it can naturally allow for the incorporation of prior statistics of a rich variety of shapes. This stems from the fact that, unlike other modeling schemes, in our modeling, we require relatively few parameters to describe a large class of shapes.	surface fitting , multiresolution representation , stiffness matrix , fractal surfaces , superquadrics , orthonormal wavelet basis , deformable surfaces , bayesian estimation	Multiresolution stochastic hybrid shape models with fractal priors.	en	198
199	The authors consider uniform subclasses of the nonuniform  complexity classes defined by Karp and Lipton [L'Enseign. Math., 28 (1982)] via the notion of advice functions. These  subclasses are obtained by restricting the complexity of  computing correct advice.  Also, the effect of allowing  advice functions of limited complexity to depend on the  input rather than on the input's length is investigated.  Among other results, using the notions described above,  new characterizations of (a) $NP^{NP\cap SPARSE}$, (b) $\NP$ with  a restricted access to an $\NP$ oracle, and (c) the odd  levels of the boolean hierarchy are given.  As a consequence, it is shown that every set that is nondeterministically truth-table reducible to SAT in the  sense of Rich [J. Comput. System Sci., 38 (1989),  pp. 511--523] is already deterministically truth-table reducible  to SAT. Furthermore, it turns out that the $\NP$  reduction classes of bounded versions of this reducibility  coincide with the odd levels of the boolean hierarchy.	nonuniform complexity classes , truth-table reducibility , relativization , boolean hierarchy , restricted oracle access , sparse NP sets , advice classes , optimization functions	Complexity-Restricted Advice Functions.	en	199
200	"One of the most important sets associated with a poset  ${\cal P}$ is its set of linear extensions, $E({\cal P})$. This paper presents an algorithm to generate all of the linear extensions of a poset in constant amortized time, that is, in time $O(e(\cP))$, where |E(\cP)|$.  The fastest previously known algorithm for generating the linear extensions of a poset runs in time $O(n \! \cdot \! e(\cP))$, where $n$ is the number of elements of the poset. The algorithm presented here is the first constant amortized time algorithm for generating a ""naturally defined"" class of combinatorial objects for which the corresponding counting problem is #P-complete. Furthermore, it is shown that linear extensions can be generated in constant amortized time where each extension differs from its predecessor by one or two adjacent transpositions. The algorithm is practical and can be modified to count linear extensions efficiently and to compute $P (x < y)$, for all pairs $x,y$, in time $O(n^2 e({\cal P}))$."	transposition , poset , combinatorial Gray code , linear extension	Generating Linear Extensions Fast.	en	200
201	The authors analyze the computational complexity of sparse rational interpolation, and give the first deterministic algorithm for this problem with singly exponential bounds on the number of arithmetic  operations.	interpolation , sparse rational functions , arithmetic operations , computational complexity	Computational Complexity of Sparse Rational Interpolation.	en	201
202	The external path length of a tree $T$ is the sum of the lengths of the paths from the root to each external node. The maximal path length difference, $\Delta$, is the difference between the lengths of the longest and shortest such paths.  Tight lower and upper bounds are proved on the external path length  of binary trees with $N$ external nodes and  maximal path  length difference $\Delta$ is prescribed.   In particular, an upper bound is given that, for each value of  $\Delta$, can be exactly achieved for infinitely many values of $N$.  This improves on the previously known upper bound that could  only be achieved up to a factor proportional to $N$. An elementary proof of the known upper bound is also presented as  a preliminary result.  Moreover, a lower bound is proved that can be exactly achieved for each value of $N$ and $\Delta\leq N/2$.	binary search trees , path length	Tight Upper and Lower Bounds on the Path Length of Binary Trees.	en	202
203	A basic question about NP is whether or not search reduces  in polynomial time to decision.  This paper indicates that  the answer is negative: Under a complexity assumption (that  deterministic and nondeterministic double-exponential time  are unequal) a language in NP for which search does not  reduce to decision is constructed.  These ideas extend in a natural way to interactive proofs  and program checking.  Under similar assumptions, the  authors present languages in NP for which it is harder to  prove membership interactively than it is to decide this  membership, and languages in NP that are not checkable.	sparse sets , interactive proofs , quadratic residuosity , NP-completeness , program checking , self-reducibility	The Complexity of Decision Versus Search.	en	203
204	In this paper we present a new technique for piecewise-linear surface reconstruction from a series of parallel polygonal cross-sections. This is an important problem in medical imaging, surface reconstruction from topographic data, and other applications. We reduce the problem, as in most previous works, to a series of problems of piecewise-linear interpolation between each pair of successive slices. Our algorithm uses a partial curve matching technique for matching parts of the contours, an optimal triangulation of 3-D polygons for resolving the unmatched parts, and a minimum spanning tree heuristic for interpolating between non simply connected regions. Unlike previous attempts at solving this problem, our algorithm seems to handle successfully any kind of data. It allows multiple contours in each slice, with any hierarchy of contour nesting, and avoids the introduction of counter-intuitive bridges between contours, proposed in some earlier papers to handle interpolation between multiply connected regions. Experimental results on various complex examples, involving actual medical imaging data, are presented, and show the good and robust performance of our algorithm.	slice interpolation , branching surfaces , surface fitting , polyhedra , geometric hashing , curve matching , tiling , dynamic programming , surface reconstruction , triangulation	Piecewise-linear interpolation between polygonal slices.	en	204
205	We describe two improvements to Chaitin-style graph coloring register allocators. The first, optimistic coloring, uses a stronger heuristic to find a k-coloring for the interference graph. The second extends Chaitin's treatment of rematerialization to handle a larger class of values. These techniques are complementary. Optimistic coloring decreases the number of procedures that require spill code and reduces the amount of spill code when spilling is unavoidable. Rematerialization lowers the cost of spilling some values. This paper describes both of the techniques and our experience building and using register allocators that incorporate them. It provides a detailed description of optimistic coloring and rematerialization. It presents experimental data to show the performance of several versions of the register allocator on a suite of FORTRAN programs. It discusses several insights that we discovered only after repeated implementation of these allocators.	graph coloring , register allocation , code generation	Improvements to graph coloring register allocation.	en	205
206	We describe a framework for compositional verification of finite-state processes. The framework is based on two ideas: a subset of the logic CTL for which satisfaction is preserved under composition, and a preorder on structures which captures the relation between a component and a system containing the component. Satisfaction of a formula in the logic corresponds to being below a particular structure (a tableau for the formula) in the preorder. We show how to do assume-guarantee-style reasoning within this framework. Additionally, we demonstrate efficient methods for model checking in the logic and for checking the preorder in several special cases. We have implemented a system based on these methods, and we use it to give a compositional verification of a CPU controller.	temporal logics , formal verification , computer-aided verification , CTL , moore machines , model checking	Model checking and modular verification.	en	206
207	A first-order multiparty interaction is an abstraction mechanism that defines communication among a set of formal process roles. Actual processes participate in a first-order interaction by enroling into roles, and execution of the interaction can proceed when all roles are filled by distinct processes. As in CSP, enrolement statements can serve as guards in alternative commands. The enrolement guard-scheduling problem then is to enable the execution of first-order interactions through the judicious scheduling of roles to processes that are currently ready to execute enrolement guards. We present a fully distributed and message-efficient algorithm for the enrolement guard-scheduling problem, the first such  solution of which we are aware. We also describe several extensions of the algorithm, including: generic roles; dynamically changing environments, where processes can be created and destroyed at run time; and nested-enrolement, which allows interactions to be nested.	multiparty interaction , rendezvous , interaction scheduling , distributed languages , committee coordination , distributed algorithms , first-order interaction	Coordinating first-order multiparty interactions.	en	207
208	A new approach to ambiguity of context-free grammars is presented, and within this approach the LL and LR techniques are generalized to solve the following problems for large classes of ambiguous grammars: Construction of a parser that accepts all sentences generated by the grammar, and which always terminates in linear time. Identification of the structural ambiguity: a finite set of pairs of partial parse trees is constructed; if for each pair the two partial parse trees are semantically equivalent, the ambiguity of the grammar is semantically irrelevant. The user may control the parser generation so as to get a parser which finds some specific parse trees for the sentences. The generalized LL and LR techniques will still guarantee that the resulting parser accepts all sentences and terminates in linear time on all input.	grammatic ambiguity , semantic unambiguity	Controlled grammatic ambiguity.	en	208
209	Modern computer architectures increasingly depend on mechanisms that estimate future control flow decisions to increase performance. Mechanisms such as speculative execution and prefetching are becoming standard architectural mechanisms that rely on control flow prediction to prefetch and speculatively execute future instructions. At the same time, computer programmers are increasingly turning to object-oriented languages to increase their productivity. These languages commonly use run time dispatching to implement object polymorphism. Dispatching is usually implemented using an indirect function call, which presents challenges to existing control flow prediction techniques. We have measured the occurrence  of indirect function calls in a collection of C++ programs. We show that, although it is more important to predict branches accurately, indirect call prediction is also an important factor in some programs and will grow in importance with the growth of object-oriented programming. We examine the improvement offered by compile-time optimizations and static and dynamic prediction techniques, and demonstrate how compilers can use existing branch prediction mechanisms to improve performance in C++ programs. Using these methods with the programs we examined, the number of instructions between mispredicted breaks in control can be doubled on existing computers.	profile-based optimization , customization , optimization , object oriented programming	Reducing indirect function call overhead in C++ programs.	en	209
210	"This paper describes new algorithms for approximately solving the concurrent multicommodity flow problem with uniform capacities. These algorithms are much faster than algorithms discovered previously. Besides being an important problem in its own right, the uniform-capacity concurrent flow problem has many interesting applications.  Leighton and Rao used uniform-capacity concurrent flow to find an approximately ""sparsest cut"" in a graph and thereby approximately solve a wide variety of graph problems, including minimum feedback arc set, minimum cut linear arrangement, and minimum area layout. However, their method appeared to be impractical as it required solving a large linear program. This paper shows that their method might be practical by giving an $O(m^2 \log m)$ expected-time randomized algorithm for their concurrent flow problem on an $m$-edge graph.  Raghavan and Thompson used uniform-capacity concurrent flow to solve approximately a channel width minimization problem in very large scale integration.  An randomized algorithm and an $O(k\min{n,k} (m+n\log n)\log k)$ deterministic algorithm is given for this problem when the channel width is $\Omega(\log n)$, where $k$ denotes the number of wires to be routed in an $n$-node, $m$-edge network."	approximation , multicommodity flow , VLSI routing , concurrent flow , graph separators	Faster Approximation Algorithms for the Unit Capacity Concurrent Flow Problem with Applications to Routing and Finding Sparse Cuts.	en	210
211	Algorithms are developed for reliable and accurate evaluations of the complex elementary functions required in FORTRAN 77 and FORTRAN 9, namely, cabs, csqrt, cexp, clog, csin, and ccos. The algorithms are presented in a pseudocode that has a convenient exception-handling facility. A tight error bound is derived for each algorithm. Corresponding FORTRAN programs for an IEEE environment have also been developed to illustrate the practicality of the algorithms, and these programs have been tested very carefully to help confirm the correctness of the algorithms and their error bounds. The results of these tests are included in the paper, but the FORTRAN programs are not.	implementation , complex elementary functions	Implementing complex elementary functions using exception handling.	en	211
212	In this article, we explore the use of genetic algorithms (GAs) as a key element in the design and implementation of robust concept learning systems. We describe and evaluate a GA-based system called GABIL that continually learns and refines concept classification rules from its interaction with the environment. The use of GAs is motivated by recent studies showing the effects of various forms of bias built into different concept learning systems, resulting in systems that perform well on certain concept classes (generally, those well matched to the biases) and poorly on others. By incorporating a GA as the underlying adaptive search mechanism, we are able to construct a concept learning system that has a simple, unified architecture with several important features. First, the system is surprisingly robust even with minimal bias. Second, the system can be easily extended to incorporate traditional forms of bias found in other concept learning systems. Finally, the architecture of the system encourages explicit representation of such biases and, as a result, provides for an important additional feature: the ability to dynamically adjust system bias. The viability of this approach is illustrated by comparing the performance of GABIL with that of four other more traditional concept learners (AQ14, C4.5, ID5R, and IACL) on a variety of target concepts. We conclude with some observations about the merits of this approach and about possible extensions.	genetic algorithms , bias adjustment , concept learning	Using Genetic Algorithms for Concept Learning.	en	212
213	Neural networks, despite their empirically proven abilities, have been little used for the refinement of existing knowledge because this task requires a three-step process. First, knowledge must be inserted into a neural network. Second, the network must be refined. Third, the refined knowledge must be extracted from the network. We have previously described a method for the first step of this process. Standard neural learning techniques can accomplish the second step. In this article, we propose and empirically evaluate a method for the final, and possibly most difficult, step. Our method efficiently extracts symbolic rules from trained neural networks. The four major results of empirical tests of this method are that the extracted rules 1) closely reproduce the accuracy of the network from which they are extracted&semi; 2) are superior to the rules produced by methods that directly refine symbolic rules&semi; are superior to those produced by previous techniques for extracting rules from trained neural networks&semi; and are human comprehensible. Thus, this method demonstrates that neural networks can be used to effectively refine symbolic knowledge. Moreover, the rule-extraction technique developed herein contributes to the understanding of how symbolic and connectionist approaches to artificial intelligence can be profitably integrated.	representational shift , rule extraction from neural networks , integrated learning , theory refinement	Extracting Refined Rules from Knowledge-Based Neural Networks.	en	213
214	An $n \times n$ matrix polynomial $L(\lambda)$ (with real or complex coefficients) is called self-adjoint if Factorizations of selfadjoint and symmetric matrix polynomials of the form are studied, where $D$ is a constant matrix and $M(\lambda)$ is a matrix polynomial.  In particular, the minimal possible size of $D$ is described in terms of the elementary divisors of $L(\lambda)$ and (sometimes) signature of the Hermitian values of $L(\lambda)$.	factorization , matrix polynomials , symmetries	Factorization of Matrix Polynomials with Symmetries.	en	214
215	Loops are a large source of parallelism for many numerical applications. An important issue in the parallel execution of loops is how to schedule them so that the workload is well balanced among the processors. Most existing loop scheduling algorithms were designed for shared-memory multiprocessors, with uniform memory access costs. These approaches are not suitable for distributed-memory multiprocessors where data locality is a major concern and communication costs are high. This paper presents a new scheduling algorithm in which data locality is taken into account. Our approach combines both worlds, static and dynamic scheduling, in a two-level (overlapped) fashion. This way data locality is considered and communication costs are limited. The performance of the new algorithm is evaluated on a CM-5 message-passing distributed-memory multiprocessor.	distributed-memory multiprocessors , dynamic and static scheduling , loop scheduling , load balancing , message-passing	Combining static and dynamic scheduling on distributed-memory multiprocessors.	en	215
216	Because the spatial locality of numerical codes is significant, the potential for performance improvements is important. However, large cache lines cannot be used in current on-chip data caches because of the important pollution they breed. In this paper, we propose a hardware design, called the Virtual Line Scheme, that allows the utilization of large virtual cache lines when fetching data from memory for better exploitation of spatial locality, while the actual physical cache line is smaller than currently found cache lines for better exploitation of temporal locality. Simulations show that a 17% to 64% reduction of the average memory access time can be obtained for a 20-cycle memory latency. It is also shown how simple software informations can be used to significantly decrease memory traffic, a flaw associated with the utilization of large cache lines.	numerical codes , memory hierarchy , temporal locality , spatial locality , cache architecture	Using virtual lines to enhance locality exploitation.	en	216
217	We address the development of efficient methods for performing data redistribution of arrays on distributed-memory machines. Data redistribution is important for the distributed-memory implementation of data parallel languages such as High Performance Fortran. An algebraic representation of regular data distributions is used to develop an analytical model for evaluating the communication cost of data redistribution. Using this algebraic representation and the analytical model, an approach to communication-efficient data redistribution is developed. Implementation results on the Intel iPSC/860 are reported.	tensor products , data distribution , high performance Fortran , data communication , distributed-memory machine	An approach to communication-efficient data redistribution.	en	217
218	The main theorem of this paper is that, for every real number $\alpha<1$ (e.g., $\alpha=0.99$), only a measure 0 subset of the languages decidable in exponential time are $\leq^{P}_{n^{\alpha} - tt}$-reducible to languages that are not exponentially dense.  Thus every $\leq^{P}_{n^{\alpha} - tt}$-hard language for E is exponentially dense.  This strengthens Watanabe's 1987 result, that every $\leq^{P}_{(O \log n)-tt}$-hard language for E is exponentially dense.  The combinatorial technique used here, the sequentially most frequent query selection, also gives a new, simpler proof of Watanabe's result.  The main theorem also has implications for the structure of NP under strong hypotheses.  Ogiwara and Watanabe (1991) have shown that the hypothesis $\p\ne\NP$ implies that every $\leq^{P}_{btt}$-hard language for NP is nonsparse (i.e., not polynomially sparse).  Their technique does not appear to allow significant relaxation of either the query bound or the sparseness criterion.  It is shown here that a stronger hypothesis---namely, that NP does not have measure 0 in exponential time---implies the stronger conclusion that, for every real $\alpha<1$, every $\leq^{P}_{n^{\alpha} - tt}$-hard language for NP is exponentially dense.  Evidence is presented that this stronger hypothesis is reasonable.   The proof of the main theorem uses a new, very general weak stochasticity theorem, ensuring that almost every language in E is statistically unpredictable by feasible deterministic algorithms, even with linear nonuniform advice.	dense languages , resource-bounded measure , complexity classes , computational complexity , sparse languages , weak stochasticity , polynomial reductions	Measure, Stochasticity, and the Density of Hard Languages.	en	218
219	The impact of cache interferences on program performance (particularly numerical codes, which heavily use the memory hierarchy) remains unknown. The general knowledge is that cache interferences are highly irregular, in terms of occurrence and intensity. In this paper, the different types of cache interferences that can occur in numerical loop nests are identified. An analytical method is developed for detecting the occurrence of interferences and, more important, for computing the number of cache misses due to interferences. Simulations and experiments on real machines show that the model is generally accurate and that most interference phenomena are captured. Experiments also show that cache interferences can be intense and frequent. Certain parameters such as array base addresses or dimensions can have a strong impact on the occurrence of interferences. Modifying these parameters only can induce global execution time variations of 30% and more. Applications of these modeling techniques are numerous and range from performance evaluation and prediction to enhancement of data locality optimizations techniques.	numerical codes , performance evaluation , data locality , cache interferences or conflicts , modeling	Cache interference phenomena.	en	219
220	Three different interfaces were used to browse a large (1296 items) table of contents. A fully expanded stable interface, expand/contract interface, and multipane interface were studied in a between-groups experiment with 41 novice participants. Nine timed fact retrieval tasks were performed; each task is analyzed and discussed separately. We found that both the expand/contract and multipane interfaces produced significantly faster times than the stable interface for many tasks using this large hierarchy; other advantages of the expand/contract and multipane interfaces over the stable interface are discussed. The animation characteristics of the expand/contract interface appear to play a major role. Refinements to the multipane and expand/contract interfaces are suggested. A predictive model for measuring navigation effort of each interface is presented.	user interfaces , browsing , table of contents , hierarchies	An exploratory evaluation of three interfaces for browsing large hierarchical tables of contents.	en	220
221	This paper presents an optimized general-purpose algorithm for polyvariant, static analyses of higher-order applicative programs. A polyvariant analysis is a very accurate form of analysis that produces many more abstract descriptions for a program than does a conventional analysis. It may also compute intermediate abstract descriptions that are irrelevant to the final result of the analysis. The optimized algorithm addresses this overhead while preserving the accuracy of the analysis. The algorithm is also parameterized over both the abstract domain and degree of polyvariance. We have implemented an instance of our algorithm and evaluated its performance compared to the unoptimized algorithm. Our implementation runs significantly faster on average than the other algorithm for benchmarks reported here.	program analysis , fixpoint algorithm , abstract interpretation	Fixpoint computation for polyvariant static analyses of higher-order applicative programs.	en	221
222	Composite systems are generally comprised of heterogeneous components whose specifications are developed by many development participants. The requirements of such systems are invariably elicited from multiple perspectives that overlap, complement, and contradict each other. Furthermore, these requirements are generally developed and specified using multiple methods and notations, respectively. It is therefore necessary to express and check the relationships between the resultant specification fragments. We deploy multiple ViewPoints that hold partial requirements specifications, described and developed using different representation schemes and development strategies. We discuss the notion of inter-ViewPoint communication in the context of this ViewPoints framework, and propose a general model for ViewPoint interaction and integration. We elaborate on some of the requirements for expressing and enacting inter-ViewPoint relationships-the vehicles for consistency checking and inconsistency management. Finally, though we use simple fragments of the requirements specification method CORE to illustrate various components of our work, we also outline a number of larger case studies that we have used to validate our framework. Our computer-based ViewPoints support environment, The Viewer, is also briefly described.	CORE , requirements specification method , viewpoints framework , inconsistency management , requirements specification , inter-ViewPoint communication , the viewer , multiple ViewPoints , consistency checking , heterogeneous components , computer-based ViewPoints support , formal specification , multiple views , partial requirements specifications	A Framework for Expressing the Relationships Between Multiple Views in Requirements Specification.	en	222
223	"Features are often the basic unit of development for a very large software system and represent long-term efforts, spanning up to several years from inception to actual use. Developing an experiment to monitor (by means of sampling) such lengthy processes requires a great deal of care in order to minimize casts and to maximize benefits. Just as prototyping is often a necessary auxiliary step in a large-scale, long-term development effort, so, too, is prototyping a necessary step in the development of a large-scale, long-term process monitoring experiment. Therefore, we have prototyped our experiment using a representative process and reconstructed data from a large and rich feature development. This approach has yielded three interesting sets of results. First, we reconstructed a 30-month time diary for the lead engineer of a feature composed of both hardware and software. These data represent the daily state (where the lead engineer spent the majority of his time) for a complete cycle of the development process. Second, we found that we needed to modify our experimental design. Our initial set of states did not represent the data as well as we had hoped. This is exemplified by the fact that the ""Other"" category is too large. Finally, the data provide evidence for both a waterfall view and an interactive, cyclic view of software development. We conclude that the prototyping effort is a necessary part of developing and installing any large-scale process monitoring experiment."	process computer control , interactive cyclic view , process monitoring experiment , large-scale long-term development , software , very large software system , software development , computerised monitoring , experimental design , long-term process monitoring , software prototyping , large-scale process monitoring experiment , waterfall view , prototyping , hardware	Prototyping a Process Monitoring Experiment.	en	223
224	We consider time slot interchangers (TSIs) which are built from 2/spl times/2 exchange switches and delays and which are useful for time division multiplexed (TDM) systems in telecommunications and pipelined systems such as time multiplexed optical multiprocessors. We formulate a general method for constructing TSIs based on multistage interconnection networks in the space domain via space-to-time mapping. Two types of TSIs, time slot permuters and time slot sorters, are considered. We review the time slot permuter based on the Benes network, and obtain the /spl Lambdaspl tilde/ time slot permuter based on the bit-controlled, self-routing /spl Lambda/ permutation network. The time slot sorter, S/sub N/, is obtained from the Batcher spatial sorting network. The generalized Lambda time slot permuter /spl Lambdasub Nsup q/ is obtained, in an algorithmic approach, by combining the idea of the /spl Lambdaspl tilde/ time slot permuter and Q-way bitonic decomposition (Q=2/sup q/). The numbers of switches, control complexities, and frame delays of these architectures are compared, and the problem of crosstalk in optical implementation is discussed. It is shown that control complexity can be traded against the number of switches.	benes network , time slot sorters , telecommunications , time division multiplexing , multiprocessor interconnection networks , optical implementations , optical information processing , serial array time slot interchangers , bitonic decomposition , crosstalk , time division multiplexed systems , multistage interconnection networks , control complexity , exchange switches , space-to-time mapping , pipelined systems , logic arrays , optical interconnections , time multiplexed optical multiprocessors , batcher spatial sorting network , self-routing /spl Lambda/ permutation network , time slot permuters	Serial Array Time Slot Interchangers and Optical Implementations.	en	224
225	Several techniques have been proposed to allow parallel access to a shared memorylocation by combining requests. They have one or more of the following attributes:requirements for a priori knowledge of the request to combine, restrictions on the routingof messages in the network, or the use of sophisticated interconnection network nodes.We present a new method of combining requests that does not have the aboverequirements. We obtain this new method for request combining by developing aclassification scheme for the existing methods of request combining. This classificationscheme is facilitated by separating the request combining process into a two partoperation: determining the combining set, which is the set of requests that participate ina combined access; and distributing the results of the combined access to the membersof the combining set. The classification of combining strategies is based upon whichsystem component, processor elements, or interconnection network performs each ofthese tasks. Our approach, which uses the interconnection network to establish thecombining set and the processor elements to distribute the results, lies in an unexploredarea of the design space. We also present simulation results to assess the benefits of theproposed approach.	message passing , parallel architectures , multiprocessors , index termsmultiprocessor interconnection networks , combining strategies , message routing , hot spots , arbitrary interconnection networks , design space , combining set , simulationresults , shared memory systems , parallel access , shared memory location , classification scheme , virtual machines , processor elements	Request Combining in Multiprocessors with Arbitrary Interconnection Networks.	en	225
226	It is important for a distributed computing system to be able to route messages aroundwhatever faulty links or nodes may be present. We present a fault-tolerant routingalgorithm that assures the delivery of every message as long as there is a path betweenits source and destination. The algorithm works on many common mesh architecturessuch as the torus and hexagonal mesh. The proposed scheme can also detect thenonexistence of a path between a pair of nodes in a finite amount of time. Moreover, thescheme requires each node in the system to know only the state (faulty or not) of eachof its own links. The performance of the routing scheme is simulated for both square andhexagonal meshes while varying the physical distribution of faulty components. It isshown that a shortest path between the source and destination of each message istaken with a high probability, and, if a path exists, it is usually found very quickly.	software reliability , parallel architectures , torus , message routing , hexagonal meshes , parallelalgorithms , fault-tolerant routing , square meshes , mesh architectures , source , destination , fault-tolerant routing algorithm , index termsmessage passing , distributedcomputing system , hexagonal mesh , routing scheme performance , network routing , fault tolerant computing , high probability	Fault-Tolerant Routing in Mesh Architectures.	en	226
227	Active learning differs from learning from examples in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples.In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers useful. We test our implementation, called an SG-network, on three domains and observe significant improvement in generalization.	active learning , version space , neural networks , queries , generalization	Improving Generalization with Active Learning.	en	227
228	We present a linear algebraic formulation for a class of index transformations such asGray code encoding and decoding, matrix transpose, bit reversal, vector reversal,shuffles, and other index or dimension permutations. This formulation unifies, simplifies,and can be used to derive algorithms for hypercube multiprocessors. We show how all the widely known properties of Gray codes, and some not so well-known properties as well, can be derived using this framework. Using this framework, we relate hypercube communications algorithms to Gauss-Jordan elimination on a matrix of 0's and 1's.	gray code encoding , matrix transpose , hypercube communications algorithms , index termslinear algebra , shuffles , bit reversal , encoding , hypercube multiprocessors , linear algebra framework , gray codes , indextransformation algorithms , vector reversal , gauss-jordan elimination , decoding , hypercube networks	Index Transformation Algorithms in a Linear Algebra Framework.	en	228
229	It is expected that in the near future, tens of millions of users will have access to distributed information systems through wireless connections. The technical characteristics of the wireless medium and the resulting mobility of both data resources and data consumers raise new challenging questions regarding the development of information systems appropriate for mobile environments. In this paper, we report on the development of such a system. First, we describe the general architecture of the information system and the main considerations of our design. Then, based on these considerations, we present our system support for maintaining the consistency of replicated data and for providing transaction schemas that account for the frequent but predictable disconnections, the mobility, and the vulnerability of the wireless environment.	mobile computing , new applications , information systems , consistency , transaction management	Building information systems for mobile environments.	en	229
230	The development of software for minimization problems is often based on a line search method. We consider line search methods that satisfy sufficient decrease and curvature conditions, and formulate the problem of determining a point that satisfies these two conditions in terms of finding a point in a set T(&mgr;).  We describe a search algorithm for this problem that produces a sequence of iterates that converge to a point in T(&mgr;) and that, except for pathological cases, terminates in a finite number of steps. Numerical results for an implementation of the search algorithm on a set of test functions show that the algorithm terminates within a small number of iterations.	variable metric algorithms , line search algorithms , conjugate gradient algorithms , nonlinear optimization , truncated Newton algorithms	Line search algorithms with guaranteed sufficient decrease.	en	230
231	We present a method for determining a posteriori bounds and estimates for local and total errors in radiosity solutions. The ability to obtain bounds and estimates for the total error is crucial fro reliably judging the acceptability of a solution. Realistic estimates of the local error improve the efficiency of adaptive radiosity algorithms, such as hierarchical radiosity, by indicating where adaptive refinement is necessary.First, we describe a hierarchical radiosity algorithm that computes conservative lower and upper bounds on the exact radiosity function, as well as on the approximate solution. These bounds account for the propagation of errors due to interreflections, and provide a conservative upper bound on the error. We also describe a non-conservative version of the same algorithm that is capable of computing tighter bounds, from which more realistic error estimates can be obtained. Finally, we derive an expression for the effect of a particular interaction on the total error. This yields a new error-driven refinement strategy for hierarchical radiosity, which is shown to be superior to brightness-weighted refinement.	adaptive refinement , a posteriori error bounds and estimates , importance , hierarchical radiosity , global illumination	Bounds and error estimates for radiosity.	en	231
232	In this paper we identify sources of error in global illumination algorithms and derive bounds for each distinct category. Errors arise from three sources: inaccuracies in the boundary data, discretization, and computation. Boundary data consists of surface geometry, reflectance functions, and emission functions, all of which may be perturbed by errors in measurement or simulation, or by simplifications made for computational efficiency. Discretization error is introduced by replacing the continuous radiative transfer equation with a finite-dimensional linear system, usually by means of boundary elements and a corresponding projection method. Finally, computational errors perturb the finite-dimensional linear system through imprecise form factors, inner products, visibility, etc., as well as by halting iterative solvers after a finite number of steps. Using the error taxonomy introduced in the paper we examine existing global illumination algorithms and suggest new avenues of research.	discretization , linear operators , boundary elements , radiosity , error bounds , global illumination , projection methods , reflectance functions	A framework for the analysis of error in global illumination algorithms.	en	232
233	We describe a data visualization system based on spreadsheets. Cells in our spreadsheet contain graphical objects such as images, volumes, or movies. Cells may also contain widgets such as buttons, sliders, or curve editors. Objects are displayed in miniature inside each cell. Formulas for cells are written in a general-purpose programming language (Tcl) augmented with operators for array manipulation, image processing, and rendering.Compared to flow chart visualization systems, spreadsheets are more expressive, morescalable, and easier to program. Compared to conventional numerical spreadsheets, spreadsheets for images pose several unique design problems: larger formulas, longer computation times, and more complicated intercelldependencies. In response to these problems, we have extended the spreadsheet paradigm in three ways: formulas can display their results anywhere in the spreadsheet, cells can be selectively disabled, and multiple cells can be edited at once. We discuss these extensions and their implications, and we also point out some unexpected uses for our spreadsheets: as a visual database browser, as a graphical user interface builder, as a smart clipboard for the desktop, and as a presentation tool.	visual programming languages , user interfaces , flow charts , data visualization , spreadsheets	Spreadsheets for images.	en	233
234	Lambert's model for body reflection is widely used in computer graphics. It is used extensively by rendering techniques such as radiosity and ray tracing. For several real-world objects, however, Lambert's model can prove to be a very inaccurate approximation to the body reflectance. While the brightness of a Lambertian surface is independent of viewing direction, that of a rough surface increases as the viewing direction approaches the light source direction. In this paper, a comprehensive model is developed that predicts body reflectance from rough surfaces. The surface is modeled as a collection of Lambertian facets. It is shown that such a surface is inherently non-Lambertian due to the foreshortening of the surface facets. Further, the model accounts for complex geometric and radiometric phenomena such as masking, shadowing, and interreflections between facets. Several experiments have been conducted on samples of rough diffuse surfaces, such as, plaster, sand, clay, and cloth. All these surface demonstrate significant deviation from Lambertian behavior. The reflectance measurements obtained are in strong agreement with the reflectance predicted by the model.	moon reflectance , BRDF , reflection models , rough surfaces , lambert's model	Generalization of Lambert''s reflectance model.	en	234
235	We present an approach to modeling with truly mutable yet completely controllable free-form surfaces of arbitrary topology. Surfaces may be pinned down at points and along curves, cut up and smoothly welded back together, and faired and reshaped in the large. This style of control is formulated as a constrained shape optimization, with minimization of squared principal curvatures yielding graceful shapes that are free of the parameterization worries accompanying many patch-based approaches. Triangulated point sets are used to approximate these smooth variational surfaces, bridging the gap between patch-based and particle-based representations. Automatic refinement, mesh smoothing, and re-triangulation maintain a good computational mesh as the surface shape evolves, and give sample points and surface features much of the freedom to slide around in the surface that oriented particles enjoy. The resulting surface triangulations are constructed and maintained in real time.	delaunay triangulation , functional minimization , adaptive meshing , fair surface design , polygonal models	Free-form shape design using triangulated surfaces.	en	235
236	This article describes a new approach to the unit testing of object-oriented programs, a set of tools based on this approach, and two case studies. In this approach, each test case consists of a tuple of sequences of messages, along with tags indicating whether these sequences should put objects of the class under test into equivalent states and/or return objects that are in equivalent states. Tests are executed by sending the sequences to objects of the class under test, then invoking a user-supplied equivalence-checking mechanism. This approach allows for substantial automation of many aspects of testing, including test case generation, test driver generation, test execution, and test checking. Experimental prototypes of tools for test generation and test execution are described.  The test generation tool requires the availability of an algebraic specification of the abstract data type being tested, but the test execution tool can be used when no formal specification is available. Using the test execution tools, case studies involving execution of tens of thousands of test cases, with various sequence lengths, parameters, and combinations of operations were performed. The relationships among likelihood of detecting an error and sequence length, range of parameters, and relative frequency of various operations were investigated for priority queue and sorted-list implementations having subtle errors. In each case, long sequences tended to be more likely to detect the error, provided that the range of parameters was sufficiently large and likelihood of detecting an  error tended to increase up to a threshold value as the parameter range increased.	abstract data types , software testing , object-oriented programming , algebraic specification	The ASTOOT approach to testing object-oriented programs.	en	236
237	We describe a multiresolution curve representation, based on wavelets, that conveniently supports a variety of operations: smoothing a curve; editing the overall form of a curve while preserving its details; and approximating a curve within any given error tolerance for scan conversion. We present methods to support continuous levels of smoothing as well as direct manipulation of an arbitrary portion of the curve; the control points, as well as the discrete nature of the underlying hierarchical representation, can be hidden from the user. The multiresolution representation requires no extra storage beyond that of the original control points, and the algorithms using the representation are both simple and fast.	curve fitting , curve smoothing , direct manipulation , curve compression , scan conversion , wavelets , curve editing	Multiresolution curves.	en	237
238	This article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. Experience has shown that most software engineers find standard temporal logics difficult to understand and use. The objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. To illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. The article also describes the tool set and the implementation.	concurrent systems , visual languages , timing diagrams , automated proof-checking , formal specifications , temporal logic , graphical interval logic	A graphical interval logic for specifying concurrent systems.	en	238
239	We present a new particle-based approach to sampling and controlling implicit surfaces. A simple constraint locks a set of particles onto a surface while the particles and the surface move. We use the constraint to make surfaces follow particles, and to make particles follow surfaces. We implement control points for direct manipulation by specifying particle motions, then solving for surface motion that maintains the constraint. For sampling and rendering, we run the constraint in the order direction, creating floater particles that roam freely over the surface. Local repulsion is used to make floaters spread evenly across the surface. By varying the radius of repulsion adaptively, and fissioning or killing particles based on the local density, we can achieve good sampling distributions very rapidly, and maintain them even in the face of rapid and extreme deformations and changes in surface topology.	constrained optimization , physically based modeling , interaction , adaptive sampling	Using particles to sample and control implicit surfaces.	en	239
240	We present a general method for automatic reconstruction of accurate, concise, piecewise smooth surface models from scattered range data. The method can be used in a variety of applications such as reverse engineeringthe automatic generation of CAD models from physical objects. Novel aspects of the method are its ability to model surfaces of arbitrary topological type and to recover sharp features such as creases and corners. The method has proven to be effective, as demonstrated by a number of examples using both simulated and real data.A key ingredient in the method, and a principal contribution of this paper, is the introduction of a new class of piecewise smooth surface representations based on subdivision. These surfaces have a number of properties that make  them ideal for use in surface reconstruction: they are simple to implement, they can model sharp features concisely, and they can be fit to scattered range data using an unconstrained optimization procedure.	subdivision surfaces , surface fitting , shape recovery , geometric modeling , range data analysis	Piecewise smooth surface reconstruction.	en	240
241	We present an approach for accelerating hierarchical radiosity by clustering objects. Previous approaches constructed effective hierarchies by subdividing surfaces, but could not exploit a hierarchical grouping on existing surfaces. This limitation resulted in an excessive number of initial links in complex environments. Initial linking is potentially the most expensive portion of hierarchical radiosity algorithms, and constrains the complexity of the environments that can be simulated. The clustering algorithm presented here operates by estimating energy transfer between collections of objects while maintaining reliable error bounds on each transfer. Two methods of bounding the transfers are employed with different tradeoffs between accuracy and time. In contrast with the O(s2) time and space complexity of the initial linking in previous hierarchical radiosity algorithms, the new methods have complexities of O(slogs) and O(s) for both time and space. Using these methods we have obtained speedups of two orders of magnitude for environments of moderate complexity while maintaining comparable accuracy.	hierarchical radiosity , error bounds , clustering , global illumination	A clustering algorithm for radiosity in complex environments.	en	241
242	Character User Interfaces (CUI) generally display only pieces of text and semi-graphical objects, whereas Graphical User Interfaces (GUI) display interaction objects (IO) such as icons, check boxes, list boxes, radio buttons and push buttons. Traditional GUI do not often go beyond such existing IO. Multimedia GUI add interactive objects such as pictures, images, video sequences that could serve as a base for sophisticated user interaction. All these types of user interfaces have in common the problem of determining a basic layout of IO. The complexity of this problem is proportional to the variety of IO accessible for the designer. This paper summarises visual techniques exported from the area of visual design to be further exploited for user interface. These visual techniques  provide the designer a wide range of means for laying out IO. A small set of guidelines for effectively applying these visual techniques is given.	layout , interactive objects , graphical applications , visual interaction , multimedia applications , visual techniques , visual placement , interaction objects , visual interface design and management	Visual techniques for traditional and multimedia layouts.	en	242
243	This paper presents the architecture for an extensible toolkit used in construction and rapid prototyping of three dimensional interfaces, interactive illustrations, and three dimensional widgets. The toolkit provides methods for the direct manipulation of 3D primitives which can be linked together through a visual programming language to create complex constrained behavior. Features of the toolkit include the ability to visually build, encapsulate, and parameterize complex models, and impose limits on the models. The toolkit's constraint resolution technique is based on a dynamic object model similar to those in prototype delegation object systems. The toolkit has been used to rapidly prototype tools for mechanical modelling, scientific visualization, construct 3D widgets, and build mathematical illustrations.	constraints , delegation , visual programming , user interface toolkits , direct manipulation , interaction techniques	An architecture for an extensible 3D interface toolkit.	en	243
244	Multimodal interaction combines input from multiple sensors such as pointing devices or speech recognition systems, in order to achieve more fluid and natural interaction. Two-handed interaction has been used recently to enrich graphical interaction. Building applications that use such combined interaction requires new software techniques and frameworks. Using additional devices means that user interface toolkits must be more flexible with regard to input devices and event types. The possibility of parallel interactions must also be taken into account, with consequences on the structure of toolkits. Finally, frameworks must be provided for the combination of events and status of several devices. This paper reports on the extensions we made to the direct manipulation interface toolkit Whizz in order to experiment two-handed interaction. These extensions range from structural adaptations of the toolkit to new techniques for specifying the time-dependent fusion of events.	interaction styles , graphical toolkit , direct manipulation , two-handed interaction , multimodal interaction	Extending a graphical toolkit for two-handed interaction.	en	244
245	Array and pointer references are often ambiguous in that compile time analysis cannot always determine if distinct references are to the same object. Ambiguously aliased objects are not allocated to registers by conventional compilers due to the cost of the loads and stores required to keep register copies consistent with memory and each other. There are several hardware and software strategies that can be used to solve the ambiguous alias problem: we have implemented one such scheme called CRegs in a compiler and instruction level simulator. We present a modification to Briggs' Optimistic Coloring Algorithm that allows us to allocate local and parameter arrays to CRegs. The CRegs register file operation and instruction set modifications required to implement this scheme are discussed. Underlying hardware issues such as pipeline impact and chip area are briefly discussed. Several benchmarks are compared in terms of dynamic instructions executed for two CReg set sizes. The measured reduction in memory operations is significant, averaging 23% for the benchmarks shown.	CRegs , register allocation , ambiguous alias , live range , graph coloring	Reducing memory traffic with CRegs.	en	245
246	Numerical applications frequently contain nested loop structures that process large arrays of data. The execution of these loop structures often produces memory reference patterns that poorly utilize data caches. Limited associativity and cache capacity result in cache conflict misses. Also, non-unit stride access patterns can cause low utilization of cache lines. Data copying has been proposed and investigated in order to reduce cache conflict misses, but this technique has a high execution overhead since it performs the copy operations entirely in software.We propose a combined hardware and software technique called data relocation and prefetching which eliminates much of the overhead of data copying through the use of special hardware. Furthermore, by relocating the data while performing software prefetching, the overhead of copying the data can be reduced further. Experimental results for data relocation and prefetching are encouraging and show a large improvement in cache performance.	cache conflicts , data relocation , software prefetching , program optimization , data copying	Data relocation and prefetching for programs with large data sets.	en	246
247	This paper explores a novel way to incorporate hardware-programmable resources into a processor microarchitecture to improve the performance of general-purpose applications. Through a coupling of compile-time analysis routines and hardware synthesis tools, we automatically configure a given set of the hardware-programmable functional units (PFUs) and thus augment the base instruction set architecture so that it better meets the instruction set needs of each application. We refer to this new class of general-purpose computers as PRogrammable Instruction Set Computers (PRISC). Although similar in concept, the PRISC approach differs from dynamically programmable microcode because in PRISC we define entirely-new primitive datapath operations. In this paper, we concentrate on the microarchitectural design of the simplest form of PRISCa RISC microprocessor with a single PFU that only evaluates combinational functions. We briefly discuss the operating system and the programming language compilation techniques that are needed to successfully build PRISC and, we present performance results from a proof-of-concept study. With the inclusion of a single 32-bit-wide PFU whose hardware cost is less than that of a 1 kilobyte SRAM, our study shows a 22% improvement in processor performance on the SPECint92 benchmarks.	compile-time optimization , general-purpose microarchitectures , logic synthesis , automatic instruction set design , programmable logic	A high-performance microarchitecture with hardware-programmable functional units.	en	247
248	We propose a new navigation paradigm based on a spatial metaphor to help users access and navigate within large sets of documents. This metaphor is implemented by a computer artifact called an Interactive Dynamic Map (IDM). An IDM plays a role similar to the role of a real map with respect to physical space. Two types of IDMs are computed from the documents: Topic IDMs represent the semantic contents of a set of documents while Document IDMs visualize a subset of documents such as those resulting from a query. IDMs can be used for navigating, browsing, and querying. They can be made active, they can be customized and they can be shared among users. The article presents the SHADOCS document retrieval system and describes the role, use and generation of IDMs in SHADOCS.	visualization , navigation , information retrieval , interaction paradigm	Accessing hyperdocuments through interactive dynamic maps.	en	248
249	In semantically rich hypertexts it is attractive to enable presentation of a network of nodes and link at different levels of abstraction. It is also important that the user can interact with the hypertext using a command repertoire that reflects the chosen abstraction level. Based on a characterization of rich hypertext we introduce the concept of an interaction engine that governs the separation between internal hypertext representation and external screen presentation. This separation is the key principle of the HyperPro system. The HyperPro interaction engine is based on simple rules for presentation, interpretation of events, and menu set up. Much of the power of the interaction engine framework comes from the organization of these rules relative to the type of hierarchy of nodes and links, and relative to a hierarchy of so-called interaction schemes. The primary application domain discussed in the paper is program development and program documentation.	program development , event control , interaction engine , tailorability , aggregated views	An interaction engine for rich hypertexts.	en	249
250	The correct and timely creation of systems for coordination of group work depends on the ability to express, analyze, and experiment with protocols for managing multiple work threads. We present an evolution of the Trellis model that provides a formal basis for prototyping the coordination structure of a collaboration system. In Trellis, group interaction protocols are represented separately from the interface processes that use them for coordination. Protocols are interpreted (rather than compiled into applications) so group interactions can be changed as a collaborative task progresses. Changes can be made either by a person editing the protocol specification on the fly or by a silent observation process that participates in an application solely to  perform behavioral adaptations.Trellis uniquely mixes hypermedia browsing with collaboration support. We term this combination a hyperprogram, and we say that a hyperprogram integrates the description of a collaborative task with the information required for that task. As illustration, we describe a protocol for a moderated meeting and show a Trellis prototype conference tool controlled by this protocol.	dynamic protocol , coordination structure , formal methods , moderated meeting , process-based hypertext/hypermedia , colored Petri net , trellis	Interpreted collaboration protocols and their use in groupware prototyping.	en	250
251	The need to merge different versions of an object to a common state arises in collaborative computing due to several reasons including optimistic concurrency control, asynchronous coupling, and absence of access control. We have developed a flexible object merging framework that allows definition of the merge policy based on the particular application and the context of the collaborative activity. It performs automatic, semi-automatic, and interactive merges, supports semantics-determined merges, operates on objects with arbitrary structure and semantics, and allows fine-grained specification of merge policies. It is based on an existing collaborative applications framework and consists of a merge matrix, which defines merge functions and their parameters and allows definition of  multiple merge policies, and a merge algorithm, which performs the merge based on the results computed by the merge functions. In conjunction with our framework we introduce a set of merge policies for several useful kinds of merges we have identified. This paper motivates the need for a general approach to merging, identifies some important merging issues, surveys previous research in merging, identifies a list of merge requirements, describes our merging framework and illustrates it with examples, and evaluates the framework with respect to the requirements and other research efforts in merging objects.	optimistic concurrency control , merging , flexible coupling , versions	A flexible object merging framework.	en	251
252	Computer technology is available to build video-based tools for supporting presentations to distributed audiences, but it is unclear how such an environment affects participants' ability to interact and to learn.  We built and tested a tool called Forum that broadcasts live audio, video and slides from a speaker, and enables audiences to interact with the speaker and other audience members in a variety of ways. The challenge was to enable effective interactions while overcoming obstacles introduced by the distributed nature of the environment, the large size of the group, and the asymmetric roles of the participants. Forum was most successful in enabling effective presentations in cases when the topic sparked a great deal of audience participation or when the purpose of the talk was  mostly informational and did not require a great deal of interaction. We are exploring ways to enhance Forum to expand the effectiveness of this technology.	remote collaboration , multimedia , distance learning , distributed presentations , user interface design , broadcast video	A forum for supporting interactive presentations to distributed audiences.	en	252
253	The chaining problem is defined as follows. Given values The chaining problem appears as a subproblem in many contexts. There are known algorithms that solve the chaining problem on CRCW PRAMs in $O(\alpha(n))$ time, where $\alpha(n)$ is the inverse of Ackerman's function, and is a very slowly growing function. The author studies a  class of algorithms (called oblivious algorithms) for this problem.  A simple oblivious chaining algorithm running in $O(\alpha(n))$ time is presented. More importantly, the optimality of the algorithm is demonstrated by showing a matching lower  bound for oblivious algorithms using $n$ processors. The first  steps toward a lower bound for all chaining algorithms  are also provided by showing that any chaining algorithm that  runs in two steps must use a superlinear number of processors.  The proofs use prefix graphs and  weak superconcentrators.  An interesting connection  between the two is demonstrated and this idea is used to obtain improved bounds on the size of prefix graphs.	ackerman's function , parallel , superconcentrators , lower bound , prefix graphs , chaining	Tight Bounds on Oblivious Chaining.	en	253
254	Managing tradeoffs between program structure and program efficiency is one of the most difficult problems facing software engineers. Decomposing programs into abstractions simplifies the construction and maintenance of software and results in fewer errors. However, the introduction of these abstractions often introduces significant inefficiencies.This paper describes a strategy for eliminating many of these inefficiencies. It is based upon providing alternative implementations of the same abstraction, and using information contained in formal specifications to allow a compiler to choose the appropriate one. The strategy has been implemented in a prototype compiler that incorporates theorem proving technology.	compilers , program optimization , program modularity , formal specifications , software interfaces	Using specialized procedures and specification-based analysis to reduce the runtime costs of modularity.	en	254
255	Tapeworm II is a software-based simulation tool that evaluates the cache and TLB performance of multiple-task and operating system intensive workloads. Tapeworm resides in an OS kernel and causes a host machine's hardware to drive simulations with kernel traps instead of with address traces, as is conventionally done. This allows Tapeworm to quickly and accurately capture complete memory referencing behavior with a limited degradation in overall system performance. This paper compares trap-driven simulation, as implemented in Tapeworm, with the more common technique of trace-driven memory simulation with respect to speed, accuracy, portability and flexibility.	trap-driven simulation , trace-driven simulation , cache , TLB , memory system	Trap-driven simulation with Tapeworm II.	en	255
256	Several researchers have proposed algorithms for basic block reordering. We call these branch alignment algorithms. The primary emphasis of these algorithms has been on improving instruction cache locality, and the few studies concerned with branch prediction reported small or minimal improvements. As wide-issue architectures become increasingly popular the importance of reducing branch costs will increase, and branch alignment is one mechanism which can effectively reduce these costs.In this paper, we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and the branch prediction architecture when performing the basic block reordering. We show that branch alignment algorithms can improve a broad range  of static and dynamic branch prediction architectures. We also show that a program performance can be improved by approximately 5% even when using recently proposed, highly accurate branch prediction architectures. The programs are compiled by any existing compiler and then transformed via binary transformations. When implementing these algorithms on a Alpha AXP 21604 up to a 16% reduction in total execution time is achieved.	profile-based optimization , trace scheduling , branch target buffers , branch prediction	Reducing branch costs via branch alignment.	en	256
257	An articulated figure is often modeled as a set of rigid segments connected with joints. Its configuration can be altered by varying the joint angles. Although it is straight forward to compute figure configurations given joint angles (forward kinematics), it is more difficult to find the joint angles for a desired configuration (inverse kinematics). Since the inverse kinematics problem is of special importance to an animator wishing to set a figure to a posture satisfying a set of positioning constraints, researchers have proposed several different approaches. However, when we try to follow these approaches in an interactive animation system where the object on which to operate is as highly articulated as a realistic human figure, they fail in either generality or performance. So, we  approach this problem through nonlinear programming techniques. It has been successfully used since 1988 in the spatial constraint system within Jack, a human figure simulation system developed at the University of Pennsylvania, and proves to be satisfactorily efficient, controllable, and robust. A spatial constraint in our system involves two parts: one constraint on the figure, the end-effector, and one on the spatial environment, the goal. These two parts are dealt with separately, so that we can achieve a neat modular implementation. Constraints can be added one at a time with appropriate weights designating the importance of this constraint relative to the others and are always solved as a group. If physical limits prevent  satisfaction of all the constraints, the system stops with the (possibly local) optimal solution for the given weights. Also, the rigidity of each joint angle can be controlled, which is useful for redundant degrees of freedom.	nonlinear programming , articulated figures , inverse kinematics	Inverse kinematics positioning using nonlinear programming for highly articulated figures.	en	257
258	Perturbed nonlinear control problems with data depending  on a vector parameter are considered. Using second-order sufficient  optimality conditions, it is shown that the optimal solution and the adjoint  multipliers are differentiable functions of the parameter.  The proof  exploits the close connections between solutions of a Riccati differential  equation and shooting methods for solving the associated boundary value  problem.  Solution differentiability provides a firm theoretical basis for  numerical feedback schemes that have been developed for computing  neighbouring extremals.  The results are illustrated by an example that  admits two extremal solutions. Second-order sufficient conditions single out  one optimal solution for which a sensitivity analysis is carried out.	parametric control problems , solution differentiability , feedback controls , shooting methods , second-order sufficient conditions	Solution Differentiability for Nonlinear Parametric Control Problems.	en	258
259	It is shown that indirect pole-zero placement adaptive controllers are robust for systems with time-varying parameters as well as unmodeled dynamics and disturbances.  A parameter estimator with projection is used.  No special signal normalization is employed to ensure robustness.  The nominal system parameters need only be bounded, and their variations need only be small in an average sense.  This allows them to vary slowly with time, as well as to take large jumps occasionally. The adaptive controllers can also simultaneously tolerate small unmodeled dynamics, as well as bounded disturbances, with no restriction on the magnitude of the bound.	robust performance , robustness , adaptive systems , indirect adaptivecontrol , unmodeled dynamics , time-varyingplants , stability , disturbances	Robust Indirect Adaptive Control of Time-Varying Plants with Unmodeled Dynamics and Disturbances.	en	259
260	The page migration problem is to manage a globally addressed shared memory in a multiprocessor system.  Each physical page of memory is located at a given processor, and memory references to that page by other processors incur a cost proportional to the network distance. At times the page may migrate between processors at cost proportional to the distance times $D$, a page size factor.  The problem is to schedule movements on-line so that the total cost of memory references is within a constant factor $c$ of the best off-line schedule.  An algorithm that does so is called c-competitive.  Black and Sleator gave 3-competitive deterministic on-line algorithms for uniform networks (complete graphs with unit edge lengths) and for trees with arbitrary edge lengths. No good deterministic algorithm is known for general networks with arbitrary edge lengths.  Randomized algorithms are presented for the migration problem that are both simple and better than 3-competitive against an oblivious adversary. An algorithm for uniform graphs is given. It is approximately 2.28-competitive as $D$ grows large. A second, more powerful algorithm that works on graphs with arbitrary edge distances is also given. This algorithm is approximately 2.62-competitive (or, 1 plus the golden ratio) for large $D$. Both these algorithms use random bits only during an initialization phase, and from then on run deterministically. The competitiveness of a very simple coin-flipping algorithm is also examined.	multiprocessors , page migration , competitive analysis , memory management , on-line algorithms	Randomized Algorithms for Multiprocessor Page Migration.	en	260
261	Many data-parallel algorithmsFast Fourier Transform, Batcher's sorting schemes, and the prefix-sumexhibit recursive structure. We propose a data structure called powerlist that permits succinct descriptions of such algorithms, highlighting the roles of both parallelism and recursion. Simple algebraic properties of this data structure can be explotied to derive properties of these algorithms and to establish equivalence of different algorithms that solve the same problem.	hypercube , algebra of parallel programs , prefix sum , batcher sort , parallel programs , recursion , Fast Fourier Transform	Powerlist: a structure for parallel recursion.	en	261
262	Communication between processors has long been the bottleneck of distributed network computing. However, recent progress in switch-based high-speed Local Area Networks (LANs) may be changing this situation. Asynchronous Transfer Mode (ATM) is one of the most widely-accepted and emerging high-speed network standards which can potentially satisfy the communication needs of distributed network computing. In this paper, we investigate distributed network computing over local ATM networks. We first study the performance characteristics involving end-to-end communication in an environment that includes several types of workstations interconnected via a Fore Systems' ASX-100 ATM Switch. We then compare the communication performance of four different Application Programming Interfaces (APIs). The four APIs were Fore Systems ATM API, BSD socket programming interface, Sun's Remote Procedure Call (RPC), and the Parallel Virtual Machine (PVM) message passing library. Each API represents distributed programming at a different communication protocol layer. We evaluate parallel Matrix Multiplication over the local ATM network. The experimental results show that network computing is promising over local ATM networks.	performance measurement , Asynchronous Transfer Mode , application programming interface , distributed network computing	Distributed network computing over local ATM networks.	en	262
263	AbstractThere are three projective invariants of a set of six points in general position in space. It is well known that these invariants cannot be recovered from one image, however an invariant relationship does exist between space invariants and image invariants. This invariant relationship is first derived for a single image. Then this invariant relationship is used to derive the space invariants, when multiple images are available.This paper establishes that the minimum number of images for computing these invariants is three, and the computation of invariants of six points from three images can have as many as three solutions. Algorithms are presented for computing these invariants in closed form.The accuracy and stability with respect to image noise, selection of the triplets of images and distance between viewing positions are studied both through real and simulated images.Applications of these invariants are also presented. Both the results of Faugeras [1] and Hartley et al. [2] for projective reconstruction and Sturms method [3] for epipolar geometry determination from two uncalibrated images with at least seven points are extended to the case of three uncalibrated images with only six points.	self-calibration , epipolar geometry , uncalibrated images , projective reconstruction , projective geometry , this invariant	Invariants of Six Points and Projective Reconstruction From Three Uncalibrated Images.	en	263
264	AbstractWe develop efficient algorithms for low and intermediate level image processing on the scan line array processor, a SIMD machine consisting of a linear array of cells that processes images in a scan line fashion. For low level processing, we present algorithms for block DFT, block DCT, convolution, template matching, shrinking, and expanding which run in real-time. By real-time, we mean that, if the required processing is based on neighborhoods of size mm, then the output lines are generated at a rate of O(m) operations per line and a latency of O(m) scan lines, which is the best that can be achieved on this model. We also develop an algorithm for median filtering which runs in almost real-time at a cost of O(m${\rm log}$m) time per scan line and a latency of ${\bf \lfloor^{\underline m}_{\,\, 2}\rfloor}$ scan lines. For intermediate level processing, we present optimal algorithms for translation, histogram computation, scaling, and rotation. We also develop efficient algorithms for labelling the connected components and determining the convex hulls of multiple figures which run in O(n${\rm log}$n) and O(n${\rm log}$2n) time, respectively. The latter algorithms are significantly simpler and easier to implement than those already reported in the literature for linear arrays.	parallel algorithms , image processing , scan line array processors , video procesor , linear array , SIMD algorithms	Efficient Image Processing Algorithms on the Scan Line Array Processor.	en	264
265	AbstractRegion-based image segmentation methods require some criterion for determining when to merge regions. This paper presents a novel approach by introducing a Bayesian probability of homogeneity in a general statistical context. Our approach does not require parameter estimation and is therefore particularly beneficial for cases in which estimation-based methods are most prone to error: when little information is contained in some of the regions and, therefore, parameter estimates are unreliable. We apply this formulation to three distinct parametric model families that have been used in past segmentation schemes: implicit polynomial surfaces, parametric polynomial surfaces, and Gaussian Markov random fields. We present results on a variety of real range and intensity images.	bayes factor , range images , statistical image segmentation , bayesian methods , markov random field , texture segmentation , likelihoods	A Bayesian Segmentation Methodology for Parametric Image Models.	en	265
266	A program correctness checker is an algorithm for checking the output of a computation. That is, given a program and an instance on which the program is run, the checker certifies whether the output of the program on that instance is correct. This paper defines the concept of a program checker. It designs program checkers for a few specific and carefully chosen problems in the class FP of functions computable in polynomial time. Problems in FP for which checkers are presented in this paper include Sorting, Matrix Rank and GCD. It also applies methods of modern cryptography, especially the idea of a probabilistic interactive proof, to the design of program checkers for group theoretic computations.Two structural theorems are proven here. One is a characterization of problems that can be checked. The other theorem establishes equivalence classes of problems such that whenever one problem in a class is checkable, all problems in the class are checkable.	interactive proofs , program verification , testing , program checking , probabilistic algorithms	Designing programs that check their work.	en	266
267	Recent years have seen a shift in perception of the nature of HCI and interactive systems. As interface work has increasingly become a focus of attention for the social sciences, we have expanded our appreciation of the importance of issues such as work practice, adaptation, and evolution in interactive systems. The reorientation in our view of interactive systems has been accompanied by a call for a new model of design centered around user needs and participation. This article argues that a new process of design is not enough and that the new view necessitates a similar reorientation in the structure of the systems we build. It outlines some requirements for systems that support a deeper conception of interaction and argues that the traditional system design techniques are not suited to creating such systems. Finally, using examples from ongoing work in the design of an open toolkit for collaborative applications, it illustrates how the principles of computational reflection and metaobject protocols can lead us toward a new model based on open abstraction that holds great promise in addressing these issues.	computational reflection , collaborative applications , meta-object protocol , system architecture , open implementations	Developing a reflective model of collaborative systems.	en	267
268	An environment for general research into and prototyping of algorithms for reliable constrained and unconstrained global nonlinear optimization and reliable enclosure of all roots of nonlinear systems of equations, with or without inequality constraints, is being developed. This environment should be portable, easy to learn, use, and maintain, and sufficiently fast for some production work. The motivation, design principles, uses, and capabilities for this environment are outlined. The environment includes an interval data type, a symbolic form of automatic differentiation to obtain an internal representation for functions, a special technique to allow conditional branches with operator overloading and interval computations, and generic routines to give interval and noninterval function and derivative information. Some of these generic routines use a special version of the backward mode of automatic differentiation. The package also includes dynamic data structures for exhaustive search algorithms.	global optimization , symbolic computation , automatic differentiation , nonlinear algebraic systems , fortran 90	A Fortran 90 environment for research and prototyping of enclosure algorithms for nonlinear equations and global optimization.	en	268
269	Flow analyses of untyped higher-order functional programs have in the past decade been presented by Ayers, Bondorf, Consel, Jones, Heintze, Sestoft, Shivers, Steckler, Wand, and others. The analyses are usually defined as abstract interpretations and are used for rather different tasks such as type recovery, globalization, and binding-time analysis. The analyses all contain a global closure analysis that computes information about higher-order control-flow. Sestoft proved in 1989 and 1991 that closure analysis is correct with respect to call-by-name and call-by-value semantics, but it remained open if correctness holds for arbitrary beta-reduction.This article answers the question; both closure analysis and others are correct with respect to arbitrary beta-reduction. We also prove a subject-reduction result: closure information is still valid after beta-reduction. The core of our proof technique is to define closure analysis using a constraint system. The constraint system is equivalent to the closure analysis of Bondorf, which in turn is based on Sestoft's.	constraints , correctness proof , flow analysis	Closure analysis in constraint form.	en	269
270	Compiler-directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high-performance processors. However, prefetching is not without costs, particularly on a shared-memory multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and data sharing. We simulate the effects of a compiler-directed prefetching algorithm, running on a range of bus-based multiprocessors. We show that, despite a high memory latency, this architecture does not necessarily support prefetching well, in some cases actually causing performance degradations. We pinpoint several problems with prefetching on a shared-memory architecture (additional conflict misses, no reduction in the data-sharing traffic and associated latencies, a multiprocessor's greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance) and measure their effect on performance. We then solve those problems through architectural techniques and heuristics for prefetching that could be easily incorporated into a compiler: (1) victim caching, which eliminates most of the cache conflict misses caused by prefetching in a direct-mapped cache, (2) special prefetch algorithms for shared data, which significantly improve the ability of our basic prefetching algorithm to prefetch individual misses, and (3) compiler-based shared-data restructuring, which eliminates many of the invalidation misses the basic prefetching algorithm does not predict. The combined effect of these improvements is to make prefetching effective over a much wider range of memory architectures.	bus-based multiprocessors , cache prefetching , false sharing , memory latency hiding	Effective cache prefetching on bus-based multiprocessors.	en	270
271	Modern optimizing compilers use several passes over a program's intermediate representation to generate good code. Many of these optimizations exhibit a phase-ordering problem. Getting the best code may require iterating optimizations until a fixed point is reached. Combining these phases can lead to the discovery of more facts about the program, exposing more opportunities for optimization. This article presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the frame work provides insight into when a combination yields better results. To make the ideas more concrete, this article presents a framework for combining constant propagation, value numbering, and unreachable-code elimination. It is an open question as to what other frameworks can be combined in this way.	optimizing compilers , value numbering , data-flow analysis , constant propagation	Combining analyses, combining optimizations.	en	271
272	We show how to specify components of concurrent systems. The specification of a system is the conjunction of its components' specifications. Properties of the system are proved by reasoning about its components. We consider both the decomposition of a given system into parts, and the composition of given parts to form a system.	concurrent programming , safety properties , composition , liveness properties , modular specification , decomposition , temporal logic	Conjoining specifications.	en	272
273	The current algebraic models for nondeterminism focus on the notion of possibility rather than necessity and consequently equate (nondeterministic) terms that one would intuitively not consider equal. Furthermore, existing models for nondeterminism depart radically from the standard models for (equational) specifications of deterministic operators. One would prefer that a specification language for nondeterministic operators be based on an extension of the standard model concepts, preferably in such a way that the reasoning system for (possibly nondeterministic) operators becomes the standard equational one whenever restricted to the deterministic operatorsthe objective should be to minimize the departure from the standard frameworks. In this article we define a specification language for nondeterministic operators and multialgebraic semantics. The first complete reasoning system for such specifications is introduced. We also define a transformation of specifications of nondeterministic operators into derived specifications of deterministic ones, obtaining a computational semantics of nondeterministic specification by adopting the standard semantics of the derived specification as the semantics of the original one. This semantics turns out to be a refinement of multialgebra semantics. The calculus is shown to be sound and complete also with respect to the new semantics.	algebraic specifications , reasoning with nondeterminism	A complete calculus for the multialgebraic and functional semantics of nondeterminism.	en	273
274	If $A$ is a square matrix with spectral radius less than 1 then $A^k \to 0$ as $k \to \infty$, but the powers computed in finite precision arithmetic may or may not converge. We derive a sufficient condition for $fl(A^k) \to 0$ as $k\to\infty$ and a bound on $\norm{fl(A^k)}$, both expressed in terms of the Jordan canonical form of $A$. Examples show that the results can be sharp. We show that the sufficient condition can be rephrased in terms of a pseudospectrum of $A$ when $A$ is diagonalizable, under certain assumptions. Our analysis leads to the rule of thumb that convergence or divergence of the computed powers of $A$ can be expected according as the spectral radius computed by any backward stable algorithm is less than or greater than 1.	rounding errors , pseudospectrum , jordan canonical form , nonnormal matrices , matrix powers	Matrix Powers in Finite Precision Arithmetic.	en	274
275	Algorithms based on Nested Generalized Exemplar (NGE) theory (Salzberg, 1991) classify new data points by computing their distance to the nearest generalized exemplar (i.e., either a point or an axis-parallel rectangle). They combine the distance-based character of nearest neighbor (NN) classifiers with the axis-parallel rectangle representation employed in many rule-learning systems. An implementation of NGE was compared to the k-nearest neighbor (kNN) algorithm in 11 domains and found to be significantly inferior to kNN in 9 of them. Several modifications of NGE were studied to understand the cause of its poor performance. These show that its performance can be substantially improved by preventing NGE from creating overlapping rectangles, while still allowing complete nesting of rectangles. Performance can be further improved by modifying the distance metric to allow weights on each of the features (Salzberg, 1991). Best results were obtained in this study when the weights were computed using mutual information between the features and the output class. The best version of NGE developed is a batch algorithm (BNGE FWMI) that has no user-tunable parameters. BNGE FWMI's performance is comparable to the first-nearest neighbor algorithm (also incorporating feature weights). However, the k-nearest neighbor algorithm is still significantly superior to BNGE FWMI in 7 of the 11 domains, and inferior to it in only 2. We conclude that, even with our improvements, the NGE approach is very sensitive to the shape of the decision boundaries in classification problems. In domains where the decision boundaries are axis-parallel, the NGE approach can produce excellent generalization with interpretable hypotheses. In all domains tested, NGE algorithms require much less memory to store generalized exemplars than is required by NN algorithms.	nearest neighbors , nested generalized exemplars , feature weights , exemplar-based learning , instance-based learning	An Experimental Comparison of the Nearest-Neighbor and Nearest-Hyperrectangle Algorithms.	en	275
276	We analyze the performance of queues that serve readers and writers. Readers are served concurrently, while writers require exclusive service. We approximately analyze a first-come-first-serve (FCFS) reader/writer queue, and derive simple formulae for computing waiting times and capacity under the assumption of Poisson arrivals and exponential service. We extend the analysis to handle a one-writer queue, and a queue that includes write-intention locks. The simple analyses that we present can be used as rules-of-thumb for designing concurrent systems.	performance analysis , lock queue , queuing system , concurrent system , reader/writer	Approximate Analysis of Reader/Writer Queues.	en	276
277	AbstractThis paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Niblacks method with the addition of the postprocessing step of Yanowitz and Brucksteins method added performed the best and was also one of the fastest binarization methods.	evaluation , locally adaptive binarization , thresholding , document images , utility maps	Evaluation of Binarization Methods for Document Images.	en	277
278	We consider the probabilistic relationship between the value of a random asymmetric traveling  salesman problem $ATSP(M)$ and the value of its assignment relaxation $AP(M)$. We assume here  that the costs are given by an $n\times n$ matrix $M$ whose entries are independently and  identically distributed. We focus on the relationship between $Pr(ATSP(M)=AP(M))$ and the  probability $p_n$ that any particular entry is zero. If $np_n\rightarrow \infty$ with $n$  then we prove that $ATSP(M)=AP(M)$ with probability 1-o(1). This is shown to be best possible  in the sense that if $np(n)\rightarrow c$, $c>0$ and constant, then $Pr(ATSP(M)=AP(M))<1-\phi(c)$  for some positive function $\phi$. Finally, if $np_n\rightarrow 0$ then $Pr(ATSP(M)=AP(M))\rightarrow 0$.	probabilistic analysis , traveling salesman	When is the Assignment Bound Tight for the Asymmetric Traveling-Salesman Problem?.	en	278
279	The relative power of determinism, randomness, and nondeterminism for search problems in the Boolean decision tree model is studied. It is shown that the gaps between the nondeterministic, the randomized, and the deterministic complexities can be arbitrarily large for search problems.  An interesting connection of this model to the complexity of resolution proofs is also mentioned.	decision trees , boolean functions	Search Problems in the Decision Tree Model.	en	279
280	A good software architecture facilitates application system development, promotes achievement of functional requirements, and supports system reconfiguration. We present a domain-specific software architecture (DSSA) that we have developed for a large application domain of adaptive intelligent systems (AIS's). The DSSA provides: a) an AIS reference architecture designed to meet the functional requirements shared by applications in this domain, b) principles for decomposing expertise into highly reusable components, and c) an application configuration method for selecting relevant components from a library and automatically configuring instances of those components in an instance of the architecture. The AIS reference architecture incorporates features of layered, pipe and filter, and blackboard style architectures. We describe three studies demonstrating the utility of our architecture in the subdomain of mobile office robots and identify software engineering principles embodied in the architecture.	intelligent agents , software reuse , domain-specific software architectures , mobile robots , software architecture	A Domain-Specific Software Architecture for Adaptive Intelligent Systems.	en	280
281	Recent developments in mobile communication and personal computer technology have laid a new foundation for mobile computing. Performance of the data communication system as seen by an application program is a fundamental factor when communication infrastructure at the application layer is designed. This paper provides results of performance measurements of data transmission over two different cellular telephone networks, a digital GSM-network and an analogue NMT-network. Since our emphasis is on performance as seen by application programs, we use the standard TCP/IP protocols in the measurements. The performance is measured using three basic operations: establishment of a wireless dial-up connection, exchange of request-reply messages, and bulk data transfer. The external conditions under which the measurements were carried out present a normal office environment when the field strength of the cellular link is good or fairly good.	performance measurements , wireless dial-up connections , NMT , GSM , cellular telephones	Measured performance of data transmission over cellular telephone networks.	en	281
282	Earlier work concerning control of discrete event systems usually assumed that a correct model of the system to be controlled was available. A goal of this work is to provide an algorithm for determining the correct model from a set of models. The result of the algorithm is a finite language that can be used to test for the correct model or notification that the remaining models cannot be controllably distinguished. We use the finite state machine model with controllable and uncontrollable events presented by Ramadge and Wonham.	system identification , discrete event systems	Model Uncertainty in Discrete Event Systems.	en	282
283	A widely used approach to parameter identification is the output least-squares formulation. Numerical methods for solving the resulting minimization problem almost invariably require the computation of the gradient of the output least-squares functional. When the identification problem involves time-dependent distributed parameter systems (or approximations thereof), numerical evaluation of the gradient can be extremely time consuming. The costate method can greatly reduce the cost of computing these gradients. However, questions have been raised concerning the accuracy and convergence of costate approximations, even when the numerical methods being used are known to converge rapidly on the forward problem.  In this paper it is shown that the use of time-marching schemes that yield high-order accuracy on the forward problem does not necessarily lead to high-order accurate costate approximations. In fact, in some cases these approximations do not converge at all. However, under certain circumstances, rapidly converging gradient approximations do result because of rapid weak-star-type convergence of the costate approximations. These issues are treated both theoretically and numerically.	costate method , evolution equations , parameter estimation	Analysis of Costate Discretizations in Parameter Estimation for Linear Evolution Equations.	en	283
284	This paper presents a globally convergent successive approximation  method for solving $F(x)=0$ where $F$ is a continuous function. At each step of the method, $F$ is approximated by a smooth function $f_{k},$ with $\pa f_{k}-F\pa \rightarrow 0$ as $k \rightarrow \infty$. The direction $-f'_{k}(x_{k})^{-1}F(x_{k})$ is then used in a line search on a sum of squares objective. The approximate function  $f_k$ can be constructed for nonsmooth equations arising from variational inequalities, maximal monotone operator problems, nonlinear complementarity problems, and nonsmooth partial differential equations. Numerical examples are given to illustrate the method.	successive approximation , global convergence , integration convolution	A Globally Convergent Successive Approximation Method for Severely Nonsmooth Equations.	en	284
285	This paper considers the exact number of character comparisons needed to  find all occurrences of a pattern of length $m$ in a text of length $n$ using on-line and general algorithms. For on-line algorithms, a lower bound of about  $(1+\frac{9}{4(m+1)})\cdot n$ character comparisons is obtained. For general algorithms, a lower bound of about $(1+\frac{2}{m+3})\cdot n$ character comparisons is obtained. These lower bounds complement an on-line upper bound  of about $(1+\frac{8}{3(m+1)})\cdot n$ comparisons obtained recently by Cole and Hariharan.  The lower bounds are obtained by finding patterns with interesting combinatorial properties. It is also shown that for some patterns off-line algorithms can be more efficient than on-line algorithms.	string matching , complexity , lower bounds , comparisons , pattern matching	Tighter Lower Bounds on the Exact Complexity of String Matching.	en	285
286	This paper proves that for a strongly connected planar directed graph of size $n$, a depth-first search tree rooted at a specified vertex can be computed in $O(\log^{5}n)$ time with $n/\log{n}$ processors.  Previously, for planar directed graphs that may not be strongly connected, the best depth-first search algorithm runs in $O(\log^{10}n)$ time with $n$ processors.  Both algorithms run on a parallel random access machine that allows concurrent reads and concurrent writes in its shared memory, and in case of a write conflict, permits an arbitrary processor to succeed.	strong connectivity , depth-first search , s-t graphs , linear-processor NC algorithms , graph separators , planar directed graphs , bubblegraphs	Planar Strong Connectivity Helps in Parallel Depth-First Search.	en	286
287	The following file distribution problem is considered: Given  a network of processors represented by an undirected graph  $G=(V,E)$ and a file size $k$, an arbitrary file w  of $k$ bits is to be distributed among all nodes of $G$. To  this end, each node is assigned a memory device such that by  accessing the memory of its own and of its adjacent nodes,  the node can reconstruct the contents of w. The  objective is to minimize the total size of memory in the  network. This paper presents a file distribution scheme  which realizes this objective for $k \gg \log \Delta_G$,  where $\Delta_G$ stands for the maximum degree in $G$: For  this range of $k$, the total memory size required by the  suggested scheme approaches an integer programming lower  bound on that size. The scheme is also constructive in the  sense that given $G$ and $k$, the memory size at each node  in $G$, as well as the mapping of any file w into the  node memory devices, can be computed in time complexity  which is polynomial in $k$ and $|V|$. Furthermore, each node  can reconstruct the contents of such a file w in  $O(k^2)$ bit operations. Finally, it is shown that the  requirement of $k$ being much larger than $\log \Delta_G$ is  necessary in order to have total memory size close to the  integer programming lower bound.	resource sharing , distributed networks , integer programming , linear programming , derandomization , set cover , linear codes , file assignment , probabilistic algorithms	Optimal File Sharing in Distributed Networks.	en	287
288	Measure-theoretic aspects of the $\leq^{\rm P}_{\rm m}$-reducibility  structure of the exponential time complexity classes E=DTIME($2^{\rm linear}$) and $E_{2}={\rm DTIME}(2^{\rm polynomial})$ are investigated. Particular attention is given to the complexity (measured  by the size of complexity cores) and distribution  (abundance in the sense of measure) of languages that are $\leq^{\rm P}_{\rm m}$-hard for E and other complexity classes.  Tight upper and lower bounds on the size of complexity cores of hard languages are derived.  The upper bound says that the  $\leq^{\rm P}_{\rm m}$-hard languages for E are unusually simple, in the  sense that they have smaller complexity cores than most  languages in E.  It follows that the $\leq^{\rm P}_{\rm m}$-complete languages for E form a measure 0 subset of E (and similarly in $E_2$).  This latter fact is seen to be a special case of a more general theorem, namely, that {\it every} \pmr-degree (e.g., the degree of all \pmr-complete languages for NP) has measure 0 in E and in \Ep.	resource-bounded measure , polynomial reducibilities , complexity classes , complexitycores , computational complexity , complete problems	The Complexity and Distribution of Hard Problems.	en	288
289	The subject of this work is the possibility of private distributed computations of $n$-argument functions defined over the integers.  A function $f$ is $t$-private if there exists a    protocol for computing $f$, so that no coalition of at most $t$ participants can infer any additional information from the execution of the protocol.  It is known that over finite domains, every function can be computed $\left\lfloor{(n-1)/2}\right\rfloor$-privately. Some functions, like addition, are even $n$-private.   We prove that this result cannot be extended to infinite domains. The possibility of privately computing $f$ is shown to be  closely related to the communication complexity of $f$. Using this relation, we show, for example, that $n$-argument addition is  $\left\lfloor{(n-1)/2}\right\rfloor$-private over the nonnegative integers, but not even $1$-private over all the integers.  Finally, a complete characterization of $t$-private Boolean functions over countable domains is given. A Boolean function is $1$-private if and only if its communication complexity is bounded. This characterization enables us to prove that every Boolean function falls into one of the following three categories: It is either $n$-private, $\left\lfloor{(n-1)/2}\right\rfloor$-private but not $\left\lceil{n/2}\right\rceil $-private, or not $1$-private.	private distributed computations , communication complexity	Private Computations Over the Integers.	en	289
290	"We present a new approach to formal language theory using Kolmogorov complexity. The main results presented here are an alternative for pumping lemma(s), a new characterization for regular languages, and a new method to separate deterministic context-free languages and nondeterministic context-free languages. The use of the new ""incompressibility arguments"" is illustrated by many examples. The approach is also successful at the high end of the Chomsky hierarchy since one can quantify nonrecursiveness in terms of Kolmogorov complexity."	kolmogorov complexity , pumping lemmas , deterministic context-free languages , formal language theory , regular languages , finite automata	A New Approach to Formal Language Theory by Kolmogorov Complexity.	en	290
291	The spectral method is the best currently known technique to prove lower bounds on expansion. Ramanujan graphs, which have asymptotically optimal second eigenvalue, are the best-known explicit expanders. The spectral method yielded a lower bound of k/4 on the expansion of linear-sized subsets of k-regular Ramanujan graphs. We improve the lower bound on the expansion of Ramanujan graphs to approximately k/2. Moreover, we construct a family of k-regular graphs with asymptotically optimal second eigenvalue and linear expansion equal to k/2. This shows that k/2 is the best bound one can obtain using the second eigenvalue method. We also show an upper bound of roughly     1+k-1  on the average degree of linear-sized induced subgraphs of Ramanujan graphs. This compares positively with the classical bound   2k-1 . As a byproduct, we obtain improved results on random walks on expanders and construct selection networks (respectively, extrovert graphs) of smaller size (respectively, degree) than was previously known.	expander graphs , load balancing , selection networks , random walks , induced subgraphs , ramanujan graphs , eigenvalues	Eigenvalues and expansion of regular graphs.	en	291
292	Constraint networks have been shown to be useful in formulating such diverse problems as scene labeling, natural language parsing, and temporal reasoning. Given a constraint network, we often wish to (i) find a solution that satisfies the constraints and (ii) find the corresponding minimal network where the constraints are as explicit as possible. Both tasks are known to be NP-complete in the general case. Task (1) is usually solved using a backtracking algorithm, and task (ii) is often solved only approximately by enforcing various levels of local consistency. In this paper, we identify a property of binary constraint called row convexity and show its usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee that a network is both minimal and globally consistent. Globally consistent networks have the property that a solution can be found without backtracking. We show that one can test for the row convexity property efficiently and we show, by examining applications of constraint networks discussed in the literature, that our results are useful in practice. Thus, we identify a class of binary constraint networks for which we can solve both tasks (i) and (ii) efficiently. Finally, we generalize the results for binary constraint networks to networks with nonbinary constraints.	consecutive ones property , row convexity , relations , constraint networks , constraint-based reasoning , constraint satisfaction problems , path consistency	On the minimality and global consistency of row-convex constraint networks.	en	292
293	Consider a switching component in a packet-switching network, where messages from several incoming channels arrive and are routed to appropriate outgoing ports according to a service policy. One requirement in the design of such a system is to determine the buffer storage necessary at the input of each channel and the policy for serving these buffers that will prevent buffer overflow and the corresponding loss of messages. In this paper, a class of buffer service policies, called Least Time to Reach Bound (LTRB), is introduced that guarantees no overflow, and for which the buffer size required at each input channel is independent of the number of channels and their relative speeds. Further, the storage requirement is only twice the maximal length of a message in all cases, and as a consequence the class is shown to be optimal in the sense that any nonoverflowing policy requires at least as much storage as LTRB.	multiplexor , parallel queues , service discipline , gradual input , switch , buffer overflow	An optimal service policy for buffer systems.	en	293
294	"The MEG (minimum equivalent graph) problem is the following: ""Given a directed graph,  find a smallest subset of the edges that maintains all reachability relations between nodes."" This problem is NP-hard; this paper gives an approximation algorithm achieving a performance guarantee of about 1.64 in polynomial time. The algorithm achieves a performance guarantee of 1.75 in the time required for transitive closure.  The heart of the MEG problem  is the minimum SCSS (strongly connected spanning subgraph) problem --- the MEG problem restricted to strongly connected digraphs. For the minimum SCSS problem,  the paper gives a practical, nearly linear-time implementation achieving a performance guarantee of 1.75.  The algorithm and its analysis  are based on the simple idea of contracting long cycles. The analysis applies directly to $2$-\Exchange,  a general ""local improvement"" algorithm, showing that its performance guarantee is 1.75."	local improvement , strong connectivity , directed graph , approximation algorithm	Approximating the Minimum Equivalent Digraph.	en	294
295	Primal-dual interior-point methods for  linear complementarity and linear programming problems solve a linear system of equations to obtain a  modified Newton step at each iteration. These linear systems become increasingly ill-conditioned in the later stages of the algorithm, but the computed steps are often sufficiently accurate to be useful. We use error analysis techniques tailored to the special structure of these linear systems to explain this observation and examine how theoretically superlinear convergence of a path-following  algorithm is affected by the roundoff errors.	primal-dual interior-point methods , error analysis , stability	Stability of Linear Equations Solvers in Interior-Point Methods.	en	295
296	This paper studies a notion called polynomial-time membership comparable sets. For a function $g$, a set $A$ is polynomial-time $g$-membership comparable if there is a polynomial-time computable function $f$ such that for any $x_1, \cdots, x_m$ with $m \geq g(\max\{ |x_1|, \cdots, |x_m| \})$, outputs $b \in \{0,1\}^m$ such that $(A(x_1), \cdots, A(x_m)) \neq b$. The following is a list of major results proven in the paper.  1. Polynomial-time membership comparable sets construct  a proper hierarchy according to the bound on the number  of arguments.  2. Polynomial-time membership comparable sets have  polynomial-size circuits.  3. For any function $f$ and for any constant $c>0$,  if a set is $\leq^p_{f(n)-tt}$-reducible to a P-selective set,  then the set is polynomial-time $(1+c)\log f(n)$-membership  comparable.  4. For any $\cal C$ chosen from $\{ {\rm PSPACE, UP, FewP, NP, C_{=}P, PP, MOD_{2}P, MOD_{3}}, \cdots \}$,  if $\cal C \subseteq {\rm P-mc}(c\log n)$ for some $c<1$,  then $\cal As a corollary of the last two results, it is shown that if there is some constant $c<1$ such that all of $\cal C$ are polynomial-time $n^c$-truth-table reducible to some P-selective sets, then $\cal which resolves a question that has been left open for a long time.	polynomial-size circuits , polynomial-time reducibilities , p-selective sets	Polynomial-time Membership Comparable Sets.	en	296
297	There has been rapid growth in the demand for mobile communications over the past few years. This has led to intensive research and development efforts for complex PCS (personal communication service) networks. Capacity planning and performance modeling is necessary to maintain a high quality of service to the mobile subscriber while minimizing cost to the PCS provider. The need for flexible analysis tools and the high computational requirements of large PCS network simulations make it an excellent candidate for parallel simulation.Here, we describe our experiences in developing two PCS simulation models on a general purpose distributed simulation platform based on the Time Warp mechanism. These models utilize two widely used approaches to simulating PCS networks: (i) the  call-initiated and (ii) the portable-initiated models. We discuss design decisions that were made in mapping these models to the Time Warp executive, and characterize the workloads resulting from these models in terms of factors such as communication locality and computation granularity. We then present performance measurements for their execution on a network of workstations. These measurements indicate that the call-initiated model generally outperforms the portable initiated model, but is not able to capture phenomenon such as the busy line effect. Moreover, these studies indicate that the high locality in large-scale PCS network simulations make them well-suited for execution on general purpose parallel and distributed simulation platforms.	performance measurements , mobile communications , PCS networks , time warp , personal communication service , discrete event simulation , telecommunication computing , time warp simulation , distributed , parallel , flexible analysis tools , simulation platforms , mobile communication	A case study in simulating PCS networks using Time Warp.	en	297
298	In a distributed memory environment the communication overhead of Time Warp as induced by the rollback procedure due to overoptimistic progression of the simulation is the dominating performance factor. To limit optimism to an extent that can be justified from the inherent model parallelism, an optimism control mechanism is proposed, which by maintaining a history record of virtual time differences from the time stamps carried by arriving messages, and forecasting the timestamps of forthcoming messages, probabilistically delays the execution of scheduled events to avoid potential rollback and associated communication overhead (antimessages). After investigating statistical forecast methods which express only the central tendency of the arrival process, we demonstrate  that arrival processes in the context of Time Warp simulations of timed Petri nets have certain predictable and consistent ARIMA characteristics, which encourage the use of sophisticated and recursive forecast procedures based on those models. Adaptiveness is achieved in two respects: the synchronization behavior of logical processes automatically progressing and conservatively blocking, that is the most adequate for (i) the specific simulation model and (ii) the communication/computation speed characteristics of the underlying execution platform.	RS6000 cluster , time warp , forecast models , optimism control , PVM , petri nets	Probabilistic adaptive direct optimism control in Time Warp.	en	298
299	Critical path analysis has been suggested as a technique for establishing a lower bound on the completion times of parallel discrete event simulations. A protocol is super-critical if there is at least one simulation that can complete in less than the critical path time using that protocol. Previous studies have shown that several practical protocols are super-critical while others are not. We present a sufficient condition to demonstrate that a protocol is super-criticality. It has been claimed that super-criticality requires independence of one or more messages (states) on events in the logical past of those messages (states). We present an example which contradicts this claim and examine the implications of this contradiction on lower bounds.	protocol , critical path analysis , discrete event simulation , parallel discrete event simulations , lower bound , protocols	Super-criticality revisited.	en	299
300	Mechanisms for managing message buffers in Time Warp parallel simulations executing on cache-coherent shared-memory multiprocessors are studied. Two simple buffer management strategies called the sender pool and receiver pool mechanisms are examined with respect to their efficiency, and in particular, their interaction with multiprocessor cache-coherence protocols. Measurements of implementations on a Kendall Square Research KSR-2 machine using both synthetic workloads and benchmark applications demonstrate that sender pools offer significant performance advantages over receiver pools. However, it is also observed that both schemes, especially the sender pool mechanism, are prone to severe performance degradations due to poor locality of reference in large simulations using  substantial amounts of message buffer memory. A third strategy called the partitioned buffer pool approach is proposed that exploits the advantages of sender pools, but exhibits much better locality. Measurements of this approach indicate that the partitioned pool mechanism yields substantially better performance than both the sender and receiver pool schemes for large-scale, small-granularity parallel simulation applications.The central conclusions from this study are: (1) buffer management strategies play an important role in determining the overall efficiency of multiprocessor-based parallel simulators, and (2) the partitioned buffer pool organization offers significantly better performance than the sender and receiver pool schemes. These studies demonstrate that poor  performance may result if proper attention is not paid to realizing an efficient buffer management mechanism.	message passing , message buffers , mall-granularity parallel simulation applications , severe performance degradations , storage management , shared memory systems , buffer management , shared-memory time warp systems , Kendall Square Research KSR-2 machine , receiver pool , discrete event simulation , time warp simulation , message buffer memory , multiprocessor cache-coherence protocols , sender pool , partitioned pool mechanism , cache-coherent shared-memory multiprocessors , partitioned buffer pool approach , multiprocessing programs , multiprocessor-based parallel simulators , buffer storage	Buffer management in shared-memory Time Warp systems.	en	300
301	In practice, time critical portions of hard real-time systems are still implemented in low-level programming languages and manually tuned to meet all the timing requirements. Without a real-time language that supports an appropriate way of specifying timing constraints for a generic hard real-time systems, and high precision timing analysis that is transparent to users, the users will ever suffer from the complex coding and analysis, particularly for systems requiring fast turnaround responses.In this paper, we propose novel language constructs that can be added to any imperative programming language so that the extended language provides users a way to specify relative timing constraints between arbitrary operations at instruction-level. The compilation techniques unique to transformation of the proposed language are also presented as a part of CHaRTS, the Compiler for Hard Real-Time Systems, which generates a valid instruction sequence for a target execution model.	hard real-time , timing constraint , CHaRTS , real-time language	Language constructs and transformation for hard real-time systems.	en	301
302	A new method is presented for visualizing data as they are generated from real-time applications. These techniques allow viewers to perform simple data analysis tasks such as detection of data groups and boundaries, target detection, and estimation. The goal is to do this rapidly and accurately on a dynamic sequence of data frames. Our techniques take advantage of an ability of the human visual system called preattentive processing. Preattentive processing refers to an initial organization of the visual system based on operations believed to be rapid, automatic, and spatially parallel. Examples of visual features that can be detected in this way include hue, orientation, intensity, size, curvature, and line length. We believe that studies from preattentive processing should be used to assist in the design of visualization tools, especially those for which high speed target,  boundary, and region detection are important. Previous work has shown that results from research in preattentive processing can be used to build visualization tools that allow rapid and accurate analysis of individual, static data frames. We extend these techniques to a dynamic real-time environment. This allows users to perform similar tasks on dynamic sequences of frames, exactly like those generated by real-time systems such as visual interactive simulation. We studied two known preattentive features, hue and curvature. The primary question investigated was whether rapid and accurate target and boundary detection in dynamic sequences is possible using these features. Behavioral experiments were run that simulated displays from our preattentive visualization tools. Analysis of the  results of the experiments showed that rapid and accurate target and boundary detection is possible with both hue and curvature. A second question, whether interactions occur between the two features in a real-time environment, was answered positively.	target detection , curvature , visual interactive simulation , preattentive , scientific visualization , multivariate data , human vision , boundary detection , cognitive psychology	Visualizing real-time multivariate data using preattentive processing.	en	302
303	We describe a system for off-line production and real-time playback of motion for articulated human figures in 3D virtual environments. The key notion are (1) the logical storage of full-body motion in posture graphs, which provides a simple motion access method for playback, and (2) mapping the motions of high DOF figures to lower DOF figures using slaving to provide human models at several levels of detail, both in geometry and articulation, for later playback. We present our system in the context of a simple problem: animating human figures in a distributed simulation, using DIS protocols for communicating the human state information. We also discuss several related techniques for real-time animation of articulated figures in visual simulation.	multiresolution motion , posture graphs , animation , visual simulation , real-time animation	Production and playback of human figure motion for visual simulation.	en	303
304	AbstractThis correspondence presents a new technique for calibrating a camera mounted on a controllable head/eye platform. It uses the trajectories of an arbitrary number of tracked corner features to improve the calibration parameter estimates over time, utilizing a novel variable state dimension form of recursive filter. No special visual stimuli are required and no assumptions are made about the structure of the scene, other than that it is stationary relative to the head. The algorithm runs at 4 frames per second on a single Inmos T805 transputer, and is fully integrated into a real-time active vision system. Updated calibration parameters are regularly passed to the vision modules that require them. Although the algorithm requires an initial estimate of camera focal length, results are presented from real experiments demonstrating that convergence is achieved for initial errors up to 50%.	recursive filter , active vision , real-time vision , camera calibration	Active Camera Calibration for a Head-Eye Platform Using the Variable State-Dimension Filter.	en	304
305	AbstractA multiscale morphological dilation-erosion smoothing operation and its associated scale-space expansion for multidimensional signals are proposed. Properties of this smoothing operation are developed and, in particular a scale-space monotonic property for signal extrema is demonstrated. Scale-space fingerprints from this approach have advantages over Gaussian scale-space fingerprints in that they are defined for negative values of the scale parameter; have monotonic properties in two and higher dimensions, do not cause features to be shifted by the smoothing, and allow efficient computation. The application of reduced multiscale dilation-erosion fingerprints to the surface matching of terrain is demonstrated.	scale-space fingerprints , scale-space filtering , monotonic property , multiscale morphology , signal analysis	Scale-Space Properties of the Multiscale Morphological Dilation-Erosion.	en	305
306	We study different genetic algorithm operators for one permutation problem associated with the Human Genome Projectthe assembly of DNA sequence fragments from a parent clone whose sequence is unknown into a consensus sequence corresponding to the parent sequence. The sorted-order representation, which does not require specialized operators, is compared with a more traditional permutation representation, which does require specialized operators. The two representations and their associated operators are compared on problems ranging from 2K to 34K base pairs (KB). Edge-recombination crossover used in conjunction with several specialized operators is found to perform best in these experiments&semi; these operators solved a 10KB sequence, consisting of 177 fragments, with no manual intervention. Natural building blocks in the problem are exploited at progressively higher levels through macro-operators. This significantly improves performance.	edge-recombination crossover , DNA fragment assembly , ordering problems , genetic algorithms , building blocks , human genome project	Genetic Algorithms, Operators, and DNA Fragment Assembly.	en	306
307	The MEME algorithm extends the expectation maximization (EM) algorithm for identifying motifs in unaligned biopolymer sequences. The aim of MEME is to discover new motifs in a set of biopolymer sequences where little or nothing is known in advance about any motifs that may be present. MEME innovations expand the range of problems which can be solved using EM and increase the chance of finding good solutions. First, subsequences which actually occur in the biopolymer sequences are used as starting points for the EM algorithm to increase the probability of finding globally optimal motifs. Second, the assumption that each sequence contains exactly one occurrence of the shared motif is removed. This allows multiple appearances of a motif to occur in any sequence and permits the algorithm to ignore sequences with no appearance of the shared motif, increasing its resistance to noisy data. Third, a method for probabilistically erasing shared motifs after they are found is incorporated so that several distinct motifs can be found in the same set of sequences, both when different motifs appear in different sequences and when a single sequence may contain multiple motifs. Experiments show that MEME can discover both the CRP and LexA binding sites from a set of sequences which contain one or both sites, and that MEME can discover both the 10 and 35 promoter regions in a set of E. coli sequences.	binding site , motif , expectation maximization , DNA , unsupervised learning , promoter , biopolymer , consensus sequence , protein , sequence analysis	Unsupervised Learning of Multiple Motifs in Biopolymers Using Expectation Maximization.	en	307
308	In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.	concept learning	Evaluation and Selection of Biases in Machine Learning.	en	308
309	AbstractRun-time data redistribution can enhance algorithm performance in distributed-memory machines. Explicit redistribution of data can be performed between algorithm phases when a different data decomposition is expected to deliver increased performance for a subsequent phase of computation. Redistribution, however, represents increased program overhead as algorithm computation is discontinued while data are exchanged among processor memories. In this paper, we present a technique that minimizes the amount of data exchange for BLOCK to CYCLIC(c) (or vice-versa) redistributions of arbitrary number of dimensions. Preserving the semantics of the target (destination) distribution pattern, the technique manipulates the data to logical processor mapping of the target pattern. When implemented on an IBM SP, the mapping technique demonstrates redistribution performance improvements of approximately 40% over traditional data to processor mapping. Relative to the traditional mapping technique, the proposed method affords greater flexibility in specifying precisely which data elements are redistributed and which elements remain on-processor.	data redistribution , data-parallel programming , data decomposition , processor mapping , distributed-memory architectures , High Performance Fortran	Processor Mapping Techniques Toward Efficient Data Redistribution.	en	309
310	The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be closely matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially in this domain.	experimental comparisons , text-to-speech , backpropagation	A Comparison of ID3 and Backpropagation for English Text-To-Speech Mapping.	en	310
311	A weak completeness phenomenon is investigated in the complexity class ${\rm E}= {\rm DTIME}(2^{\rm linear})$.  According to  standard terminology, a language $H$ is $\leq^{\rm P}_{m}$-hard for E if the set ${\rm P}_{m}(H)$, consisting of all languages $A \leq^{\rm P}_{m}H$, contains the entire class E.  A language $C$ is $\leq^{\rm P}_{m}$-complete for E if it is $\leq^{\rm P}_{m}$-hard for E and is also an element of E.  Generalizing this, a language $H$ is weakly $\leq^{\rm P}_{m}$-hard for E if the set ${\rm P}_{m}(H)$ does not have measure 0 in E.  A language $C$ is weakly $\leq^{\rm P}_{m}$-complete for E if it is weakly $\leq^{\rm P}_{m}$-hard for E and is also an element of E.  The main result of this paper is the construction of a language that is weakly $\leq^{\rm P}_{m}$-complete, but not $\leq^{\rm P}_{m}$-complete, for E. The existence of such languages implies that previously known strong lower bounds on the complexity of weakly $\leq^{\rm P}_{m}$-hard problems for E (given by work of Lutz, Mayordomo, and Juedes) are indeed more general than the corresponding bounds for $\leq^{\rm P}_{m}$-hard problems for E.  The proof of this result introduces a new diagonalization method, called martingale diagonalization.  Using this method, one simultaneously develops an infinite family of polynomial time computable martingales (betting strategies) and a corresponding family of languages that defeat these martingales (prevent them from winning too much money) while also pursuing another agenda.  Martingale diagonalization may be useful for a variety of applications.	resource-bounded measure , weak completeness , complexity classes , computational complexity , complete problems	Weakly Hard Problems.	en	311
312	Pattern matching is an important operation used in many applications such as functional programming, rewriting, and rule-based expert systems.  By preprocessing the patterns into a DFA-like automaton, we can rapidly select the matching pattern(s) in a single scan of the relevant portions of the input term.  This automaton is typically based on left-to-right traversal of the patterns.  By adapting the traversal order to suit the set of input patterns, it is possible to considerably reduce the space and matching time requirements of the automaton.  The design of such adaptive automata is the focus of this paper. We first formalize the notion of an adaptive traversal. We then present several strategies for synthesizing adaptive traversal orders aimed at reducing space and matching time complexity.  In the worst case, however, the space requirements can be exponential in the size of the patterns. We show this by establishing  an exponential lower bounds on space that is independent of the traversal order used. We then discuss an orthogonal approach to space minimization based on direct construction of optimal dag automata. Finally, our work brings forth the impact of typing in pattern matching. In particular, we show that several important problems (e.g., lazy pattern matching in ML) are computationally hard in the presence of type disciplines, whereas they can be solved efficiently in the untyped setting.	indexing , discrimination nets , functional programming , algorithms and complexity , pattern matching	Adaptive Pattern Matching.	en	312
313	We consider the following problem: given a labelled directed graph $G$  and a regular expression $R$, find all pairs of nodes connected by a  simple path such that the concatenation of the labels along the path  satisfies $R$. The problem is motivated by the observation that many  recursive queries in relational databases can be expressed in this  form, and by the implementation of a query language, ${\bf G}^+$, based on  this observation.  We show that the problem is in general intractable,  but present an algorithm than runs in polynomial time in the size of  the graph when the regular expression and the graph are free of   conflicts.  We also present a class of languages whose expressions  can always be evaluated in time polynomial in the size of both the  graph and the expression, and characterize syntactically the  expressions for such languages.	regular expressions , NP-completeness , labelled directed graphs , simple paths , polynomial-time algorithms	Finding Regular Simple Paths in Graph Databases.	en	313
314	Remotely sensed imagery has been used for developing and validating various studies regarding land cover dynamics. However, the large amounts of imagery collected by the satellites are largely contaminated by the effects of atmospheric particles. The objective of atmospheric correction is to retrieve the surface reflectance from remotely sensed imagery by removing the atmospheric effects. We introduce a number of computational techniques that lead to a substantial speedup of an atmospheric correction algorithm based on using look-up tables. Excluding I/O time, the previous known implementation processes one pixel at a time and requires about 2.63 seconds per pixel on a SPARC-10 machine, while our implementation is based on processing the whole image and takes about 4-20 microseconds per pixel on the same machine. We also develop a parallel version of our algorithm that is scalable in terms of both computation and I/O. Experimental results obtained show that a Thematic Mapper (TM) image (36 MB per band, 5 bands need to be corrected) can be handled in less than 4.3 minutes on a 32-node CM-5 machine, including I/O time.	High Performance Computing , remote sensing , AVHRR , atmospheric correction , Scalable Parallel Processing , Parallel I/O	Efficient algorithms for atmospheric correction of remotely sensed data.	en	314
315	We present and analyze a portable, high-performance algorithm for finding connected components on modern distributed memory multiprocessors. The algorithm is a hybrid of the classic DFS on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm on the global collection of subgraphs. We implement the algorithm in Split-C and measure performance on the the Cray T3D, the Meiko CS-2, and the Thinking Machines CM-5 using a class of graphs derived from cluster dynamics methods in computational physics. On a 256 processor Cray T3D, the implementation outperforms all previous solutions by an order of magnitude. A characterization of graph parameters allows us to select graphs that highlight key performance features. We study the effects of these parameters and machine characteristics on the balance of time between the local and global phases of the algorithm and find that edge density, surface-to-volume ratio, and relative communication cost dominate performance. By understanding the effect of machine characteristics on performance, the study sheds light on the impact of improvements in computational and/or communication performance on this challenging problem.	distributed memory , hybrid algorithm , connected components , parallel machines , performance modeling modelling	Towards modeling the performance of a fast connected components algorithm on parallel machines.	en	315
316	The message passing programs are executed with the Parallel Virtual Machine (PVM) library and the shared memory programs are executed using TreadMarks. The programs are Water and Barnes-Hut from the SPLASH benchmark suite; 3-D FFT, Integer Sort (IS) and Embarrassingly Parallel (EP) from the NAS benchmarks; ILINK, a widely used genetic linkage analysis program; and Successive Over-Relaxation (SOR), Traveling Salesman (TSP), and Quicksort (QSORT). Two different input data sets were used for Water (Water-288 and Water-1728), IS (IS-Small and IS-Large), and SOR (SOR-Zero and SOR-NonZero). Our execution environment is a set of eight HP735 workstations connected by a 100Mbits per second FDDI network. For Water-1728, EP, ILINK, SOR-Zero, and SOR-NonZero, the performance of TreadMarks is within 10%of PVM. For IS-Small, Water-288, Barnes-Hut, 3-D FFT, TSP, and QSORT, differences are on the order of 10%to 30%. Finally, for IS-Large, PVM performs two times better than TreadMarks. More messages and more data are sent in TreadMarks, explaining the performance differences. This extra communication is caused by 1) the separation of synchronization and data transfer, 2) extra messages to request updates for data by the invalidate protocol used in TreadMarks, accumulation for migratory data in TreadMarks.	message passing , lazy release consistency , PVM , distributed shared memory , treadmarks	Message passing versus distributed shared memory on networks of workstations.	en	316
317	We evaluate the impact of a gigabit network on the implementation of a distributed chemical process optimization application. The optimization problem is formulated as a stochastic Linear Assignment Problem and was solved using the Thinking Machines CM-2 (SIMD) and the Cray C-90 (vector) computers at PSC, and the Intel iWarp (MIMD) system at CMU, connected by the Gigabit Nectar testbed. We report our experience distributing the application across this heterogeneous set of systems and present measurements that show how the communication requirements of the application depend on the structure of the application. We use detailed traces to build an application performance model that can be used to estimate the elapsed time of the application for different computer system and network combinations. Our results show that the application benefits from the high-speed network, and that the need for high network throughput is increasing as computer systems get faster. We also observed that supporting high burst rates is critical, although structuring the application so that communication is overlapped with computation relaxes the bandwidth requirements.	distributed computing , gigabit networks , heterogeneous computing , optimal resource allocation , chemical process optimization , stochastic linear assignment problem	Distributing a chemical process optimization application over a gigabit network.	en	317
318	This paper presents an extensive empirical evaluation of an interprocedural parallelizing compiler, developed as part of the Stanford SUIF compiler system. The system incorporates a comprehensive and integrated collection of analyses, including privatization and reduction recognition for both array and scalar variables, and symbolic analysis of array subscripts. The interprocedural analysis framework is designed to provide analysis results nearly as precise as full inlining but without its associated costs. Experimentation with this system shows that it is capable of detecting coarser granularity of parallelism than previously possible. Specifically, it can parallelize loops that span numerous procedures and hundreds of lines of codes, frequently requiring modifications to array data structures such as privatization and reduction transformations. Measurements from several standard benchmark suites demonstrate that an integrated combination of interprocedural analyses can substantially advance the capability of automatic parallelization technology.	shared memory multiprocessors , compiler optimizations , parallelizing compilers , interprocedural data-flow analysis	Detecting coarse-grain parallelism using an interprocedural parallelizing compiler.	en	318
319	Because large scientific codes are rarely static objects, developers are often faced with the tedious task of accounting for discrepancies between new and old versions. In this paper, we describe a new technique called relative debugging that addresses this problem by automating the process of comparing a modified code against a correct reference code. We examine the utility of the relative debugging technique by applying a relative debugger called Guard to a range of debugging problems in a large atmospheric circulation model. Our experience confirms the effectiveness of the approach. Using Guard, we are able to validate a new sequential version of the atmospheric model, and to identify the source of a significant discrepancy in a parallel version in a short period of time.	parallelism , guard , relative debugging , tools , debugging , scientific computing , meteorology	Relative debugging and its application to the development of large numerical models.	en	319
320	Release consistency is a widely accepted memory model for distributed shared memory systems. Eager release consistency represents the state of the art in release consistent protocols for hardware-coherent multiprocessors, while lazy release consistency has been shown to provide better performance for software distributed shared memory (DSM). Several of the optimizations performed by lazy protocols have the potential to improve the performance of hardware-coherent multiprocessors as well, but their complexity has precluded a hardware implementation. With the advent of programmable protocol processors it may become possible to use them after all. We present and evaluate a lazy release-consistent protocol suitable for machines with dedicated protocol processors. This protocol admits multiple concurrent writers, sends write notices concurrently with computation, and delays invalidations until acquire operations. We also consider a lazier protocol that delays sending write notices until release operations. Our results indicate that the first protocol outperforms eager release consistency by as much as 20% across a variety of applications. The lazier protocol, on the other hand, is unable to recoup its high synchronization overhead. This represents a qualitative shift from the DSM world, where lazier protocols always yield performance improvements. Based on our results, we conclude that machines with flexible hardware support for coherence should use protocols based on lazy release consistency, but in a less ''aggressively lazy'' form than is appropriate for DSM.	Lazy Release Consistency , cache coherence , shared memory , protocol processors	Lazy release consistency for hardware-coherent multiprocessors.	en	320
321	We describe pHPF, an research prototype HPF compiler for the IBM SP series parallel machines. The compiler accepts as input Fortran 90 and Fortran 77 programs, augmented with HPF directives; sequential loops are automatically parallelized. The compiler supports symbolic analysis of expressions. This allows parameters such as the number of processors to be unknown at compile-time without significantly affecting performance. Communication schedules and computation guards are generated in a parameterized form at compile-time. Several novel optimizations and improved versions of well-known optimizations have been implemented in pHPF to exploit parallelism and reduce communication costs. These optimizations include elimination of redundant communication using data-availability analysis; using collective communication; new techniques for mapping scalar variables; coarse-grain wavefronting; and communication reduction in multi-dimensional shift communications. We present experimental results for some well-known benchmark routines. The results show the effectiveness of the compiler in generating efficient code for HPF programs.	compiler , automatic parallelization , fortran , IBM SP2 , HPF , distributed memory compilation , optimizing compiler , communication optimization	An HPF compiler for the IBM SP2.	en	321
322	This paper discusses the comprehensive performance profiling, improvement and benchmarking of a Computational Fluid Dynamics code, one of the Grand Challenge applications, on three popular multiprocessors. In the process of analyzing performance we considered language, compiler, architecture, and algorithmic changes and quantified each of them and their incremental contribution to bottom-line performance. We demonstrate that parallelization alone cannot result in significant gains if the granularity of parallel threads and the effect of parallelization on data locality are not taken into account. Unlike benchmarking studies that often focus on the performance or effectiveness of parallelizing compilers on specific loop kernels, we used the entire CFD code to measure the global effectiveness of compilers and parallel architectures. We probed the performance bottlenecks in each case and derived solutions which eliminate or neutralize the performance inhibiting factors. The major conclusion of our work is that overall performance is extremely sensitive to the synergetic effects of compiler optimizations, algorithmic and code tuning, and architectural idiosyncrasies.	parallel architectures , performance evaluation , parallelizing compilers , program optimization , parallel processing , parallel algorithms and programs , profiling and program tuning , CFD computational fluid dynamics , cache optimization	The synergetic effect of compiler, architecture, and manual optimizations on the performance of CFD on multiprocessors.	en	322
323	Abstract: Designers of embedded systems are facing ever tighter constraints on design time, but computer-aided design tools for embedded systems have not kept pace with these trends. The Chinook co-synthesis system addresses the automation of the most time-consuming and error-prone tasks in embedded controller design, namely the synthesis of interface hardware and software needed to integrate system components, the migration of functions between processors or custom logic, and the co-simulation of the design before, during and after synthesis. This paper describes the principal elements of Chinook and discuss its application to a variety of embedded designs.	custom logic , software tools , microprocessors , logic CAD , error-prone tasks , real-time systems , computer-aided design tools , interface software , system components integration , chinook hardware/software co-synthesis system , interface hardware , microcontrollers , function migration , embedded controller design , logic design , design co-simulation , design time constraints	The Chinook hardware/software co-synthesis system.	en	323
324	Abstract: One of the challenging tasks in code generation for embedded systems is register assignment. When more live variables than registers exist, some variables are necessarily accessed from data memory. Because loops are typically executed many times and are often time-critical, good register assignment in loops is exceedingly important, since accessing data memory can degrade performance. The issue of finding an optimal register assignment to loops, one which minimizes the number of spills between registers and memory, has been open for some time. In this paper, we address this issue and present an optimal, but exponential, algorithm which assigns registers to loop bodies such that the resulting spill code is minimal. We also show that a heuristic modification performs as well as the exponential approach on typical loops from scientific code.	heuristic modification , loops , storage allocation , minimal spill code , real-time systems , scientific code , data memory access , optimal register assignment , embedded code generation , program control structures , optimisation , exponential algorithm , live variables	Optimal register assignment to loops for embedded code generation.	en	324
325	Abstract: Software synthesis is a new approach which focuses on the support of embedded systems without the use of operating systems. Compared to traditional design practices, a better utilization of the available time and hardware resources can be achieved, because the static information provided by the system specification is fully exploited and an application-specific solution is automatically generated. On-going research on a software synthesis approach for real-time information processing systems is presented which starts from a concurrent process system specification and tries to automate the mapping of this description to a single processor. An internal representation model which is well-suited for the support of concurrency and timing constraints is proposed, together with flexible execution models for multi-tasking with real-time constraints. The method is illustrated on a personal terminal receiver demodulator for mobile satellite communication.	hardware resource utilization , internal representation model , real-time multi-tasking , concurrency control , real-time systems , multiprocessing programs , concurrent process system specification , information processing systems , automatically generated application-specific solution , embedded systems , static information , automatic processor mapping , software synthesis , computer aided software engineering , flexible execution models , mobile satellite communication , personal terminal receiver demodulator , time utilization , timing constraints , processor scheduling	Real-time multi-tasking in software synthesis for information processing systems.	en	325
326	An important challenge in the area of distributed computing is to automate the selection of the parameters that control the distributed computation. A performance-critical parameter is the grain size of the computation, i.e., the interval between successive synchronization points in the application. This parameter is hard to select since it depends both on compile time (loop structure and data dependences, computational complexity) and run time components (speed of compute nodes and network). On networks of workstations that are shared with other users, the run-time parameters can change over time. As a result, it is also necessary to consider the interactions with dynamic load balancing, which is needed to achieve good performance in this environment. In this paper we present a method for automatically selecting the grain size of the computation consisting of nested DO loops. The method is based on close cooperation between the compiler and the runtime system. We evaluate the method using both simulation and measurements for an implementation on the Nectar multicomputer.	automatic parallelization , dynamic load balancing , network of workstations , grain size	Controlling application grain size on a network of workstations.	en	326
327	Abstract: DSP algorithms are, in most cases, subject to hard real-time constraints. In the case of programmable DSPs, meeting those constraints must be ensured by appropriate code generation techniques. For processors offering instruction-level parallelism, the task of code generation includes code compaction. The exact timing behavior of a DSP program is only known after compaction. Therefore, real-time constraints should be taken into account during the compaction phase. While most known DSP code generators rely on rigid heuristics for that phase, this paper proposes a novel approach to local code compaction based on an integer programming model, which obeys exact timing constraints. Due to a general problem formulation, the model also obeys encoding restrictions and possible side-effects.	exact timing behavior , integer programming , instruction-level parallelism , encoding restrictions , side-effects , timing , programmable DSP , real-time systems , time-constrained code compaction , local code compaction , digital signal processing algorithms , rigid heuristics , integer programming model , hard real-time constraints , code generation techniques , source coding , automatic programming , digital signal processing chips	Time-constrained code compaction for DSPs.	en	327
328	Abstract: To construct complete systems on silicon, application specific DSP accelerators are needed to speed up the execution of high throughput DSP algorithms. In this paper, a methodology is presented to synthesize high throughput DSP functions into accelerator processors containing a datapath of highly pipelined, bit-parallel hardware units. Emphasis is put on the definition of a controller architecture that allows efficient run-time schedules of these DSP algorithms on such highly pipelined data paths. The methodology is illustrated by means of an FFT butterfly accelerator block.	highly pipelined data paths , parallel architectures , pipelined DSP accelerator synthesis , application specific DSP accelerators , datapath , dynamic scheduling , pipeline processing , controller architecture , circuit CAD , application specific integrated circuits , pipelined bit-parallel hardware , scheduling , FFT butterfly accelerator block , network synthesis , run-time schedules , silicon , digital signal processing chips , DSP algorithms	Synthesis of pipelined DSP accelerators with dynamic scheduling.	en	328
329	Abstract: This paper describes an exact solution methodology, implemented in Rensselaer's Voyager design space exploration system, for solving the scheduling problem in a 3-dimensional (3D) design space: the usual 2D design space (which trades off area and schedule length), plus a third dimension representing clock length. Unlike design space exploration methodologies which rely on bounds or estimates, this methodology is guaranteed to find the globally optimal solution to the 3D scheduling problem. Furthermore, this methodology efficiently prunes the search space, eliminating provably inferior design points through: a careful selection of candidate clock lengths; and tight bounds on the number of functional units of each type or on the schedule length.	high level synthesis , clocks , three dimensional scheduling , voyager design space exploration system , candidate clock lengths , 3D scheduling problem , 2D design space , clock length , globally optimal solution , three-dimensional design space , schedule length , 3D design space , two dimensional design space , search problems , tight bounds , optimisation , scheduling , network synthesis , search space pruning	An exact methodology for scheduling in a 3D design space.	en	329
330	Abstract: This paper describes the application of a measurement based power analysis technique for an embedded DSP processor. An instruction-level power model for the processor has been developed using this technique. Significant points of difference have been observed between this model and the ones developed earlier for some general-purpose commercial microprocessors. In particular, the effect of circuit state on the power cost of an instruction stream is more marked in the case of this DSP processor. In addition, the DSP processor has a special architectural feature that allows instructions to be packed into pairs. The energy reduction possible through the use of this feature is studied. The on-chip Booth multiplier on the processor is a major source of energy consumption for DSP programs. A micro-architectural power model for the multiplier is developed and analyzed for further energy minimization. A scheduling algorithm incorporating these new techniques is proposed to reduce the energy consumed by DSP software. Energy reductions varying from 11% to 56% have been observed for several example programs. These energy savings are real and have been verified through physical measurement.	energy minimization , embedded DSP software , circuit state , instruction sets , general-purpose commercial microprocessors , instruction-level power model , real-time systems , energy consumption , micro-architectural power model , low-power scheduling , energy reduction , on-chip Booth multiplier , circuit CAD , application specific integrated circuits , DSP processor , scheduling , measurement based power analysis , scheduling algorithm , digital signal processing chips , power analysis	Power analysis and low-power scheduling techniques for embedded DSP software.	en	330
331	We study the minimum-cost bounded-skew routing tree problem under the Elmore delay model. We present two approaches to construct bounded-skew routing trees: (i) the Boundary Merging and Embedding (BME) method which utilizes merging points that are restricted to the boundaries of merging regions, and (ii) the Interior Merging and Embedding (IME) algorithm which employs a sampling strategy and dynamic programming to consider merging points that are interior to, rather than on the boundary of, the merging regions. Our new algorithms allow accurate control of Elmore delay skew, and show the utility of merging points inside merging regions.	bounded-skew , zero-skew , elmore delay , routing trees , global routing , pathlength delay , clock routing , VLSI	Bounded-skew clock and Steiner routing under Elmore delay.	en	331
332	We present efficient, optimal algorithms for timing optimization by discrete wire sizing and buffer insertion. Our algorithms are able to minimize dynamic power dissipation subject to given timing constraints. In addition, we compute the complete power-delay tradeoff curve for added flexibility. We extend our algorithm to take into account the effect of signal slew on buffer delay which can contribute substantially to overall delay. The effectiveness of these methods is demonstrated experimentally.	timing optization , elmore delay , Dynamic Power Dissipation , dynamic programming , signal slew	Optimal wire sizing and buffer insertion for low power and a generalized delay model.	en	332
333	Abstract: Functions that map boolean vectors into the integers are important for the design and verification of arithmetic circuits. MTBDDs and BMDs have been proposed for representing this class of functions. We discuss the relationship between these methods and describe a generalization called hybrid decision diagrams which is often much more concise. We show how to implement arithmetic operations efficiently for hybrid decision diagrams. In practice, this is one of the main limitations of BMDs since performing arithmetic operations on functions expressed in this notation can be very expensive. In order to extend symbolic model checking algorithms to handle arithmetic properties, it is essential to be able to compute the BDD for the set of variable assignments that satisfy an arithmetic relation. In our paper, we give an efficient algorithm for this purpose. Moreover, we prove that for the class of linear expressions, the time complexity of our algorithm is linear in the number of variables.	circuit analysis computing , hybrid decision diagrams , multi-terminal binary decision diagrams , digital arithmetic , MTBDDs , arithmetic circuits verification , integers , binary decision diagrams , symbolic model checking algorithms , computational complexity , time complexity , boolean vectors , linear expressions	Hybrid decision diagrams.	en	333
334	This paper presents a method to synthesize labeled Petri nets from state-based models. Although state-based models (such as Finite State Machines) are a powerful formalism to describe the behavior of sequential systems, they cannot explicitly express the notions of concurrency, causality and conflict. Petri nets can naturally capture these notions. The proposed method in based on deriving an Elementary Transition System (ETS) from a specification model. Previous work has shown that for any ETS there exists a Petri net with minimum transition count (one transition for each label) with a reachability graph isomorphic to the original ETS. This paper presents the first known approach to obtain an ETS from a non-elementary TS and derive a place-irredundant Petri net. Furthermore, by imposing constraints on the synthesis method, different classes of Petri nets can be derived from the same reachability graph (pure, free choice, unique choice). This method has been implemented and efficiently applied in different frameworks: Petri net composition, synthesis of Petri nets from asynchronous circuits, and resynthesis of Petri nets.	asynchronous circuits , Finite State Machines , transition systems , synthesis , petri nets	Synthesizing Petri nets from state-based models.	en	334
335	We present a simulation-based method for combinational design verification that aims at complete coverage of specified design errors using conventional ATPG tools. The error models used in prior research are examined and reduced to four types: gate substitution errors (GSEs), gate count errors (GCEs), input count errors (ICEs), and wrong input errors (WIEs). Conditions are derived for a gate to be completely testable for GSEs; These conditions lead to small test sets for GSEs. Near-minimal test sets are also derived for GCEs. We analyze redundancy in design errors and relate this to single stuck-line (SSL) redundancy. We show how to map all the foregoing error types into SSL faults, and describe an extensive set of experiments to evaluate the proposed method. Our experiments demonstrate that high coverage of the modeled design errors can be achieved with small test sets.	logic simulation , error models , test generation , design verification	Design verification via simulation and automatic test pattern generation.	en	335
336	Abstract: Precise failure analysis requires accurate fault diagnosis. A previously proposed method for diagnosing bridging faults using single stuck-at dictionaries was applied only to small circuits, produced large and imprecise diagnoses, and did not take into account the Byzantine Generals Problem for bridging faults. We analyze the original technique and improve it by introducing the concepts of match restriction, match requirement, and failure recovery. Our new technique, which requires no information other than that used by standard stuck-at methods, produces diagnoses that are an order of magnitude smaller than those produced by the original technique and produces many fewer misleading diagnoses than that of traditional stuck-at diagnosis.	fault diagnosis , single stuck-at information , stuck-at methods , stuck-at diagnosis , failure recovery , match restriction , match requirement , realistic bridging faults diagnosis , single stuck-at dictionaries , failure analysis , fault location , logic testing	Diagnosis of realistic bridging faults with single stuck-at information.	en	336
337	We address the problem of instruction selection in code generation for embedded DSP microprocessors. Such processors have highly irregular data-paths, and conventional code generation methods typically result in inefficient code. Instruction selection can be formulated as directed acyclic graph (DAG) covering. Conventional methods for instruction selection use heuristics that break up the DAG into a forest of trees and then cover them independently. This breakup can result in suboptimal solutions for the original DAG. Alternatively, the DAG covering problem can be formulated as a binate covering problem, and solved exactly or heuristically using branch-and-bound methods. We show that optimal instruction selection on a DAG in the case of accumulator-based architectures requires a partial scheduling of nodes in the DAG, and we augment the binate covering formulation to minimize spills and reloads. We show how the irregular data transfer costs of typical DSP data-paths can be modeled in the binate covering formulation.	instruction selection , digital signal processors , code generation	Instruction selection using binate covering for code size optimization.	en	337
338	The optimal wiresizing problem for nets with multiple sources is studied under the distributed Elmore delay model. We decompose such a net into a source subtree (SST) and a set of loading subtrees (LSTs), and show the optimal wiresizing solution satisfies a number of interesting properties, including: the LST separability, the LST monotone property, the SST local monotone property and the general dominance property. Furthermore, we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property. These properties lead to efficient algorithms to compute the lower and upper bounds of the optimal solutions. Experiment results on nets from an Intel processor layout show an interconnect delay reduction of up to 35.9\% when compared to the minimum-width solution. In addition, the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy, when compared with the fixed grid based methods.	performance driven layout , optimal wiresizing , VLSI routing , interconnect optimization	Optimal wiresizing for interconnects with multiple sources.	en	338
339	We address the problem of minimizing power consumption in behavioral synthesis of data-dominated circuits. The complex nature of power as a cost function implies that the effects of several behavioral synthesis tasks like module selection, clock selection, scheduling, and resource sharing on supply voltage and switched capacitance need to be considered simultaneously to fully derive the benefits of design space exploration at the behavior level. Recent work has established the importance of behavioral synthesis in low power VLSI design. However, most of the algorithms that have been proposed separate these tasks and perform them sequentially, and are hence not able to explore the tradeoffs possible due to their interaction. We present an efficient algorithm for performing scheduling, clock selection, module selection, and resource allocation and assignment simultaneously with an aim of reducing the power consumption in the synthesized data path. The algorithm, which is based on an iterative improvement strategy, is capable of escaping local minima in its search for a low power solution. The algorithm considers diverse module libraries and complex scheduling constructs such as multicycling, chaining, and structural pipelining. We describe supply voltage and clock pruning strategies that significantly improve the efficiency of our algorithm by cutting down on the computational effort involved in exploring candidate supply voltages and clock periods that are unlikely to lead to the best solution. Experimental results are reported to demonstrate the effectiveness of the algorithm. Our techniques can be combined with other known methods of behavioral power optimization like data path replication and transformations, to result in a complete data path synthesis system for low power applications.	Low power VLSI design , behavioral synthesis , power consumption	An iterative improvement algorithm for low power data path synthesis.	en	339
340	Sather extends the notion of an iterator in a powerful new way. We argue that iteration abstractions belong in class interfaces on an equal footing with routines. Sather iterators were derived from CLU iterators but are much more flexible and better suited for object-oriented programming. We retain the property that iterators are structured, i.e., strictly bound to a controlling structured statement. We motivate and describe the construct along with several simple examples. We compare it with iteration based on CLU iterators, cursors, riders, streams, series, generators, coroutines, blocks, closures, and lambda expressions. Finally, we describe experiences with iterators in the Sather compiler and libraries.	iteration abstraction , general control structures , sather	Iteration abstraction in Sather.	en	340
341	One popular family of low dicrepancy sets is the (t, m, s)-nets. Recently a randomization of these nets that preserves their net property has been introduced. In this article a formula for the mean square L2-discrepancy of (0, m, s)-nets in base b is derived. This formula has a computational complexity of only O(s log(N) + s2) for large N or s, where bm is the number of points. Moreover, the root mean square L2-discrepancy of (0, m, s)-nets is show to be O(N-1[log(N)](s-1)/2) as N tends to infinity, the same asymptotic order as the known lower bound for  the  L2-discrepancy of an arbitrary set.	quasi-random sets , multidimensional integration , quasi-Monte Carlo methods , quadrature , number-theoretic nets and sequences	The mean square discrepancy of randomized nets.	en	341
342	This article studies an analytic model of parallel discrete-event simulation, comparing the YAWNS conservative synchronization protocol with Bounded Time Warp. The assumed simulation problem is a heavily loaded queuing network where the probability of an idle server  is closed to zero. We model workload and job routing in standard ways, then develop and validate methods for computing approximated performance measures as a function of the degree of optimism allowed, overhead costs of state-saving, rollback, and barrier synchronization, and workload aggregation. We find that Bounded Time Warp is superior when the number of servers per physical processor is low (i.e., sparse load), but that aggregating workload improves YAWNS relative performance.	synchronization protocol , parallel simulation	Analysis of bounded time warp and comparison with YAWNS.	en	342
343	AbstractWe introduce a computation model for developing and analyzing parallel algorithms on distributed memory machines. The model allows the design of algorithms using a single address space and does not assume any particular interconnection topology. We capture performance by incorporating a cost measure for interprocessor communication induced by remote memory accesses. The cost measure includes parameters reflecting memory latency, communication bandwidth, and spatial locality. Our model allows the initial placement of the input data and pipelined prefetching.We use our model to develop parallel algorithms for various data rearrangement problems, load balancing, sorting, FFT, and matrix multiplication. We show that most of these algorithms achieve optimal or near optimal communication complexity while simultaneously guaranteeing an optimal speed-up in computational complexity. Ongoing experimental work in testing and evaluating these algorithms has thus far shown very promising results.	parallel algorithms , parallel model , personalized communication , matrix multiplication , load balancing , broadcasting , sorting , Fast Fourier Transform	The Block Distributed Memory Model.	en	343
344	AbstractThis paper develops the theoretical background for the design of deadlock-free adaptive routing algorithms for virtual cut-through and store-and-forward switching. This theory is valid for networks using either central buffers or edge buffers. Some basic definitions and three theorems are proposed, developing conditions to verify that an adaptive algorithm is deadlock-free, even when there are cyclic dependencies between routing resources. Moreover, we propose a necessary and sufficient condition for deadlock-free routing. Also, a design methodology is proposed. It supplies fully adaptive, minimal and non-minimal routing algorithms, guaranteeing that they are deadlock-free.The theory proposed in this paper extends the necessary and sufficient condition for wormhole switching previously proposed by us. The resulting routing algorithms are more flexible than the ones for wormhole switching. Also, the design methodology is much easier to apply because it automatically supplies deadlock-free routing algorithms.	design methodologies , virtual cut-through , interconnection networks , deadlock avoidance , adaptive routing , store-and-forward	A Necessary and Sufficient Condition for Deadlock-Free Routing in Cut-Through and Store-and-Forward Networks.	en	344
345	AbstractThe fault-tolerance of distributed algorithms is investigated in asynchronous message passing systems with undetectable process failures. Two specific synchronization problems are considered, the dining philosophers problem and the binary committee coordination problem. The abstraction of a bounded doorway is introduced as a general mechanism for achieving individual progress and good failure locality. Using it as a building block, optimal fault-tolerant algorithms are constructed for the two problems.	concurrency , fault-tolerance , lower bounds , distributed algorithms , synchronization	Localizing Failures in Distributed Synchronization.	en	345
346	AbstractWe consider the problem of statically assigning many tasks to a (smaller) system of homogeneous processors, where a task's structure is modeled as a branching process, all tasks are assumed to have identical behavior, and the tasks may synchronize frequently. We show how the theory of majorization can be used to obtain a partial order among possible task assignments. We show that if the vector of numbers of tasks assigned to each processor under one mapping is majorized by that of another mapping, then the former mapping is better than the latter with respect to a large number of objective functions. In particular, we show how the metrics of finishing time, the space-time product, and reliability are all captured. We also apply majorization to the problem of partitioning a pool of processors for distribution among parallelizable tasks. Limitations of the approach, which include the static nature of the assignment, are also discussed.	majorization , performance of parallel systems , processor allocation , load balancing , task allocation , task assignment , resource allocation	Static Assignment of Stochastic Tasks Using Majorization.	en	346
347	AbstractManagement of replicated data has received considerable attention in the last few years. Several replica control schemes have been proposed which work in the presence of both node and communication link failures. However, this resiliency to failure inflicts a performance penalty in terms of the communication overhead incurred. Though the issue of performance of these schemes from the standpoint of availability of the system has been well addressed, the issue of message overhead has been limited to the analysis of worst case and best case message bounds. In this paper we derive expressions for computing the average message overhead of several well known replica control protocols and provide a comparative study of the different protocols with respect to both average message overhead and system availabilities.	quorum consensus , replica control , availability , message overhead , replicated databases , update synchronization	An Analysis of the Average Message Overhead in Replica Control Protocols.	en	347
348	AbstractWe address the problem of mapping divide-and-conquer programs to mesh connected multicomputers with wormhole or store-and-forward routing. We propose the binomial tree as an efficient model of parallel divide-and-conquer and present two mappings of the binomial tree to the 2D mesh. Our mappings exploit regularity in the communication structure of the divide-and-conquer computation and are also sensitive to the underlying flow control scheme of the target architecture. We evaluate these mappings using new metrics which are extensions of the classical notions of dilation and contention. We introduce the notion of communication slowdown as a measure of the total communication overhead incurred by a parallel computation. We conclude that significant performance gains can be realized when the mapping is sensitive to the flow control scheme of the target architecture.	routing , wormhole routing , mesh connected machines , mapping , dilation , binomial tree , divide-and-conquer algorithms , embedding , store-and-forward routing , contention	Parallel Divide and Conquer on Meshes.	en	348
349	AbstractPhenomenal improvements in the computational performance of multiprocessors have not been matched by comparable gains in I/O system performance. This imbalance has resulted in I/O becoming a significant bottleneck for many scientific applications. One key to overcoming this bottleneck is improving the performance of multiprocessor file systems. The design of a high-performance multiprocessor file system requires a comprehensive understanding of the expected workload. Unfortunately, until recently, no general workload studies of multiprocessor file systems have been conducted. The goal of the CHARISMA project was to remedy this problem by characterizing the behavior of several production workloads, on different machines, at the level of individual reads and writes. The first set of results from the CHARISMA project describe the workloads observed on an Intel iPSC/860 and a Thinking Machines CM-5. This paper is intended to compare and contrast these two workloads for an understanding of their essential similarities and differences, isolating common trends and platform-dependent variances. Using this comparison, we are able to gain more insight into the general principles that should guide multiprocessor file-system design.	workload characterization , parallel file system , parallel I/O , multiprocessor , scientific computing	File-Access Characteristics of Parallel Scientific Workloads.	en	349
350	AbstractAs massively parallel computers proliferate, there is growing interest in finding ways by which performance of massively parallel codes can be efficiently predicted. This problem arises in diverse contexts such as parallelizing compilers, parallel performance monitoring, and parallel algorithm development. In this paper, we describe one solution where one directly executes the application code, but uses a discrete-event simulator to model details of the presumed parallel machine, such as operating system and communication network behavior. Because this approach is computationally expensive, we are interested in its own parallelization, specifically the parallelization of the discrete-event simulator. We describe methods suitable for parallelized direct execution simulation of message-passing parallel programs, and report on the performance of such a system, LAPSE (Large Application Parallel Simulation Environment), we have built on the Intel Paragon. On all codes measured to date, LAPSE predicts performance well, typically within 10% relative error. Depending on the nature of the application code, we have observed low slowdowns (relative to natively executing code) and high relative speedups using up to 64 processors.	message-passing programs , parallel simulation , direct execution simulation , architectural simulation , synchronization , MIMD , contention	Parallelized Direct Execution Simulation of Message-Passing Parallel Programs.	en	350
351	We describe a divide-and-conquer tridiagonalization approach for  matrices with repeated eigenvalues. Our algorithm hinges on the  fact that, under easily constructively verifiable conditions,   a symmetric matrix with band width  $b$ and $k$ distinct eigenvalues must be block diagonal with  diagonal blocks of size at most $b k$. A slight modification of  the usual orthogonal band-reduction algorithm allows us to reveal  this structure, which then leads to potential parallelism in the form of  independent diagonal blocks.  Compared to the usual Householder  reduction algorithm, the new approach exhibits improved data  locality, significantly more scope for parallelism, and the potential to  reduce arithmetic complexity by close to 50% for matrices that have only  two numerically distinct eigenvalues. The actual improvement   depends to a large extent on the number of distinct eigenvalues and  a good estimate thereof. However, at worst the algorithms behave like  a successive band-reduction approach to tridiagonalization.  Moreover, we provide   a numerically reliable and effective algorithm for computing  the eigenvalue decomposition of a   symmetric matrix with two numerically distinct eigenvalues.  Such matrices arise, for example, in invariant subspace decomposition  approaches to the symmetric eigenvalue problem.	repeated eigenvalues , eigenvalue decomposition , tridiagonalization	On Tridiagonalizing and Diagonalizing Symmetric Matrices with Repeated Eigenvalues.	en	351
352	An approximate minimum degree (AMD) ordering algorithm  for preordering a  symmetric sparse matrix  prior to numerical factorization is presented.  We use techniques  based on the quotient graph for matrix factorization that allow us to  obtain computationally cheap bounds for the minimum degree. We show that  these bounds are often equal to the actual degree.  The resulting  algorithm is typically much faster than previous minimum degree ordering  algorithms and produces results that are comparable in quality with the  best orderings from other minimum degree algorithms.	ordering algorithms , quotient graph , graph algorithms , approximate minimum degree ordering algorithm , sparse matrices	An Approximate Minimum Degree Ordering Algorithm.	en	352
353	Certain interesting classes of functions on a real inner product space are invariant  under an associated group of orthogonal linear transformations.  This invariance can  be made explicit via a simple decomposition.  For example, rotationally invariant  functions on {\bf R}$^2$ are just even functions of the Euclidean norm, and functions on  the Hermitian matrices (with trace inner product) which are invariant under unitary  similarity transformations are just symmetric functions of the eigenvalues.  We  develop a framework for answering geometric and analytic (both classical and  nonsmooth) questions about such a function by answering the corresponding question  for the (much simpler) function appearing in the decomposition.  The aim is to  understand and extend the foundations of eigenvalue optimization, matrix  approximation, and semidefinite programming.	convexity , semidefinite program , fenchel conjugate , von Neumann's lemma , spectral function , extreme point , group invariance , nonsmooth analysis , unitarily invariant norm , schur convex , eigenvalue optimization , subdifferential	Group Invariance and Convex Matrix Analysis.	en	353
354	AbstractMemory hierarchies have long been studied by many means: system building, trace-driven simulation, and mathematical analysis. Yet little help is available for the system designer wishing to quickly size the different levels in a memory hierarchy to a first-order approximation. In this paper, we present a simple analysis for providing this practical help and some unexpected results and intuition that come out of the analysis. By applying a specific, parameterized model of workload locality, we are able to derive a closed-form solution for the optimal size of each hierarchy level. We verify the accuracy of this solution against exhaustive simulation with two case studies: a three-level I/O storage hierarchy and a three-level processor-cache hierarchy. In all but one case, the configuration recommended by the model performs within 5% of optimal. One result of our analysis is that the first place to spend money is the cheapest (rather than the fastest) cache level, particularly with small system budgets. Another is that money spent on an n-level hierarchy is spent in a fixed proportion until another level is added.	cache , optimization of cache configurations , memory , and storage hierarchies , trace-driven simulations	An Analytical Model for Designing Memory Hierarchies.	en	354
355	Object-code compatibility between processor generations is an open issue for VLIW architectures. A potential solution is a technique termed dynamic rescheduling, which performs run-time software rescheduling at the first-time page faults. The time required for rescheduling the pages constitutes a large portion of the overhead of this method. A disk caching scheme that uses a persistent rescheduled-page cache (PRC) is presented. The scheme reduces the overhead associated with dynamic rescheduling by saving rescheduled pages on disk, across program executions. Operating system support is required for dynamic rescheduling and management of the PRC. The implementation details for the PRC are discussed. Results of simulations used to gauge the effectiveness of PRC indicate that: the PRC is effective in reducing the overhead of dynamic rescheduling; and due to different overhead requirements of programs, a split PRC organization performs better than a unified PRC. The unified PRC was studied for two different page replacement policies: LRU and overhead-based replacement. It was found that with LRU replacement, all the programs consistently perform better with increasing PRC sizes, but the high-overhead programs take a consistent performance hit compared to the low-overhead programs. With overhead-based replacement, the performance of high-overhead programs improves substantially, while the low-overhead programs perform only slightly worse than in the case of the LRU replacement.	low overhead object code compatibility , run-time software rescheduling , program executions , persistent rescheduled-page cache , high-overhead programs , page replacement policies , first-time page faults , simulations , LRU replacement , cache storage , VLIW architectures , program performance , operating system support , disk caching scheme , overhead-based replacement , dynamic rescheduling	A persistent rescheduled-page cache for low overhead object code compatibility in VLIW architectures.	en	355
356	Code scheduling to exploit instruction level parallelism (ILP) is a critical problem in compiler optimization research in light of the increased use of long-instruction-word machines. Unfortunately optimum scheduling is computationally intractable, and one must resort to carefully crafted heuristics in practice. If the scope of application of a scheduling heuristic is limited to basic blocks, considerable performance loss may be incurred at block boundaries. To overcome this obstacle, basic blocks can be coalesced across branches to form larger regions such as super blocks. In the literature, these regions are typically scheduled using algorithms that are either oblivious to profile information (under the assumption that the process of forming the region has fully utilized the profile information), or use the profile information as an addendum to classical scheduling techniques. We believe that even for the simple case of linear code regions such as super blocks, additional performance improvement can be gained by utilizing the profile information in scheduling as well. We propose a general paradigm for converting any profile-insensitive list scheduler to a profile-sensitive scheduler. Our technique is developed via a theoretical analysis of a simplified abstract model of the general problem of profile-driven scheduling over any acyclic code region, yielding a scoring measure for ranking branch instructions.	long-instruction-word machines , scheduling heuristic , compiler optimization , optimum scheduling , abstract model , ranking branch instructions , optimising compilers , profile-driven instruction level parallel scheduling , profile-sensitive scheduler , linear code regions , code scheduling	Profile-driven instruction level parallel scheduling with application to super blocks.	en	356
357	Much of the previous work on modulo scheduling has targeted numeric programs, in which, often, the majority of the loops are well-behaved loop-counter-based loops without early exits. In control-intensive non-numeric programs, the loops frequently have characteristics that make it more difficult to effectively apply modulo scheduling. These characteristics include multiple control flow paths, loops that are not based on a loop counter, and multiple exits. In these loops, the presence of unimportant paths with high resource usage or long dependence chains can penalize the important paths. A path that contains a hazard such as another nested loop can prohibit modulo scheduling of the loop. Control dependences can severely restrict the overlap of the blocks within and across iterations. This paper describes a set of methods that allow effective modulo scheduling of loops with multiple exits. The techniques include removal of control dependences to enable speculation, extensions to modulo variable expansion, and a new epilogue generation scheme. These methods can be used with superblock and hyperblock techniques to allow modulo scheduling of the selected paths of loops with arbitrary control flow. A case study is presented to show how these methods, combined with superblock techniques, enable modulo scheduling to be effectively applied to control-intensive non-numeric programs. Performance results for several SPEC CINT92 benchmarks and Unix utility programs are reported and demonstrate the applicability of modulo scheduling to this class of programs.	speculation , software pipelining , control-intensive , instruction-level parallelism , modulo scheduling , modulo variable expansion	Modulo scheduling of loops in control-intensive non-numeric programs.	en	357
358	Many high performance processors predict conditional branches and consume processor resources based on the prediction. In some situations, resource allocation can be better optimized if a confidence level is assigned to a branch prediction; i.e. if the quantity of resources allocated is a function of the confidence level. To support such optimizations, we consider hardware mechanisms that partition conditional branch predictions into two sets: those which are accurate a relatively high percentage of the time, and those which are accurate a relatively low percentage of the time. The objective is to concentrate as many of the mispredictions as practical into a relatively small set of low confidence dynamic branches. We first study an ideal method that profiles branch predictions and sorts static branches into high and low confidence sets, depending on the accuracy with which they are dynamically predicted. We find that about 63 percent of the mispredictions can be localized to a set of static branches that account for 20 percent of the dynamic branches. We then study idealized dynamic confidence methods using both one and two levels of branch correctness history. We find that the single level method performs at least as well as the more complex two level method and is able to isolate 89 percent of the mispredictions into a set containing 20 percent of the dynamic branches. Finally, we study practical, less expensive implementations and find that they achieve most of the performance of the idealized methods.	dynamic branches , processor resources , branch correctness , static branches , conditional branch predictions , resource allocation	Assigning confidence to conditional branch predictions.	en	358
359	VLIW architectures use very wide instruction words in conjunction with high bandwidth to the instruction cache to achieve multiple instruction issue. This report uses the TINKER experimental testbed to examine instruction fetch and instruction cache mechanisms for VLIWs. A compressed instruction encoding for VLIWs is defined and a classification scheme for i-fetch hardware for such an encoding is introduced. Several interesting cache and i-fetch organizations are described and evaluated through trace-driven simulations. A new i-fetch mechanism using a silo cache is found to have the best performance.	parallel architectures , i-fetch hardware , instruction words , TINKER experimental testbed , compressed encodings , compressed instruction encoding , silo cache , instruction cache , instruction fetch mechanisms , VLIW architectures , multiple instruction issue , trace-driven simulations	Instruction fetch mechanisms for VLIW architectures with compressed encodings.	en	359
360	AbstractThis paper presents a new model for estimating optical flow based on the motion of planar regions plus local deformations. The approach exploits brightness information to organize and constrain the interpretation of the motion by using segmented regions of piecewise smooth brightness to hypothesize planar regions in the scene. Parametric flow models are estimated in these regions in a two step process which first computes a coarse fit and estimates the appropriate parameterization of the motion of the region (two, six, or eight parameters). The initial fit is refined using a generalization of the standard area-based regression approaches. Since the assumption of planarity is likely to be violated, we allow local deformations from the planar assumption in the same spirit as physically-based approaches which model shape using coarse parametric models plus local deformations. This parametric+deformation model exploits the strong constraints of parametric approaches while retaining the adaptive nature of regularization approaches. Experimental results on a variety of images indicate that the parametric+deformation model produces accurate flow estimates while the incorporation of brightness segmentation provides precise localization of motion boundaries.	segmentation , parameterized flow models , robust regression , local deformation , optical flow	Estimating Optical Flow in Segmented Images Using Variable-Order Parametric Models With Local Deformations.	en	360
361	"AbstractFeature detectors using a quadratic nonlinearity in the filtering stage are known to have some advantages over linear detectors; here, we consider their scale-space properties. In particular, we investigate whether, like linear detectors, quadratic feature detectors permit a scale selection scheme with the ""causality property,"" which guarantees that features are never created as scale is coarsened. We concentrate on the design most common in practice, i.e., one dimensional detectors with two constituent filters, with scale selection implemented as convolution with a scaling function. We consider two special cases of interest: constituent filter pairs related by the Hilbert transform, and by the first spatial derivative. We show that, under reasonable assumptions, Hilbert-pair quadratic detectors cannot have the causality property. In the case of derivative-pair detectors, we describe a family of scaling functions related to fractional derivatives of the Gaussian that are necessary and sufficient for causality. In addition, we report experiments that show the effects of these properties in practice. Thus we show that at least one class of quadratic feature detectors has the same desirable scaling property as the more familiar detectors based on linear filtering."	feature detection , quadratic filters , edge detection , causality , nonlinear filtering , energy filters , scale space	Scale-Space Properties of Quadratic Feature Detectors.	en	361
362	AbstractThe rapid advances in high-performance computer architecture and compilation techniques provide both challenges and opportunities to exploit the rich solution space of software pipelined loop schedules. In this paper, we develop a framework to construct a software pipelined loop schedule which runs on the given architecture (with a fixed number of processor resources) at the maximum possible iteration rate ( la rate-optimal) while minimizing the number of buffersa close approximation to minimizing the number of registers.The main contributions of this paper are: First, we demonstrate that such problem can be described by a simple mathematical formulation with precise optimization objectives under a periodic linear scheduling framework. The mathematical formulation provides a clear picture which permits one to visualize the overall solution space (for rate-optimal schedules) under different sets of constraints. Secondly, we show that a precise mathematical formulation and its solution does make a significant performance difference. We evaluated the performance of our method against three leading contemporary heuristic methods. Experimental results show that the method described in this paper performed significantly better than these methods.The techniques proposed in this paper are useful in two different ways: 1) As a compiler option which can be used in generating faster schedules for performance-critical loops (if the interested users are willing to trade the cost of longer compile time with faster runtime). 2) As a framework for compiler writers to evaluate and improve other heuristics-based approaches by providing quantitative information as to where and how much their heuristic methods could be further improved.	software pipelining , instruction-level parallelism , superscalar and VLIW architectures , instruction scheduling , integer linear programming	A Framework for Resource-Constrained Rate-Optimal Software Pipelining.	en	362
363	Let W be a Coxeter group. We define an element w  W to be fully commutative if any reduced expression for w can be obtained from any other by means of braid relations that only involve commuting generators. We give several combinatorial characterizations of this property, classify the Coxeter groups with finitely many fully commutative elements, and classify the parabolic quotients whose members are all fully commutative. As applications of the latter, we classify all parabolic quotients with the property that (1) the Bruhat ordering is a lattice, (2) the Bruhat ordering is a distributive lattice, (3) the weak ordering is a distributive lattice, and (4) the weak ordering and Bruhat ordering coincide.	coxeter group , weak order , reduced word , bruhat order	On the Fully Commutative Elements of Coxeter Groups.	en	363
364	Elmore delay has been widely used as an analytical estimate of interconnect delays in the performance-driven synthesis and layout of VLSI routing topologies. However, for typical RLC interconnections with ramp input, Elmore delay can deviate by up to 100% or more from SPICE-computed delay since it is independent of rise time of the input ramp signal. We develop new analytical delay models based on the first and second moments of the interconnect transfer function when the input is a ramp signal with finite rise time. Delay estimates using our first moment based analytical models are within 4% of SPICE-computed delay, and models based on both first and second moments are within 2.3% of SPICE, across a wide range of interconnect parameter values. Evaluation of our analytical models is several orders of magnitude faster than simulation using SPICE. We also describe extensions of our approach for estimation of source-sink delays in arbitrary interconnect trees.	interconnect transfer function , ramp input , analytical delay models , VLSI routing topologies layout , RLC interconnections , performance-driven synthesis , source-sink delays , elmore delay , SPICE-computed delay , interconnect delays , VLSI , VLSI interconnects , arbitrary interconnect trees	Analytical delay models for VLSI interconnects under ramp input.	en	364
365	We present techniques for estimating switching activity and power consumption in register-transfer level (RTL) circuits. Previous work on this topic has ignored the presence of glitching activity at various data path and control signals, which can lead to significant underestimation of switching activity. For data path blocks that operate on word-level data, we construct piecewise linear models that capture the variation of output glitching activity and power consumption with various word-level parameters like mean, standard deviation, spatial and temporal correlations, and glitching activity at the block's inputs. For RTL blocks that operate on data that need not have an associated word-level value, we present accurate bit-level modeling techniques for glitching activity as well as power consumption. This allows us to perform accurate power estimation for control-flow intensive circuits, where most of the power consumed is dissipated in non-arithmetic components like multiplexers, registers, vector logic operators, etc. Since the final implementation of the controller is not available during high-level design iterations, we develop techniques that estimate glitching activity at control signals using control expressions and partial delay information. Experiments on example RTL designs resulted in power estimates that were within 7% of those produced by an inhouse power analysis tool on the final gate-level implementation.	switching activity , power consumption , gate-level implementation , RTL designs , glitching , register-transfer level estimation , logic design	Register-transfer level estimation techniques for switching activity and power consumption.	en	365
366	In this paper, we study the simultaneous transistor and interconnect sizing (STIS) problem. We define a class of optimization problems as CH-posynomial programs and reveal a general dominance property for all CH-posynomial programs. We show that the STIS problems under a number of transistor delay models are CH-posynomial programs and propose an efficient and near-optimal STIS algorithm based on the dominance property. When used to solve the simultaneous driver/buffer and wire sizing problem for real designs, it reduces the maximum delay by up to 16.1%, and more significantly, reduces the power consumption by a factor of 1.63X, when compared with the original designs. When used to solve the transistor sizing problem, it achieves a smooth area-delay trade-off. Moreover, the algorithm optimizes a clock net of 367 drivers/buffers and 59304 /spl mu/m-long wire in 120 seconds, and a 32-bit adder with 1026 transistors in 66 seconds on a SPARC-5 workstation.	STIS , driver/buffer , CH-posynomial programs , transistor sizing , circuit CAD , transistor and interconnect sizing , wire sizing problem	An efficient approach to simultaneous transistor and interconnect sizing.	en	366
367	"Move-based iterative improvement partitioning methods such as the Fiduccia-Mattheyses (FM) algorithm and Krishnamurthy's Look-Ahead (LA) algorithm are widely used in VLSI CAD applications largely due to their time efficiency and ease of implementation. This class of algorithms is of the ""local improvement"" type. They generate relatively high quality results for small and medium size circuits. However, as VLSI circuits become larger, these algorithms are not so effective on them as direct partitioning tools. We propose new iterative-improvement methods that select cells to move with a view to moving clusters that straddle the two subsets of a partition into one of the subsets. The new algorithms significantly improve partition quality while preserving the advantage of time efficiency. Experimental results on 25 medium to large size ACM/SIGDA benchmark circuits show up to 70% improvement over FM in cutsize, with an average of per-circuit percent improvements of about 25%, and a total cut improvement of about 35%. They also outperform the recent placement-based partitioning tool Paraboli and the spectral partitioner MELO by about 17% and 23%, respectively, with less CPU time. This demonstrates the potential of iterative improvement algorithms in dealing with the increasing complexity of modern VLSI circuitry."	fiduccia-mattheyses algorithm , cluster-removal , iterative improvement techniques , VLSI , ACM/SIGDA benchmark circuits , look-ahead algorithm , spectral partitioner MELO , partition quality , CAD , VLSI circuit partitioning	VLSI circuit partitioning by cluster-removal using iterative improvement techniques.	en	367
368	"AbstractA fail-silent node is a self-checking node that either functions correctly or stops functioning after an internal failure is detected. Such a node can be constructed from a number of conventional processors. In a software-implemented fail-silent node, the nonfaulty processors of the node need to execute message order and comparison protocols to ""keep in step"" and check each other, respectively. In this paper, the design and implementation of efficient protocols for a two processor fail-silent node are described in detail. The performance figures obtained indicate that in a wide class of applications requiring a high degree of fault-tolerance, software-implemented fail-silent nodes constructed simply by utilizing standard ""off-the-shelf"" components are an attractive alternative to their hardware-implemented counterparts that do require special-purpose hardware components, such as fault-tolerant clocks, comparator, and bus interface circuits."	replicated processing , distributed processing , fault-tolerance , fail-silence , reliability	Implementing Fail-Silent Nodes for Distributed Systems.	en	368
369	AbstractPrefix computation is a basic operation at the core of many important applications, e.g., some of the Grand Challenge problems, circuit design, digital signal processing, graph optimizations, and computational geometry.1 In this paper, we present new and strict time-optimal parallel schedules for prefix computation with resource constraints under the concurrent-read-exclusive-write (CREW) parallel random access machine (PRAM) model. For prefix of N elements on p processors (p independent of N) when N > p(p + 1)/2, we derive Harmonic Schedules that achieve the strict optimal time (steps), $\left\lceil {{{2\left( {N-1} \right)} \mathord{\left/ {\vphantom {{2\left( {N-1} \right)} {\left( {p+1} \right)}}} \right. \kern-\nulldelimiterspace} {\left( {p+1} \right)}}} \right\rceil $. We also derive Pipelined Schedules that have better program-space efficiency than the Harmonic Schedule, yet only require a small constant number of steps more than the optimal time achieved by the Harmonic Schedule. Both the Harmonic Schedules and the Pipelined Schedules are simple and easy to implement. For prefix of N elements on p processors (p independent of N) where Np(p + 1)/2, the Harmonic Schedules are not time-optimal. For these cases, we establish an optimization method for determining key parameters of time-optimal schedules, based on connections between the structure of parallel prefix and Pascal's triangle. Using the derived parameters, we devise an algorithm to construct such schedules. For a restricted class of values of N and p, we prove that the constructed schedules are strictly time-optimal. We also give strong empirical evidence that our algorithm constructs strict time-optimal schedules for all cases where Np(p	pascal's triangle , loop-carried dependences , loop parallelization , combinatorial optimization , parallel prefix computation , scan operator resource-constrained parallel algorithms , tree-height reduction , strict time-optimal schedules , associative operations	The Strict Time Lower Bound and Optimal Schedules for Parallel Prefix with Resource Constraints.	en	369
370	In a gate-level description of a finite state machine (\fsm), there is a tradeoff between the number of latches and the size of the logic implementing the next-state and output functions. Typically, an initial implementation is generated via explicit state assignment or translation from a high-level language, and the tradeoff is subsequently only lightly explored. We efficiently explore good latch/logic tradeoffs for large designs generated from high-level specifications. We reduce the number of latches while controlling the logic size. We demonstrate the efficacy of our techniques on some large industrial examples.	state assignment , high-level synthesis , sequential optimisation	Latch optimization in circuits generated from high-level descriptions.	en	370
371	"AbstractAn algorithm is described which rapidly verifies the potential rigidity of three-dimensional point correspondences from a pair of two-dimensional views under perspective projection. The output of the algorithm is a simple yes or no answer to the question ""Could these corresponding points from two views be the projection of a rigid configuration?"" Potential applications include 3D object recognition from a single previous view and correspondence matching for stereo or motion over widely separated views. The rigidity checking problem is different from the structure-from-motion problem because it is often the case that two views cannot provide an accurate structure-from-motion estimate due to ambiguity and ill conditioning, whereas it is still possible to give an accurate yes/no answer to the rigidity question. Rigidity checking verifies point correspondences using 3D recovery equations as a matching condition. The proposed algorithm improves upon other methods that fall under this approach because it works with as few as six corresponding points under full perspective projection, handles correspondences from widely separated views, makes full use of the disparity of the correspondences, and is integrated with a linear algorithm for 3D recovery due to Kontsevich. Results are given for experiments with synthetic and real image data. A complete implementation of this algorithm is being made publicly available."	perspective projection , nonlinear parameter estimation , rigidity checking , image matching , structure-from-motion , point correspondences	Rigidity Checking of 3D Point Correspondences Under Perspective Projection.	en	371
372	AbstractStructures of dynamic scenes can only be recovered using a real-time range sensor. Depth from defocus offers an effective solution to fast and dense range estimation. However, accurate depth estimation requires theoretical and practical solutions to a variety of problems including recovery of textureless surfaces, precise blur estimation, and magnification variations caused by defocusing. Both textured and textureless surfaces are recovered using an illumination pattern that is projected via the same optical path used to acquire images. The illumination pattern is optimized to maximize accuracy and spatial resolution in computed depth. The relative blurring in two images is computed using a narrow-band linear operator that is designed by considering all the optical, sensing, and computational elements of the depth from defocus system. Defocus invariant magnification is achieved by the use of an additional aperture in the imaging optics. A prototype focus range sensor has been developed that has a workspace of 1 cubic foot and produces up to 512  480 depth estimates at Hz with an average RMS error of 0.2%. Several experimental results are included to demonstrate the performance of the sensor.	active illumination pattern , tuned focus operator , optical transfer function , constant magnification defocusing , image sensing , depth from defocus , real-time range sensor , depth estimation	Real-Time Focus Range Sensor.	en	372
373	AbstractThis paper presents a motion estimation algorithm based on a new multiresolution representation, the quadtree spline. This representation describes the motion field as a collection of smoothly connected patches of varying size, where the patch size is automatically adapted to the complexity of the underlying motion. The topology of the patches is determined by a quadtree data structure, and both split and merge techniques are developed for estimating this spatial subdivision. The quadtree spline is implemented using another novel representation, the adaptive hierarchical basis spline, and combines the advantages of adaptively-sized correlation windows with the speedups obtained with hierarchical basis preconditioners. Results are presented on some standard motion sequences.	quadtrees , image pyramids , hierarchical basis functions , motion analysis , splines , multiresolution analysis , image registration , local parametric motion models , motion segmentation , optical flow	Motion Estimation with Quadtree Splines.	en	373
374	AbstractAs part of our continuing research on using Petri nets to support automated analysis of Ada tasking behavior, we have investigated the application of Petri net reduction for deadlock analysis. Although reachability analysis is an important method to detect deadlocks, it is in general inefficient or even intractable. Net reduction can aid the analysis by reducing the size of the net while preserving relevant properties. We introduce a number of reduction rules and show how they can be applied to Ada nets, which are automatically generated Petri net models of Ada tasking. We define a reduction process and a method by which a useful description of a detected deadlock state can be obtained from the reduced net's information. A reduction tool and experimental results from applying the reduction process are discussed.	reachability analysis , net reduction , concurrent software , ada tasking , petri nets , deadlock analysis	An Application of Petri Net Reduction for Ada Tasking Deadlock Analysis.	en	374
375	AbstractThis paper discusses detection of global predicates in a distributed program. A run of a distributed program results in a set of sequential traces, one for each process. These traces may be combined to form many global sequences consistent with the single run of the program. A strong global predicate is true in a run if it is true for all global sequences consistent with the run. We present algorithms which detect if the given strong global predicate became true in a run of a distributed program. Our algorithms can be executed on line as well as off line. Moreover, our algorithms do not assume that underlying channels satisfy FIFO ordering.	predicate detection , unstable predicates , distributed debugging , distributed algorithms	Detection of Strong Unstable Predicates in Distributed Programs.	en	375
376	AbstractIn many designs a large portion of path delay faults is not robustly testable. In this paper, we investigate testing strategies for robustly untestable faults. We show that the quality of nonrobust tests may be very poor in detecting small defects caused by manufacturing process variation. We demonstrate that better quality nonrobust tests can be obtained by including timing information into the process of test generation. A good nonrobust test can tolerate larger timing variations on the off-inputs. We also show that not all nonrobustly untestable path delay faults may be ignored in high quality delay testing. Functional sensitizable paths are nonrobustly untestable but, under some faulty conditions, may degrade the performance of the circuit. However, up till now, there was no strategy for generating tests for such faults.In this paper, we present algorithms for generating high quality nonrobust and functional sensitizable tests. We also devise an algorithm for generating tests for validatable nonrobust faults which have a high quality in detecting defects but are hard to be generated automatically. Our experimental results show that the quality of delay testing increases if validatable and high quality nonrobust tests, as well as tests for functional sensitizable path delay faults are included.	automatic test generation , timing defects , robust , nonrobust , path delay faults , delay testing , VLSI testing	Generation of High Quality Tests for Robustly Untestable Path Delay Faults.	en	376
377	We consider linear plants controlled by dynamic output feedback which are subjected to blockdiagonal stochastic parameter perturbations. The stability radii of these systems are characterized, and it is shown that, for real data, the real and the complex stability radii coincide. A corresponding result does not hold in the deterministic case, even for perturbations of single-output feedback type. In a second part of the paper we study the problem of optimizing the stability radius by dynamic linear output feedback. Necessary and sufficient conditions are derived for the existence of a compensator which achieves a suboptimal stability radius. These conditions consist of a parametrized Riccati equation, a parametrized Liapunov inequality, a coupling inequality, and a number of linear matrix inequalities (one for each disturbance term). The corresponding problem in the deterministic case, the optimal $\mu$-synthesis problem, is still unsolved.	dynamic output feedback , linear matrix inequalities , multiperturbations , stability radius , state-dependent noise , riccati inequalities , stochastic systems , scaling	Stability Radii of Systems with Stochastic Uncertainty and Their Optimization by Output Feedback.	en	377
378	A technique to construct a low-order finite difference preconditioner for solving orthogonal collocation equations for boundary value problems is presented.  It is shown numerically and theoretically that the spectral condition numbers of the preconditioned collocation matrices are bounded by constants independent of the number of mesh nodes when certain exact low-order finite difference preconditionings are used. Preconditioners based on incomplete LU factorization are also discussed. Numerical experiments show the efficiency and robustness of the preconditioning.	collocation , boundary value problem , preconditioning	Finite Difference Preconditioning for Solving Orthogonal Collocation Equations for Boundary Value Problems.	en	378
379	Repositories for software reuse are faced with two interrelated problems: (1) acquiring the knowledge to initially construct the repository and (2) modifying the repository to meet the evolving and dynamic needs of software development organizations. Current software repository methods rely heavily on classification, which exacerbates acquistition and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively, This article outlines an approach that avoids these problems by choosing a retrieval method that utilizes minimal repository structure to effectively support the process of finding software conponents. The approach is demonstrated through a pair of proof-of-concept prototypes: PEEL, a tool to semiautomatically  identify reusable components, and CodeFinder, a retrieval system that compensates for the lack of explicit knowledge structures through a spreading activation retrieval process. CodeFinder also allows component representations to be modified while users are searching for information. This mechanism adapts to the changing nature of the information in the repository and incrementally improves the repository while people use it. The combination of these techniques holds potential for designing software repositories that minimize up-front costs, effectively support the search process, and evolve with an organization's changing needs.	software reuse , component repositories , information retrieval	An evolutionary approach to constructing effective software reuse repositories.	en	379
380	AbstractA graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise. By combining graduated nonconvexity, two-way (assignment) constraints, and sparsity, large improvements in accuracy and speed are achieved. Its low order computational complexity [O(lm), where l and m are the number of links in the two graphs] and robustness in the presence of noise offer advantages over traditional combinatorial approaches. The algorithm, not restricted to any special class of graph, is applied to subgraph isomorphism, weighted graph matching, and attributed relational graph matching. To illustrate the performance of the algorithm, attributed relational graphs derived from objects are matched. Then, results from twenty-five thousand experiments conducted on 100 node random graphs of varying types (graphs with only zero-one links, weighted graphs, and graphs with node attributes and multiple link types) are reported. No comparable results have been reported by any other graph matching algorithm before in the research literature. Twenty-five hundred control experiments are conducted using a relaxation labeling algorithm and large improvements in accuracy are demonstrated.	model matching , graduated assignment , continuation method , attributed relational graphs , softassign , weighted graphs , relaxation labeling , graph matching	A Graduated Assignment Algorithm for Graph Matching.	en	380
381	"We show that for most complexity classes of interest, all sets complete under first-order projections (fops) are isomorphic under first-order isomorphisms.  That is, a very restricted version of the Berman--Hartmanis conjecture holds.  Since ""natural"" complete problems seem to stay complete via fops, this indicates that up to first-order isomorphism there is only one ""natural"" complete problem for each ""nice"" complexity class."	complexity classes , reduction , first-order projection , descriptive complexity	A First-Order Isomorphism Theorem.	en	381
382	In this paper, we consider the stochastic profile scheduling problem of a partially ordered set of tasks on uniform processors. The set of available processors varies in time. The running times of the tasks are independent random variables with exponential distributions. We obtain a sufficient condition under which a list policy stochastically minimizes the makespan within the class of preemptive policies. This result allows us to obtain a simple optimal policy when the partial order is an interval order, an in-forest, or an out-forest.	stochastic scheduling , profile scheduling , stochastic ordering , makespan , precedence constraint , interval order , uniform processors , out-forest , in-forest	Stochastic Scheduling with Variable Profile and Precedence Constraints.	en	382
383	This paper investigates the computational complexity of approximating  several \NP-optimization problems using the number of queries to an  \NP\ oracle as a complexity measure.  The results show a tradeoff  between the closeness of the approximation and the number of queries  required.  For an approximation factor $k(n)$, $\log \log_{k(n)} n$  queries to an \NP\ oracle can be used to approximate the maximum  clique size of a graph within a factor of $k(n)$.   However, this approximation cannot be achieved using fewer than  $\log \log_{k(n)} n - c$ queries to any oracle unless  is a constant that does not depend on $k$.   These results hold for approximation factors $k(n) \geq 2$ that  belong to a class of functions which includes any integer constant  function, $\log n$, $\log^{a} n$, and $n^{1/a}$.  Similar results  are obtained for Graph Coloring, Set Cover, and other \NP-optimization problems.	bounded queries , set cover , NP-completeness , maximum clique , chromatic number , approximation algorithm	On Bounded Queries and Approximation.	en	383
384	Self-stabilizing message-driven protocols are defined and discussed.      The class weak exclusion that contains many natural tasks      such as $\ell$-exclusion and token passing is defined,      and it is shown that in any execution of any self-stabilizing      protocol for a task in this class, the configuration size must grow      at least in a logarithmic rate. This last lower bound is valid even      if the system is supported by a time-out mechanism that prevents      communication deadlocks. Then we present three self-stabilizing      message-driven protocols for token passing. The rate of growth of      configuration size for all three protocols matches the aforementioned      lower bound. Our protocols are presented for two-processor systems      but can be easily adapted to rings of arbitrary size. Our results      have an interesting interpretation in terms of automata theory.	message passing , self-stabilization , token passing , shared memory	Resource Bounds for Self-Stabilizing Message-Driven Protocols.	en	384
385	"AbstractIn this paper, we examine the wormhole routing problem in terms of the ""congestion"" c and ""dilation"" d for a set of packet paths. We show, with mild restrictions, that there is a simple randomized algorithm for routing any set of P packets in $O\left( {cd\eta +cL\eta \,\,{\rm log}\,\,P} \right)$ time with high probability, where L is the number of flits in a packet, and only a constant number of flits are stored in each queue at any time. Using this result, we show that a fat-tree network of area (A) can simulate wormhole routing on any network of comparable area with O(log3A) slowdown, when all worms have the same length. Variable-length worms are also considered. We run some simulations on the fat-tree which show that not only does wormhole routing tend to perform better than the more heavily studied store-and-forward routing in this context, but that performance superior to our provable bound is attainable in practice."	wormhole routing , packet routing , greedy routing , fat-tree interconnection network , area-universal networks , randomized routing	Universal Wormhole Routing.	en	385
386	AbstractReducing communication latency, which is a performance bottleneck in optically interconnected multiprocessor systems, is of prominent importance. A conventional approach for establishing connections in multiplexed networks uses a set of independent time slots (or virtual channels) along a path for each connection. This approach requires the use of switching devices capable of interchanging time slots, and thus introduces latency in addition to hardware and control complexity. In this paper, we propose an approach to all-optical Time Division Multiplexed (TDM) communications in multiprocessor systems. The idea is to establish a connection along a path using a set of time slots (or virtual channels) that are dependent on each other, so that no time-slot interchanging is required. We compare the proposed approach with the conventional one in terms of the overall communication latency. We found that, despite the possibility that establishing a connection may take a longer time, the proposed approach will result in lower overall communication latency as it eliminates the delays introduced by the time-slot interchanging switching devices.	time division multiplexing , fiber-optical interconnects , communication latency , switching networks , time slot interchangers	Reducing Communication Latency with Path Multiplexing in Optically Interconnected Multiprocessor Systems.	en	386
387	AbstractThe bound on component failures and their spatial distribution govern the fault tolerance of any candidate error-detecting algorithm. For distributed memory multiprocessors, the specific algorithm and the topology of the processor interconnection network define these bounds. This paper introduces the maximal fault index, derived from the system topology and local communication patterns, to demonstrate how a maximal number of simultaneous component failures can be tolerated for a particular interconnection network and error-detecting algorithm. The index is used to design a mapping of processes to processor groups such that the error-detecting ability of the algorithm is preserved for certain multiple simultaneous processor failures.	error detection , architecture , fault tolerance , mapping , fault-tolerant algorithms , multicomputers	A General Method for Maximizing the Error-Detecting Ability of Distributed Algorithms.	en	387
388	AbstractParallel scheduling is a new approach for load balancing. In parallel scheduling, all processors cooperate to schedule work. Parallel scheduling is able to accurately balance the load by using global load information at compile-time or runtime. It provides high-quality load balancing. This paper presents an overview of the parallel scheduling technique. Scheduling algorithms for tree, hypercube, and mesh networks are presented. These algorithms can fully balance the load and maximize locality at runtime. Communication costs are significantly reduced compared to other existing algorithms.	distributed memory computers , trees , runtime parallel scheduling , load balancing , meshes , scheduling algorithms , hypercubes	On Runtime Parallel Scheduling for Processor Load Balancing.	en	388
389	AbstractThe performance of the Time Warp mechanism is experimentally evaluated when only a limited amount of memory is available to the parallel computation. An implementation of the cancelback protocol is used for memory management on a shared memory architecture, viz., KSR to evaluate the performance vs. memory tradeoff. The implementation of the cancelback protocol supports canceling back more than one memory object when memory has been exhausted (the precise number is referred to as the salvage parameter) and incorporates a non-work-conserving processor scheduling technique to prevent starvation.Several synthetic and benchmark programs are used that provide interesting stress cases for evaluating the limited memory behavior. The experiments are extensively monitored to determine the extent to which various factors may affect performance. Several observations are made by analyzing the behavior of Time Warp under limited memory: 1) Depending on the available memory and asymmetry in the workload, canceling back several memory objects at one time (i.e., a salvage parameter value of more than one) improves performance significantly, by reducing certain overheads. However, performance is relatively insensitive to the salvage parameter except at extreme values. 2) The speedup vs. memory curve for Time Warp programs has a well-defined knee before which speedup increases very rapidly with memory and beyond which there is little performance gain with increased memory. performance nearly equivalent to that with large amounts of memory can be achieved with only a modest amount of additional memory beyond that required for sequential execution, if memory management overheads are small compared to the event granularity. These results indicate that contrary to the common belief, memory usage by Time Warp can be controlled within reasonable limits without any significant loss of performance.	checkpointing , time warp , performance evaluation , virtual time , discrete event simulation , memory management , parallel and distributed simulation , rollback	An Empirical Evaluation of Performance-Memory Trade-Offs in Time Warp.	en	389
390	AbstractAn end-to-end data delivery protocol for dynamic communication networks is presented. The protocol uses bounded sequence numbers and can tolerate both link failures and (intermediate) processor crashes. Previous bounded end-to-end protocols could not tolerate crashes.We present a self-stabilizing version of the algorithm that can recover from crashes of the sender and the receiver as well as of intermediate processors. Starting with the network in an arbitrary state, the self-stabilizing version guarantees proper transmission of messages following a finite convergence period.	dynamic networks , communication networks , self-stabilization , crash failures , end-to-end protocols	Crash Resilient Communication in Dynamic Networks.	en	390
391	AbstractThis paper describes a generalized sequential diagnosis algorithm whose analysis leads to strong diagnosability results for a variety of multiprocessor interconnection topologies. The overall complexity of this algorithm in terms of total testing and syndrome decoding time is linear in the number of edges in the interconnection graph and the total number of iterations of diagnosis and repair needed by the algorithm is bounded by the diameter of the interconnection graph. The degree of diagnosability of this algorithm for a given interconnection graph is shown to be directly related to a graph parameter which we refer to as the partition number. We approximate this graph parameter for several interconnection topologies and thereby obtain lower bounds on degree of diagnosability achieved by our algorithm on these topologies. If we let N denote total number of vertices in the interconnection graph and  denote the maximum degree of any vertex in it, then our results may be summarized as follows. We show that a symmetric d-dimensional grid graph is sequentially $\Omega \left( {N^{{{d \over {d+1}}}}} \right)$-diagnosable for any fixed d. For hypercubes, symmeteric log N-dimensional grid graphs, it is shown that our algorithm leads to a surprising $\Omega \left( {{{{N\,{\rm log\,log}\,N} \over {log\,N}}}} \right)$ degree of diagnosability. Next we show that the degree of diagnosability of an arbitrary interconnection graph by our algorithm is $\Omega \left( {\sqrt {{{N \over \Delta }}}} \right).$ This bound translates to an $\Omega \left( {\sqrt N} \right)$ degree of diagnosability for cube-connected cycles and an $\Omega \left( {\sqrt {{{N \over k}}}} \right)$ degree of diagnosability for k-ary trees. Finally, we augment our algorithm with another algorithm to show that every topology is $\Omega \left( {N^{{{1 \over 3}}}} \right)$-diagnosable.	system-level diagnosis , analysis of algorithms , sequential diagnosis , graph partitioning , fault-tolerance , degree of diagnosability , multiprocessor systems	A Graph Partitioning Approach to Sequential Diagnosis.	en	391
392	AbstractIn this paper, we analyze several issues involved in developing low latency adaptive wormhole routing schemes for two-dimensional meshes. It is observed that along with adaptivity, balanced distribution of traffic has a significant impact on the system performance. Motivated by this observation, we develop a new fully adaptive routing algorithm called positive-first-negative-first for two-dimensional meshes. The algorithm uses only two virtual channels per physical channel creating two virtual networks. The messages are routed positive-first in one virtual network and negative-first in the other. Because of this combination, the algorithm distributes the system load uniformly throughout the network and is also fully adaptive. It is shown that the proposed algorithm results in providing better performance in terms of the average network latency and throughput when compared with the previously proposed routing algorithms.	two-dimensional mesh , traffic distribution , region of adaptivity , adaptive wormhole routing , positive-first-negative-first algorithm	A Traffic-Balanced Adaptive Wormhole Routing Scheme for Two-Dimensional Meshes.	en	392
393	"AbstractWe consider the problem of communications over a wireless channel in support of data transmissions from the perspective of small portable devices that must rely on limited battery energy. We model the channel outages as statistically correlated errors. Classic ARQ strategies are found to lead to a considerable waste of energy, due to the large number of transmissions. The use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria. As an example, a simple probing scheme, which slows down the transmission rate when the channel is impaired, is shown to be more energy efficient, with a slight loss in throughput. A modified scheme that yields slightly better performance but requires some additional complexity is also studied. Some references on the modeling of battery cells are discussed to highlight the fact that battery charge capacity is strongly influenced by the available ""relaxation time"" between current pulses. A formal approach that can track complex models for power sources, including dynamic charge recovery, is also developed."	mobile communications , wireless systems , mobile computing , energy consumption , error control , channel probing , battery modeling	Control and Energy Consumption in Communications for Nomadic Computing.	en	393
394	AbstractNFS is a widely used remote file access protocol that has been tuned to perform well on traditional LANs which exhibit low error rates. Users migrating to mobile hosts would like continued remote file access via NFS. However, low bandwidth and high error rates degrade performance on mobile hosts using wireless links, hindering the use of NFS. We conducted experiments to study the behavior of NFS in a wireless testbed. Based on these experiments, we incorporated modifications into the mobile NFS client. This paper presents two mechanisms which improve NFS performance over wireless links: an aggressive NFS client and link-level retransmissions. Our experiments show that these mechanisms improve response time by up to 62%, which brings the performance to within 5% of that obtained in zero error conditions.	mobile computing , link-level retransmission , performance evaluation , wireless LAN , NFS , file systems	Improving NFS Performance Over Wireless Links.	en	394
395	AbstractRecently, DQDB (IEEE 802.6) MAN has been proposed as a component of Personal Communication Networks, in which base stations of wireless infrastructures are connected by a number of DQDBs which in turn are connected via bridges. We propose a protocol for call setup and path migration in a cluster of DQDBs. The protocol uses a link-state-like routing method for path selection and a source-routing-based scheme for path establishment. In addition, we propose a labeling scheme that makes it possible to carry the path information needed by the source routing protocol in a single 53-octet DQDB slot. Without such a labeling scheme, source routing would be inefficient for our purpose.	routing , DQDB , path migration , isochronous channels , call setup	An Efficient Protocol for Call Setup and Path Migration in IEEE 802.6 Based Personal Communication Networks.	en	395
396	"In this paper we extend our earlier work on supervisory control of nondeterministic systems using prioritized synchronization as the mechanism of control and trajectory model as the modeling formalism by considering design of supervisors under partial observation. We introduce the notion of observation-compatible systems and show that prioritized synchronous composition (PSC) of observation-compatible systems can be used as a mechanism of control of nondeterministic systems under partial observation in presence of driven events. Necessary and sufficient conditions that depend on the trajectory model as opposed to the language model of the plant are obtained for the existence of centralized as well as decentralized supervision. Our work on centralized control shows that the results of the traditional supervisory control can be ``extended"" to the above setting, provided that the supervisor is deterministic and the observation mask is projection type. On the other hand, our work on decentralized control is based on a new relation between controllability, observability, co-observability, and PSC that we derive in this paper."	observability , controllability , discrete event systems , driven events , trajectory models , co-observability , partial observation , prioritized synchronization , supervisory control , nondeterministic automata	Centralized and Decentralized Supervisory Control of Nondeterministic Systems Under Partial Observation.	en	396
397	An interior path-following algorithm is proposed for solving the nonlinear saddle point  problem $$ {\rm minimax}\ c^Tx+\ph(x)+b^Ty-\psi(y)-y^TAx $$ \vspace*{-18pt} $$ {\rm subject\ to\ }(x,y)\in \X\ti \Y\su R^n\ti R^m, $$ \noindent where $\ph(x)$ and $\ps(y)$ are smooth convex functions and $\X$ and $\Y$ are boxes (hyperrectangles). This problem is closely related to the models in stochastic programming and optimal control studied by Rockafellar and Wets (Math. Programming Studies, 28 (1986), pp. 63--93; SIAM J. Control Optim., 28 (1990), pp. 810--822). Existence and error-bound results on a central path are derived. Starting from an initial solution near the central path with duality gap $O(\mu)$, the algorithm finds an $\ep$-optimal solution of the problem in $O(\sqrt{m+n}\,|\log\mu/\ep|)$ iterations if both $\ph(x)$ and $\ps(y)$ satisfy a scaled Lipschitz condition.	optimal control , nonlinear complementarity problem , interior point methods , saddle point problem , stochastic programming	A Predictor-Corrector Algorithm for a Class of Nonlinear Saddle Point Problems.	en	397
398	We survey recent developments in high level synthesis technology for VLSI design. The need for higher-level design automation tools are discussed first. We then describe some basic techniques for various subtasks of high-level synthesis. Techniques that have been proposed in the past few years (since 1994) for various subtasks of high-level synthesis are surveyed. We also survey some new synthesis objectives including testability, power efficiency, and reliability.	VLSI design , high level synthesis , design methodology , design automation	Recent developments in high-level synthesis.	en	398
399	"This paper proposes a complete orthogonal decomposition (COD) algorithm for solving weighted least-squares problems. In applications, the weight matrix can be highly ill conditioned, and this can cause standard methods like QR factorization to return inaccurate answers in floating-point arithmetic. Stewart and Todd independently established a norm bound for the weighted least-squares problem that is independent of the weight matrix. Vavasis proposed a definition of a ""stable"" solution of weighted least squares based on this norm bound: The solution computed by a stable algorithm must satisfy an accuracy bound that is not affected by ill conditioning in the weight matrix. A forward error analysis shows that the COD algorithm is stable in this sense, but it is simpler and more efficient than the algorithm proposed by Vavasis. Our forward error bound is contrasted to the backward error analysis of other previous works on weighted least squares."	numerical stability , interior-point methods , weighted least squares , equilibrium systems , QR factorization , forward error analysis	Complete Orthogonal Decomposition for Weighted Least Squares.	en	399
400	We consider the construction of absorbing boundary layers using asymptotic expansions, the small parameter being the width of the layer. This allows us to define the order of the layer. We compute layers of various orders and analyze their stability properties. Finally, we perform some numerical tests to evaluate the practical utility of our layers.	wave equation , absorbing layers , absorbing boundary conditions	A New Theoretical Approach to Absorbing Layers.	en	400
401	We discuss the integration of autonomous Hamiltonian systems via dynamical rescaling of the vector field (reparameterization of time). Appropriate rescalings (e.g., based on normalization of the vector field or on minimum particle separation in an N-body problem) do not alter the time-reversal symmetry of the flow, and it is desirable to maintain this symmetry under discretization. For standard form mechanical systems without rescaling, this can be achieved by using the explicit leapfrog--Verlet method; we show that explicit time-reversible integration of the reparameterized equations is also possible if the parameterization depends on positions or velocities only.  For general rescalings, a scalar nonlinear equation must be solved at each step, but only one force evaluation is needed. The new method also conserves the angular momentum for an N-body problem. The use of reversible schemes, together with a step control based on normalization of the vector field (arclength reparameterization), is demonstrated in several numerical experiments, including a double pendulum, the Kepler problem, and a three-body problem.	n-body problems , variable stepsize methods , hamiltonian systems , leapfrog , verlet , time-reversible methods , symplectic methods	The Adaptive Verlet Method.	en	401
402	We describe the spectra and pseudospectra of continuous time and discrete time waveform relaxation operators.  Since the spectrum of the finite-interval waveform relaxation operator is only a singleton, the spectrum of the infinite-interval operator is typically used to describe the behavior of waveform relaxation algorithms.  Here, we show that the pseudospectrum is a more useful tool for analyzing convergence of waveform iterations on finite intervals and prove that the pseudospectrum of the infinite-interval operator is in fact the limit of the pseudospectra of the finite-interval operators.	dynamic iteration , pseudospectrum , waveform relaxation , spectrum	Spectra and Pseudospectra of Waveform Relaxation Operators.	en	402
403	In this paper, we describe a general approach to scaling data mining applications that we have come to call meta-learning. Meta-Learning refers to a general strategy that seeks to learn how to combine a number of separate learning processes in an intelligent fashion. We desire a meta-learning architecture that exhibits two key behaviors. First, the meta-learning strategy must produce an accurate final classification system. This means that a meta-learning architecture must produce a final outcome that is at least as accurate as a conventional learning algorithm applied to all available data. Second, it must be fast, relative to an individual sequential learning algorithm when applied to massive databases of examples, and operate in a reasonable amount of time. This paper focussed primarily on issues related to the accuracy and efficacy of meta-learning as a general strategy. A number of empirical results are presented demonstrating that meta-learning is technically feasible in wide-area, network computing environments.	data mining , scalability , meta-learning , machine learning , classifiers	On the Accuracy of Meta-learning for Scalable Data Mining.	en	403
404	This paper deals with nondegeneracy of polyhedra and linear programming (LP) problems. We allow for the possibility that the polyhedra and the feasible polyhedra of the LP problems under consideration be non-pointed. polyhedron is pointed if it has a vertex.) With respect to a given polyhedron, we consider two notions of nondegeneracy and then provide several equivalent characterizations for each of them. With respect to LP problems, we study the notion of constant cost nondegeneracy first introduced by Tsuchiya [25] under a different name, namely dual nondegeneracy. (We do not follow this terminology since the term dual nondegeneracy is already used to refer to a related but different type of nondegeneracy.) We show two main results about constant cost nondegeneracy of an LP problem. The first one shows that constant cost nondegeneracy of an LP problem is equivalent to the condition that the union of all minimal faces of the feasible polyhedron be equal to the set of feasible points satisfying a certain generalized strict complementarity condition. When the feasible polyhedron of an LP is nondegenerate, the second result shows that constant cost nondegeneracy is equivalent to the condition that the set of feasible points satisfying the generalized condition be equal to the set of feasible points satisfying the same complementarity condition strictly. For the purpose of giving a preview of the paper, the above results specialized to the context of polyhedra and LP problems in standard form are described in the introduction.	linear programming , nondegeneracy , polyhedron , constant cost face , complementary slackness	Nondegeneracy of Polyhedra and Linear Programs.	en	404
405	We present a new method to represent variable bindings in the Warren Abstract Machine (WAM), so that the ages of variable bindings can be easily found using this new representation in our intelligent backtracking schema. The age of a variable bound to a non-variable term is the youngest choice point such that backtracking to that choice point can make that variable an unbound variable again. The procedure backtracking point is the choice point of the procedure currently being executed or the choice point of its first ancestor having a choice point. Variable ages and procedure backtracking points are used in the process of figuring out backtracking points in our intelligent backtracking schema. Our intelligent backtracking schema performs much better than the results of other intelligent backtracking methods in the literature for deterministic programs, and its performance for non-deterministic programs are comparable with their results.	logic proagramming , prolog , abstract machine , intelligent backtracking	An intelligent backtracking schema in a logic programming environment.	en	405
406	AbstractWe present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.	clustering , word morphology , iterative scaling , kullback-leibler divergence , statistical learning , natural language processing , EM algorithm , random field , maximum entropy	Inducing Features of Random Fields.	en	406
407	"AbstractSimilarity measurements between 3D objects and 2D images are useful for the tasks of object recognition and classification. We distinguish between two types of similarity metrics: metrics computed in image-space (image metrics) and metrics computed in transformation-space (transformation metrics). Existing methods typically use image metrics; namely, metrics that measure the difference in the image between the observed image and the nearest view of the object. Example for such a measure is the Euclidean distance between feature points in the image and their corresponding points in the nearest view. (This measure can be computed by solving the exterior orientation calibration problem.) In this paper we introduce a different type of metrics: transformation metrics. These metrics penalize for the deformations applied to the object to produce the observed image.In particular, we define a transformation metric that optimally penalizes for ""affine deformations"" under weak-perspective. A closed-form solution, together with the nearest view according to this metric, are derived. The metric is shown to be equivalent to the Euclidean image metric, in the sense that they bound each other from both above and below. It therefore provides an easy-to-use closed-form approximation for the commonly-used least-squares distance between models and images. We demonstrate an image understanding application, where the true dimensions of a photographed battery charger are estimated by minimizing the transformation metric."	exterior orientation calibration , object recognition , affine deformations , 3D-to-2D metric	Distance Metric Between 3D Models and 2D Images for Recognition and Classification.	en	407
408	We describe efficient algorithms for finding even cycles in undirected graphs. Our main results are the following: (i) For every $k \geq 2$, there is an $O(V^2)$ time algorithm that decides whether an undirected graph $G=(V,E)$ contains a simple cycle of length $2k$, and finds one if it does. (ii) There is an $O(V^2)$ time algorithm that finds a shortest even cycle in an undirected graph $G=(V,E)$.	graph algorithms , cycles	Finding Even Cycles Even Faster.	en	408
409	We present a general theorem that can be used to identify the limiting distribution for a class of combinatorial schemata. For example, many parameters in random mappings can be covered in this way. In particular, we can derive the limiting distribution of those points with a given number of total predecessors.	combinatorial constructions , random mappings , limiting distributions	Images and Preimages in Random Mappings.	en	409
410	"AbstractWe define two measures on views: view likelihood and view stability. View likelihood measures the probability that a certain view of a given 3D object is observed; it may be used to identify typical, or ""characteristic,"" views. View stability measures how little the image changes as the viewpoint is slightly perturbed; it may be used to identify ""generic"" views. Both definitions are shown to be identical up to the prior probability of camera orientations, and determined by the 2D metric used to compare images. We analytically derive the stability and likelihood measures for two feature-based 2D metrics, where the most stable and most likely view is shown to be the flattest view of the 3D shape.Incorporating view likelihood or stability in 3D object recognition and 3D reconstruction increases the chance of robust performance. In particular, we propose to use these measures to enhance 3D object recognition and 3D reconstruction algorithms, by adding a second step where the most likely solution is selected among all feasible solutions. These applications are demonstrated using simulated and real images."	bayesian vision , object recognition , generic views , characteristic views , canonical views , view likelihood , 3D reconstruction , view stability	On View Likelihood and Stability.	en	410
411	"AbstractThis paper attacks the problem of generalized multisensor mixture estimation. A distribution mixture is said to be generalized when the exact nature of components is not known, but each of them belongs to a finite known set of families of distributions. Estimating such a mixture entails a supplementary difficulty: One must label, for each class and each sensor, the exact nature of the corresponding distribution. Such generalized mixtures have been studied assuming that the components lie in the Pearson system. Adaptations of classical algorithms, such as Expectation-Maximization, Stochastic Expectation-Maximization, or Iterative Conditional Estimation, can then be used to estimate such mixtures in the context of independent identically distributed data and hidden Markov random fields. We propose a more general procedure with applications to estimating generalized multisensor hidden Markov chains. Our proposed method is applied to the problem of unsupervised image segmentation. The method proposed allows one to: (i) identify the conditional distribution for each class and each sensor, (ii) estimate the unknown parameters in this distribution, (iii) estimate priors, and (iv) estimate the ""true"" class image."	mixture estimation , unsupervised segmentation , hidden Markov chain , multisensor data , bayesian segmentation , generalized mixture estimation	Estimation of Generalized Multisensor Hidden Markov Chains and Unsupervised Image Segmentation.	en	411
412	Computational methods based on the use of adaptively constructed nonuniform meshes reduce the amount of computation and storage necessary to perform many scientific calculations. The adaptive construction of such nonuniform meshes is an important part of these methods. In this paper, we present a parallel algorithm for adaptive mesh refinement that is suitable for implementation on distributed-memory parallel computers. Experimental results obtained on the Intel are presented to demonstrate that for scientific computations involving the finite element method, the algorithm exhibits scalable performance and has a small run time in comparison with other aspects of the scientific computations examined. It is also shown that the algorithm has a fast expected running time under the parallel random access machine (PRAM) computation model.	parallel algorithms , distributed memory computers , unstructured mesh computation , sparse matrices , adaptive mesh refinement	Parallel Algorithms for Adaptive Mesh Refinement.	en	412
413	A second-order accurate interface tracking method for the solution of incompressible Stokes flow problems with moving interfaces on a uniform Cartesian grid is presented. The interface may consist of an elastic boundary immersed in the fluid or an interface between two different fluids. The interface is represented by a cubic spline along which the singularly supported elastic or surface tension force can be computed.   The Stokes equations are then discretized using the second-order accurate finite difference methods for elliptic equations with singular sources developed in our previous paper [SIAM J. Numer. Anal., 31(1994), pp. 1019--1044]. The resulting velocities are interpolated to the interface to determine the motion of the interface.  An implicit quasi-Newton method is developed that allows reasonable time steps to be used.	bubbles , stokes flow , discontinuous coefficients , cartesian grids , creeping flow , interface tracking , immersed interface methods	Immersed Interface Methods for Stokes Flow with Elastic Boundaries or Surface Tension.	en	413
414	A parallel preconditioner is presented for the solution of general sparse linear systems of equations. A sparse approximate inverse is computed explicitly and then applied as a preconditioner to an iterative method. The computation of the preconditioner is inherently parallel, and its application only requires a matrix-vector product. The sparsity pattern of the approximate inverse is not imposed a priori but captured automatically. This keeps the amount of work and the number of nonzero entries in the preconditioner to a minimum. Rigorous bounds on the clustering of the eigenvalues and the singular values are derived for the preconditioned system, and the proximity of the approximate to the true inverse is estimated. An extensive set of test problems from scientific and industrial applications provides convincing evidence of the effectiveness of this approach.	parallel algorithms , approximate inverses , sparse linear systems , sparse matrices , iterative methods , preconditioning	Parallel Preconditioning with Sparse Approximate Inverses.	en	414
415	"A concept of generalized discrepancy, which involves pseudodifferential operators to give a criterion of equidistributed pointsets, is developed on the sphere. A simply structured formula in terms of elementary functions is established for the computation of the generalized discrepancy. With the help of this formula five kinds of point systems on the sphere, namely lattices in polar coordinates, transformed two-dimensional  sequences,  rotations on the sphere, triangulations, and ""sum of three squares sequence,"" are investigated. Quantitative tests are done, and the results are compared with one another. Our calculations exhibit different orders of convergence of the generalized discrepancy for different types of point systems."	low discrepancy quasi-Monte-Carlo method , pointsets , sphere , approximate integration , equidistribution , pseudodifferential operators , generalized discrepancy	Equidistribution on the Sphere.	en	415
416	"We consider the problem of ray shooting amidst spheres in 3-space: given n arbitrary (possibly intersecting) spheres in 3-space and any $\epsilon$ > 0, we show how to preprocess the spheres in time $O(n^{3+\epsilon})$ into a data structure of size $O(n^{3+\epsilon})$ so that any ray-shooting query can be answered in time $O(n^\epsilon)$. Our result improves previous techniques (see [P. K. Aggarwal, L. Guibas, M. Pellegrini, and M. Sharir, ""Ray shooting amidst spheres,"" unpublished note] and [P. K. Aggarwal and J. Matousek, Discrete Comput. Geom., 11 (1994), pp. 393-418]), where roughly $O(n^4)$ storage was required to support fast queries. Our result shows that ray shooting amidst spheres has complexity comparable with that of ray shooting amidst planes in 3-space.  Our technique applies to more general (convex) objects in 3-space, and we also discuss those extensions."	ray shooting , computational geometry	Ray Shooting Amidst Spheres in Three Dimensions and Related Problems.	en	416
417	"Let G be an undirected graph with V vertices and E edges. Many algorithms have been developed for enumerating all spanning trees in G. Most of the early algorithms use a technique called ""backtracking."" Recently, several algorithms using a different technique have been proposed by Kapoor and Ramesh (1992), Matsui (1993), and Shioura and Tamura (1993). They find a new spanning tree by exchanging one edge of a current one. This technique has the merit of enabling us to compress the whole output of all spanning trees by outputting only relative changes of edges. Kapoor and Ramesh first proposed an O(N E)-time algorithm by adopting such a ""compact"" output, where N is the number of spanning trees. Another algorithm with the same time complexity was constructed by Shioura and Tamura. These are optimal in the sense of time complexity but not in terms of space complexity because they take O(VE) space. We refine Shioura and Tamura's algorithm and decrease the space complexity from O(VE) to O(V E) while preserving the time complexity. Therefore, our algorithm is optimal in the sense of both time and space complexities."	spanning trees , undirected graphs , optimal algorithm	An Optimal Algorithm for Scanning All Spanning Trees of Undirected Graphs.	en	417
418	AbstractIn this paper, we consider a model of lossless image compression in which each band of a multispectral image is coded using a prediction function involving values from a previously coded band of the compression, and examine how the ordering of the bands affects the achievable compression.We present an efficient algorithm for computing the optimal band ordering for a multispectral image. This algorithm has time complexity O(n2) for an n-band image, while the naive algorithm takes time (n!). A slight variant of the optimal ordering problem that is motivated by some practical concerns is shown to be NP-hard, and hence, computationally infeasible, in all cases except for the most trivial possibility.In addition, we report on our experimental findings using the algorithms designed in this paper applied to real multispectral satellite data. The results show that the techniques described here hold great promise for application to real-world compression needs.	image compression , multispectral images , NP-completeness , lossless compression , compression , satellite data	Band Ordering in Lossless Compression of Multispectral Images.	en	418
419	AbstractIn this paper, we consider a load-balancing process allocation method for fault-tolerant multicomputer systems that balances the load before as well as after faults start to degrade the performance of the system. In order to be able to tolerate a single fault, each process (primary process) is duplicated (i.e., has a backup process). The backup process executes on a different processor from the primary, checkpointing the primary process and recovering the process if the primary process fails. In this paper, we formalize the problem of load-balancing process allocation and propose a new process allocation method and analyze the performance of the proposed method. Simulations are used to compare the proposed method with a process allocation method that does not take into account the different load characteristics of the primary and backup processes. While both methods perform well before the occurrence of a fault, only the proposed method maintains a balanced load after the occurrence of such a fault.	fault-tolerant multicomputer , checkpointing , backup process , load balancing , process allocation	Replicated Process Allocation for Load Distribution in Fault-Tolerant Multicomputers.	en	419
420	The 3-D motion of a camera within a static environment produces a sequence of time-varying images that can be used for reconstructing the relative motion between the scene and the viewer. The problem of reconstructing rigid motion from a sequence of perspective images may be characterized as the estimation of the state of a nonlinear dynamical system, which is defined by the rigidity constraint and the perspective measurement map. The time-derivative of the measured output of such a system, which is called the 2-D motion field and is approximated by the optical flow, is bilinear in the motion parameters, and may be used to specify a subspace constraint on the direction of heading independent of rotation and depth, and a pseudo-measurement for the rotational velocity as a function of the estimated heading. The subspace constraint may be viewed as an implicit dynamical model with parameters on a differentiable manifold, and the visual motion estimation problem may be cast in a system-theoretic framework as the identification of such an implicit model. We use techniques which pertain to nonlinear estimation and identification theory to recursively estimate 3-D rigid motion from a sequence of images independent of the structure of the scene. Such independence from scene-structure allows us to deal with a variable number of visible feature-points and occlusions in a principled way. The further decoupling of the direction of heading from the rotational velocity generates a filter with a state that belongs to a two-dimensional and highly constrained state-space. As a result, the filter exhibits robustness properties which are highlighted in a series of experiments on real and noisy synthetic image sequences. While the position of feature-points is not part of the state of the model, the innovation process of the filter describes how each feature is compatible with a rigid motion interpretation, which allows us to test for outliers and makes the filter robust with respect to errors in the feature tracking/optical flow, reflections, T-junctions. Once motion has been estimated, the 3-D structure of the scene follows easily. By releasing the constraint that the visible points lie in front of the viewer, one may explain some psychophysical effects on the nonrigid percept of rigidly moving objects.	implicit Extended Kalman Filter , dynamic vision , recursive rigid motion estimation , nonlinear identification	Recursive 3-D Visual Motion Estimation Using Subspace Constraints.	en	420
421	"AbstractACL2 is a reimplemented extended version of Boyer and Moore's Nqthm and Kaufmann's Pc-Nqthm, intended for large scale verification projects. This paper deals primarily with how we scaled up Nqthm's logic to an ""industrial strength"" programming languagenamely, a large applicative subset of Common Lispwhile preserving the use of total functions within the logic. This makes it possible to run formal models efficiently while keeping the logic simple. We enumerate many other important features of ACL2 and we briefly summarize two industrial applications: a model of the Motorola CAP digital signal processing chip and the proof of the correctness of the kernel of the floating point division algorithm on the AMD5K86 microprocessor by Advanced Micro Devices, Inc."	automatic theorem proving , formal verification , partial functions , computational logic , total functions , floating point division , type checking , digital signal processing , microcode verification	An Industrial Strength Theorem Prover for a Logic Based on Common Lisp.	en	421
422	This paper addresses the problems of detecting Hopf bifurcations in systems of ordinary differential equations and following curves of Hopf points in two-parameter families of vector fields.  The established approach to this problem relies upon augmenting the equilibrium condition so that a Hopf bifurcation occurs at an isolated, regular point of the extended system. We propose two new methods of this type based on classical algebraic results regarding the roots of polynomial equations and properties of Kronecker products for matrices.  In addition to their utility as augmented systems for use with standard Newton-type continuation methods, they are also particularly well adapted for solution by computer algebra techniques for vector fields of small or moderate dimension.	bialternate product , hopf bifurcation , resultant	Computing Hopf Bifurcations I.	en	422
423	The purpose of this paper is to develop a convergence theory for multigrid methods applied to nearly singular linear elliptic partial differential equations of the type produced from a positive definite system by a shift with the identity. One of the important aspects of this theory is that it allows such shifts to vary anywhere in the multigrid scheme, enabling its application to a wider class of eigenproblem solvers. The theory is first applied to a method for computing eigenvalues and eigenvectors that consists of multigrid iterations with zero right-hand side and updating the shift from the Rayleigh quotient before every cycle. It is then applied to the Rayleigh quotient multigrid (RQMG) method, which is a more direct multigrid procedure for solving eigenproblems. Local convergence of the multigrid V-cycle and global convergence for a full  multigrid version of both methods is obtained.	singular equations , rayleigh quotient , multigrid , eigenvalue problem	Multigrid Methods for Nearly Singular Linear Equations and Eigenvalue Problems.	en	423
424	We present algorithms for exactly learning unknown environments that can be described by deterministic finite automata. The learner performs a walk on the target automaton, where at each step it observes the output of the state it is at, and chooses a labeled edge to traverse to the next state. The learner has no means of a reset, and does not have access to a teacher that answers equivalence queries and gives the learner counterexamples to its hypotheses. We present two algorithms: The first is for the case in which the outputs observed by the learner are always correct, and the second is for the case in which the outputs might be corrupted by random noise. The running times of both algorithms are polynomial in the cover time of the underlying graph of the target automaton.	learning automata , learning with noise , exact learning	Exactly Learning Automata of Small Cover Time.	en	424
425	Pre-pruning and Post-pruning are two standard techniques for handling noise in decision tree learning. Pre-pruning deals with noise during learning, while post-pruning addresses this problem after an overfitting theory has been learned. We first review several adaptations of pre- and post-pruning techniques for separate-and-conquer rule learning algorithms and discuss some fundamental problems. The primary goal of this paper is to show how to solve these problems with two new algorithms that combine and integrate pre- and post-pruning.	Inductive Rule Learning , Inductive Logic Programming , noise handling , pruning	Pruning Algorithms for Rule Learning.	en	425
426	AbstractThis paper describes algorithms for scheduling preemptive, imprecise, composite tasks in real-time. Each composite task consists of a chain of component tasks, and each component task is made up of a mandatory part and an optional part. Whenever a component task uses imprecise input, the processing times of its mandatory and optional parts may become larger. The composite tasks are scheduled by a two-level scheduler. At the high level, the composite tasks are scheduled preemptively on one processor, according to an existing algorithm for scheduling simple imprecise tasks. The low-level scheduler then distributes the time budgeted for each composite task across its component tasks so as to minimize the output error of the composite task	real-time systems and applications , imprecise computation , end-to-end timing constraints , scheduling , error	Algorithms for Scheduling Real-Time Tasks with Input Error and End-to-End Deadlines.	en	426
427	AbstractAtomic actions are an important dynamic structuring technique that aid the construction of fault-tolerant concurrent systems. Although they were developed some years ago, none of the well-known commercially-available programming languages directly support their use. This paper summarizes software fault tolerance techniques for concurrent systems, evaluates the Ada 95 programming language from the perspective of its support for software fault tolerance, and shows how Ada 95 can be used to implement software fault tolerance techniques. In particular, it shows how packages, protected objects, requeue, exceptions, asynchronous transfer of control, tagged types, and controlled types can be used as building blocks from which to construct atomic actions with forward and backward error recovery, which are resilient to deserter tasks and task abortion.	exception handling , atomic actions , software fault tolerance , conversations , ada 95 , recovery blocks	Implementing Atomic Actions in Ada 95.	en	427
428	Combining different machine learning algorithms in the same system can produce benefits above and beyond what either method could achieve alone.  This paper demonstrates that genetic algorithms can be used in conjunction with lazy learning to solve examples of a difficult class of delayed reinforcement learning problems better than either method alone. This class, the class of differential games, includes numerous important control problems that arise in robotics, planning, game playing, and other areas, and solutions for differential games suggest solution strategies for the general class of planning and control problems. We conducted a series of experiments applying three learning approaches  lazy Q-learning, k-nearest neighbor (k-NN), and a genetic algorithm  to a particular differential game called a pursuit game.  Our experiments demonstrate that k-NN had great difficulty solving the problem, while a lazy version of Q-learning performed moderately well and the genetic algorithm performed even better.  These results motivated the next step in the experiments, where we hypothesized k-NN was having difficulty because it did not have good examples  a common source of difficulty for lazy learning.  Therefore, we used the genetic algorithm as a bootstrapping method for k-NN to create a system to provide these examples.  Our experiments demonstrate that the resulting joint system learned to solve the pursuit games with a high degree of accuracy  outperforming either method alone  and with relatively small memory requirements.	reinforcement learning , differential games , lazy learning , pursuit games , nearest neighbor , genetic algorithms , teaching	A Teaching Strategy for Memory-Based Control.	en	428
429	Generalizing the two-commodity flow theorem of Rothschild and Whinston [Oper. Res., 14 (1966), pp. 377--387] and the multiflow theorem of Lovsz [Acta Mat. Akad. Sci. Hungaricae, 28 (1976), pp. 129--138] and Cherkasky [Ekonom.-Mat. Metody, 13 (1977), pp. 143--151], Karzanov and Lomonosov [Mathematical Programming,  O. I. Larichev, ed., Institute for System Studies, 1978, pp. 59--66] in 1978 proved a min-max theorem on maximum multiflows.  Their original proof is quite long and technical and relies on earlier investigations into  metrics.  The main purpose of the present paper is to provide a relatively simple proof of this theorem.  Our proof relies on the locking theorem, which is another result of Karzanov and Lomonosov, and the polymatroid intersection theorem of Edmonds [Combinatorial Structures and Their Applications, R. Guy, H. Hanani, N. Sauer, and J. Schnheim, eds., Gordon and Breach, 1970, pp. 69--87]. For completeness, we also provide a simplified proof of the locking theorem.  Finally, we introduce the notion of a node demand problem and, as another application of the locking theorem, we derive a feasibility theorem concerning it.The presented approach gives rise to (combinatorial) polynomial-time algorithms.	multiflow , locking , network flows , node demands , polymatroid	On Integer Multiflow Maximization.	en	429
430	An explicit finite element method is used to solve the linear  convection--diffusion-reaction equations governing contaminant transport in ground water flowing through an adsorbing porous medium. The use of discontinuous finite elements for the convective part of the equations combined with mixed finite elements for the diffusive part renders the method for the concentration solution, which displays strong gradients, trivially conservative and fully parallelizable. We carry out a stability and convergence analysis. In particular, the method is proven to satisfy a maximum principle, to be total variation bounded, and to converge to the unique weak solution of the equations. Special attention is paid to the convective part of the equations. Numerical simulations are presented and discussed.	conservation law , finite element and volume method , convergence , mixed method , convection-diffusion-reaction equation , stability	Stability and Convergence of a Finite Element Method for Reactive Transport in Ground Water.	en	430
431	The role of the interval subdivision-selection rule is investigated in branch-and-bound algorithms for global optimization. The class of rules that allows convergence for the model algorithm is characterized, and it is shown that the four rules investigated satisfy the conditions of convergence. A numerical study with a wide spectrum of test problems indicates that there are substantial differences between the rules in terms of the required CPU time, the number of function and derivative evaluations, and space complexity, and two rules can provide substantial improvements in efficiency.	interval arithmetic , interval subdivision , global optimization	Subdivision Direction Selection in Interval Methods for Global Optimization.	en	431
432	"In this paper, we consider the so-called ""inexact Uzawa"" algorithm for iteratively solving linear block saddle point problems.  Such saddle point problems arise, for example, in finite element and finite difference discretizations of Stokes equations, the equations of elasticity, and mixed finite element discretization of second-order problems.  We consider both the linear and nonlinear variants of the inexact Uzawa iteration. We show that the linear method always converges as long as the preconditioners defining the algorithm are properly scaled.  Bounds for the rate of convergence are provided in terms of the rate of convergence for the preconditioned Uzawa algorithm and the reduction factor corresponding to the preconditioner for the upper left-hand block.  In the case of nonlinear iteration, the inexact Uzawa algorithm is shown to converge provided that the nonlinear process approximating the inverse of the upper left-hand block is of sufficient accuracy.  Bounds for the nonlinear iteration are given in terms of this accuracy parameter and the rate of convergence of the preconditioned linear Uzawa algorithm.  Applications to the Stokes equations and mixed finite element discretization of second-order elliptic problems are discussed and, finally, the results of numerical experiments involving the algorithms are presented."	uzawa algorithm , indefinite systems , stokes equations , saddle point problems , iterative methods , preconditioners	Analysis of the Inexact Uzawa Algorithm for Saddle Point Problems.	en	432
433	Transversal homoclinic orbits of maps are known to generate shift dynamics on a set with Cantor-like structure. In this paper a numerical method is developed for computation of the corresponding homoclinic orbits. They are approximated by finite-orbit segments subject to asymptotic boundary conditions. We provide a detailed error analysis including a shadowing-type result by which one can infer the existence of a transversal homoclinic orbit from a finite segment. This approach is applied to several examples. In some of them parameters appear and closed loops of homoclinic orbits are found by a path-following algorithm.	numerical methods , shadowing , homoclinic points for maps , dynamical systems	The Numerical Computation of Homoclinic Orbits for Maps.	en	433
434	An identity-based non-interactive public key distribution system is presented that is based on a novel trapdoor one-way function allowing a trusted authority to compute the discrete logarithms modulo a publicly known composite number m while this is infeasible for an adversary not knowing the factorization of m. Without interaction with a key distribution center or with the recipient of a given message, a user can generate a mutual secure cipher key based solely on the recipient's identity and his own secret key, and subsequently send the message, encrypted with the generated cipher used in a conventional cipher, over an insecure channel to the recipient. In contrast to previously proposed identity-based systems, no public keys, certificates for public keys or other information need to be exchanged and thus the system is suitable for certain applications that do not allow for interaction. The paper solves an open problem proposed by Shamir in 1984.	key management , public-key cryptography , trapdoor functions , discrete logarithms	A Non-interactive Public-Key Distribution System.	en	434
435	With the profusion of text databases on the Internet, it is becoming increasingly hard to find the most useful databases for a given query. To attack this problem, several existing and proposed systems employ brokers to direct user queries, using a local database of summary information about the available databases. This summary information must effectively distinguish relevant databases and must be compact while allowing efficient access. We offer evidence that one broker, GlOSS, can be effective at locating databases of interest even in a system of hundreds of databased and can examine the performance of accessing the GlOSS summeries for two promising storage methods: the grid file and partitioned hashing. We show that both methods can be tuned to provide good performance for a  particular workload (within a broad range of workloads), and we discuss the tradeoffs between the two data structures. As a side effect of our work, we show that grid files are more broadly applicable than previously thought; inparticular, we show that by varying the policies used to construct the grid file we can provide good performance for a wide range of workloads even when storing highly skewed data.	broker architecture , distributed information , GlOSS , partitioned hashing , grid files , broker performance	Data structures for efficient broker implementation.	en	435
436	We introduce Kleene algebra with tests, an equational system for manipulating programs. We give a purely equational proof, using Kleene algebra with tests and commutativity conditions, of the following classical result: every while program can be simulated by a while program can be simulated by a while program with at most one while loop. The proof illustrates the use of Kleene algebra with tests and commutativity conditions in program equivalence proofs.	kleene algebra , dynamic logic , specification	Kleene algebra with tests.	en	436
437	We establish a general connection between fixpoint logic and complexity. On one side, we have fixpoint logic, parameterized by the choices of 1st-order operators (inflationary or noninflationary) and iteration constructs (deterministic, nondeterministic, or alternating). On the other side, we have the complexity classes between P and EXPTIME. Our parameterized fixpoint logics capture the complexity classes P, NP, PSPACE, and EXPTIME, but equally is achieved only over ordered structures. There is, however, an inherent mismatch between complexity and logicwhile computational devices work on encodings of problems, logic is applied directly to the underlying mathematical structures. To overcome this mismatch, we use a theory of relational complexity, which bridges the gap between standard complexity and fixpoint logic. On one hand, we show that questions about containments among standard complexity classes can be translated to questions about containments among relational complexity classes. On the other hand, the expressive power of fixpoint logic can be precisely characterized in terms of relational complexity classes. This tight, three-way relationship among fixpoint logics, relational complexity and standard complexity yields in a uniform way logical analogs to all containments among the complexity classes P, NP, PSPACE, and EXPTIME. The logical formulation shows that some of the most tantalizing questions in complexity theory boil down to a single question: the relative power of inflationary vs. noninflationary 1st-order operators.	fixpoint logic , complexity classes , computational complexity , relational complexity	Fixpoint logics, relational machines, and computational complexity.	en	437
438	AbstractQuery processing is a crucial component of various application domains including information retrieval, database design and management, pattern recognition, robotics, and VLSI. Many of these applications involve data stored in a matrix satisfying a number of properties. One property that occurs time and again specifies that the rows and the columns of the matrix are independently sorted. It is customary to refer to such a matrix as sorted. An instance of the Batched Searching and Ranking problem, (BSR, for short) involves a sorted matrix A of items from a totally ordered universe, along with a collection Q of queries. Q is an arbitrary mix of the following query types: For a search query qj, one is interested in an item of A that is closest to qj; for a rank query qj one is interested in the number of items of A that are strictly smaller than qj. The BSR problem asks for solving all queries in Q. In this work, we consider the BSR problem in the following context: The matrix A is pretiled, one item per processor, onto an enhanced mesh of size $\sqrt n\times \sqrt n$; the m queries are stored, one per processor, in the first ${{m \over {\sqrt n}}}$ columns of the platform. Our main contribution is twofold. First, we show that any algorithm that solves the BSR problem must take at least $\Omega ({\rm max\{log}n,\sqrt m\})$ time in the worst case. Second, we show that this time lower bound is tight on meshes of size $\sqrt n\times \sqrt n$ enhanced with multiple broadcasting, by exhibiting an algorithm solving the BSR problem in $\Theta ({\rm max\{log}\!\!n,\sqrt m\})$ time on such a platform.	parallel algorithms , time-optimal algorithms , searching , VLSI , ranking , enhanced meshes , pattern recognition , robotics , database design	Time-Optimal Domain-Specific Querying on Enhanced Meshes.	en	438
439	AbstractUsing runtime information of load distributions and processor affinity, we propose an adaptive scheduling algorithm and its variations from different control mechanisms. The proposed algorithm applies different degrees of aggressiveness to adjust loop scheduling granularities, aiming at improving the execution performance of parallel loops by making scheduling decisions that match the real workload distributions at runtime. We experimentally compared the performance of our algorithm and its variations with several existing scheduling algorithms on two parallel machines: the KSR-1 and the Convex Exemplar. The kernel application programs we used for performance evaluation were carefully selected for different classes of parallel loops. Our results show that using runtime information to adaptively adjust scheduling granularity is an effective way to handle loops with a wide range of load distributions when no prior knowledge of the execution can be used. The overhead caused by collecting runtime information is insignificant in comparison with the performance improvement. Our experiments show that the adaptive algorithm and its five variations outperformed the existing scheduling algorithms.	shared-memory systems , parallel loops , adaptive scheduling algorithms , dynamic information , load balancing , processor affinity	Adaptively Scheduling Parallel Loops in Distributed Shared-Memory Systems.	en	439
440	AbstractA Multistage Bus Network (MBN) is proposed in this paper to overcome some of the shortcomings of the conventional multistage interconnection networks (MINs), single bus, and hierarchical bus interconnection networks. The MBN consists of multiple stages of buses connected in a manner similar to the MINs and has the same bandwidth at each stage. A switch in an MBN is similar to that in a MIN switch except that there is a single bus connection instead of a crossbar. MBNs support bidirectional routing and there exists a number of paths between any source and destination pair. In this paper, we develop self routing techniques for the various paths, present an algorithm to route a request along the path with minimum distance, and analyze the probabilities of a packet taking different routes. Further, we derive a performance analysis of a synchronous packet-switched MBN in a distributed shared memory environment and compare the results with those of an equivalent bidirectional MIN (BMIN). Finally, we present the execution time of various applications on the MBN and the BMIN through an execution-driven simulation. We show that the MBN provides similar performance to a BMIN while offering simplicity in hardware and more fault-tolerance than a conventional MIN.	routing , performance analysis , interconnection network , packet-switching , execution-driven simulation , queuing model	Performance of Multistage Bus Networks for a Distributed Shared Memory Multiprocessor.	en	440
441	In this paper we present a new probabilistic clock synchronization algorithm, its prototype implementation and experimental results. The algorithm follows the client-server programming paradigm and is designed to work in a departmental environment with few servers and a number of clients connected through an arbitrary network topology. At the core of the algorithm is a remote clock reading method that mitigates the negative effects of message delay uncertainty. The implementation proves the effectiveness of this approach and corroborates the theoretical speculations.	TCP/IP , clock synchronization , distributed systems , distributed algorithms , probabilistic algorithms , local area networks	Implementing a Probabilistic Clock Synchronization Algorithm.	en	441
442	A new method is investigated to reduce the roundoff error in computing derivatives using Chebyshev collocation methods. By using a grid mapping derived by Kosloff and Tal-Ezer, and the proper choice of the parameter $\alpha$, the roundoff error of the $k$th derivative can be reduced from $O(N^{2k})$ to $O((N \lge)^k)$, where $\epsilon$ is the machine precision and $N$ is the number of collocation points.  This drastic reduction of roundoff error makes mapped Chebyshev methods competitive with any other algorithm in computing second or higher derivatives with large $N$. Several other aspects of the mapped Chebyshev differentiation matrix are also studied, revealing that    the mapped Chebyshev methods require much less than $\pi$ points to resolve a wave; the eigenvalues are less sensitive to perturbation by roundoff error; and  larger time steps can be used for solving PDEs.   All these advantages of the mapped Chebyshev methods can be achieved while maintaining spectral accuracy.	differentiation matrix , tal-ezer mapping , chebyshev collocation , roundoff error	Accuracy Enhancement for Higher Derivatives using Chebyshev Collocation and a Mapping Technique.	en	442
443	Analyses based on Symmetric Daubechies Wavelets (SDW) lead to complex-valued multiresolution representations of real signals. After a recall of the construction of the SDW, we present some specific properties of these new types of Daubechies wavelets. We then discuss two applications in image processing: enhancement and restoration. In both cases, the efficiency of this multiscale representation relies on the information encoded in the phase of the complex wavelet coefficients.	daubechies wavelets , complex signals , restoration , image processing	Image Processing with Complex Daubechies Wavelets.	en	443
444	In this paper, we develop a framework for computing upper and lower bounds of an exponential form for a large class of single resource systems with Markov additive inputs. Specifically, the bounds are on quantities such as backlog, queue length, and response time. Explicit or computable expressions for our bounds are given in the context of queuing theory and numerical comparisons with other bounds and exact results are presented. The paper concludes with two applications to admission control in multimedia systems.	markov additive process , call admission control , queues , markov chain , exponential bound , ergodicity , large deviation principle , matrix analysis , effective bandwidth , tail distribution	Exponential bounds with applications to call admission.	en	444
445	In this paper we study the problem of on-line allocation of routes to virtual circuits (both point-to-point and multicast) where the goal is to route all requests while minimizing the required bandwidth.  We concentrate on the case of Permanent virtual circuits (i.e., once a circuit is established it exists forever), and describe an algorithm that achieves on O (log n) competitive ratio with respect to maximum congestin, where nis the number of nodes in the network.  Informally, our results show that instead of knowing all of the future requests, it is sufficient to increase the bandwidth of the communication links by an O (log n) factor.  We also show that this result is tight, that is, for any on-line algorithm there exists a scenario in which ***(log n) increase in  bandwidth is necessary in directed networks. We view virtual circuit routing as a generalization of an on-line load balancing problem, defined as follows: jobs arrive on line and each job must be assigned to one of the machines immediately upon arrival.  Assigning a job to a machine increases the machine's load by an amount that depends both on the job and on the machine.  The goal is to minimize the maximum load. For the related machines case, we describe the first algorithm that achieves constant competitive ratio. for the unrelated case (with nmachines), we describe a new method that yields O(logn)-competitive algorithm.  This stands in contrast to the natural greed approach, whose competitive ratio is exactly n. show that this result is tight, that is, for any on-line algorithm there exists a scenario in which ***(log n) increase in bandwidth is necessary in directed networks.	routing , high-speed networks , optimization , on-line algorithms	On-line routing of virtual circuits with applications to load balancing and machine scheduling.	en	445
446	Strictness analysis is an important technique for optimization of lazy functional languages.  It is well known that all strictness analysis methods are incomplete, i.e., fail to report some strictness properties.  In this paper, we provide a precise and formal characterization of the loss of information that leads to this incompletenss.  Specifically, we establish the following characterization theorem for Mycroft's strictness analysis method and a generalization of this method, called ee-analysis, that reasons about exhaustive evaluation in nonflat domains: Mycroft's method will  deduce a strictness property for program P iff the property is independent of any constant appearing in any evaluation of P. To prove this, we specify a small set of equations, called E-axioms, that capture the information loss in Mycroft's method and develop a new proof technique called E-rewriting. E-rewriting extends the standard notion of rewriting to permit the use of reductions using E-axioms interspersed with standard reduction steps.  E-axioms are a syntactic characterization of information loss and E-rewriting provides and algorithm-independent proof technique for characterizing the power of analysis methods.  It can be used to answer questions on completeness and incompleteness of Mycroft's method on certain natural classes of programs. Finally, the techniques developed in this paper provide a general principle for establishing similar results for other analysis methods such as those based on abstract interpretation.  As a demonstration of the generality of our technique, we give a characterization theorem for another variation of Mycroft's method called dd-analysis.	program analysis , strictness analysis , completeness , abstract interpretation	On the power and limitations of strictness analysis.	en	446
447	LAPACK and LINPACK both solve symmetric indefinite linear systems using the diagonal pivoting method with the partial pivoting strategy of Bunch and Kaufman [Math. Comp., 31 (1977), pp. 163--179]. No proof of the stability of this method has appeared in the literature. It is tempting to argue that the diagonal pivoting method is stable for a given pivoting strategy if the growth factor is small. We show that this argument is false in general and give a sufficient condition for stability. This condition is not satisfied by the partial pivoting strategy because the multipliers are unbounded. Nevertheless, using a more specific approach we are able to prove the stability of partial pivoting, thereby filling a gap in the body of theory supporting LAPACK and LINPACK.	numerical stability , rounding error analysis , diagonal pivoting method , LAPACK , partial pivoting , growth factor , LINPACK , symmetric indefinite matrix , LDLT factorization	Stability of the Diagonal Pivoting Method with Partial Pivoting.	en	447
448	This paper provides an error analysis of the generalized Schur algorithm of Kailath and Chun [SIAM J. Matrix Anal. Appl., 15 (1994), pp. 114--128]---a class of algorithms which can be used to factorize Toeplitz-like matrices, including block-Toeplitz matrices, and matrices of the form $T^{T}T$, where $T$ is Toeplitz. The conclusion drawn is that if this algorithm is implemented with hyperbolic transformations in the factored form which is well known to provide numerical stability in the context of Cholesky downdating, then the generalized Schur algorithm will be stable.  If a more direct implementation of the hyperbolic transformations is used, then it will be unstable.  In this respect, the algorithm is analogous to Cholesky downdating; the details of implementation of the hyperbolic transformations are essential for stability.  An example which illustrates this instability is given.  This result is in contrast to the ordinary Schur algorithm for which an analysis by Bojanczyk, Brent, De Hoog, and Sweet [SIAM J. Matrix Anal. Appl., 16 (1995), pp. 40--57] shows that the sta- bility of the algorithm is not dependent on the implementation of the hyperbolic transformations.	toeplitz matrices , structured matrices , stability , schur algorithm	Stability Issues in the Factorization of Structured Matrices.	en	448
449	Sparse matrix factorization algorithms for general problems are typically characterized by irregular memory access patterns that limit their performance on parallel-vector supercomputers. For symmetric problems, methods such as the multifrontal method avoid indirect addressing in the innermost loops by using dense matrix kernels. However, no efficient LU factorization algorithm based primarily on dense matrix kernels exists for matrices whose pattern is very unsymmetric. We address this deficiency and present a new unsymmetric-pattern multifrontal method based on dense matrix kernels. As in the classical multifrontal method, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix, thus enabling the use of Level 2 and Level 3 BLAS. The performance is compared with the classical multifrontal method and other unsymmetric solvers on a CRAY C-98.	LU factorization , unsymmetric sparse matrices , multifrontal methods	An Unsymmetric-Pattern Multifrontal Method for Sparse LU Factorization.	en	449
450	Some implementations of interior-point algorithms obtain their search directions by solving symmetric indefinite systems of linear equations. The conditioning of the coefficient matrices in these so-called augmented systems deteriorates on later iterations, as some of the diagonal elements grow without bound. Despite this apparent difficulty, the steps produced by standard factorization procedures are often accurate enough to allow the interior-point method to converge to high accuracy. When the underlying linear program is nondegenerate, we show that convergence to arbitrarily high accuracy occurs, at a rate that closely approximates the theory. We also explain and demonstrate what happens when the linear program is degenerate, where convergence to acceptable accuracy (but not arbitrarily high accuracy) is usually obtained.	symmetric indefinite matrices , interior-point methods	Stability of Augmented System Factorizations in Interior-Point Methods.	en	450
451	We extend type specialisation to a computational lambda calculus with first-class references. The resulting specialiser has been used to specialise a self-interpreter for this typed computational lambda calculus optimally. Furthermore, this specialiser can perform operations on references at specialisation time, when possible.	type systems , specialisation , monads , program transformation	Type specialisation for imperative languages.	en	451
452	We argue that runtime program transformation, partial evaluation, and dynamic compilation are essential tools for automated generation of flexible, highly interactive graphical interfaces. In particular, these techniques help bridge the gap between a high-level, functional description and an efficient implementation. To support our claim, we describe our application of these techniques to a functional implementation of n-Vision, a real-time visualization system that represents multivariate relations as nested 3D interactors, and to Auto Visual, a rule-based system that designs n-Vision visualizations from high-level task specifications. n-Vision visualizations are specified using a simple functional language. These programs are transformed into a cached dataflow graph. A partial evaluator is used on particular computation-intensive function applications, and the results are compiled to native code. The functional representation simplifies generation of correct code, and the program transformations ensure good performance. We demonstrate why these transformations improve performance and why they cannot be done at compile time.	dataflow , multivariate data visualization , partial evaluation , program transformation , virtual worlds	Generating efficient virtual worlds for visualization using partial evaluation and dynamic compilation.	en	452
453	In this paper we investigate how partial evaluation and program transformations can be used on a real problem, namely that of speeding up airline crew scheduling.Scheduling of crews is subject to many rules and restrictions. These restrictions are expressed in a rule language. However, in a given planning situation much is known to be fixed, so the rule set can be partially evaluated with respect to this known input.The approach is somewhat novel in that it uses truly static input data as well as static input data where the values are known only to belong to a set of values.The results of the partial evaluation is quite satisfactory: both compilation and running times have decreased by using it. The partial evaluator is now part of the crew scheduling system that Carmen Systems AB markets and which is in use at most of the major European airlines and in daily production.	airline crew scheduling , partial evaluation , generalized constant propagation , program transformation	Partial evaluation in aircraft crew planning.	en	453
454	Let $\lambda({\cal N})$ denote the weight of a minimum cut in an edge-weighted undirected network ${\cal N}$, and  $n$ and $m$ denote the numbers of vertices and edges, respectively. It is known that $O(n^{2k})$ is an upper bound on the number of cuts with weights less than $k\lambda({\cal N})$, where $k\geq 1$ is a given constant. This paper first shows that  all cuts of weights less than $k\lambda({\cal N})$ can be enumerated in $O(m^2n+n^{2k}m)$ time without using the maximum flow algorithm. The paper then proves for $k<\four$ that $n\choose 2$ is a tight upper bound on the number of cuts of weights less than $k\lambda({\cal N})$, and that all those cuts can be enumerated in $O(m^2n+mn^2\log n)$ time.	polynomial algorithm , graphs , minimum cuts , edge-splitting	Computing All Small Cuts in an Undirected Network.	en	454
455	AbstractInability to identify weaknesses or to quantify advancements in software system robustness frequently hinders the development of robust software systems. Efforts have been made to develop benchmarks of software robustness to address this problem, but they all suffer from significant shortcomings. This paper presents the various features that are desirable in a benchmark of system robustness, and evaluates some existing benchmarks according to these features. A new hierarchically structured approach to building robustness benchmarks, which overcomes many deficiencies of past efforts, is also presented. This approach has been applied to building a hierarchically structured benchmark that tests part of the Unix file and virtual memory systems. The resultant benchmark has successfully been used to identify new response class structures that were not detected in a similar situation by other less organized techniques.	software validation , robustness benchmarking , extensible benchmarks , system reliability , test suite organization , software dependability , object-oriented benchmarks	Measuring Software Dependability by Robustness Benchmarking.	en	455
456	AbstractWe report on a formal requirements analysis experiment involving an avionics control system. We describe a method for specifying and verifying real-time systems with PVS. The experiment involves the formalization of the functional and safety requirements of the avionics system as well as its multilevel verification. First level verification demonstrates the consistency of the specifications whilst the second level shows that certain system safety properties are satisfied by the specification. We critically analyze methodological issues of large scale verification and propose some practical ways of structuring verification activities for optimizing the benefits.	requirements analysis , formal verification , safety critical systems , avionics systems , formal specification	Formal Requirements Analysis of an Avionics Control System.	en	456
457	AbstractDynamic verification is a new approach to formal verification, applicable to generic algorithms such as those found in the Standard Template Library (STL, part of the Draft ANSI/ISO C++ Standard Library). Using behavioral abstraction and symbolic execution techniques, verifications are carried out at an abstract level such that the results can be used in a variety of instances of the generic algorithms without repeating the proofs. This is achieved by substituting for type parameters of generic algorithms special data types that model generic concepts by accepting symbolic inputs and deducing outputs using inference methods. By itself, this symbolic execution technique supports testing of programs with symbolic values at an abstract level. For formal verification we also need to generate multiple program execution paths and use assertions (to handle while loops, for example), but we show how this can be achieved via directives to a conventional debugger program and an analysis database. The assertions must still be supplied, but they can be packaged separately and evaluated as needed by appropriate transfers of control orchestrated via the debugger. Unlike all previous verification methods, the dynamic verification method thus works without having to transform source code or process it with special interpreters. We include an example of the formal verification of an STL generic algorithm.	generic algorithms , templates , Standard Template Library , software libraries , specification , verification	Dynamic Verification of C++ Generic Algorithms.	en	457
458	A hardware self-managing heap memory (RCM) for languages like Lisp, Smalltalk, and Java has been designed, built, tested and benchmarked. On every pointer write from the processor, reference-counting transactions are performed in real time within this memory, and garbage cells are reused without processor cycles. A processor allocates new nodes simply by reading from a distinguished location in its address space. The memory hardware also incorporates support for off-line, multiprocessing, mark-sweep garbage collection.Performance statistics are presented from a partial implementation of Scheme over five different memory models and two garbage collection strategies, from main memory (no access to RCM) to a fully operational RCM installed on an external bus. The performance of the RCM memory is more than competitive with main memory.	uniprocessor , garbage collection , performance	Research Demonstration of a Hardware Reference-Counting Heap.	en	458
459	AbstractWe present a new class of interconnection topologies called the Linear Recursive Networks (LRNs) and examine their possible applications in distributed systems. Each LRN is characterized by a recursive pattern of interconnection which can be specified by simple parameters. Basic properties such as node degree, diameter, and the performance of routing algorithms for all LRNs are then collectively analyzed in terms of these parameters. By choosing appropriate values for the parameters, our results can assist a network designer in selecting a topology with required routing performance and cost of interconnection. A subclass of LRNs, called Congruent LRNs (CLRNs), is also identified here and shown to possess desirable properties for more tightly coupled systems. It is shown that the CLRNs include existing networks such as hypercube and generalized Fibonacci cubes. These results suggest that the linear recursive networks potentially have applications in interconnecting distributed systems.	hypercube , self-similar networks , fibonacci cube , routing algorithms , recursive networks , design and analysis of interconnection topologies	Linear Recursive Networks and Their Applications in Distributed Systems.	en	459
