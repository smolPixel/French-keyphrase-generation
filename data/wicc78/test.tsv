	title	sentences	label	language	index
0	Requerimientos para aplicaciones web	﻿ El objetivo del presente proyecto es la elaboración  de un conjunto de políticas, estándares, procesos,  metodologías, técnicas, herramientas y métricas  para aplicar en el desarrollo de aplicaciones Web,  específicamente en la obtención y representación de  los requerimientos que debe satisfacer el sistema o  aplicación Web. Estos distintos elementos se  desarrollarán en forma gradual y eventualmente se  integrarán en un sitio Web que centralice toda la  información.   Hay varias líneas de investigación previstas en este  proyecto. Las enumeramos someramente a continuación.   • Se realizará un estudio comparativo de los  diferentes tipos de requerimientos que se  identifican en las metodologías para el desarrollo de aplicaciones Web.  • Se tratará de precisar el concepto de stakeholder en general y en el contexto de las aplicaciones Web en particular. Para ello se  establecerá un proceso de identificación de los  stakeholders de dichas aplicaciones. Hay un  reconocimiento generalizado de que tener en  cuenta a los stakeholders es crítico para un  proyecto de software [17]. En el caso de las  aplicaciones Web, muchos de estos stakeholders no son accesibles y a veces ni siquiera  están identificados.   • A partir de la revisión del estado del arte en  cuanto a procesos de elicitación de requerímientos no funcionales para aplicaciones Web,  se propondrá un proceso basado en la extensión  de los procesos actualmente utilizados en la  Ingeniería de Requerimientos.   • En un desarrollo de aplicaciones Web es necesario tener en cuenta y analizar una cantidad  considerable de documentación. Para ello se  investigará un proceso para la generación automática del Léxico Extendido del Lenguaje  (LEL) [11], [15].   Contexto  Internet ha penetrado en múltiples aspectos de la  vida cotidiana de las personas y las organizaciones  económicas, sociales, culturales, políticas y otras.  Se la utiliza tanto para entretenimiento (como los  juegos online) como para aplicaciones empresariales  (como los transacciones financieras).   La Ingeniería de Aplicaciones Web o Ingeniería  Web tiene como objetivo la aplicación de enfoques  de la ingeniería de software al desarrollo y  evolución de las aplicaciones Web [12]. La cantidad de dominios de aplicación y de usuarios a lo  largo del tiempo crece cada vez con mayor aceleración. Como resultado de esto ha crecido la  cantidad de personas que desarrollan aplicaciones  Web. El desarrollo de aplicaciones Web ha  demostrado ser una tarea compleja: parafraseando  el célebre “software crisis” el término “Web Crisis”  comenzó a utilizarse en 2001 [9].   La calidad de los desarrollos Web ha sido severamente cuestionada en diversas investigaciones  específicas. Una investigación ampliamente  difundida [13] revela que algunos de los principales  problemas de las aplicaciones Web son: fallas en  satisfacer las necesidades de negocio (84%), falta  de funcionalidad (53%) y baja calidad de los  entregables (52%). Justamente éstos son los tipos  de problemas que ataca la Ingeniería de Requerimientos. Quienes desarrollan aplicaciones Web  deben aplicar metodologías de desarrollo de la  misma manera que lo hacen quienes desarrollan  software en general [4].   En este contexto, se ha ido abriendo paso en las  comunidades de Ingeniería Web y de Ingeniería de  Requerimientos el reconocimiento de la necesidad  de entender en profundidad los objetivos y  necesidades que debe satisfacer una aplicación  Web. Desde el punto de vista de la comunidad  científica la realización de un Workshop dedicado  al tema de aplicaciones Web en el contexto de la  mayor conferencia de Ingeniería de Requerimientos  (el WeRe, International Workshop on the Web and  Requirements Engineering, durante la decimoctava  conferencia de Ingeniería de Requerimientos en  Sydney, Australia) muestra la importancia que se  está asignando a este tema.   El diseño de la arquitectura de información de un  sitio Web es una tarea compleja y requiere un  análisis cuidadoso de sus objetivos y de las  necesidades que debe satisfacer desde el punto de  vista de los distintos usuarios. Por tal razón, es  imprescindible contar con una definición precisa de  todos los requerimientos del sitio que será  desarrollado, lo que incluye aspectos tales como los  objetivos del sitio, los stakeholders involucrados,  las tareas y necesidades que se pretenden abarcar en  el sitio y sus objetivos de comunicación [10]. La  Ingeniería de Requerimientos se encarga precisamente de establecer un proceso ingenieril para la  captura, análisis, comprensión, documentación y  representación de los requerimientos de un sistema  [17], en particular de un sitio Web.   Introducción   Una aplicación Web es un sistema de software al  que se accede a través de Internet (o Intranet): las  aplicaciones Web constituyen una clase especial de  aplicaciones de software que se construyen de  acuerdo con ciertas tecnologías y estándares. Una  tipología bastante difundida identifica los siguientes tipos de aplicaciones Web [12]:   • Sitios Web centrados en documentos   • Aplicaciones Web Interactivas   • Aplicaciones Web transaccionales   • Aplicaciones Web basadas en flujos de trabajos   • Aplicaciones Web de colaboración   • Web social   • Aplicaciones Web orientadas a portales, y dentro  de éstos, portales generales y portales especializados de negocios, de mercados o de comunidades   • Aplicaciones de Web ubicua   • Web semántica   La Ingeniería Web tiene como objetivo la aplicación y desarrollo de enfoques de la Ingeniería de  Software a las aplicaciones Web. Existen varias  características que ubican a las aplicaciones Web  en una posición especial dentro de la Ingeniería de  Software. Algunas de ellas son;   • La importancia del contenido y de la presentación  en un sitio Web hacen necesaria una estrecha  colaboración dentro de un equipo en el que  coexisten programadores, especialistas en contenido y diseñadores gráficos.   • Existen, además de las tradicionales aplicaciones  centradas en documentos muchas otras. Por citar  sólo algunas, podemos mencionar las aplicaciones interactivas, transaccionales, sociales y  colaborativas.   • Los requerimientos de calidad se vuelven más  complejos, ya que hay que considerar la calidad  del contenido (consistencia, confiabilidad, actualización, relevancia entre otros.) Asimismo es  necesario tener en cuenta la calidad de la estructura de hipertexto que generalmente subyace en  un sitio Web.   • El contexto social se vuelve extremadamente  complejo, ya que el perfil de usuarios es casi  imposible de determinar. Hay también diferencias  educativas y culturales que hacen que los  parámetros de usabilidad se vuelvan relativos. El  aspecto de “retención” de un usuario es  especialmente relevante dada la facilidad con la  que un usuario puede dejar un sitio Web en el  caso de que, por ejemplo, algo no sea de su  agrado.   • El contexto técnico es también complejo. La  multiplicidad de plataformas desde las cuales se  puede acceder una aplicación Web cambia tanto  que es difícil para un usuario normal tener un  panorama general. Esto hace que el tradicional  “mantra” que se suele invocar cuando se habla de  requerimientos (hay que concentrarse en “qué” y  no en “cómo” [17]) se vuelve impracticable en el  caso de aplicaciones Web [25] por la especial  interacción que hay entre los requerimientos y la  arquitectura.   • Finalmente, la localización geográfica del usuario  requiere de las aplicaciones capacidad para  adaptarse a diversos contextos locales (por  ejemplo, diferencias de hora,  idioma, cultura,  etc.)   Líneas de investigación  El proyecto tiene varias líneas de investigación  relacionadas con requerimientos orientados a aplicaciones Web. Los describimos a continuación.   Análisis de procesos de requerimientos. Para  obtener información sobre los procesos actualmente  utilizados para la obtención de requerimientos, se  diseñó una investigación de las prácticas habituales  de las áreas que desarrollan aplicaciones Web. En  parte esta medición ya ha sido ejecutada en otros  contextos [18]. La investigación se basa en una  encuesta a un amplio número de desarrolladores y  cuenta con un cuestionario con  preguntas cerradas  que se responde mediante acceso a Internet.  Actualmente se está evaluando el instrumento con  un grupo de prueba y se está generando la base de  datos de desarrolladores de aplicaciones Web a los  que se invitará a contestar la encuesta.   Identificación de stakeholders. El concepto de  stakeholder está ya muy bien establecido en la  comunidad de la Ingeniería de Software  sin  embargo en la Ingeniería de Requerimientos no hay  un concepto homogéneo. Algunos autores parecen  limitar los stakeholders a las personas directamente  involucradas en el proyecto [2] y otros lo extienden  a personas interesadas o que puedan ser afectadas  directa o indirectamente por el proyecto [1]; en la  literatura de dirección de proyectos incluso se suele  identificar como stakeholders no sólo a seres  humanos sino también a organizaciones [21].  Aunque hay consenso en la literatura acerca de la  importancia de considerar a los stakeholders en el  proceso de obtención de los requerimientos, una  simple revisión basada en la frecuencia de  aparición de palabras en conferencias revela que la  situación es otra: las menciones a stakeholders son  pocas. Esto revelaría, de acuerdo con la lingüística,  que el concepto en sí no es considerado en la  investigación con una relevancia similar a la que se  manifiesta cuando se considera su papel en la  obtención de los requerimientos. También en el  ámbito de la Ingeniería Web [8], [9] se ha puesto de  manifiesto la importancia de los stakeholders. El  universo de stakeholders en una aplicación Web es  considerablemente más complejo en cuanto a  variedad y cantidad de tipos. Actualmente está en  desarrollo un proceso de revisión de los enfoques  existentes en cuanto a este concepto y los procesos  disponibles para identificarlos.   Léxico Extendido de Lenguaje y abordajes  afines. El Léxico Extendido del Lenguaje (LEL)  [11], [15] ha demostrado ser una herramienta  poderosa para la elicitación de los requerimientos  de un sistema de Software,  sin embargo no es muy  utilizado en la práctica profesional. Hay indicios de  que esto se debería a cierta dificultad en su construcción. Por otra parte las aplicaciones Web requieren revisiones extensas de documentos. Esto ha  motivado que se explore la posibilidad de que el  LEL pueda ser construido automáticamente. Ya se  han desarrollado algunos esfuerzos para la generación automática del LEL a partir de documentos  con estrategias básicas [5]. Las técnicas de generación automática de resúmenes [23], [24] han  abierto un camino con promisorios resultados  iniciales y la aplicación de técnicas de text mining  [26], [27] a este terreno puede ser también una  herramienta útil para desarrollar un proceso  automatizado de construcción del LEL. Se encarará  una línea de investigación para poder avanzar en  este aspecto de generación automatizada del LEL  basada en el análisis masivo de documentos del  dominio de aplicación. Estrictamente hablando se  pretende, por lo menos en esta etapa del trabajo,  desarrollar una enfoque de generación asistida por  computadora. Una línea que se planea integrar con  este enfoque es la utilización, mediante adaptaciones y extensiones, de una herramienta de soporte  de Ingeniería de Requerimientos [28].   Lenguaje y requerimientos. En la Ingeniería de  Software y en particular en la Ingeniería de Requerimientos la problemática del lenguaje está muy  presente. Investigaciones recientes establecieron  que la mayoría de los documentos utilizados en el  análisis de requerimientos son provistos por los  consumidores y están escritos en lenguaje natural  común [19]. En la Ingeniería de Requerimientos se  ha puesto foco en el universo de discurso, que  incluye todas las fuentes de información y todas las  personas relacionadas al Software. Estas son conocidas como los actores de ese universo de discurso  [14], [16]. De una u otra manera, la Ingeniería de  Requerimientos ha puesto énfasis en entender ese  discurso que viene dado en el lenguaje del dominio  de aplicación. No sólo es necesario conocer este  lenguaje del dominio de aplicación para entender el  problema, sino también para especificar los  requerimientos, toda vez que éstos deben ser  entendidos por los usuarios y otros stakeholders  que se desempeñan en el dominio de aplicación.  Esto se ve complicado por la ambigüedad inherente  del lenguaje natural [3]. Es por eso que se ha  encarado un programa de trabajo interdisciplinario  con el campo de la Lingüística (Análisis del Discurso) de la Facultad de Comunicaciones y Diseño,  para tratar de delimitar un campo de investigación  en la interacción del lenguaje natural con la  Ingeniería de Requerimientos. Dado el estado  preliminar de las definiciones, se ha planificado  comenzar con un seminario de interacción entre el  campo de la Lingüística y de Ingeniería de Requerimientos.   Metodologías de Requerimientos Web. El  objetivo de esta línea de investigación es la  realización de un estudio comparativo de los  enfoques disponibles para el tratamiento de los  diferentes tipos de requerimientos. Para ello se  analizarán las metodologías establecidas para el  desarrollo de aplicaciones Web [6], [7], [22]. Si  bien el estado del arte de desarrollo de aplicaciones  Web en la actualidad no aprovecha plenamente los  avances de la Ingeniería de Software en los años  recientes, las comunidades de Ingeniería Web y de  Ingeniería de Requerimientos están en camino de  reconocer la importancia de un trabajo conjunto,  como se comentó más arriba. Siguiendo la  clasificación habitual de requerimientos en funcionales y no funcionales, de plena validez en los  requerimientos de aplicaciones Web, se llevará a  cabo un estudio comparativo de los enfoques  metodológicos para el tratamiento de los  requerimientos no funcionales [6], [7].  Se investigará especialmente:   • Estándares, procesos, metodologías propuestas  para el desarrollo de aplicaciones Web.   • Los requerimientos en general y los requerimientos en las aplicaciones Web en particular.   • Técnicas de elicitación según los tipos de requerimientos.   • Los requerimientos en aplicaciones Web en el  contexto de la problemática que plantea este  tipo de aplicaciones.   Resultados y objetivos  Los objetivos del proyecto en su estado actual son  los siguientes:  • Desarrollar una investigación de campo de las  prácticas de obtención de requerimientos de  aplicaciones Web  • Establecer un proceso para obtener los Requerimientos No Funcionales (NFR) de una  aplicación Web  • Establecer un concepto de stakeholder aplicable a las aplicaciones Web y definir un proceso para su identificación en el caso de una  aplicación Web  • Desarrollar la primera versión de un proceso de  construcción del Léxico extendido del Lenguaje (LEL) de una aplicación Web que pueda  ser soportado por un sistema software  • Desarrollar una seminario para comenzar a  definir el campo de colaboración entre la  Lingüística y la Ingeniería de Requerimientos  En el corto plazo esperamos obtener los siguientes  resultados.   • Capacitación del grupo de investigación en el  terreno específico de requerimientos para  aplicaciones Web.   • Estimulación de la cultura de investigación  entre los alumnos de la facultad y entre  profesores que tuvieren interés y, por una razón  u otra, no hayan podido realizar investigación  antes.   • Transferencia tecnológica con algunas de las  empresas participantes en la encuesta. En una  primera etapa a través de las comunicaciones  de los resultados de la encuesta y en una  segunda etapa a través de la propuesta de  mejores prácticas de Ingeniería de Requerimientos.   • Trabajo multidisciplinario con otras facultades.  En una primera etapa se hará un seminario  conjunto entre especialistas en informática y en  lingüística.   Formación de Recursos  Humanos  El estado actual de formación de recursos humanos  es el siguiente:  • Tesis de grado. En el contexto de la carrera de  Ingeniería en Informática de la UADE están  ejecutando tres Tesinas de Grado (PFI –  Proyecto Final de Ingeniería) de la carrera  Ingeniería en Informática  • Se encuentra en desarrollo un Trabajo Final  Integrador para la Especialización en Ingeniería  de Software de la Facultad de Informática de la  UNLP. El mismo se utilizará formará parte de  una tesis de Maestría  • Se encuentra en proceso de definición de un  plan de trabajo para el proyecto de Doctorado  en la UNLP de uno de los miembros del grupo.  • Se encuentra en proceso de definición la tesis  de Maestría de en Ingeniería de Software de la  Facultad de Informática de la UNLP de un ex  alumno de la UADE  La formación de recursos humanos es tal vez la  principal motivación de este proyecto de investigación. En el campo de la Ingeniería de Software existe una desigual competencia entre la  Universidad y la industria por los recursos humanos  y es por lo tanto extremadamente difícil conseguir  personas capacitadas para investigación. No obstante, siempre hay personas que se interesan por la  investigación y es parte de la responsabilidad del  grupo de este proyecto acercar a los alumnos de la  carrera Ingeniería en Informática a la investigación.  Alineado con este objetivo se desarrollan actividades conjuntas con la cátedra de Ingeniería de  Requerimientos de la carrera de grado para incorporar, desarrollar y evaluar diferentes métodologías  de enseñanza de técnicas de obtención de requerimientos, en especial las del área de elicitación de  requerimientos.	﻿Requerimientos , Aplicaciones Web	es	20125
1	FQTrie Desbalanceado	﻿ El modelo de Espacios Métricos permite formalizar el concepto de búsqueda por similitud en bases de datos no tradicionales. El objetivo es construir ·ndices que permitan reducir el tiempo necesario para resolver una búsqueda por similitud. Uno de los enfoques para la construcción de índices es el usado por los algoritmos basados en pivotes. Sobre bases de datos tradicionales se sabe que, mientras más balanceado sea un índice, mejor será su desempeño durante una búsqueda. Los supuestos que llevan a esta conclusión no son ciertos en espacios métricos y por lo tanto un enfoque desbalanceado suele ser la mejor opción para los espacios de alta dimensión. En este trabajo nos proponemos estudiar técnicas para lograr desbalancear el Fixed Queries Trie. Palabras claves: Bases de Datos, Espacios M·etricos, ·Indices, Pivotes. 1. Introducción El concepto de b·usquedas por similitud o por proximidad, es decir buscar elementos de una base de datos que sean similares o cercanos a uno dado, aparece en diversas áreas de computación, tales como reconocimiento de voz, reconocimiento de imágenes, compresión de texto, biología computacional, inteligencia artificial, minería de datos, entre otras. En [4] se muestra que el problema se puede expresar como sigue: dado un conjunto de objetos X y una función de distancia d definida entre ellos que mide cuán diferentes son, el objetivo es recuperar todos aquellos elementos que sean similares a uno dado. Esta función d cumple con las propiedades características de una función de distancia: Positividad Estricta: d(x, y) = 0 ⇔ x = y. Simetr·a: d(x, y) = d(y, x). Desigualdad Triangular: d(x, z) ≤ d(x, y) + d(y, z). El par (X , d) se denomina espacio m·etrico. La base de datos será un subconjunto finito U ⊆ X de cardinalidad n. En este nuevo modelo de bases de datos, una de las consultas típicas que implica recuperar objetos similares es la b·usqueda por rango, que denotaremos con (q, r)d. Dado un elemento q ∈ X , al que llamaremos query y un radio de tolerancia r, una búsqueda por rango consiste en recuperar los objetos de la base de datos cuya distancia a q no sea mayor que r, es decir: (q, r)d = {u ∈ U : d(q, u) ≤ r} Se han logrado avances importantes sobre búsquedas por similitud en espacios métricos en torno al concepto de construir un ·ndice, es decir, una estructura de datos que permita reducir el tiempo necesario para resolver una consulta. En este tiempo influyen tres factores, a saber: la cantidad de evaluaciones de la función de distancia d hechas, la cantidad de operaciones adicionales realizadas que implican un tiempo extra de CPU y la cantidad de accesos a páginas de disco que implican un tiempo extra de I/O. En consecuencia, el tiempo total de resolución de una búsqueda puede ser calculado como: T = #d× complejidad(d) + Tpo CPU + Tpo I/O En muchas aplicaciones la evaluación de la función d es tan costosa que las demás componentes pueden ser despreciadas. Éste es el modelo de complejidad usado en la mayoría de los trabajos de investigación hechos en esta temática. Sin embargo, hay que prestar especial atención al tiempo extra de CPU, dado que reducir este tiempo produce que en la práctica la búsqueda sea más rápida, aún cuando estemos realizando la misma cantidad de evaluaciones de la función d. De igual manera, el tiempo de I/O puede jugar un papel importante en algunas aplicaciones. En [4] se presenta un desarrollo unificador de las soluciones existentes en la temática. En dicho trabajo se muestra que todos los enfoques para la construcción de índices en espacios métricos consisten en: particionar el espacio en clases de equivalencia. indexar las clases de equivalencia. durante la búsqueda, usando el índice y la desigualdad triangular, descartar algunas clases y buscar exhaustivamente en las restantes. La diferencia entre los distintos algoritmos radica en cómo construyen estas clases de equivalencia. Básicamente se pueden distinguir dos enfoques: algoritmos basados en pivotes y algoritmos basados en particiones compactas. En este trabajo nos centraremos en los algoritmos basados en pivotes, más específicamente el Fixed Queries Trie (FQTrie). Comenzamos dando una introducción a este índice; en las secciones siguientes describimos el concepto de dimensión y analizamos el efecto de desbalancear el índice sobre espacios de alta y de baja dimensión. Luego, proponemos una técnica para lograr un desbalance en el FQTrie. Finalizamos dando el trabajo futuro. 2. Fixed Queries Trie Esta estructura fue presentada en [2] y forma parte de los familia de estructuras basadas en pivotes. La idea subyacente de los algoritmos basados en pivotes es la siguiente. Se seleccionan k pivotes {p1, p2, . . . , pk}, y se le asigna a cada elemento a de la base de datos, el vector o firma Φ(a) = (d(a, p1), d(a, p2), . . . , d(a, pk)). Ante una búsqueda (q, r)d, se computa Φ(q) = (d(q, p1), d(q, p2), . . . , d(q, pk)). Luego, se descartan todos aquellos elementos a, tales que para algún pivote pi, | d(q, pi) − d(a, pi) |> r, es decir max1≤i≤k | d(q, pi) − d(a, pi) |= L∞(Φ(a), Φ(q)) > r. Los elementos no descartados por la condición anterior, se comparan directamente con la query q. Esto significa que, todos los algoritmos basados en pivotes, proyectan el espacio métrico original en un espacio vectorial k dimensional con la función de distancia L∞. La diferencia entre todos ellos, radica en cómo implementan la búsqueda en el espacio mapeado. La familia de estructuras FQ (FQT, FHQT, FQA, FQtrie) [1, 3] forman parte de las estructuras basadas en pivotes; cada una de ellas fue presentada como una mejora de la anterior En el FQtrie [2], se utiliza un ·Arbol Digital o Trie [5] para indexar las firmas de los elementos de la base de datos. Un Trie es un árbol m-ario para búsqueda lexicográfica. En esta estructura, cada elemento se considera como una secuencia de caracteres sobre un alfabeto Σ; la cardinalidad del alfabeto determina la aridad del árbol, es decir m = |Σ|. Un nodo en un trie o es un nodo externo y contiene un elemento; o es un nodo interno y contiene m punteros a subtries. Dada una cadena, se usan los caracteres que la conforman para direccionar la búsqueda en el árbol. Estando en un nodo de nivel i, la selección del subtrie que le corresponde se realiza en función del i-ésimo caracter de la cadena. El nodo raíz usa el primer caracter, los nodos hijos de la raíz usan el segundo caracter, y así sucesivamente. En un trie m-ario la búsqueda toma un tiempo que es proporcional a la longitud de la cadena, independientemente de cual sea el tamaño de la base de datos. 3. La maldición de la dimensión Uno de los principales obstáculos en el diseño de buenas técnicas de indización es lo que se conoce con el nombre de maldici·on de la dimensionalidad. El concepto de dimensionalidad está relacionado a la dificultad o facilidad de buscar en un determinado espacio métrico. La dimensión intrínseca de un espacio métrico se define en [4] como ρ = µ22σ2 , siendo µ y σ2 la media y la varianza respectivamente de su histograma de distancias. Es decir que, a medida que la dimensionalidad intrínseca crece, la media crece y su varianza se reduce. Esto significa que el histograma de distancia se concentra más alrededor de su media, lo que influye negativamente en los algoritmos de indización. La figura 1 da una idea intuitiva de por qué el problema de búsqueda se torna más difícil cuando el histograma es más concentrado. Consideremos una búsqueda (q, r)d, y un índice basado en pivotes elegidos aleatoriamente. Los histogramas de la figura representan posibles distribuciones de distancias entre q y algún pivote p. La regla de eliminación dice que podemos descartar aquellos puntos y tales que y /∈ [d(p, q) − r, d(p, q) + r]. Las áreas sombreadas muestran los puntos que no podrán descartarse. A medida que el histograma se concentra más alrededor de su media, disminuye la cantidad de puntos que pueden descartarse usando como dato d(p, q). Este fenómeno es independiente de la naturaleza del espacio métrico, y nos brinda una forma de cuantificar cuán dura es una búsqueda sobre el mismo. 4. Desbalanceando el Índice Sobre bases de datos tradicionales se sabe que mientras más balanceado sea un índice mejor será su desempeño durante una búsqueda. Estructuras tales como AVL, Skip List y Árbol B han sido diseñadas persiguiendo este objetivo. Esta idea de balancear una estructura de datos se basa en dos supuestos: el tipo de búsqueda que se quiere resolver es la búsqueda exacta. existe un orden lineal en el conjunto sobre el que se realizarán las búsquedas. d(p,q)d(p,q) 2r d(p,x) 2r d(p,x) Figura 1: Histogramas de distancias de baja dimensionalidad (izquierda), y de alta dimensionalidad (derecha) Ninguno de estos dos supuestos es válido sobre espacios métricos. La única información con la que se cuenta al momento de organizar un índice (generalmente un árbol) en espacios métricos es la distancia entre elementos. Se impone un orden lineal a los elementos ordenándolos de acuerdo a su distancia a la raíz del árbol. Sin embargo, la mayoría de los autores tratan de balancear estos árboles dividiendo los rangos de distancia en partes uniformes, con el fin de lograr que cada subárbol tenga aproximadamente la misma cantidad de elementos. Los problemas con este enfoque surgen cuando consideramos el tipo de búsqueda que realizaremos y la distribución de los elementos en el espacio métrico. En primer lugar, la búsqueda no es exacta sino que existe un radio de tolerancia r que se conoce en el momento de realizar la búsqueda y no cuando se construye el índice. En segundo lugar, la distribución de los elementos en el espacio no se corresponde necesariamente con una distribución uniforme. Como vimos en la sección anterior, los espacios métricos de baja dimensionalidad presentan un histograma más uniforme que los espacios de alta dimensionalidad. Como el histograma de espacios de baja dimensionalidad no es concentrado, una partición donde cada subárbol tenga la misma cantidad de elementos produce franjas de aproximadamente el mismo tamaño y, en consecuencia, la búsqueda entra en un número razonable de subárboles. Pero si el espacio es de alta dimensionalidad, el histograma está concentrado en un pequeño rango de valores, a donde probablemente también pertenecerá la query q. Esto produce que una proporción grande de elementos no puedan descartarse cuando se compara la query q con la raíz del árbol. Si balanceamos el árbol hacemos aún más ineficiente la búsqueda. Como el histograma es más concentrado, particionar en partes con la misma cantidad de elementos produce que las zonas sean cada vez más pequeñas; pero como el radio de búsqueda es el mismo, una mayor cantidad de zonas serán intersectadas por la query, lo que producirá que una mayor cantidad de subárboles deban ser revisados. Por esta razón balancear los árboles en espacios de alta dimensionalidad no es una buena opción. Si armamos un árbol particionando de manera tal que cada parte tenga el mismo ancho evitamos este problema y obtenemos algunos beneficios adicionales. Dado que el ancho es independiente de la dimensión, no necesitamos revisar más subárboles cuando la dimensión crece. Además, los subárboles que contienen aquellas partes que conforman el núcleo del espacio tendrá mayor cantidad de elementos logrando así cada vez un mayor desbalance. 5. Desbalanceando el FQTrie Como todos los elementos del espacio tiene el mismo tamaño de firma, el trie utilizado por el FQTrie queda necesariamente balanceado. Este trie puede verse como un índice sobre U* (el conjunto de firmas). Lo que nosotros en realidad debemos desbalancear es el índice sobre U (el espacio métrico). El índice base sobre el que se construye el FQTrie es el FQT. Fixed Height Fixed Queries Tree (FHQT)[1]. Si miramos el FHQT subyacente, desbalancearlo significa conseguir pivotes que distribuyan los elementos en forma no uniforme dentro del índice. Para lograr esto formaremos la firma de los elementos de manera tal que no todos los pivotes contribuyan a la firma de todos los elementos. El procesa a utilizar es el siguiente. Se selecciona un pivote p1 como raíz. Los M elementos más cercanos a p1 forman el primer grupo de elementos. Con los restantes se procede recursivamente: se elije un pivote p2 y los M elementos más cercanos a p2 forman el segundo grupo de elementos. Este proceso continúa hasta que no queden más elementos para clasificar. Como resultado final tendreM elementos grupo 1 M elementos grupo 2 M elementos grupo 3 <=M  elementos grupo (n/M) p1 p2 p3 p(n/M) Figura 2: Proceso de creación de grupos mos los n elementos del espacio clasificados en n/M grupos (ver figura 2). La firma de cada elemento x se forma ahora teniendo en cuenta el grupo al que pertenece x. Si se utilizan k pivotes para la creación de cada firma, entonces: los elementos del primer grupo forman su firma con los pivotes p2, · · · , pk+1; los elementos del segundo grupo forman su firma con los pivotes p3, · · · , pk+2; en general los elementos del grupo i forman su firma con los pivotes pi+1, · · · , pk+i. Cabe señalar que se necesitarán elegir k pivotes adicionales a los ya utilizados a fin de poder completar la firma de los (k − 1) últimos grupos. El proceso de búsqueda debe ahora tener en cuenta esta situación a fin de descartar correctamente los grupos que correspondan. En el peor caso la query q deberá compararse con k + n/M pivotes. 6. Trabajo Futuro Nos proponemos estudiar cuál es el efecto de la técnica de desbalance propuesta sobre la performance de las búsquedas. El trabajo a realizar puede resumirse en los siguientes puntos: Establecer empíricamente el valor más adecuado para M . Estudiar el efecto de esta técnica de desbalance experimentando sobre espacios de alta y también de baja dimensionalidad. Comparar los resultados obtenidos con el FQTrie original. Analizar otras técnica posibles para el desbalance del índice.	﻿Índices , Bases de Datos , Espacios Métricos , Pivotes	es	20789
2	Grupos de aprendizaje colaborativo asistidos por agentes para promover el equilibrio de roles	"﻿ Organizar a los estudiantes en grupos  para que trabajen juntos no garantiza que  alcancen un aprendizaje colaborativo  efectivo. Las conductas que manifiesta cada  uno de los integrantes, es decir los roles que  se asuman, son fundamentales para alcanzar  con éxito los objetivos de enseñanza y de  aprendizaje. Por ello, en este trabajo se  presenta un proyecto que propone un  modelo multiagente que monitorea la  participación de los estudiantes dentro del  grupo, reconoce los roles que ellos  desenvuelven al trabajar colaborativamente,  construye automáticamente sus perfiles de  usuario, diagnostica el estado de la  colaboración considerando el equilibrio de  roles como situación ideal, y propone  acciones correctivas cuando el  comportamiento del grupo se aleja de dicho  equilibrio. El modelo a desarrollar será  implementado en un entorno de aprendizaje  a distancia, y para validar su  funcionamiento será utilizado por grupos de  estudiantes reales durante sesiones de  trabajo colaborativo.    Palabras clave: Técnicas de aprendizaje  de máquina, Roles de grupo, Equilibrio de  roles, Agentes de software,  Personalización, Aprendizaje Colaborativo  Soportado por Computadora.    Contexto  En este trabajo se presenta una de las  líneas de investigación planteadas en el  proyecto “Sistemas de información web  basados en agentes para promover el  Aprendizaje Colaborativo Soportado por  computadoras (ACSC)”, dentro del  Programa de investigación titulado  “Sistemas de Información Web basados en  Tecnología de Agentes”, correspondiente a  la convocatoria 2011 de la Secretaria de  Ciencia y Tecnología de la Universidad  Nacional de Santiago del Estero (SICYT -  UNSE). Cabe aclarar que esta propuesta es  una continuación de la línea de  investigación “Sistemas Adaptativos  Inteligentes”, iniciada en 2005-2009 por el  proyecto “Herramientas conceptuales,  metodológicas y técnicas de la Informática  Teórica y Aplicada”  (COD. 23/C062),  continuada en 2009-2010 por el proyecto  ""Personalización en Sistemas de Enseñanza  Virtual"" (Código P09/C002), y en 20102011 por el proyecto ""Fundamentos  Conceptuales y Soportes Tecnológicos de la  Informática Educativa"" (Código 23/C089),  todos estos proyectos aprobados y  financiados por SICYT – UNSE.     Introducción  El Aprendizaje Colaborativo (AC)  describe una situación en la que se esperan  ocurran ciertas formas de interacción entre  las personas, susceptibles de promover  WICC 2012 83 2012 XIV Workshop de Investigadores en Ciencias de la Computación mecanismos de aprendizaje, aunque sin  garantías de que ello ocurra (Maissoneuve,  1998). El uso de medios computacionales  en el dominio del AC originó nuevos  escenarios de enseñanza y de aprendizaje  enmarcados en lo que se conoce como  Aprendizaje Colaborativo Soportado por  Computadoras (ACSC), con la tecnología  facilitando tanto la colaboración como la  comunicación. El ACSC rápidamente fue  adoptado en el ámbito de la educación a  distancia porque, a través del soporte  computacional, logra independizar a los  estudiantes de las variables tiempo y  espacio. Así, en un sistema de ACSC, los  estudiantes pueden trabajar  colaborativamente ubicados en puntos  geográficos distantes, e incluso,  contribuyendo en momentos diferentes en  el tiempo.   En ACSC el concepto de grupo es  fundamental. Un grupo es un conjunto  dinámico de estudiantes que trabajan  juntos, discutiendo algún tema, para  alcanzar eventualmente alguna meta común  prefijada, donde cada uno de ellos es  responsable por sus acciones, pero trabajan  juntos sobre el mismo problema respetando  las habilidades y contribuciones de cada  uno. Por otro lado, formar grupos y luego  incentivar a sus miembros a trabajar  colaborativamente no alcanza para  garantizar que el AC se produzca, ni que el  grupo trabaje de manera coordinada y  eficiente. El éxito o fracaso de la  experiencia de aprendizaje depende, entre  otras cosas, de los roles que sean capaces de  desempeñar los miembros del grupo.   Un rol se define como la tendencia a  comportarse, contribuir e interrelacionarse  de una determinada manera con el resto de  los integrantes de un equipo (Belbin, 1996).  Como resultado de sus investigaciones,  Belbin (1996) propuso una clasificación  basada en nueve roles que ha sido  ampliamente utilizada. Posteriormente,  continuando con sus investigaciones,  Belbin afirmó que sólo existiendo un  adecuado equilibrio de roles dentro de un  grupo, éste podrá lograr un trabajo  coordinado (Belbin, 2001). Dicho equilibrio  aparece cuando en un grupo se manifiesta la  mayor cantidad de roles posibles, y los  mismos no se repiten entre los integrantes  del grupo.   En muchos sistemas se almacenan en un  modelo de usuario las características  conductuales que los usuarios manifiestan  al interactuar. Generalmente, la creación y  mantenimiento de estos modelos se logra  con la inclusión de agentes inteligentes. El  denominado paradigma de agentes aborda  el desarrollo de entidades que puedan actuar  de forma autónoma y razonada (Maes,  1994). Uno de los tipos más conocidos de  agentes son los de interfaz. Un agente de  interfaz captura los intereses del usuario en  forma no intrusiva y guarda esa  información en el modelo de usuario que  crea y mantiene actualizado (Nwana, 1996).  Por otro lado, se habla de sistema  multiagente cuando existen en el sistema  varios agentes que interactúan entre sí.  Dado lo expuesto, en este trabajo se  propone crear un modelo multiagente capaz  de analizar las interacciones registradas por  los estudiantes dentro de un grupo de  ACSC, diagnosticar el estado de la  colaboración evaluando los roles  manifestados por sus miembros, y de ser  necesario proponer acciones correctivas  para alcanzar el equilibrio de roles  Los siguientes trabajos previos del grupo  de investigación servirán de base para este  proyecto: (Balmaceda et al., 2010;  Costaguta, 2006; Costaguta, 2009a;  Costaguta, 2009b; Costaguta y Durán 2006,  2007, 2011; Costaguta et al., 2011;  Costaguta y Fares, 2011; Costaguta y  Amandi, 2008; Durán et al., 2007, 2009a,  2009b, 2010; Fares y Costaguta, 2011a,  2011b;  Monteserín et al., 2010, Ozán y  Costaguta, 2010; Sachiaffino et al., 2008,  Schiaffino y Amandi, 2009). Aunque  también se considerarán otros antecedentes  sobre el tema (Botti y Julián, 2000;  Constantino et al., 2003; Constantino y  Suthers, 2004; Durán y Amandi, 2011;  WICC 2012 84 2012 XIV Workshop de Investigadores en Ciencias de la Computación Jermann et al., 2001; Liao et al., 2008;  Paiva, 1997; Peña, 2005; Santos et al.,  2003, 2004; Schellens et al., 2005; Soller,  2001; Strijbos et al., 2005; Vizcaino, 2005).    Líneas de investigación y  desarrollo/Objetivos  En un grupo de aprendizaje es  fundamental que los integrantes  desempeñen diferentes funciones o roles  que permitan realizar un trabajo  coordinado, y así alcanzar un aprendizaje  colaborativo exitoso. Si bien las conductas  o roles de un individuo en un equipo  pueden ser infinitas, el rango de conductas  útiles que realizan una contribución efectiva  al equipo es finito (Belbin, 2001). Existen  nueve roles diferentes que al ser  manifestados por los integrantes de un  grupo impactan en la efectividad del  trabajo, ellos son: Impulsor,  Implementador, Finalizador, Coordinador,  Cohesionador, Investigador de Recursos,  Cerebro, Monitor – Evaluador, y  Especialista (Belbin, 1996). Cuando un  equipo puede equilibrar la aparición de  estos roles, el alcanzar con éxito un trabajo  coordinado está asegurado (Belbin, 2001).   De lo anterior se desprende la necesidad  de identificar los roles que efectivamente  manifiesta cada uno de los integrantes de un  grupo de aprendizaje, y diagnosticar el  estado actual de la colaboración a fin de  intervenir en el momento adecuado,  propiciando alcanzar un equilibrio de roles  y un AC exitoso. En consecuencia, esta  investigación pretende responder al  siguiente interrogante: ¿Es posible crear un  modelo multiagente en el ámbito del  ACSC, capaz tanto de reconocer los roles  que manifiestan los estudiantes mientras  trabajan en un grupo, como de intervenir  con acciones correctivas cuando los roles  no se manifiesten adecuadamente?   Así, el objetivo general de este trabajo  de investigación consiste en desarrollar  técnicas que mejoren el desempeño de los  estudiantes dentro de un grupo de ACSC,  propiciando que los roles se manifiesten  adecuadamente. Se fijan como objetivos  específicos los siguientes:    Definir agentes inteligentes capaces de:  analizar las interacciones de los  estudiantes que conforman un grupo de  AC, identificar los roles que esos  estudiantes desempeñan, y reconocer las  situaciones en que dichos roles no se  manifiestan de manera equilibrada  durante la dinámica de trabajo.   Definir técnicas de recomendación que  permitan sugerir a los estudiantes  cambios de conductas o acciones  correctivas personalizadas orientadas a  mejorar sus desempeños individuales y  el del grupo como un todo.   Para alcanzar estos objetivos, primero se  realizará la búsqueda y análisis de  bibliografía vinculada con la consideración  de roles y de agentes en ACSC. Luego se  llevará a cabo el diseño del modelo  multiagente que permitirá resolver la  problemática descripta en la sección previa.  Finalmente, el modelo será implementado  sobre un entorno de aprendizaje a distancia.  Para validar su funcionamiento, se llevarán  a cabo experiencias con estudiantes reales  que lo utilizarán en sesiones de trabajo  colaborativo planificadas por docentes del  Departamento de Informática (FCEyT –  UNSE). El modelo multiagente propuesto  se encuadra en la categoría más alta  planteada por Jermann et al. (2001), es  decir, los sistemas que aconsejan.    Resultados  esperados  Con la concreción de este proyecto se  esperan obtener los siguientes resultados  principales:    Estado del arte referido a la  consideración de roles desempeñados  por integrantes de grupos colaborativos  en sistemas web de ACSC.   Módulos que apliquen técnicas de  aprendizaje de máquina para reconocer  WICC 2012 85 2012 XIV Workshop de Investigadores en Ciencias de la Computación roles y diagnosticar el estado actual de la  colaboración.   Módulos que apliquen técnicas de  aprendizaje de máquina para recomendar  acciones correctivas personalizadas.   Modelo multiagente que integre a los  módulos creados (diagnosticadores y  recomendadores).   Modelo multiagente implementado en  sistemas web de ACSC.  Actualmente se están ejecutando las  actividades vinculadas con la obtención del  primero de los resultados esperados.  Por otra parte, considerando las líneas de  investigación existentes en el medio, es  posible afirmar que el desarrollo de este  proyecto permitirá conformar un nuevo  grupo de investigación, que iniciará  oficialmente una línea en el área del ACSC.  De esta manera también se verán  consolidados los esfuerzos y la  investigación realizados con anterioridad en  esta temática, por parte de algunos de los  integrantes que integraron los proyectos  citados en la sección Contexto.     Formación de Recursos Humanos  El equipo de trabajo de este proyecto  está formado por dos docentes  investigadores formados y uno en  formación. También integra el equipo un  becario CONICET que desarrollará su tesis  doctoral en este proyecto. Además, se  incorporará al equipo un estudiante  avanzado de la carrera Licenciatura en  Sistemas de Información (FCEyT – UNSE),  para desarrollar su Tesis Final de  Graduación.    Referencias  Balmaceda, J., García, P., Schiaffino, S.: Detección  Automática de Roles de Equipo. In: X Simposio  Argentino de Inteligencia Artificial (39 JAIIO), p.  235-238 (2010)  Belbin, M.: Team Roles at Work (2nd Ed.).  Butterworth-Heinemann, Oxford (1996)  Belbin, M.: Managing without Power. ButterworthHeinemann, Oxford (2001)  Botti, V., Julián V.,: Agentes inteligentes: el  siguiente paso de la Inteligencia Artificial. Novatica,  vol. 145, pp. 95-99 (2000)  Constantino Gonzalez, M., Suthers, D.: Automated  Coaching of Collaboration based Workspace  Analysis: Evaluation and Implications for Future  Learning Environments. In: 36th IEEE Hawaii  International Conference on System Sciences, Big  Island, Hawaii, USA (2003)  Constantino Gonzalez, M., Suthers, D., Escamilla de  los Santos, J.: Coaching Web-based Collaborative  Learning based on Problem Solution Differences  and Participation. International Journal of Artificial  Intelligence in Education, vol. 13 (2003)  Costaguta, R.: ""Una Revisión de Desarrollos  Inteligentes para Aprendizaje Colaborativo  Soportado por Computadora"". Revista Ingeniería  Informática. Universidad de Concepción, Chile. Vol.  3, Nro. 1, pp.68-81 (2006).  Costaguta, R.: ""Habilidades de Colaboración  Manifestadas por los Estudiantes de Ciencias de la  Computación"". Revista Nuevas Propuestas, Nº 4344, pp. 55-69 (2009).  Costaguta, R.: “Algunos marcos utilizables para el  desarrollo de aplicaciones colaborativas”. Revista  Nuevas Propuestas, Vol. 46, pp. 55-69 (2009).  Costaguta, R:. ""Entrenamiento de estudiantes en la  práctica de destrezas colaborativas"". VII Simposio de  Inteligencia Artificial (ASAI-JAIIO), pp. 6-10  (2010).  Costaguta, R., Amandi, A.: ""Training Collaboration  Skills to Improve Group Dynamics"". Proceedings  ACM Euro American Conference on Telematics and  Information Systems (EATIS) (2008).   Costaguta, R., Fares, R.: “Coordinación de Agentes:  Tipos, Técnicas, Modelos y Lenguajes”, Revista  Avances en Sistemas e Informática (RASI), Vol. 7  (3), pp. 33-42 (2010).  Costaguta, R., Garcia, P., Amandi, A.: “Using  Agents for Training Students Collaborative Skills”,  IEEE Latinoamerican Transactions, Vol. 9 (7), pp.  1118-1124 (2011).  Durán, E., Amandi A..: “Personalised collaborative  skills for student models”, Interactive Learning  Environment. Joseph Psotka and Steve Wheeler  (eds.), Routledge, Taylor & Francis Group, vol.  19(2), pp. 143-162 (2011).  Durán, E., Costaguta, R., Farías, R., Trejo, M.,  Torales, F., Ozán, V., Martínez, P.:  ""Personalización,  en Sistemas de Enseñanza  Virtual"", Anales XI Workshop de Investigadores en  WICC 2012 86 2012 XIV Workshop de Investigadores en Ciencias de la Computación Ciencias de la Computación, Universidad Nacional  de San Juan. (2009).  Durán, E., Costaguta, R., Maldonado, M., Únzaga,  S., Menini, M., Chequer, G., Fernandez, N., Missio,  D.: “Técnicas de Aprendizaje de Máquina y  Personalización en Educación”, Anales XII  Workshop de Investigadores en Ciencias de la  Computación, Universidad Nacional de la Patagonia  San Juan Bosco, pp. 704-710 (2010).  Durán, E., Costaguta, R., Maldonado, M., Únzaga,  S.: ""Sistemas Adaptativos Inteligentes"", Anales IX  Workshop de Investigadores en Ciencias de la  Computación. Zulema Rosanigo et al., (eds).  Universidad Nacional de la Patagonia San Juan  Bosco (2007).  Durán, E., Costaguta, R., Maldonado, M., Unzaga,  S.: ""Nuevos desarrollos para sistemas adaptativos  inteligentes"", Anales XI Workshop de Investigadores  en Ciencias de la Computación, Universidad  Nacional de San Juan (2009).  Fares, R., Costaguta, R.: ""A multi-agent model that  promotes team-role balance in Computer Supported  Collaborative Learning"", Proc. Second International  Conference Advances in New Technologies,  Interactive Interfaces and Communicability  (ADNTIIC). Huerta Grande, Córdoba (2011).  Fares, R., Costaguta, R.: “Modelo multiagente que  propicia el equilibrio de roles en Aprendizaje  Colaborativo Soportado por Computadora”, XII  Simposio de Inteligencia Artificial (ASAI- JAIIO),  Córdoba (2011).  Jermann, P., Soller, A., Mühlenbrock, M.: From  Mirroring to Guiding: A Review of State of Art  Technology for Supporting Collaborative Learning.  In: 1st European Conference on Computer  Supported Collaborative Learning, pp. 324-331  (2001)  Liao, J., Li, Y., Chen, P., Huang, R.: Using Data  Mining as Strategy for Discovering User Roles in  CSCL. In: 8th IEEE Internatinal Conference on  Advanced Learning Technologies. España (2008)  Maes, P.: “Agents that reduce work and information  overload”, Communication of the ACM, vol. 37 (7)  (1994)  Maisonneuve, J.: La dinámica de los grupos (11va  Ed.). Nueva Visión, Argentina (1998)  Monteserín, A., Schiaffino, S., García, P., Amandi,  A.: Análisis de la formación de grupos en  Aprendizaje Colaborativo Soportado por  Computadoras. In: XXI Simpósio Brasileiro de  Informática na Educação (SBIE). Brasil (2010)  Nwana, H.: “Software Agents: An Overview”,  Knowledge Engineering Review, vol. 11(3) (1996)  Ozán, V., Costaguta, R.: “Descubrimiento de  habilidades de colaboración vinculadas con roles en  grupos de aprendizaje”, XI Simposio de Inteligencia  Artificial (ASAI-JAIIO). Buenos Aires (2010).  Paiva, A.: “Learner Modelling for Collaborative  Learning Environments”, du Boulay and Mizoguchi  (Eds.). Proc. Artificial Intelligence in Education.  Japón (1997).  Peña, A.: “Collaborative Student Modeling by  Cognitive Maps”, Proc. First International  Conference on Distributed Frameworks for  Multimedia Applications. IEEE Press (2005).  Santos, O., Barrera, C., Gaudioso, E., Boticario, J.:  ALFANET: An adaptive e-learning platform. In:  2nd International Meeting on Multimedia and  Information and Communication Technologies in  Education, pp. 1938-1942 (2003)  Santos, O., Boticario, J., Barrera, C.: A Machine  Learning Mult-Agent architecture to provide  adaptation in a Standard-based Learning  Management System. WSEAS Transactions on  Information Science and Applications, vol. 1 (1), pp.  468-473 (2004)   Schellens, T, Van Keer, H., Valcke, M., De Wever,  B.: The Impact of role Assignment as Scripting Tool  on Knowledge Construction in Asynchronous  Discussion Groups. In: ACM Conference on  Computer Support for Collaborative Learning.  Taiwan (2005)   Schiaffino, S., Garcia, P., Amandi, A.: eTeacher:  Providing personalized assistance to e-learning  students. Computers and Education, vol. 51 (4), pp.  1744-1754 (2008)  Schiaffino, S., Amandi, A.: Building an expert travel  agent as a software agent. Expert Systems with  Applications, vol. 36 (2), pp. 1291-1299 (2009)  Soller, A.: Supporting Social Interaction in an  Intelligent Collaborative Learning System.  International  Journal of Artificial Intelligence in  Education, vol. 12, pp. 40-62 (2001)  Strijbos, J., de Laat, M., Martens, R., Jochems, W.:  Functional versus Spontaneous Roles during CSCL.  In: ACM Conference on Computer Support for  Collaborative Learning. Taiwan (2005)  Vizcaíno, A.: “A Simulated Student Can Improve  Collaborative Learning”, International Journal of  Artificial Intelligence, vol. 15, pp 3-40 (2005).      WICC 2012 87 2012 XIV Workshop de Investigadores en Ciencias de la Computación"	﻿aprendizaje colaborativo soportado por computadora , técnicas de aprendizaje de máquina , roles de grupo , equilibrio de roles , agentes de software , personalización	es	18386
3	Sistemas inteligentes que promuevan el aprendizaje colaborativo soportado por computadoras	"﻿ Las ventajas de los sistemas de  Aprendizaje Colaborativo Soportado por  Computadora (ACSC) están ampliamente  reconocidas. Sin embargo, utilizar estos  sistemas, organizar a los estudiantes en  grupos e instar a los mismos a desarrollar  sus actividades de manera colaborativa, no  garantiza la construcción de un aprendizaje  colaborativo adecuado. El éxito o fracaso  de las experiencias de aprendizaje depende  principalmente del comportamiento que los  integrantes del grupo manifiesten. Así, las  conductas que demuestren tanto docentes  como estudiantes son fundamentales para  alcanzar con éxito los objetivos de  enseñanza y de aprendizaje.   Este artículo describe un proyecto que  propone desarrollar módulos para sistemas  de ACSC que puedan personalizar los  procesos de enseñanza y de aprendizaje, a  fin de promover las conductas adecuadas y   asegurar el éxito de las experiencias de  colaboración. Para ello se desarrollarán  módulos basados en agentes, los cuales  mediante técnicas de aprendizaje de  máquina puedan analizar las interacciones y  administrar los modelos de estudiante  necesarios para lograr la personalización.  La validación de los productos  desarrollados se realizará mediante la  experimentación con grupos de estudiantes  universitarios reales durante sesiones de  trabajo colaborativo especialmente  diseñadas. Los datos recabados serán  procesados mediante técnicas estadísticas y  métricas específicas de personalización.     Palabras clave: Técnicas de aprendizaje  de máquina, Modelo de estudiante, Agentes  de software, Personalización, Aprendizaje  Colaborativo Soportado por Computadora.    Contexto  En este trabajo se presenta uno de los  cuatro proyectos que integran el Programa  de investigación  “Sistemas de Información  Web basados en Tecnología de Agentes”,  correspondiente a la convocatoria 2011 de  la Secretaria de Ciencia y Tecnología de la  Universidad Nacional de Santiago del  Estero (CICyT - UNSE). El proyecto se  titula “Sistemas de información web  basados en agentes para promover el  Aprendizaje Colaborativo Soportado por  computadoras (ACSC)”. Cabe aclarar que  esta propuesta es una continuación de la  línea de investigación “Sistemas  Adaptativos Inteligentes”, iniciada en 20052009 por el proyecto “Herramientas  conceptuales, metodológicas y técnicas de  la Informática Teórica y Aplicada”  (COD.  23/C062), continuada en 2009-2010 por el  proyecto ""Personalización en Sistemas de  Enseñanza Virtual"" (Código P09/C002), y  en 2010-2011 por el proyecto  ""Fundamentos Conceptuales y Soportes  Tecnológicos de la Informática Educativa""  (Código 23/C089), todos estos proyectos  aprobados y financiados por CICyT –  UNSE.   WICC 2012 88 2012 XIV Workshop de Investigadores en Ciencias de la Computación Introducción  El uso de medios computacionales en el  dominio del Aprendizaje Colaborativo  (AC) ha permitido definir nuevos  escenarios de enseñanza-aprendizaje,  originando los conocidos sistemas de  Aprendizaje Colaborativo Soportado por  Computadoras (ACSC) o Computer  Supported Collaborative Learning (CSCL).  Estos sistemas web ofrecen versiones  electrónicas de muchas actividades y  recursos presentes en las aulas de enseñanza  tradicional (presencial). Así, es posible  disponer de espacios para trabajo  compartido, lecturas y presentaciones online, resultados de evaluaciones y  calificaciones, listados de bibliografía,  repositorios de ejercicios y materiales, etc.  Contando también con herramientas de  comunicación síncrona y/o asíncrona como  chat, foro y e-mail, todo lo cual da soporte  tanto a la comunicación como a la  colaboración entre los estudiantes.    Los sistemas de ACSC rápidamente  fueron adoptados en el ámbito de la  educación a distancia por las facilidades  expuestas en el párrafo anterior, y porque a  través del soporte web logran  independizarse de las variables tiempo y  espacio.  En ACSC el concepto de grupo es  fundamental. Un grupo de aprendizaje  colaborativo se define como un conjunto  dinámico de estudiantes que trabajan juntos  para alcanzar eventualmente alguna meta  común prefijada, donde cada uno de ellos es  responsable por sus acciones, pero todos  trabajan juntos sobre el mismo problema  respetando las habilidades y contribuciones  de cada uno. Así, los estudiantes que  conforman grupos en ACSC son  responsables de su propio aprendizaje y  también del de sus compañeros, las  capacidades de cada individuo sirven como  recursos para cada uno de los otros  miembros del equipo, y por esto, el éxito de  uno ayuda al éxito de todos. Cuando un  estudiante no se comporta adecuadamente,  su participación es deficiente y perjudica al  desenvolvimiento del grupo como equipo.   La aparición de comportamientos  individuales disfuncionales impactan de  manera negativa en el rendimiento grupal e  impiden alcanzar un ACSC adecuado  (Maisonneuve, 1998). Por lo tanto, formar  grupos y luego incentivar a sus miembros a  trabajar colaborativamente no alcanza para  garantizar que la experiencia de enseñanza  y de aprendizaje sea exitosa. El éxito o  fracaso está influenciado por la tendencia a  comportarse, contribuir e interrelacionarse  de una determinada manera de cada  integrante con respecto al resto de  miembros del grupo.   En muchos sistemas se almacenan en un  modelo de usuario las características  conductuales que los usuarios manifiestan  al interactuar. Generalmente, la creación y  mantenimiento de estos modelos se logra  con la inclusión de agentes inteligentes. El  denominado paradigma de agentes aborda  el desarrollo de entidades que puedan actuar  de forma autónoma y razonada (Maes,  1994). Uno de los tipos más conocidos de  agentes son los de interfaz. Un agente de  interfaz captura los intereses del usuario en  forma no intrusiva y guarda esa  información en el modelo de usuario que  crea y mantiene actualizado (Nwana, 1996).   El desarrollo de sistemas web apoyados  por agentes de software, aplicados a ACSC,  permitirá ofrecer sistemas con la capacidad  de monitorear las interacciones y adaptarse  al comportamiento de sus usuarios. Estas  capacidades de monitoreo y adaptación  contribuirán de manera positiva en los  procesos de enseñanza y de aprendizaje; ya  que, al estar basadas en modelos centrados  en el estudiante, favorecen un aprendizaje  significativo y activo. Por esto, en este  proyecto se propone investigar teórica y  metodológicamente las contribuciones que  puedan realizarse en el desarrollo de  sistemas web para entornos de ACSC,  construyendo modelos de estudiante,  aplicando técnicas de aprendizaje de  máquina, y creando agentes de software.   WICC 2012 89 2012 XIV Workshop de Investigadores en Ciencias de la Computación Los siguientes trabajos previos del grupo  de investigación servirán de base para este  proyecto: (Costaguta, 2006; Costaguta,   2009a; Costaguta, 2009b; Costaguta y  Durán 2006, 2007, 2011; Costaguta et al.,  2011; Costaguta y Fares, 2011; Costaguta y  Amandi, 2008; Durán et al., 2007, 2009a,  2009b, 2011; Durán y Costaguta, 2008;  Fares y Costaguta, 2011a, 2011b; Menini et  al., 2011; Ozán y Costaguta, 2010; Santana  Mansilla et al; 2011). Aunque también se  considerarán otros antecedentes sobre el  tema (Ayala, 2003; Bull et al., 2007; Chen  et al., 2003; Durán y Amandi, 2011;  Gonzalez et al., 2006; Monteserín et al.,  2010; Paiva, 1997; Peña, 2005; Vassileva et  al., 2003; Vizcaíno, 2005).    Líneas de investigación y  desarrollo/Objetivos  El desarrollo de sistemas web basados en  agentes para ser aplicados al ámbito del  ACSC permitirá ofrecer sistemas con la  capacidad de adaptarse al comportamiento  de sus usuarios. Por esto, en este proyecto  se propone investigar teórica y  metodológicamente las contribuciones que  puedan realizarse en el desarrollo de  sistemas web para entornos de ACSC,  aplicando modelos de usuario, agentes y  técnicas de aprendizaje de máquina.   Se establecen los siguientes objetivos  generales para esta investigación:   Favorecer el desarrollo de conocimiento  científico-tecnológico de relevancia para  el desarrollo de sistemas de información  web personalizados en el área del ACSC   Realizar propuestas metodológicas y  desarrollos de módulos para sistemas de  información web en ACSC, mediante la  inclusión de modelos de usuario,  agentes, y técnicas de aprendizaje, a fin  de promover a los procesos de enseñanza  y de aprendizaje   Transferir y ofrecer servicios al medio a  través del asesoramiento y la  capacitación de estudiantes de grado y  posgrado por medio de cursos y  seminarios, así como también difundir  los resultados obtenidos mediante la  realización de publicaciones en revistas  especializadas y de presentaciones en  congresos, simposios y jornadas.    Formar recursos humanos mediante la  realización de tesis de grado y posgrado,  y la dirección de becas de investigación.  La pregunta central que guía el  desarrollo de este proyecto es la siguiente:  ¿Es posible mejorar el rendimiento  académico de los estudiantes universitarios,  promoviendo los procesos de enseñanza y  de aprendizaje en los sistemas de  información web de apoyo al aprendizaje  colaborativo, mediante la incorporación de  agentes y la aplicación de técnicas de  personalización?. Acorde con esto, se  fijaron los objetivos específicos que se  enuncian a continuación:   Determinar el estado actual de  conocimiento y desarrollo de sistemas de  información web que apliquen técnicas  provenientes del área de la Inteligencia  Artificial en el ámbito del ACSC.   Diseñar, construir y evaluar módulos que  apliquen técnicas de aprendizaje de  máquina para crear y mantener  actualizados modelos de estudiante, para  ser incluidos en sistemas de información  web de apoyo a los procesos de  enseñanza y de aprendizaje en el ámbito  del ACSC universitario, a fin de  personalizar tales procesos.   Diseñar, construir y evaluar módulos  para sistemas de información web, en el  ámbito del ACSC universitario, que  utilicen agentes inteligentes tanto para  diagnosticar el estado actual de la  colaboración, como para proponer  recomendaciones personalizadas que  favorezcan a los procesos de enseñanza  y de aprendizaje.   Evaluar el rendimiento académico de los  estudiantes que utilicen los sistemas de  información web personalizados,  desarrollados para promover los  procesos de enseñanza y de aprendizaje  WICC 2012 90 2012 XIV Workshop de Investigadores en Ciencias de la Computación en el ámbito del ACSC en contextos  universitarios.      Resultados  esperados  Con la concreción de este proyecto se  esperan obtener los siguientes resultados  principales:    Estado del arte referido a la  personalización en sistemas web de  ACSC mediante la aplicación de técnicas  de Inteligencia Artificial.   Modelos de estudiante para personalizar  sistemas web de ACSC.   Modelos multiagente para la  personalización de diferentes aspectos en  ACSC.   Módulos de personalización implementados en sistemas web de ACSC.  Los resultados enunciados se relacionan  de manera directa con los objetivos  específicos presentados en la sección  previa. Acorde con el cronograma de  actividades del proyecto, actualmente se  desarrollan las tareas vinculadas con la  concreción del  primero de los objetivos  específicos planteados.   Por otra parte, considerando las líneas de  investigación existentes en el medio, es  posible afirmar que el desarrollo de este  proyecto permitirá conformar un nuevo  grupo de investigación, que iniciará  oficialmente una línea en el área del ACSC.  De esta manera también se verán  consolidados los esfuerzos y la  investigación realizados con anterioridad en  esta temática, por parte de algunos de los  integrantes que integraron los proyectos  citados en la sección Contexto.     Formación de Recursos Humanos  El equipo de trabajo de este proyecto  está formado por un docente investigador  formado y cuatro docentes investigadores  en formación. También integran el equipo   dos becarios CONICET, los cuales  desarrollarán sus investigaciones y sus tesis  doctorales en el marco del proyecto.  Además, con la ejecución de este proyecto  se contribuirá a la formación en  investigación de estudiantes avanzados de  informática, por cuanto dos de ellos  desarrollarán sus Tesis Finales de Grado  como integrantes de este proyecto.    Referencias  Ayala, G.: “Towards lifelong learning environments:  Agents supporting the collaborative construction of  knowledge in virtual communities”, Proc.  International Conference on Computer Support for  Collaborative Learning, pp. 141–149 (2003).   Bull, S., Mabbott, A., Abu Issa, A.: “UMPTEEN:  Named and Anonymous Learner Model Access for  Instructors and Peers”, International Journal of  Artificial Intelligence in Education, vol. 17(3), pp.  227-253 (2007).  Chen, Z., Chou, C., Deng C., Chan, T.: “Active  Open Learner Models as Animal Companions:  Motivating Children to Learn through Interacting  with My-Pet and Our-Pet”, International Journal of  Artificial Intelligence in Education, vol. 17(2), pp.  145-167 (2007).     Costaguta, R., Amandi, A.: ""Training Collaboration  Skills to Improve Group Dynamics"". Proceedings  ACM Euro American Conference on Telematics and  Information Systems (EATIS) (2008).   Costaguta, R., Durán, E.: ""Group and students  profiles to support collaborative learning in a  multiagent model"", Proc. Second International  Conference Advances in New Technologies,  Interactive Interfaces and Communicability  (ADNTIIC). Huerta Grande, Córdoba (2011).  Costaguta, R., Durán, E.: ""Minería de Datos para  Descubrir Estilos de Aprendizaje"". Revista  Iberoamericana de Educación Nº 42/2,  Organización de Estados Iberoamericanos para la  Educación, la Ciencia y la Cultura (OEI) (2007).  Costaguta, R., Fares, R.: “Coordinación de Agentes:  Tipos, Técnicas, Modelos y Lenguajes”, Revista  Avances en Sistemas e Informática (RASI), Vol. 7  (3), pp. 33-42 (2010).  Costaguta, R., Garcia, P., Amandi, A.: “Using  Agents for Training Students Collaborative Skills”,  IEEE Latinoamerican Transactions, Vol. 9 (7), pp.  1118-1124 (2011).  Costaguta, R.: ""Habilidades de Colaboración  Manifestadas por los Estudiantes de Ciencias de la  Computación"". Revista Nuevas Propuestas, Nº 4344, pp. 55-69 (2009).  WICC 2012 91 2012 XIV Workshop de Investigadores en Ciencias de la Computación Costaguta, R.: ""Una Revisión de Desarrollos  Inteligentes para Aprendizaje Colaborativo  Soportado por Computadora"". Revista Ingeniería  Informática. Universidad de Concepción, Chile. Vol.  3, Nro. 1, pp.68-81 (2006).  Costaguta, R.: “Algunos marcos utilizables para el  desarrollo de aplicaciones colaborativas”. Revista  Nuevas Propuestas, Vol. 46, pp. 55-69 (2009).  Costaguta, R:. ""Entrenamiento de estudiantes en la  práctica de destrezas colaborativas"". VII Simposio de  Inteligencia Artificial (ASAI-JAIIO), pp. 6-10  (2010).  Durán, E., Amandi A..: “Personalised collaborative  skills for student models”, Interactive Learning  Environment. Joseph Psotka and Steve Wheeler  (eds.), Routledge, Taylor & Francis Group, vol.  19(2), pp. 143-162 (2011).  Durán, E., Costaguta, R., Farías, R., Trejo, M.,  Torales, F., Ozán, V., Martínez, P.:  ""Personalización,  en Sistemas de Enseñanza  Virtual"", Anales XI Workshop de Investigadores en  Ciencias de la Computación, Universidad Nacional  de San Juan. (2009).  Durán, E., Costaguta, R., Maldonado, M., Únzaga,  S., Menini, M., Chequer, G., Fernandez, N., Missio,  D.: “Técnicas de Aprendizaje de Máquina y  Personalización en Educación”, Anales XII  Workshop de Investigadores en Ciencias de la  Computación, Universidad Nacional de la Patagonia  San Juan Bosco, pp. 704-710 (2010).  Durán, E., Costaguta, R., Maldonado, M., Unzaga,  S.: ""Nuevos desarrollos para sistemas adaptativos  inteligentes"", Anales XI Workshop de Investigadores  en Ciencias de la Computación, Universidad  Nacional de San Juan (2009).  Durán, E., Costaguta, R., Maldonado, M., Únzaga,  S.: ""Sistemas Adaptativos Inteligentes"", Anales IX  Workshop de Investigadores en Ciencias de la  Computación. Zulema Rosanigo et al., (eds).  Universidad Nacional de la Patagonia San Juan  Bosco (2007).  Durán, E., Costaguta, R: ""Una experiencia de  enseñanza de la Simulación adaptada al Estilo de  Aprendizaje de los Estudiantes"". Revista  Internacional Formación Universitaria, Año 1,  Edición 1, pp. 19-28 (2008).   Fares, R., Costaguta, R.: ""A multi-agent model that  promotes team-role balance in Computer Supported  Collaborative Learning"", Proc. Second International  Conference Advances in New Technologies,  Interactive Interfaces and Communicability  (ADNTIIC). Huerta Grande, Córdoba (2011).  Fares, R., Costaguta, R.: “Modelo multiagente que  propicia el equilibrio de roles en Aprendizaje  Colaborativo Soportado por Computadora”, XII  Simposio de Inteligencia Artificial (ASAI- JAIIO),  Córdoba (2011).  Gonzalez, C., Burguillo, J., Llamas, M.: “A  qualitative Comparison of Techniques for Student  Modelling in Intelligent Tutoring Systems”, Proc.  36th ASEE/IEEE Frontiers in Education  Conference.. San Diego, USA (2006).  Maes, P.: “Agents that reduce work and information  overload”, Communication of the ACM, vol. 37 (7)  (1994)  Maisonneuve, J.: La dinámica de los grupos (11va  Ed.). Nueva Visión, Argentina (1998)  Menini, M., Unzaga, S., Chequer, G.: “La minería  de datos aplicada en entornos de E-learning”, Anales  VII Jornadas de Ciencia y Tecnología de Facultades  de Ingeniería del NOA. San Fernando del Valle de  Catamarca (2011).  Monteserín, A., Schiaffino, S., García, P., Amandi,  A.: “Análisis de la formación de grupos en  Aprendizaje Colaborativo Soportado por  Computadoras”. Proc: XXI Simpósio Brasileiro de  Informática na Educação (SBIE). Brasil (2010)  Nwana, H.: “Software Agents: An Overview”,  Knowledge Engineering Review, vol. 11(3) (1996)  Ozán, V., Costaguta, R.: “Descubrimiento de  habilidades de colaboración vinculadas con roles en  grupos de aprendizaje”, XI Simposio de Inteligencia  Artificial (ASAI-JAIIO). Buenos Aires (2010).  Paiva, A.: “Learner Modelling for Collaborative  Learning Environments”, du Boulay and Mizoguchi  (Eds.). Proc. Artificial Intelligence in Education.  Japón (1997).  Peña, A.: “Collaborative Student Modeling by  Cognitive Maps”, Proc. First International  Conference on Distributed Frameworks for  Multimedia Applications. IEEE Press (2005).  Santana Mansilla, P., Costaguta, R., Missio, D.:  “Clasificación de habilidades de e-tutores en  Aprendizaje Colaborativo Soportado por  Computadora”,  Segundo Congreso Internacional en  Educación en Ciencias y Tecnología. San Fernando  del Valle de Catamarca (2011).  Vassileva, J., McCalla, G., Greer, J.: “Multi-Agent  Multi-User Modeling in I-Help”. User Modelling  and User Adapted Interaction, vol. 13(1-2), pp. 179210 (2003).  Vizcaíno, A.: “A Simulated Student Can Improve  Collaborative Learning”, International Journal of  Artificial Intelligence, vol. 15, pp 3-40 (2005).    WICC 2012 92 2012 XIV Workshop de Investigadores en Ciencias de la Computación"	﻿aprendizaje colaborativo soportado por computadora , técnicas de aprendizaje de máquina , modelo de estudiante , agentes de software , personalización	es	18387
4	Revisión de creencias argumentación confianza y reputación en Sistemas Multi agentes	﻿ Esta línea de investigación se enfoca en mejorar las capacidades de razonamiento de agentes que participan en Sistemas MultiAgente (SMA). Su objetivo general es desarrollar técnicas de razonamiento avanzadas, combinando revisión de creencias, argumentación y mecanismos de confianza y reputación para su aplicación en SMA. En particular, se espera poder integrar y combinar los avances producidos en las áreas de revisión de creencias, teoría de argumentación y mecanismos de confianza y reputación. Dicha integración permitirá realizar un avance en las tres áreas de investigación, y además, proveerá de técnicas avanzadas aplicables a los modelos de razonamiento de agentes inteligentes y sistemas multi-agente para entornos dinámicos. Palabras Clave: Argumentación, Revisión de Creencias, Confianza y Reputación, Sistemas Multi-Agente. 2. Contexto Esta línea de investigación se realizará dentro del ámbito del Laboratorio de Investigación y Desarrollo en Inteligencia Artificial (LIDIA) del Departamento de Ciencias e Ingeniería de la Computación de la Universidad Nacional del Sur. Está asociada con el proyecto de investigación: “Formalismos Argumentativos aplicados a Sistemas Inteligentes para Toma de Decisiones”. Código: PGI 24/ZN18 financiado por la Universidad Nacional del Sur, y con el Proyecto de Investigación Plurianual, (PIP): “Sistemas de Apoyo a la Decisión Basados en Argumentación: formalización y aplicaciones”, financiado por CONICET. Algunos de los resultados obtenidos fueron desarrollados durante la ejecución del proyecto “Combinando Revisión de Creencias y Argumentación para mejorar las capacidades de razonamiento de agentes en SMA”, Proyecto Binacional DA/09/06 (Argentina-Alemania). Programa de Cooperación Científico-Tecnológica entre el Ministerio de Ciencia, Tecnología e Innovación Productiva de la República Argentina (MINCYT) y el Deutscher Akademischer Austausch Dienst (DAAD) de Alemania. Período: 1/1/2010 al 31/12/2011. Financiamiento MINCYT y DAAD. 3. Introducción Los sistemas de argumentación resultan útiles para toma de decisiones, tales deliberaciones pueden modelarse de una forma dialéctica usando argumentos a favor y en contra de 1 WICC 2012 122 2012 XIV Workshop de Investigadores en Ciencias de la Computación cada opción [13, 17, 7, 23, 6]. Dado que el razonamiento y la toma de decisiones son componentes centrales dentro de una arquitectura de un agente, nuestra línea de investigación pretende establecer vínculos estrechos entre estos dos componentes para poder mejorar la coherencia dentro de un modelo de agentes. El uso de credibilidad de informantes en dinámica de creencias es algo novedoso [25]. Este formalismo puede ser usado como punto de partida para combinar revisión de creencias y mecanismos de confianza y reputación en SMA. En [24], Sabater y Sierra sostienen que la importancia de la confianza y reputación en sociedades humanas está fuera de discusión, por lo cual no es sorprendente que varias disciplinas, cada una desde una perspectiva diferente, haya estudiado y utilizado ambos conceptos. La investigación científica en el área de mecanismos computacionales de confianza y reputación en sociedades virtuales, es una disciplina reciente orientada a incrementar la fiabilidad y performance de comunidades electrónicas. En artículos recientes como [12, 24, 11, 3, 5] podemos notar que en ciencias de la computación hay dos elementos que han contribuido sustancialmente a incrementar el interés en confianza y reputación: el paradigma de SMA y la evolución creciente del e-commerce. El estudio de confianza y reputación tiene muchas aplicaciones en tecnologías de Comunicación e Información. Estos sistemas han sido reconocidos como factores claves para la adopción del comercio electrónico. Los mismos son usados por agentes de software inteligentes como un mecanismo para buscar compañeros confiables y como un incentivo en la toma de decisiones acerca de si se tiene en cuenta un contrato. La reputación es usada en el mercado electrónico como un mecanismo para evitar fraudes y estafas [11, 3]. Los e-markets no son el único campo de aplicación; por ejemplo, en [5] usan la confianza para mejorar la performance de mecanismos de revisión de creencias. En el contexto de SMA, un agente puede a menudo recibir información a través de otro que, por lo general, llamamos informante. Estos informantes son agentes independientes quienes tiene sus propios intereses y, por lo tanto, no son completamente fiables. Es natural para un agente creerle más a un informante sobre otro. Es por esto que en algunos trabajos se ha propuesto la organización de los informantes en un orden parcial que compara la plausibilidad de los mismos. En esta investigación se pretende combinar formalismos de revisión de creencias, argumentación y actualización de conocimiento con técnicas de mantenimiento de confianza y reputación de agentes en un ambiente distribuido. Desde el punto de vista teórico, elaborar tales conexiones entre argumentación, revisión de creencias y mecanismos de confianza y reputación, y poder aplicarlas a los SMA, permitirá un progreso científico substancial en el campo de la representación de conocimiento [22, 4, 19]. El estudio de los vínculos y relaciones entre revisión de creencias y argumentación ha comenzado hace muy poco. Aunque ya existen en la literatura comentarios [9, 16] de que ambas áreas podrían combinarse con buenos resultados, el tema carece de resultados importantes. Por lo tanto, con la línea de investigación propuesta se esperan obtener resultados relevantes en el área de razonamiento y representación de conocimiento. Por ejemplo, las técnicas de revisión de creencias proveen a los agentes la capacidad de corregir sus creencias cuando existe una discrepancia con la información nueva que pueden obtener de su entorno [2, 8, 10, 20]. Uno de los resultados esperados es mejorar los sistemas de revisión de creencias utilizando argumentación y mecanismos de confianza y reputación para evaluar si es importante que la nueva información obtenida sea incorporada o no a las creencias del agente. 4. Líneas de Investigación y Desarrollo Esta linea de investigación toma como punto de partida publicaciones obtenidas en el proyecto de investigación binacional mencionaWICC 2012 123 2012 XIV Workshop de Investigadores en Ciencias de la Computación do en el contexto, en el cual participaron todos los autores del presente trabajo [21, 14, 15, 18]. La línea principal de investigación presentada en este trabajo, busca mejorar las capacidades de razonamiento de agentes que participan en SMA a través de la integración de mecanismos argumentativos, de revisión de creencias y mecanismos de confianza y reputación. Su objetivo es desarrollar formalismos avanzados de representación de conocimiento y razonamiento basados en una combinación entre argumentación, revisión de creencias y mecanismos de confianza y reputación. En particular, de esta línea principal se desprenden varias sub-líneas de investigación que describimos a continuación. Desarrollar métodos para equipar a los sistemas de argumentación con la posibilidad de tener en cuenta cambios dinámicos y los mecanismos de confianza y reputación; integrando, de esta manera, técnicas de revisión de creencias a la argumentación rebatible y mecanismos de confianza y reputación. Esto permitirá obtener un formalismo que puede ser utilizado para aplicaciones prácticas y permitirá realizar argumentación en entornos dinámicos utilizando los mecanismos de confianza y reputación para especificar diferentes relaciones de preferencias entre argumentos. Esto ayudará a obtener con mayor precisión argumentos garantizados. Desarrollar un sistema formal que permita describir postulados y propiedades deseables. Este formalismo permitirá establecer una forma de catalogar y evaluar propuestas de argumentación para entornos dinámicos. En particular nos permitirá evaluar la propuesta enunciada anteriormente y compararla con otras propuestas diferentes. Mejorar los sistemas de revisión de creencias utilizando argumentación para evaluar información nueva que es obtenida, y utilizar argumentos en el proceso de revisión haciendo uso de las medidas de confianza y reputación en la toma de decisiones. Esto incluye el estudio de la relación de entre “em epistemic entrechment” y “defeat” como los conceptos que guien los procesos de revisión y argumentación. La confianza y la reputación son propiedades dependientes del contexto. Un modelo de confianza y reputación que administra un único contexto es diseñado para asociar un único valor de confianza o reputación por compañero sin tomar en consideración el contexto. Sin embargo, un modelo de multi-contexto tiene los mecanismos para tratar con varios contextos, manteniendo diferentes valores de confianza y reputación asociados a estos contextos para un único compañero (como lo hacen AbdulRahman y Hailes en [1]). Considerando que este último aspecto es abordado por pocos autores en la literatura, otro de los objetivos de esta investigación consiste en extendernos hacia formalismos multi-contexto trabajando en argumentación basada en contextos determinados por los grados de confianza y reputación que los agentes tienen de sus pares en diferentes areas. La revisión de creencias en un entorno distribuido y con múltiples agentes heterogéneos podría mejorarse usando argumentación y mecanismos de confianza y reputación. En este caso, la revisión y argumentación no es llevada a cabo por un agente, sino por un grupo de agentes. Por lo tanto, esta línea estudiará como varios agentes pueden cambiar de opinión si hay un intercambio de argumentos. En particular, la revisión de creencias en SMA puede ser extendida con técnicas de argumentación y mecanismos de confianza y reputación tanto a nivel teórico como práctico. 5. Resultados y Objetivos Los resultados alcanzados hasta el momento involucran principalmente a métodos para equipar a los sistemas de argumentación con la posibilidad de tener en cuenta cambios dinámicos, aunque parte del desarrollo de un sistema formal que permita describir postulados y propiedades deseables esté en proceso actualmente al igual que la mejora de los sistemas de revisión de creencias utilizando argumentación para evaluar información nueva. Algunos de los resultados obtenidos hasta el momento relativos a estos desarrollos han sido publicados WICC 2012 124 2012 XIV Workshop de Investigadores en Ciencias de la Computación en [14, 15, 18]. Entre los resultados específicos de los desarrollos enunciados con anterioridad se encuentra la definición de operadores de contracción para modelar la dinámica de los programas lógicos rebatibles (DeLP) [17]. DeLP es un formalismo que combina argumentación rebatible y programación en lógica; y este operador de contracción está definido siguiendo trabajos de operadores de contracción para claúsulas Horn. En particular, se estudió el problema de contraer literales en un programa DeLP. Se desarrollaron diferentes nociones de contracción en base a las diferentes formas de deducción implícitas en un proceso argumentativo y la influencia que pueden exhibir los literales en el proceso de razonamiento. Para estos operadores de contracción se han presentado postulados de racionalidad basados en los ampliamente aceptados en el área de revisión de creencias [2]. Además, se estudió un operador de revisión no priorizada que utiliza argumentación para decidir que parte de la nueva información será utilizada para revisar. En este contexto, conocido como revisión selectiva, hay dos pasos: primero aplicar una función de transformación para decidir que parte de la entrada utilizar y luego incorporar el resultado de la primera utilizando un operador de revisión priorizada. Por lo tanto, se implementó una transformación que emplea argumentación deductiva para evaluar el peso de la nueva información. Hemos probado que el operador implementado satisface propiedades deseables en revisión selectiva. Actualmente estamos incorporando a este operador medidas de confianza basada en la credibilidad que tiene un agente acerca de sus informates. Con esto se busca mejorar la capacidad de los agentes a la hora de tomar una decisión. Además, se trabajó en la confección de otros dos artículos que vinculan argumentación y revisión de creencias, proponen nuevas líneas de investigación y realizan un análisis del estado del arte de estas dos áreas de trabajo [14, 15]. Respecto a los objetivos en curso de esta investigación, se dará una caracterización formal completa de los mecanismos creados en los desarrollos planteados. Además, se comenzará con el estudio y desarrollo de un modelo de revisión de creencias distribuido para SMA combinado con argumentación y mecanismos de confianza y reputación. Este objetivo está relacionado con la revisión de creencias en un entorno distribuido, donde el procesos de revisión con múltiples agentes heterogéneos podrían mejorarse usando argumentación y valores de confianza. En este caso, la revisión y argumentación no son llevadas a cabo por un agente, sino por un grupo de agentes.	﻿Sistemas Multi Agente , argumentación , revisión de creencias , confianza y reputación	es	18394
5	Tres pilares para la implantación de sistemas	﻿Para el éxito de un proyecto de software, una de las etapas relevantes es el proceso de implantación del mismo. Desde su surgimiento, la ingeniería del software ha evolucionado abordando diferentes áreas para la mejora de la especialidad. Sin embargo la implantación de sistemas  ha sido un tema que si bien se lo considera importante, no ha sido abordado metodológicamente como etapa del desarrollo de software. Por esta razón, se considera necesario que dicha etapa debe ser definida específicamente como así también sus principios básicos y sus procesos de manera de abordarla como un área dentro de la ingeniería de Software.  En el presente artículo, se expone una investigación cuyo objetivo es comenzar a enmarcar la Implantación de Sistemas como parte constitutiva del proceso software y determinar los pilares de esta. Palabras Clave: Implantación de Sistemas, Proceso Software.  CONTEXTO El Grupo de Ingeniería de Software “G.I.S.” lleva varios años trabajando en la investigación del proceso de Implantación de sistemas como una etapa de capital importancia dentro de la ingeniería de software. El objetivo principal es la sistematización de las actividades a desarrollar dentro de esta etapa. La línea de investigación que se plantea en este artículo, está constituida como una línea de transferencia de tecnología al sector, basada en el desarrollo de una propuesta metodológica para abordar la Implantación de los sistemas. INTRODUCCION En la actualidad existen diversos modelos de proceso que detallan las diferentes actividades  que deben llevarse a cabo para los proyectos de tecnología de la información, dividiéndolas en procesos y subproceso.  Los modelos más utilizados por la industria del software y servicios informáticos, son los específicos para software como el Estándar IEEE 1074 “Standard for Developing Software Life Cycle Processes” [1] y el ISO 12207 “Standard for Information Technology - Software Life Cycle Processes [2], así como el modelo integrado de ingeniería de software e ingeniería de sistemas CMMI “Capability Maturity Model Integration” [3]. Por otro lado el RUP “Proceso unificado de Software” es la metodología más utilizada en análisis e implementación de sistemas orientados a objetos.  En cuanto a  la estandarización de actividades de gestión de proyectos, la guía de PMBOK “A Guide to the Project Management Body of Knowledge”[4], define el conjunto de actividades desde una perspectiva más genérica, aplicable en diferentes tipos de proyectos. El ITIL foundation Handbook [5] trabaja sobre las actividades para la gestión del servicio y por último, el SWBOK presenta una guía al conocimiento presente en Ingeniería de Software. El estándar de IEEE 1074 [1] el proceso del software se compone de 17 grupos de actividades, cada uno de los cuales contiene actividades que son responsables de satisfacer sus requisitos asociados. El proceso se compone de un total de 65 actividades. IEEE 1074 engloba las siguientes familias de actividades: Gestión del Proyecto; Actividades Orientadas al desarrollo y Actividades integrales.  En el grupo de actividades orientadas al desarrollo, se diferencian las de Pre-desarrollo, Desarrollo en sí mismas y post-desarrollo. En este último grupo de actividades de post-desarrollo, el estándar define el proceso de Instalación, definida como el transporte y la  WICC 2012 621 2012 XIV Workshop de Investigadores en Ciencias de la Computación instalación de un sistema de software desde el entorno de desarrollo al entorno de destino. Incluye la carga –si es necesario- de la base de datos, las modificaciones necesarias del software, las comprobaciones en el entorno de destino y la aceptación del cliente. Si durante la instalación surge algún problema, se identifica e informa acerca de él. [1] En cuanto al proceso software según el estándar ISO/IEC 12207-1995[2], divide las actividades del ciclo de vida del software en tres procesos primarios, ocho procesos de soporte y cuatro procesos organizativos. Los procesos primarios son: Procesos principales del ciclo de vida; procesos de soporte del ciclo de vida, Procesos organizativos del ciclo de vida. No obstante esta separación, no incluye explícitamente un proceso que defina la puesta en marcha o implantación de software. La estructura del modelo CMMI [3], define un conjunto de 22 áreas de proceso agrupadas en 4 categorías: Gestión de Procesos, Gestión de Proyectos, Ingeniería y Soporte. Este modelo, trata los temas relacionados con la puesta en marcha del proyecto en forma separada en las áreas de Gestión de Proyectos y como requerimientos en la categoría de Ingeniería. El RUP propone un proceso iterativo e incremental dividido en cuatro fases, inicio, elaboración, construcción y transición. Por otro lado desarrolla flujos de trabajo del proceso -los que están asociados a la construcción propiamente dicha del software- que son: modelado del negocio, requisitos, análisis y diseño, implementación, pruebas y despliegue, y los flujos de trabajo de soporte que son: gestión del cambio y configuraciones, gestión de proyecto y entorno. La fase de transición tiene como propósito la entrega del producto de software a la comunidad de usuarios [6]. Respecto a la gestión de proyectos en general, la guía PMBOK [4] provee fundamentos para la gestión de proyectos, aplicables a un amplio rango de proyectos, generalmente aceptados como las mejores prácticas para la gestión.  El PMBOK reconoce 5 grupos de procesos básicos y 9 áreas de conocimiento comunes a casi todos los proyectos. Los grupos de procesos básicos incluyen los de Iniciación, Planificación, Ejecución, Seguimiento y Control, y Cierre. En esta guía, los procesos se traslapan e interactúan a través de un proyecto o fase. Se  describen en términos de entradas (documentos, planes, diseños, etc.), herramientas y técnicas (mecanismos aplicados a las entradas), y salidas (documentos, productos, etc.). Dos de sus capítulos presentan una guía básica de prácticas acerca de cómo debe gestionar los recursos humanos y las comunicaciones dentro de un proyecto.  Al analizar el ITIL, Information Technology Infrastructure Library, [5] se observa  que consta de 5 libros basados en el ciclo de vida del servicio. Estos son: Estrategia del Servicio, Diseño del Servicio,  Transición del Servicio,  Operación del Servicio y  Mejora Continua del Servicio. A su vez, la transición del servicio esta formado por 7 subprocesos. Estos son: Planificación y soporte a la transición, Gestión de Cambios, Gestión de la configuración y activos del servicio, Gestión de entregas y despliegues, Validación y pruebas, Evaluación y Gestión del conocimiento. En cuanto al SWBOK, está formado por 10 áreas de conocimiento. Esta son: Requisitos de Software, Diseño de Software, Construcción de Software, Pruebas de Software, Mantenimiento de Software, Gestión de configuración, Gestión de la Ingeniería de Software, Proceso de Ingeniería de Software, Herramientas y métodos de la Ingeniería de Software y Calidad del Software. El Proceso de Ingeniería del Software se ocupa principalmente de la implementación, evaluación, medición, gestión, cambio y mejora de los procesos del ciclo de vida. [7] El conjunto de modelos y estándares existentes, ordenan de manera prescriptiva al conjunto de actividades esenciales, no ordenadas en el tiempo, que deben llevarse a cabo para un correcto desarrollo de proyectos de construcción, adaptación y o mantenimiento de software.  Sin embargo, estos modelos, no dan cuenta de manera explícita a la definición de actividades esenciales que deben realizarse para una correcta puesta en marcha de los software que se desarrollan, adaptan y/o mantienen, entendiendo a ésta como la implantación de un sistema en el contexto específico de su uso, que requiere de un conjunto de actividades que aborden las tareas específicas referidas a la infraestructura tecnológica, a las particularidades de la implantación del  WICC 2012 622 2012 XIV Workshop de Investigadores en Ciencias de la Computación producto, así como a los recursos humanos involucrados en el cambio tecnológico que será implantado. LINEAS DE INVESTIGACION y DESARROLLO La línea principal de la presente investigación consiste en definir específicamente que se entiende por Implantación de Sistemas o Implantación de software, para poder de modo tal que permita trabajar sobre el área como un dominio específico y elaborar una propuesta metodológica que incluya las mejores prácticas para la gestión de la Implantación de los proyectos de TI que contenga recomendaciones aplicables a las diferentes etapas del proyecto en lo que respecta a esa fase de un proyecto. Por lo tanto, se trabaja en la idea de conformar una Ingeniería de Implantación que pueda modelar diferentes metodologías, de acuerdo con los diferentes contextos sociales y tecnológicos de Implantación. En este contexto, se ha detectado que la definición de el área específica no es lo suficientemente clara para los diferentes involucrados, ni aparecen claramente definidos los límites de su incumbencia. [8] RESULTADOS Y OBJETIVOS La línea principal de la presente investigación consiste en explorar sobre las dificultades y falta de sistematización en la Implantación de Sistemas o Implantación de software, de modo tal que permita elaborar una propuesta metodológica que incluya las mejores prácticas para la gestión en la puesta en marcha de los proyectos software.  Con el fin de elaborar un marco metodológico que permita incorporar buenas prácticas, se ha llevado un trabajo exploratorio, de modo tal que permita detectar las áreas conflictivas e irresueltas por los modelos vigentes.  De éste trabajo se infiere que la implantación no está claramente definida y que sus límites son vagos.[9] De las respuestas obtenidas, los aspectos relevantes que se presentaron, han sido clasificados en tres grupos: problemas de  producto software ó metodológicos, problemas de infraestructura y problemas de RRHH. Los elementos que estarían faltando sobre los productos Software y el uso de metodologías, las respuestas más relevantes son: “Necesidad de profundizar en la aplicación de actividades durante la etapa de implementación.” “Necesidad de documentar procedimientos adecuados y un alcance concreto.” “Escasa definición de procedimientos de implementación con un detalle de las actividades a realizar.” “Falta de más y mejores métodos para definir dichas actividades.” “Falta de organización y documentación de cada actividad.”  Los elementos que estarían ausentes respecto a la Infraestructura son: “Falta de análisis de la infraestructura con la que cuenta el cliente.” “Falta de madurez en las organizaciones sobre la puesta en marcha.” Los elementos que estarían faltando respecto a los Recursos Humanos son: “Insuficiente tiempo y recursos humanos para la puesta en marcha.”  “Falta de definición de planes de pruebas completos.”  “Falta de comunicación con los clientes para la puesta en marcha.”  Con éstos resultados se está avanzando en determinar los límites del proceso de implantación de sistemas así como  realizar un relevamiento de las diferentes prácticas y técnicas específicas que se utilizan para la implantación en los proyectos de TI definiendo el alcance y las limitaciones de cada una de ellas, focalizadas sobre los aspectos de los recursos humanos involucrados, los atributos de los productos software a implantar, así como la evaluación sistemática y ordenada de la infraestructura tecnológica en la cual se implantarán dichos productos.     WICC 2012 623 2012 XIV Workshop de Investigadores en Ciencias de la Computación El siguiente gráfico muestra el esquema de trabajo:   Esquema de trabajo. Fuente: elaboración propia En la definición del modelo específico se ha avanzado en considerar la separación del mismo en tres áreas de proceso. Estas áreas deben estar estrechamente vinculadas, pero conceptualmente separadas, para trabajar sobre los problemas de la puesta en marcha de los sistemas de software.  En tal sentido, se ha considerado delinear un modelo de implantación con 3 áreas básicas. La primera de éstas, está relacionada con las personas involucradas, la segunda está relacionada con el producto a ser desplegado y la tercera con el ambiente y contexto en el cual el software funcionará. Estas áreas están profundamente interrelacionadas y serán denominadas en este artículo como Recursos Humanos, Producto e Infraestructura respectivamente. El área de producto se define por la inclusión de todos los artefactos que forman parte del nuevo producto, sean estos de Hardware, Software de base ó Software aplicativo desarrollado por el equipo responsable de la puesta en marcha o adoptado para su despliegue. En el área de infraestructura se definen todos los elementos previamente existentes al despliegue del nuevo producto. En términos de Gestión de la Configuración serán todos los ítems de configuración preexistentes en la Base de Datos de Gestión de la Configuración. Respecto al área de Recursos Humanos las prácticas deben estar relacionadas con las  competencias requeridas, el compromiso con el proyecto, las expectativas con el cambio y  la capacitación de los actores involucrados, entre otros.  Cada área se define por el enfoque que debe adoptar, así como los roles que debe definir para la correcta realización, en tanto que incluya un conjunto de componentes. Los siguientes son ejemplos de componentes del área de producto: - Definición adecuada del plan de pruebas, incluyendo las condiciones y casos de prueba, el entorno de pruebas y una adecuada gestión de la configuración. - Definición de los requisitos de producto, referido a la infraestructura en la cual será desplegado y a los requisitos de performance, tiempos de respuesta y capacidad. - Carga correcta de la base de datos, incluyendo las cargas iniciales y migraciones de bases si corresponde. En cuanto al área de infraestructura, estos son: - Planeamiento de capacidad previo para asegurar que no son necesarias inversiones adicionales. - Adecuada gestión de la configuración que garantice que los ítems de configuración modificados son compatibles con los existentes en producción.  WICC 2012 624 2012 XIV Workshop de Investigadores en Ciencias de la Computación - Definición de los planes de recuperación necesarios. - Gestión del nivel de servicio. Esto incluye definir métricas  y contar con éstas previamente a la implantación para tener valores de referencia. En el  área de Recursos Humanos son: - Definición de roles y competencias requeridas. - Tratamiento de las expectativas del cambio en los usuarios. - Inclusión de los usuarios en la puesta en marcha. - Capacitación de los usuarios y los involucrados en general. - Inclusión del equipo de desarrollo en la puesta en marcha. La siguiente etapa del proyecto tiene como objetivo completa la definición del proceso de implantación de sistemas. La última etapa del proyecto se propone realizar una validación del modelo propuesto, con la definición de buenas prácticas a partir del relevamiento de casos de estudios que permitan analizar y evaluar de la propuesta metodológica y los casos de éxito en la puesta en marcha de los proyectos de TI. FORMACION DE RECURSOS HUMANOS El Grupo de Ingeniería de Software (G.I.S.) es un grupo ínter universidad, en esta línea de investigación que aborda, en la Universidad Caece, se están desarrollando dos tesis de Maestría y se han incorporado alumnos avanzados de la carrera de sistemas.	﻿implantación de sistemas , proceso de software	es	19092
6	Computación distribuida para seguridad informática CoDiSe	﻿ Tanto en el ámbito académico como en el  comercial, la utilización de equipamiento de  alto rendimiento para el cómputo de  algoritmos complejos con alta demanda  computacional es poco accesible dado su alto  costo. Sin embargo, dada la dependencia  tecnológica creciente, este tipo de cómputos  es cada vez más necesaria.  En este panorama, la incorporación de  tecnologías de computación de alto  rendimiento a bajo costo, por ejemplo la  computación voluntaria, surge como una  opción interesante, especialmente por parte de  organizaciones que cuentan con grandes  cantidades de puestos de trabajo que  presentan capacidad de cómputo ociosa.  En particular, cuando se experimenta en el  ámbito de la seguridad informática poseer  grandes capacidades de procesamiento  disminuyen los tiempos de cómputo de  manera oportuna incrementando así la  posibilidad de realizar pruebas y validaciones.  El tratar continuamente con grandes números,  hace que el campo de la seguridad de la  información en general, y la criptografía en  particular, se encuentre bajo constante  demanda de poder de cálculo.  En CoDiSe se plantea la utilización de un  sistema distribuido, utilizando un framework  del tipo Computación en Grilla de Escritorio  (Grid Desktop Computing – GDC), que  explotará la capacidad ociosa de la  infraestructura informática disponible dentro  de la organización, conformando una gran  máquina de alto rendimiento fundamental en  proyectos de investigación científica [1] en  general, y en criptografía en particular.    Palabras Clave: Computación Distribuida,  Seguridad, Computación en Grilla de  Escritorio, Factorización de Grandes  Números.    CONTEXTO  La arquitectura de GDC [2], es una forma  particular de computación distribuida que ha  atraído la atención tanto de los ejércitos alrededor del mundo como de los distintos institutos científicos debido al gran poder de  cómputo que es posible extraer de ellos. Esta  arquitectura es utilizada en áreas de interés  tan críticas como las del criptoanálisis, toma  de decisiones, análisis de imágenes por software, entre otros.   El Ejército Argentino, a través del Centro  de Investigación y Desarrollo de Software del  Ejército Argentino (CIDESO), implementó  estos conceptos escalándolos y llevándolos a  la practica en la forma de Sistemas de Simulación Distribuidos, en problemas para los  cuales era necesario poseer gran poder de  cálculo [3] [4].  Por otro lado, en el ámbito académico, la  Escuela Superior Técnica del Ejército Argentino (EST), posee un laboratorio de investigación en temas de criptografía con amplia experiencia en el desarrollo científico y  evaluación de algoritmos para la seguridad de  la información, el CriptoLab.   Dentro de las investigaciones llevadas a  cabo por el CriptoLab, la de identificar y evaluar algoritmos de factorización de grandes  números - pilar fundamental de la seguridad  de la información-, requería de gran poder de  cálculo, disponible sólo a través de la utilización de computación paralela y/o distribuida.  Dada la estrecha vinculación entre el  CIDESO y el CriptoLab, surgió la posibilidad  WICC 2012 678 2012 XIV Workshop de Investigadores en Ciencias de la Computación de realizar investigaciones en conjunto, aportando las experticias de cada uno, logrando  una sinergia en la investigación.  Así, CoDiSe nace como un proyecto de investigación conjunto entre las dos organizaciones para poder ejecutar procesos que requieren gran poder de cómputo en el ámbito  de la EST, logrando la sinergia entre ambas  líneas de investigación.     1. INTRODUCCIÓN  El concepto de computación distribuida es  usado para solucionar problemas de carácter  científico desde hace varios años. Sin embargo, cuando se hacer referencia a computación  de alto rendimiento, surgen inmediatamente  arquitecturas paralelas o distribuidas que acarrean altos costos, tanto en infraestructura  como en la formación de recursos humanos.  Aplicando el concepto de GDC, asociado a lo  que se denomina computación voluntaria, utilizando computadores personales para generar  clústeres de procesamiento que funcionan  como una solo equipo, estos costes disminuyen rápidamente otorgando a las organizaciones una ventaja estratégica para procesar sus  funciones de negocio [5]. Dentro de las arquitecturas de GDC, BOINCa es la referencia  más conocida y utilizada a nivel mundial.  Proyectos como SETI @Home o Einstein  @Home [6] la avalan, demostrando su gran  capacidad y estabilidad.   Como se ha señalado anteriormente, el  CIDESO posee experiencia en las tecnologías  de GDC [3] [4], la cual probó ser invaluable  para la mejora de rendimiento en algoritmos  de simulación que requerían cómputos  intensivos dentro de sistema Batalla Virtual,  logrando mejoras en la velocidad de cómputo  de más del 400%.  Teniendo en cuenta este logro, y el hecho  que organismos tales como compañías,  centros de investigación y universidades  tienen una participación fundamental en la  validación de las técnicas y tecnologías  existentes, así como en el desarrollo de  nuevos y más seguros métodos, las                                                  a  http://boinc.berkeley.edu/  tecnologías señaladas cobran una relevancia  critica para estos mismos.  Por otro lado, el problema de la  factorización de grandes números en el  ámbito de las investigaciones criptográficas es  uno de los más complejos desde el punto de  vista del poder computacional asociado que se  requiere. La criptografía actual se basa,  justamente, en la complejidad computacional  de dicho problema. Los números primos  asociados a las claves criptográficas son pilar  fundamental de la seguridad de la  información a nivel mundial. Así, pues, un  error, colisión o cualquier otra debilidad  detectada en la generación o utilización de  ellos representa un grave incidente de  seguridad.  Poder evaluar la calidad de los números  primo generados por las librerías de seguridad  otorga una herramienta de alto valor agregado  a nivel científico y operativo. Sin embargo, la  evaluación de estos números requiere la  ejecución de algoritmos con altas demandas  de cómputo.  CoDiSe busca utilizar las tecnologías de  GDC como una herramienta eficaz en el  ámbito académico con la cual poder realizar  estudios, experimentos y validaciones de las  diferentes técnicas y tecnologías asociadas a  la seguridad informática en general y a la  criptografía en particular. Idea que tuvo  origen conjunto entre los investigadores del   equipo de Investigación Aplicada y  Desarrollo Experimental del CIDESO y los  investigadores del CriptoLab.  Durante el segundo semestre del 2011 se  desarrolló un prototipo exploratorio con resultados positivos. Esta línea de investigación  pretende profundizar los resultados alcanzados y mejorar su aplicación.  La experimentación durante el año 2011  consistió en la factorización del valor N, de  los sistemas de clave RSA [7] generados por  la librería OpenSSLb a través del uso de las  claves públicas y privadas, obteniendo los  primos generadores. Dicha factorización ha  sido estudiada en varios trabajos de criptografía y, por más que se posea el par de claves                                                  b  http://www.openssl.org/  WICC 2012 679 2012 XIV Workshop de Investigadores en Ciencias de la Computación correspondientes al N dado, los algoritmos  requieren gran poder de cálculo para lograr la  obtención de lo primos.  El desenmascarar los primos generados por  las librerías, otorga a los especialistas en criptografía una herramienta potente que permite  evaluar potenciales colisiones que se traducen  en debilidades de los sistemas criptográficos.  Para la factorización de N se evaluaron dos  algoritmos, aprovechando la infraestructura  para lograr un benchmark entre ambos. Los  algoritmos evaluados fueron el de Menezes  [8] y otro ideado por los investigadores del  CriptoLab.  Los resultados preliminares mostraron una  mejora de rendimiento cercana al 150% (ciento cincuenta por ciento) en los tiempos de obtención de claves utilizando una infraestructura de GDC en base a la arquitectura BOINC  [9], con sólo cinco participantes en la grilla,  contra su versión standalone. Adicionalmente, se probó de manera fehaciente una ventaja  de rendimiento importante del algoritmo  “CriptoLab” por sobre el Menezes, en una  proporción mayor a 1:1000 para N de 512  bits.  Dados estos resultados, se procederá a la  formalización de la arquitectura, extendiendo  el uso de GDC a la mayor cantidad de equipos  disponibles en el ámbito de la universidad,  para ser utilizados tanto por el CriptoLab como por cualquier otro laboratorio que requiera  procesar grandes niveles de información.    2. LÍNEAS DE INVESTIGACIÓN Y  DESARROLLO  Lo que propone CoDiSe es la utilización de  la capacidad de cómputo ociosa de la infraestructura disponible, para aportar:  • Gran capacidad de cómputo a bajo  costo.  • Aprovechar la capacidad ociosa de la  infraestructura informática a través del  aporte voluntario, donde cada PC en el sistema ofrecerá los recursos que tenga disponibles en ese momento preciso.  • Utilizar lo antes mencionado para reducir drásticamente los tiempos empleados  tanto en el análisis de robustez de claves  como en el de las librerías de seguridad, tipo Kerberos o OpenSSL.  De esta forma, las líneas de investigación  que se llevaran a cabo tomando como base el  prototipo realizado serán:  • Creación de un sistema GDC para  cálculo científico-matemático distribuido  multipropósito para uso académico.  • Expansión del actual proyecto de seguridad llevándolo a un sistema más complejo que se acomode a las necesidades del  Ejercito Argentino en materia de seguridad  informática.  Para lograr los dos objetivos planteados se  debe profundizar en temas de GDC agregando  cuestiones relativas a la seguridad desde el  punto de vista de la información transmitida,  procesada y utilizada por la grilla. También se  deben incrementar las capacidades de chequeo cruzado que proponen las arquitecturas a  través de los sistemas de votación para validación de resultados.  Por otro lado, se deben realizar cambios en  el sistema nativo (BOINC) para permitir el  proceso multipropósito, convirtiendo a CoDiSe en una facilidad genérica que exceda el  uso direccionado del cómputo.  Por lo antedicho, se centrarán las líneas de  investigación en computación en grilla de escritorio, seguridad informática en sistemas  distribuidos y programación distribuida.    3. RESULTADOS ESPERADOS  Se espera en lo general para el año 2012  lograr el análisis y el diseño completo de la  solución distribuida multipropósito así como  también avanzar en el desarrollo del prototipo  realizado durante 2011.  En lo particular se pretende afianzar el  desarrollo direccionado hacia el ámbito de la  seguridad informática, profundizando el circuito de prueba de librerías generadoras de  claves. Para ello se debe consolidar el circuito  generación de tuplas RSA - obtención de primos - análisis de resultados, habiéndose realizado a nivel prototipo sólo la obtención de  primos (problema de factorización).  Además se extenderá el desarrollo para que  el sistema acepte cualquier generador de claWICC 2012 680 2012 XIV Workshop de Investigadores en Ciencias de la Computación ves y no sólo OpenSSL como en el prototipo  experimental realizado.  Para tal fin se contará con apoyo de los dos  laboratorios participantes así como también  con el alumnado de la EST, en las materias  relacionadas de Ingeniería en Informática e  Ingeniería en Electrónica.  En resumen, al final del 2012, y como  próximo paso de CoDiSe, se pretende:  • Dar el primer paso hacia la creación  de un servidor de computación distribuida  genérica, para ser usado en proyectos de  distinta índole.  • Tener a disposición de los alumnos y  profesores afectados al laboratorio de  Criptografía de la EST, una herramienta  robusta y potente, con un muy bajo costo  de mantenimiento, que cumpla con los  requisitos funcionales necesarios.  • Validar robustez de algoritmos de  seguridad a nivel experimental.  Estos resultados, además, serán de gran  interés para el CIDESO, que dispondrá de una  herramienta fuerte para la evaluación de  librerías de seguridad, permitiéndole  seleccionar la mejor para el desarrollo de  todos sus sistemas en general, y de su sistema  de comando y control en particular, Sistema  Táctico Integrado del Ejército Argentino  (SITEA).    4. FORMACIÓN DE RECURSOS  HUMANOS  CoDiSe tiene como característica principal  haber nacido de la colaboración de dos centros de investigación dentro del Ejército Argentino. Y la particularidad que uno de dichos  centros es parte funcional de la EST. Esto pone al proyecto en un ámbito privilegiado para  la formación de recursos humanos.  Por un lado, el CIDESO tiene amplia experiencia en la formación de recursos humanos  en el terreno de la investigación aplicada en  sistemas de información de diversa índole, incluyendo sistemas de simulación para el  adiestramiento, sistemas de información geográfica, sistemas de visualización, sistemas  inteligentes, sistemas móviles, sistemas de  comunicación de alta complejidad y sistemas  de cómputo de alto rendimiento.  Por el otro, el CriptoLab tiene experiencia  en investigación básica en el terreno de la  criptografía, asociado estrechamente al posgrado de Especialización en Criptografía y  Seguridad Teleinformática que se dicta en la  EST. En este laboratorio se dispone del material y los recursos humanos específicos para la  investigación en esta área de la seguridad.  Cuenta con expertos matemáticos y criptógrafos que dan cuerpo muchas veces a las investigaciones de lo trabajos finales de carrera que  se realizan en el posgrado.  Ambos laboratorios, a través del dictado de  materias de grado en Ingeniería Informática,  aportan recursos humanos a la EST. Es así  que gran cantidad investigadores de los laboratorios dan cátedras en la EST y, de manera  análoga, alumnos de la escuela aportan sus  análisis a los laboratorios a través de trabajos  prácticos de laboratorio, prácticas profesionales supervisadas o tesis y tesinas de grado y  posgrado.    En particular, el prototipo experimental del  cual nace la idea de CoDiSe fue  implementado por los alumnos de tercer año  de Ingeniería Informática y Electrónica como  trabajo práctico de laboratorio de la materia  Lenguajes de Programación I, y tutorado por  investigadores del CIDESO y el CriptoLab,  que a la vez son docentes de dicha cátedra.  Para el próximo paso, se pretende continuar  con esta interacción fluida entre los centros de  investigación y el alumnado, formado  profesionales con conocimientos de campo en  el terreno de la computación de alto  rendimiento y un conocimiento acabado sobre  temas criptográficos.  Además, al expandirse el sistema para ser  aplicado en cualquier problema que requiera  altos niveles de cómputo, se pretende  incorporar alumnos y docentes de otras  cátedras, de cualquiera de las ingenierías que  se dictan en la Facultad.  Por otro lado, se continuará con la  formación de profesionales en investigación  utilizando estas nuevas tecnologías en los dos  laboratorios participantes a través de la  utilización de la GDC.  WICC 2012 681 2012 XIV Workshop de Investigadores en Ciencias de la Computación Así, pues, se formarán recursos humanos  de todos los niveles, grado, posgrado o  investigadores activos, incorporando más  alumnos a lo laboratorios y, potencialmente,  becarios que se dediquen de modo formal (no  sólo académico) a la profundización de lo  modelos propuestos.    5- BIBLIOFRAFÍA    [1]  A. B. Ajulo, Grid Computing An  Advancement of E-Science To  Computing And Beyond, Department of  Computer Science, Federal University of  Technology.   [2]  V. Berstis, Fundamentals of Grid  Computing, Redbooks Paper, IBM Corp,  2002.   [3]  A. J. M. Repetto, «Hybrid Architecture  for Constructive Interactive Simulation:  Evaluation and Outcomes,» de  /ITSEC’10, Interservice/Industry  Training, Simulation and Education  Conference, Orlando, FL, 2010.   [4]  A. J. M. Repetto, «Grid Desktop  Computing for Constructive Battlefield  Simulation,» de XV Congreso Argentino  de Ciencias de la Computación (CACIC  2009), San Salvador de Jujuy, 2009.   [5]  Computerized Bussiness Solutions,  «Centralized vs Distributed Computing:  White Paper,» 2007.  [6]  B. Javadi, D. Kondo, J.-M. Vincent y D.  P. Anderson, Discovering Statistical  Models of Availability in Large  Distributed Systems - An Empirical  Study of SETI@home, Berkeley: Space  Sciences Laboratory, University of  California.   [7]  R. Rivest, A. Shamir y L. Adleman, «A  Method for Obtaining Digital Signatures  and Public-Key Cryptosystems,» 1978.   [8]  A. Menezes, P. van Oorschot y S.  Vanstone, Handbook of Applied  Crytptography, CRC Press, 1997.   [9]  K. M. Martin, R. Safavi-Naini, H. Wang  y P. R. Wild, Distributing the Encryption  and Decryption of a Block Cipher,  London, England: Information Security  Group, Royal Holloway, University of  London.   [10] A. Ahmar, Grid Computing Technology:  An Overview, Abbas, 2004.   [11] N.-Z. Constantinescu-Fulop, A Desktop  Grid Computing Approach for Scientific  Computing, Department of Computer  and Information Science Faculty of  Information Technology, Norwegian  University of Science and Technology,  2008.   [12] E. T. O. Opiyo, E. Ayienga, K. Getao, B.  Manderick, O. Odongo y A. Nowé,  Computing Research Challenges and  Opportunities with Grid Computing.   [13] D. P. Anderson, T. Estrada, M. Taufer y  K. Reed, EmBOINC: An Emulator for  Performance Analysis of BOINC  Projects, University of Delaware, IBM,  University of Berkeley.   [14] F. Berman, G. C. Fox y A. J. G. Hey,  Grid Computing: Making the global  infrastructure a reality, Wiley publishers,  2003.   [15] D. P. Anderson, E. Korpela y R. Walton,  High-Performance Task Distribution for  Volunteer Computing, Berkeley: Space  Sciences Laboratory, University of  California.   [16] R. Neves, N. Mestre, F. Machado y J.  Lopez, Parallel and Distributed  Computing BOINC Grid  Implementation.   [17] J. Blythe, E. Deelman, Y. Gil y C.  Kesselman, Transparent Grid  Computing: a Knowledge-Based  Approach, Marina Del Rey, CA: USC  Information Sciences Institute.       WICC 2012 682 2012 XIV Workshop de Investigadores en Ciencias de la Computación	﻿computación distribuida , computación en grilla de escritorio , seguridad , factorización de grandes números	es	19212
7	Extensión de CluSim Simulación de la arquitectura tolerante a fallos RADIC	﻿ Los sistemas de Cómputo de Altas Prestaciones se utilizan para desarrollar software en una gran cantidad de campos. Es evidente el creciente predominio e impacto de  las aplicaciones del Cómputo de Altas  Prestaciones (High Performance Computing - HPC) en la sociedad moderna. Sin  embargo, la presencia de fallos en el hardware o software de computadores paralelos  hace necesario el uso de mecanismos tolerantes a fallos para asegurar que las aplicaciones finalicen exitosamente. Para ello se  ha desarrollado RADIC, una arquitectura  transparente, descentralizada, flexible y escalable para tolerancia a fallos que provee  alta disponibilidad en sistemas de paso de  mensajes. La falta de disponibilidad física  de grandes clusters y el hecho de estar ligado a una implementación específica de  MPI como base, son las principales dificultades con las que se encontraron los desarrolladores de RADIC. Como una solución a estos problemas el presente proyecto  de investigación propone el desarrollo de  un entorno de simulación para RADIC basado en OMNeT++, a partir de CLUSIM  (Simulador de clusters basado en OMNet++).  Palabras claves: Cómputo de Altas Prestaciones, HPC, Tolerancia a Fallos, OMNeT++, Simulación, RADIC.  Contexto  Este proyecto da continuidad a una línea  de investigación iniciada en el año 2008 en  el marco de los proyectos acreditados por  la Universidad Nacional de Jujuy denominados: APLICACIONES DEL CÓMPUTO DE  ALTAS PRESTACIONES y SISTEMAS DE  CÓMPUTO DE ALTAS PRESTACIONES CON  ALTA DISPONIBILIDAD: EVALUACIÓN DE LA  PERFORMANCE EN DIFERENTES CONFIGURACIONES.  El proyecto se encuentra acreditado y financiado por la Secretaría de Ciencia y  Técnica y Estudios Regionales de la Universidad Nacional de Jujuy (SECTERUNJu) y cuenta con el asesoramiento del  Mg. Germán Montejano (Universidad Nacional de San Luis) y de la Dra. Jussara de  Marques Almeida (Universidad Federal de  Minas Gerais).  Introducción  Los sistemas de HPC se usan para desarrollar software en una gran cantidad de  campos, incluyendo física nuclear, simulación de accidentes, procesamiento de datos  de satélites, dinámica de fluidos, modelado  del clima, bioinformática y modelado financiero. La gran variedad de organizaciones científicas, gubernamentales y  comerciales presentes en esta lista ilustra el  creciente predominio e impacto de las aplicaciones de HPC en la sociedad moderna  [Carver, 2007].  La presencia de fallos en el hardware o  software de computadores paralelos genera  nuevas necesidades en el uso de mecanismos tolerantes a fallos para asegurar que  las aplicaciones finalicen exitosamente.  Recientemente, algunos centros de supercomputadores han publicado estadísticas  acerca de fallos [Cappello, 2009]. El LaboWICC 2012 718 2012 XIV Workshop de Investigadores en Ciencias de la Computación ratorio Nacional de Los Álamos (LANL)  provee información muy detallada con una  descripción de 23.000 eventos que causaron paradas en la aplicación. Los datos de  LANL corresponden a 22 clusters con hasta 4.096 CPUs, durante un período de 10  años. Esto representa alrededor de 5.000  nodos de cómputo y un total de 24.000 de  CPUs (algunos nodos son multiprocesador). En un análisis de estos datos realizados por Schroeder y Gibson [Schroeder &  Gibson, 2007] se puede observar que el  número de fallos por año puede exceder los  1.000 para algunos sistemas. Para estos  últimos, tres fallos por día implican que  aplicaciones que utilizan todos los nodos  de cómputo y demoran más de 8 horas tienen pocas posibilidades de finalizar correctamente.   Las estadísticas anteriores llevan a plantear  la necesidad de implementar un sistema tolerante a fallos para HPC. Para alcanzar alta disponibilidad, tal sistema de tolerancia  a fallos debe proveer una recuperación y  detección de fallos automática y transparente. Además, para una tolerancia a fallos  proactiva es también deseable que se puedan realizar tareas de mantenimiento preventivo, como, por ejemplo, reemplazar  máquinas susceptibles a fallos sin interrupciones al sistema. [Santos et al., 2008]  Considerando estos aspectos, Duarte y colegas [Duarte et al., 2006; Duarte et al.,  2007] han propuesto y desarrollado  RADIC (Redundant Array of Distributed  Independent Fault Tolerance Controllers),  una arquitectura transparente, descentralizada, flexible y escalable para tolerancia a  fallos que provee alta disponibilidad en sistemas de paso de mensajes que basa su  operación en el mecanismo de rollbackrecovery basado en protocolo log pesimista. En tal protocolo, se realizan checkpoints  regularmente y todos los mensajes recibidos por cada proceso de la aplicación deben ser guardados por el receptor para poder volver a utilizarlos en caso de fallo.  En la actualidad, es cada vez más frecuente  el uso de modelos de simulación computacional en HPC, ya sea como ayuda al diseño y modelado de prestaciones [Denzel et  al., 2008], como para explorar arquitecturas o aplicaciones [Hammond et al., 2009;  Minkenberg & Rodriguez, 2009] o como  una herramienta de predicción de tráfico  [Tikir et al., 2009].   En el año 2010, [Valdiviezo et al., 2010;  Pérez Ibarra et al., 2010; Lasserre et al.,  2011, García et al., 2011] el GIS desarrolló el simulador CluSim, un simulador  de clusters basado en OMNeT++, que al  presente permite parametrizar la configuración de un cluster y hace posible evaluar  y predecir el impacto en el rendimiento de  diferentes configuraciones para aplicaciones tipo Master/Worker.  De igual modo, el realizar un simulador de  RADIC permitiría disponer de una herramienta para analizar cómo afectan las características y parámetros de configuración  de RADIC a la aplicación y al sistema. Por  ejemplo, cuál sería el número ideal de nodos spare y su ubicación ideal dentro del  cluster, investigar si es posible alcanzar  mejores resultados distribuyendo los nodos  spare de acuerdo con determinados criterios (nivel de degradación aceptable, límites de memoria de un nodo, topología de  red), investigar acerca de las posibles políticas a usarse en las tareas de reemplazo de  nodo, etc.  Líneas de investigación y desarrollo   Incorporar la arquitectura tolerante  a fallos RADIC a CluSim.    Extender la funcionalidad de CluSim tolerante a fallos a aplicaciones  paralelas tipo SPMD, Pipeline y  Divide/Conquer.  Resultados y Objetivos  El proyecto ha comenzado en febrero  del año en curso, de modo que, a la fecha  aún no se tienen resultados para presentar.  El objetivo principal del proyecto es desarrollar un framework basado en OMNeT++  donde puedan simularse distintos esquemas de tolerancia a fallos y que permita  WICC 2012 719 2012 XIV Workshop de Investigadores en Ciencias de la Computación implementar los módulos de la arquitectura  de RADIC de forma parametrizable y configurable. De esta manera, el simulador  permitirá un mejor análisis y comprensión  de las funciones de RADIC interactuando  con el sistema de cómputo y con las aplicaciones. Es decir, el simulador:   permitirá el desarrollo y prueba de  nuevas políticas en sistemas que no  están disponibles físicamente,   permitirá el análisis del comportamiento del sistema (desbalanceo de  carga, cuellos de botellas causados  por fallos) mediante la inyección de  diferentes patrones de fallos,   ayudará en el proceso de toma de  decisiones (por ejemplo, si se detecta un cuello de botella: cuántos  nodos spare serán necesarios,  dónde ubicarlos, cuál es la influencia del mapeo entre protectores y  observadores) permitiendo así la  evaluación de diferentes configuraciones de RADIC.  Formación de Recursos Humanos  El equipo de trabajo de la línea de I/D presentada está formado por:   Director: Lasserre, Cecilia María  Co-Director: Pérez Otero, Nilda María  Integrantes:   Pérez Ibarra, Claudio Marcelo   García, Adelina   Verazay, Abigaíl Roxana Noemí   Quispe, Gloria Lola   Ovando, Pablo   Martínez, Jorge   Córdoba, Rafaela   Argañaraz Azua, Fabio  Como se muestra a continuación algunos  de los miembros del equipo realizan estudios de postgrado o desarrollan el proyecto  de final de carrera de Ingeniería en Informática en temáticas afines al proyecto:    Tesis de maestría en curso: 1  Trabajos finales de especialización: 2  Especialización en curso en la UNLP: 1  Proyectos de final de carrera: 1  Referencias  [Cappello, 2009] Cappello, F. (2009). Fault  Tolerance in Petascale/ Exascale  Systems: Current Knowledge, Challenges and Research Opportunities.  International Journal of High Performance Computing Applications,  23(3):212–226.  [Carver, 2007] Carver, J. C. Third international workshop on software engineering for high performance computing (HPC) applications. In ICSE  COMPANION ’07: Companion to  the proceedings of the 29th International Conference on Software Engineering, p. 147, Washington, DC,  USA. IEEE Computer Society.  [Denzel et al., 2008] Denzel, W. E.; Li, J.;  Walker, P. & Jin, Y. A framework  for end-to-end simulation of highperformance computing systems. In  Simutools ’08: Proceedings of the 1st  international conference on Simulation tools and techniques for communications, networks and systems  & workshops, pp. 1--10, ICST, Brussels, Belgium, Belgium. ICST (Institute for Computer Sciences, SocialInformatics and Telecommunications  Engineering).  [Duarte, 2007] Duarte, A. RADIC: A Powerful Fault-Tolerant Architecture.  PhD thesis, Departament d’Arquitectura de Computadors i Sistemes Operatius. Universitat Autònoma de Barcelona, http://www.tdx.cat/TDX1126107-101303.  [Duarte et al., 2006] Duarte, A.; Rexachs,  D. & Luque, E. Increasing the cluster  availability using RADIC. Cluster  Computing, 2006 IEEE International  Conference on , vol., no., pp.1-8, 2528 Sept. 2006   [Duarte et al., 2007] Duarte, A.; Rexachs,  D. & Luque, E. (2007). Functional  tests of the RADIC fault tolerance  architecture. In PDP, pp. 278–287.  IEEE Computer Society.  [García et al., 2011] García, A.; Pérez Otero, N. M.; Pérez Ibarra, C. M. y C.  WICC 2012 720 2012 XIV Workshop de Investigadores en Ciencias de la Computación M. Lasserre. Simulación de Clusters:  Integración de INET a CluSim. XVII  Congreso Argentino de Ciencias de  la Computación (CACIC 2011).  ISBN 978-950-34-0756-1. Universidad Nacional de la Plata. Buenos Aires. pp. 367-373. Octubre 2011.  [Hammond et al., 2009] Hammond, S. D.;  Mudalige, G. R.; Smith, J. A.; Jarvis,  S. A.; Herdman, J. A. & Vadgama,  A. Warpp: a toolkit for simulating  highperformance parallel scientific  codes. In Simutools ’09: Proceedings  of the 2nd International Conference  on Simulation Tools and Techniques,  pp. 1--10, ICST, Brussels, Belgium,  Belgium. ICST (Institute for Computer Sciences, Social-Informatics  and Telecommunications Engineering).  [Lasserre et al.,2011] Lasserre, C. M.;  Pérez Ibarra, C. M.; Valdiviezo, L.  M.; Verazay, A. R. N.; Quispe, G.  L.; Nolasco, S. A.; Chosco, V. H. y  N. M. Pérez Otero. Adaptación de  CluSim a clusters heterogéneos. Investigaciones en Facultades de Ingeniería del NOA. Tomo 2 – 2011.  ISSN 1853-7871. Editorial Científica  Universitaria. San Fernando del Valle de Catamarca. pp 1053-1060. Octubre 2011.  [Minkenberg & Rodriguez, 2009] Minkenberg, C. & Rodriguez, G. Tracedriven co-simulation of highperformance computing systems using OMNeT++. In Simutools ’09:  Proceedings of the 2nd International  Conference on Simulation Tools and  Techniques, pp. 1--8, ICST, Brussels, Belgium, Belgium. ICST (Institute for Computer Sciences, SocialInformatics and Telecommunications  Engineering).  [Pérez Ibarra et al., 2010] Pérez Ibarra, C.  M.; Valdiviezo L. M; Pérez Otero N.  M.; Liberatori, H. P.; Rexachs, D.;  Luque E. y C. M. Lasserre.  CLUSIM: Simulador de Clusters para aplicaciones de cómputo de altas  prestaciones basado en OMNeT++.  XVI Congreso Argentino de Ciencias  de la Computación (CACIC 2010).  ISBN 978-950-9474-49-9. Ed. Universidad de Morón. Buenos Aires.  pp. 142-151. 2010. Octubre 2010.  [Santos et al., 2008] Santos, G.; Duarte,  A.; Rexachs, D. & Luque, E. Providing non-stop service for messagepassing based parallel applications  with RADIC. In Luque, E.; Margalef, T. & Benitez, D., editores, EuroPar, volume 5168 of Lecture Notes  in Computer Science, pp. 58–67.  Springer.  [Schroeder & Gibson, 2007] Schroeder, B.  & Gibson, G. A. Understanding failures in petascale computers. Journal  of Physics: Conference Series,  78:012022 (11pp).  [Tikir et al., 2009] Tikir, M. M.; Laurenzano, M. A.; Carrington, L. & Snavely, A. PSINS: An open source event  tracer and execution simulator for  MPI applications. In Euro-Par ’09:  Proceedings of the 15th International  Euro-Par Conference on Parallel  Processing, pp. 135--148, Berlin,  Heidelberg. Springer-Verlag.  [Valdiviezo et al., 2010] Valdivezo, L. M.;  Pérez Otero, N. M.; Pérez Ibarra, C.  M. y C. M. Lasserre. Caracterización  de una aplicación paralela con distintas configuraciones en CluSim. Investigaciones en Facultades de Ingeniería del NOA – 2010. ISSN 33675072. Ed. EdiUNJu. S. S. de Jujuy.  pp. 499-504. Noviembre 2010.    WICC 2012 721 2012 XIV Workshop de Investigadores en Ciencias de la Computación	﻿cómputo de altas prestaciones , HPC , tolerancia a fallos , OMNeT , simulación , RADIC	es	19308
8	Arquitecturas multiprocesador distribuidas Cluster grid y cloud computing	"﻿   Esta línea de Investigación está dentro del proyecto “Arquitecturas Multiprocesador Distribuidas. Modelos, Software de Base y Aplicaciones” acreditado por el Ministerio de Educación  y de proyectos específicos apoyados por organismos nacionales e internacionales.  En el tema hay cooperación con varias  Universidades de Argentina  y se está trabajando con  Universidades de América Latina y Europa en  proyectos financiados por CyTED, AECID y la  OEI (Organización de Estados Iberoamericanos).  Se participa en iniciativas como el Programa  IberoTIC de intercambio de Profesores y Alumnos de Doctorado en el área de Informática.  Por otra parte,  se tiene financiamiento de  Telefónica de Argentina en Becas de grado y  posgrado.    Resumen    Caracterizar las arquitecturas multiprocesador  distribuidas enfocadas a cluster, grid y cloud  computing, con énfasis en las que utilizan procesadores de múltiples núcleos (“multicores”),  con el objetivo de modelizarlas, estudiar su escalabilidad, analizar y predecir performance de  aplicaciones paralelas y desarrollar esquemas  de tolerancia a fallas en las mismas.  Analizar y desarrollar software de base para  clusters de multicores, tratando de optimizar el  rendimiento de tales arquitecturas para diferentes modelos de programación paralela y diferentes paradigmas de resolución de aplicaciones.  En el año 2011 se han agregado dos líneas de  interés:    El estudio de arquitecturas basadas en  GPGPU y su comparación con clusters de  multicores, así como el empleo combinado  de GPUs y multicores en computadoras de  alta perfomance.   El análisis de la eficiencia energética, considerando el impacto de la arquitectura, el  sistema operativo, el modelo de programación y el algoritmo específico.    Es de hacer notar que este proyecto se coordina con otros dos proyectos en curso en el IIILIDI,  relacionados con Algoritmos Distribuidos/Paralelos y Sistemas de Software Distribuido.    Keywords: Sistemas Paralelos. Cluster, Multicluster, Grid y Cloud Computing. Paradigmas  de programación paralela. Modelos y predicción de performance. Scheduling. Virtualización. Tolerancia a fallas. GPGPUs. Eficiencia  energética.                        Introducción    La investigación en Sistemas Distribuidos y Paralelos es una de las líneas de mayor desarrollo  en la Ciencia Informática actual [1][2][3]. En  particular la utilización de arquitecturas multiprocesador configuradas en clusters, multiclusters, grids y clouds, soportadas por redes de  diferentes características y topologías se ha  generalizado, tanto para el desarrollo de algoritmos paralelos, la ejecución de procesos que  requieren cómputo intensivo y la atención de  servicios WEB concurrentes [4][5][6][7].    El cambio tecnológico, fundamentalmente a  partir de los procesadores multicore, ha impuesto la necesidad de investigar en paradigmas “híbridos”, en los cuales coexisten esquemas de memoria compartida con mensajes  [8][9][10][11]. Asimismo la utilización de procesadores gráficos (GPGPUs) como arquitecturas  paralelas presenta una alternativa para alcanWICC 2012 778 2012 XIV Workshop de Investigadores en Ciencias de la Computación zar un alto speed-up en determinadas aplicaciones [12].    Es importante en este contexto desarrollar nuevos paradigmas y herramientas para la programación eficiente de aplicaciones  [13][14][15]. A su vez el concepto de eficiencia  se refiere tanto al aspecto computacional como  el energético y el impacto del consumo sobre  arquitecturas con miles de procesadores que  trabajan concurrentemente debe tenerse en  cuenta en la estructura de los sistemas paralelos y en la planificación de la utilización de recursos por las aplicaciones [16].    Asimismo, aparecen líneas de I/D tales como el  scheduling dinámico basado en el consumo del  sistema paralelo y de cada subsistema (llegando al nivel de cada núcleo), el control en tiempo  real de la frecuencia de reloj de los procesadores para optimizar consumo, la detección en  bajo nivel de errores de concurrencia [17], el  estudio y desarrollo de lenguajes, compiladores  y estructuras de datos adecuados a estas arquitecturas y la detección y tolerancia a fallos  tratando de minimizar el overhead de tiempo y  aprovechando alguna redundancia en la misma  arquitectura [14][15][18].    Por otra parte la heterogeneidad que caracteriza a los clusters y grids, así como a las redes  de comunicaciones, se extiende a las nuevas  arquitecturas multicore y GPGPU enfocando  funcionalidades específicas para algunos núcleos, lo cual puede mejorar la performance pero al mismo tiempo complejiza el scheduling de  los procesos paralelos [13][19].    La aparición de las arquitecturas tipo Cloud  obliga a poner especial atención a los problemas de virtualización y predicción de performance (para la asignación dinámica de recursos). Naturalmente a mayor potencia del Cloud,  también crecen las complejidades al analizar la  comunicación y el acceso a memoria en arquitecturas que están distribuidas y a su vez conformadas por placas con un número variable de  procesadores multicore y/o GPGPU [20][21].  En el proyecto se ha abierto una línea específicamente dedicada a los problemas de configuración y administración eficiente de Cloud, incluyendo entre los parámetros el consumo estimado de las aplicaciones [22].       Definiciones básicas    Un procesador multicore  integra dos o más  núcleos computacionales dentro de un mismo  “chip” [23][24]. La motivación de su desarrollo  se basa en incrementar el rendimiento, reduciendo el consumo de energía en cada núcleo.   Una GPU (Graphics Processing Unit) es una  arquitectura multicore dedicada a procesamiento grafico, con un gran número de cores simples. En los últimos años, estas arquitecturas,  fueron utilizadas para aprovechar su potencia  de cómputo en aplicaciones de propósito general logrando un alto rendimiento y dando lugar  al concepto de GPGPU (General-Purpose  Computing on Graphics Processing Units)  [12][25].  Un cluster es un sistema de procesamiento paralelo compuesto por un conjunto de computadoras interconectadas vía algún tipo de red,  las cuales cooperan configurando un recurso  que se ve como “único e integrado”, más allá  de la distribución física de sus componentes.  Cada “procesador” puede tener diferente hardware y sistema operativo, e incluso puede ser  un “multiprocesador” [26]. Cuando se conectan  dos o más clusters sobre una red tipo LAN o  WAN, se tiene un multicluster [27]. La configuración más simple a considerar es la conexión  de clusters homogéneos sobre una red LAN o  WAN, utilizando un sistema operativo común  [28].    Un Grid es un tipo de sistema distribuido  que  permite seleccionar, compartir e integrar recursos autónomos geográficamente distribuidos  [28]. Un Grid es una configuración colaborativa  que se puede adaptar dinámicamente según lo  requerido por el usuario, la disponibilidad y potencia de cómputo de los recursos conectados.  El Grid puede verse como un “entorno de procesamiento virtual”, donde el usuario tiene la  visión de un sistema de procesamiento “único”  y en realidad trabaja con recursos dispersos  geográficamente [29].    Las arquitecturas tipo “Cloud” se presentan  como una evolución natural del concepto de  Clusters y Grids, integrando grandes conjuntos  de recursos virtuales (hardware, plataformas de  desarrollo y/o servicios), fácilmente accesibles  y utilizables por usuarios distribuidos, vía WEB.  Estos recursos pueden ser dinámicamente reconfigurados para adaptarse a una carga variable, permitiendo optimizar su uso  [21][30][31][32].      Aspectos de interés     El incremento en el número de procesadores disponibles en clusters, grids y clouds  obliga a poner énfasis en el desarrollo de  los algoritmos de virtualización de modo de  WICC 2012 779 2012 XIV Workshop de Investigadores en Ciencias de la Computación explotar la arquitectura con más de una  aplicación concurrente [15].   La heterogeneidad es inevitable en estos  sistemas paralelos complejos. A su vez es  un factor que condiciona la predicción de  perfomance y consumo [19].   A partir de la complejidad creciente del  hardware, se hace más desafiante el desarrollo de capas de software eficiente, desde  el middleware hasta los lenguajes de aplicación [33][34][35] [40].   Los problemas clásicos de scheduling y  mapeo de procesos a procesadores tienen  nuevos objetivos (en particular los relacionados con el consumo) y deben considerar  la migración dinámica de datos y procesos  en función de perfomance y consumo [36].   Los modelos de predicción de performance  resultan especialmente complejos. Resulta  de interés el estudio de esquemas sintéticos  (“firmas”) propios de la aplicación para estimar tiempos y consumo, ejecutando un código mínimo frente al de la aplicación real  [37].   El tema de la detección y tolerancia a fallos  de hardware y software se vuelve un punto  crítico al operar sobre arquitecturas con  gran número de procesadores, los cuales  pueden reconfigurarse dinámicamente  [38][39] [41].           Líneas de Investigación y Desarrollo    Temas de Estudio e Investigación     Arquitectura de procesadores multicore.  Clusters de multicores.  Software de base.    Modelos de predicción de performance para arquitecturas tipo cluster de multicores,  grids y clouds. Simulación de arquitecturas.    Técnicas de scheduling para sistemas paralelos, en particular en función del consumo de los procesadores.   Virtualización en clusters, grids y clouds.  Predicción de performance aplicada a la  virtualización.   Nuevas estructuras de datos, orientadas a  procesamiento paralelo sobre clusters,  grids y clouds.   Detección de errores de concurrencia, en  tiempo de ejecución.    Procesamiento paralelo basado en GPUs.  Aplicación sobre clusters de multicores.  Comparación de rendimiento con arquitecturas basadas en multicores “clásicos”.   Cloud computing. Software de base y overhead introducido por la administración de  recursos en Cloud.   Análisis comparativo de perfomance en  cluster y cloud para problemas de HPC.   Detección y Tolerancia a Fallos (de hardware y software)  en clusters, grids y  clouds.   Métricas de evaluación de performance y  escalabilidad para las nuevas arquitecturas  paralelas, a partir del uso de procesadores  de múltiples núcleos y/o GPGPUs..    Investigación experimental     Desarrollo y evaluación de aplicaciones sobre cluster de multicores y multicluster heterogéneo basado en multicores. (total 192  procesadores).   Desarrollo de software para virtualización  del cluster de multicores para emular servicios de cloud computing.    Pruebas de consumo en cluster de multicores y GPGPUs, analizando eficiencia  computacional, escalabilidad y eficiencia  energética.    Formación de Recursos Humanos    En cooperación con Universidades iberoamericanas se ha implementado la Maestría  en  Cómputo de Altas Prestaciones y se continúa  dictando la Especialización en Cómputo de altas Prestaciones y Tecnología GRID.  En esta línea de I/D existe cooperación a nivel  nacional e internacional. Hay 8 Investigadores  realizando su Doctorado, 4 realizando la Maestría y 4  alumnos avanzados están trabajando  en su Tesina de Grado de Licenciatura. En  2011 se aprobó 1 Tesis Doctoral, 1 de Maestría, 3 de Especialista y 3 Tesinas de Grado en  temas del proyecto.    Bibliografía    1. Grama A, Gupta A, Karypis G, Kumar V. “Introduction to parallel computing”. Second Edition.  Pearson Addison Wesley, 2003.  2. Dongarra J, Foster I, Fox G, Gropp W, Kennedy  K, Torczon L, White A. “The Sourcebook  of Parallel Computing”. Morgan Kauffman Publishers.  Elsevier Science, 2003.  3. Ben-Ari, M. ""Principles of Concurrent and Distributed Programming, 2/E"". Addison-Wesley, 2006.  4. Juhasz Z. (Editor), Kacsuk P. (Editor), Kranzlmuller D. (Editor). “Distributed and Parallel Systems:  Cluster and Grid Computing”. Springer; 1 edition  (September 21, 2004).  5. Miller M. “Cloud computing: web-based applications that change the way you work and collaborate online”. Que Publishing. USA 2008.  WICC 2012 780 2012 XIV Workshop de Investigadores en Ciencias de la Computación 6. Di Stefano M., “Distributed data management for  Grid Computing”. John Wiley & Sons Inc. 2005.  7. Ghosh S. “Distributed System. An Algorithmic  Approach”. Chapman & Hall/CRC Computer and  Information Science Series.  8. Mc. Cool M. “Programming models for scalable  multicore programming”. 2007.   http://www.hpcwire.com/features/17902939.html   9. Lei Chai, Qi Gao, Dhabaleswar K. Panda. “Understanding the Impact of Multi-Core Architecture  in Cluster Computing: A Case Study with Intel  Dual-Core System”.  IEEE International Symp. on  Cluster Computing and the Grid 2007 (CCGRID  2007), pp. 471-478 (May 2007).    10. Leibovich F., Gallo S., De Giusti L., Chichizola F.,  Naiouf M., De Giusti A. “Comparación de paradigmas de programación paralela en cluster de  multicores: Pasaje de mensajes e híbrido. Un caso de estudio”. Proceedings del XVII Congreso  Argentino de Ciencias de la Computación (CACIC 2011), La Plata (Argentina), 2011, ISBN:  978-950-34-0756-1. Págs: 241-250.  11. Rucci E., De Giusti A., Chichizola F., Naiouf M.,  De Giusti L. “DNA Sequence Alignment: hybrid  parallel programming on multicore cluster”. Proceedings of the International Conference on  Computers, Digital Communications and Computing (ICDCCC ‘11), Vol. 1, Nikos Mastorakis, Valeri Mladenov, Badea Lepadatescu, Hamid Reza  Karimi, Costas G. Helmis (Editors), WSEAS  Press, September 15-17, 2011, Barcelona,  Spain, ISBN: 978-1-61804-030-5, pp. 183-190.  12. General-Purpose Computation on Graphics Processing Units. http://gpgpu.org.  13. De Giusti L., Chichizola F., Naiouf M., De Giusti  A.E., Luque E. “Automatic Mapping Tasks to  Cores - Evaluating AMTHA Algorithm in Multicore Architectures”. IJCSI International Journal  of Computer Science Issues, Vol. 7, Issue 2, No  1, March 2010. ISSN (Online): 1694-0784. ISSN  (Print): 1694-0814. Págs. 1-6.  14. Olszewski M., Ansel J., Amarasinghe S. “Kendo:  Efficient Determistic Multithreading in Software”.  Architectural Support for Programming Languages and Operating Systems (ASPLOS ‘09).  15. Bertogna M., Grosclaude E., Naiouf M., De Giusti  A., Luque E. “Dynamic on Demand Virtual Clusters in Grids”. 3rd Workshop on Virtualization in  High-Performance Cluster and Grid Computing.  VHPC 08 – España. Agosto 2008.   16. Feng, W.C., “The importance of being low power  in high-performance computing”.  Cyberinfrastructure Technology Watch Quarterly  (CTWatch Quarterly). 2005.  17. Frati E., Olcos Herrero K., Piñuel Moreno L.,  Montezanti D., Naiouf M., De Giusti A. “Optimización de herramientas de monitoreo de errores de  concurrencia a través de contadores de hardware”. Proceedings del XVII Congreso Argentino de  Ciencias de la Computación (CACIC 2011), La  Plata (Argentina), 2011, ISBN: 978-950-34-07561. Págs: 337-346.  18. Shirako   J. et al. “Compiler Control Power Saving  Scheme for Multi Core Processors”. LNCS Mayo  2007 – pp. 362-376.  19. Suresh Siddha, Venkatesh Pallipadi, Asit Mallick.  “Process Scheduling Challenges in the Era of  Multicore Processors”Intel Technology Journal,  Vol. 11, Issue 04, November 2007.  20. Vaquero L.M. et al. “A Break in the Clouds: Towards a Cloud Definition”. ACM SIGCOMM  Computer Communication Review, vol. 39, num.  1, páginas 50-55, ISSN 0146-4833. Enero 2009.  21. Foster I. “There's Grid in them thar Clouds”. 2 de  Enero, 2008.  http://ianfoster.typepad.com/blog/2008/01/theresgrid-in.html. Noviembre, 2010.  22. Rodriguez I., Pettoruti J., Chichizola F., De Giusti  A. “Despliegue de un Cloud Privado para entornos de cómputo científico”. Proceedings del XVII  Congreso Argentino de Ciencias de la Computación (CACIC 2011), La Plata (Argentina), 2011,  ISBN: 978-950-34-0756-1. Págs: 251-260.  23. Burger T. W. “Intel Multi-Core Processors: Quick  Reference Guide”. http://cachewww.intel.  com/cd/00/ 00/23/19/231912_231912.pdf  24. AMD. “Evolución de la tecnología de múltiple núcleo”. http://multicore.amd.com/es-ES/AMD-MultiCore/resources/Technology-Evolution (2009).  25. Adrian Pousa, Victoria Sanz, Armando De Giusti,  “Análisis de rendimiento de un algoritmo de criptografía simétrica sobre arquitecturas multicore”,  Proceedings del XVII Congreso Argentino de  Ciencias de la Computación (CACIC 2011), La  Plata (Argentina), 2011, ISBN: 978-950-34-07561. Págs: 231-240.  26. Zoltan J., Kacsuk P., Kranzlmuller D., “Distributed  and Parallel Systems: Cluster and Grid Computing”. The International Series in Engineering and  Computer Science. Springer; 1st ed., 2004.  27. Bertogna M. L. “Planificación dinámica sobre entornos Grid”. Ph.D. thesis, Universidad Nacional  de La Plata, La Plata, Argentina, 2010.   28. Grid Computing and Distributed Systems  (GRIDS)     Laboratory - Department of Computer  Science and Software Engineering (University of  Melbourne). “Cluster and Grid Computing”. 2007.  http://www.cs.mu. oz.au/678/.  29. Grid Computing Infocentre: http://www.grid computing.com/   30. Dikaikos M. et al. “Distributed InterNet Computing  for IT and Scientific Research”. Internet  Computing IEEE. Vol 13, Nro. 5, pp 10-13  31. Ardissono L., Goy A., Petrone G., Segnan M.  “From Service Clouds to User-centric Personal  Clouds”. 2009 IEEE Second International  Conference on Cloud Computing.  32. Hemsoth N. “Outsourcing Versus Federation: Ian  Foster on Grid and Cloud”. 15 de Junio, 2010.  http://www.hpcinthecloud.com/blogs/OutsourcingVersus-Federation-Ian-Foster-on-Grid-andCloud-96326829.html. Noviembre, 2010.  33. Song Y., Kalogeropulos S., Tirumalai P.  “Design  and Implementation of a Compiler Framework for  Helper Threading on Multi-core Processors”. ProWICC 2012 781 2012 XIV Workshop de Investigadores en Ciencias de la Computación ceedings of the 14th International Conference on  Parallel Architectures and Compilation Techniques; Sept. 2005.   34. Vázquez Blanco C., Huedo E., Montero R. S.,  Llorente I. M. “Elastic Management of Clusterbased Services in the Cloud”. Proceedings pp.  19-24, ACM Digital Library 2009. ISBN 978-160558-564-2.  35. Vázquez Blanco C., Huedo E., Montero R. S.,  Llorente I. M. “Dynamic Provision of Computing  Resources from Grid Infrastructures and Cloud  Providers”. IEEE Society Press, pp.113-120,  Workshops at the Grid and Pervasive Computing  Conference, GPC 2009. ISBN 978-0-7695-36774.  36. De Giusti L.,  Naiouf M., Chichizola F., Luque E.,  De Giusti A. E. “Dynamic Scheduling in Heterogeneous Multiprocessor Architectures. Efficiency  Analysis”. Computer Science & Technology Series – XV Argentine Congress of Computer Science Selected Papers. Editores: Guillerno Simari,  Patricia Pesado, José Paganini. Págs. 85-95.  ISBN 978-950-34-0684-7. Editorial de la Universidad de La Plata (edulp).  La Plata (Argentina).  2010.  37. Corredor Franco J.  “Predicción de perfiles de  comportamiento de aplicaciones científicas en  nodos multicore”. Ph.D. Thesis, Universidad Autónoma de Barcelona, Barcelona, España, Julio  2011.  38. Lu S., Tucek J., Qin F., Zhou Y. “AVIO: detecting  atomicity violations via access interleaving invariants”. SIGPLAN Not., ACM, 2006, 41, 37-48.  39. Golander A., Weiss S., Ronen R. “Synchronizing  Redundant Cores in a Dynamic DMR Multicore  Architecture”. IEEE Transactions on Circuits and  Systems II: Express Briefs Volume 56, Issue 6,  474-478. 2009.  40. Muresano Cáceres R.  “Metodología para la aplicación eficiente de aplicaciones SPMD en clústers con procesadores multicore” Ph.D. Thesis,  Universidad Autónoma de Barcelona, Barcelona,  España, Julio 2011.  41. Fialho L.  “Fault Tolerance configuration for uncoordinated checkpoints”. Ph.D. Thesis, Universidad Autónoma de Barcelona, Barcelona, España, Julio 2011.      WICC 2012 782 2012 XIV Workshop de Investigadores en Ciencias de la Computación"	﻿grid y cloud computer , sistemas paralelos , cluster , multicluster , paradigmas de programación paralela , modelos y predicción de performance , virtualización , tolerancia a fallas , GPGPUs , eficiencia energética	es	19391
9	Optimización de cómputo paralelo científico en un entorno híbrido Multicore MultiGPU	﻿ Es creciente el número de algoritmos de cómputo científico que corren con mayor eficiencia en las placas procesadoras de video (GPGPU - General Programming Graphics Processors Units) que en los propios procesadores multicore de nuestros días. Sin embargo, la capacidad de memoria de estos dispositivos es cerca de un orden de magnitud inferior a las computadoras equipadas con multiprocesadores simétricos (SMP - Symmetric Multiprocessors). Por ello, para problemas cuyos datos requieren más memoria que la disponible en una placa GPGPU, los datos deben procesarse parcialmente en la placa, y además, planificarse el avance global del procesamiento. Frente a la existencia de múltiples cores y uno o más GPGPU(s) en el SMP, se investiga el uso optimizado de ambos recursos de procesamiento en forma conjunta, enfocándonos en problemas de cómputo científico cuyo requerimiento de memoria esté en el rango de la memoria del dispositivo gráfico y la del computador. Este problema es típico del procesamiento paralelo, por cuanto una adecuada división de datos y tareas es fundamental para obtener un resultado acorde a los recursos utilizados. En este trabajo se presentan lineas de investigación al respecto, como así también, algunos resultados preliminares. Palabras Clave: Procesamiento Paralelo - Cómputo Científico - Procesamiento híbrido multicore/GPGPU. 1. Contexto La investigación está respaldada por los proyectos de investigación ”Diseño e implementación de algoritmos y hardware optimizados para sistemas de computación paralelos en ingeniería”, proyecto presentado ante la SECYT de la UNC para el período 2012-2013 y por el proyecto “Aceleración de Algoritmos en Procesadores Multicore, GPGPU y FPGA“, subvencionado por el MINCyT de la Prov. de Córdoba, convocatoria PID 2010, de 2 años de duración. 2. Introducción La capacidad de procesamiento de las GPGPU’s en áreas de cómputo científico tiene una tendencia creciente, superior a las CPU’s multicore, en términos de FLOP’s teóricos alcanzables[Fog11], [Enc]. Existen, sin embargo, dos factores que limitan el procesamiento de las GPGPU’s. El primero referido a la dependencia de funcionamiento con una computadora, ya que necesariamente se conectan físicamente al motherboard de esta, y es un programa ejecutado en la CPU principal, quien llama a rutinas de cómputo que corren en la placa. El segundo factor limitante es la memoria de datos que disponen las GPGPU’s, cerca de un orden de magnitud inferior a la memoria de la computadora que las alojan. En problemas cuyo volumen de datos a procesar es inferior a la memoria de datos de la GPGPU, esta es candidata a realizar todo el procesamiento, siempre que existan o se programen las rutinas pertinentes optimizadas. Una situación diferente se plantea si el volumen de datos es mayor, y/o si se utilizan conjuntamente más de una placa GPGPU conectada al motherboard, y/o se dispone de un procesador multicore, el cual se desea utilizar en forma simultanea con la(s) GPGPU(s). En estos casos, usar la totalidad de la capacidad de cómputo disponible, deriva en un problema de procesamiento paralelo, siendo necesario particionar datos y tareas de cómputo y programar un algoritmo que controle la asignación de datos y tareas a cada procesador y el avance global del procesamiento. Delimitamos la investigación a aquellos programas a ejecutarse en computadoras que dispongan unidades de procesamiento multicore y de una o más placas GPGPU, cuyo volumen de datos sea mayor a la memoria de una GPGPU, por cuanto es necesario distribuir procesamiento. En cuanto a las rutinas a utilizar, se usarán bibliotecas de álgebra lineal ya optimizadas para ambos tipos de procesadores. No forma parte de nuestros objetivos la programación que maximice el uso de cada tipo de procesador, sino al contrario, utilizando las bibliotecas ya disponibles, optimizar el uso de estas con problemas cuyo tamaño justifique la utilización conjunta de ambos recursos de procesamiento. El proyecto focaliza en cómo particionar datos y tareas y sincronizar eficientemente, para que la capacidad conjunta de procesamiento sea optimamente utilizada. Los problemas a estudiar son algoritmos clásicos de Álgebra Lineal: factorización y multiplicación de matrices. Dado que no existe una forma única de dividir datos a procesar en paralelo, y teniendo en cuenta que cada tipo de procesador dispone de una capacidad de procesamiento diferente, surgen varias alternativas en la forma de distribuir los datos en cada unidad de procesamiento. Desde el punto de vista del modelo de programación paralela, estamos frente a un caso del modelo de memoria compartida. La programación paralela con hilos (threads) WICC 2012 788 2012 XIV Workshop de Investigadores en Ciencias de la Computación es lo usualmente utilizado para este tipo de hardware, utilizando OpenMP para nuestros tests. Una posible división de datos es, que el total de datos sea dividido en partes de igual tamaño, provocando que cada tipo de procesador tarde distinto tiempo en completar igual tarea, debiendo los más rápidos esperar a los más lentos. Este hecho genera un desbalance de carga indeseable. Para solucionar el problema del desbalance, consideramos dos alternativas: 1. Mantener la división de datos en partes iguales, pero los procesadores más rápidos no sincronizan con los más lentos para la siguiente tarea, sino que comiencen con esta sin esperar, es decir, un avance desigual de procesamiento. Esto genera una mejor utilización de la capacidad de procesamiento, pero implica un mayor costo de coordinación global, ya que cada procesador avanza a su propio ritmo. 2. Hacer que cada procesador trabaje sobre un bloque de datos que demande igual tiempo de procesamiento para cada tipo de procesador, es decir, una partición de datos en bloques diferentes, los de mayor tamaño destinadas a los procesadores más veloces y los de menor tamaño a los procesadores más lentos. La forma en que los datos sean divididos impacta en las tareas que se deban realizar para alcanzar el procesamiento total. Para la multiplicación de matrices, donde es casi nula la dependencia de datos, la división de datos solo está limitada por la regla de las dimensiones de los bloques para que la multiplicación pueda realizarse. Para los algoritmos de factorización, la fuerte dependencia de datos es un limitante serio para el avance desigual de tareas. Los grafos de dependencia de datos son usuales para la planificación de tareas y permiten analizar y estudiar el funcionamiento del procesamiento paralelo. Esta herramienta de modelado de procesamiento paralelo es utilizada en el proyecto PLASMA de la Universidad de Tennessee [oTICL] para el desarrollo de una biblioteca de rutinas para procesadores multicore. En particular, en el reporte técnico LAWN 191 (LAPACK Working Note 191) [BLKD07] se presenta un scheduler de tareas llamado Graph driven asynchronous execution, donde a partir del grafo de dependencia de tareas, se construye el algoritmo que define el orden de ejecución de las mismas. El reporte trabaja solo con procesadores multicore y con bloques de datos de igual tamaño. El reporte LAWN 213 [KLDB09] extiende el 191, con distintas opciones de scheduling, usando también los grafos de dependencia. Para la división desigual de bloques, orientada al avance sincróno de procesamiento, la existencia de distintas tareas impacta en la determinación del tamaño del bloque. Las GPGPU’s no son CPU’s más rápidas, sino que tienen una arquitectura diferente, por lo que si bien, en general son más veloces para cómputo numérico, no tienen la misma eficiencia relativa para distintas rutinas. La división de datos óptima sería específica por rutina, lo cual es impracticable. Así, definir un único tamaño de bloque para cada tipo de procesador provoca un rendimiento subóptimo del mismo. El reporte LAWN 250 [STD11] propone la división desigual de bloques. Se determinan bloques de datos de dos tamaños, el menor para la CPU y el mayor, para la GPGPU. Utiliza un modelo estático de asignación de bloques entre cores de CPU y GPGPU’s. El modelo de ejecución se asemeja al modelo Master/worker, y es complejo y con una alta carga de sincronización. 3. Investigación y Desarrollo La estrategia definida para la programación paralela en un SMP con uno o más GPGPU’s, es la siguiente: supongamos que se disponen de c cores en el equipo multicore y de g GPGPU’s, donde vale c ≥ g. Se dispone de c hilos, uno por cada core, de los cuales g se destinan al control de cada GPGPU y los c−g restantes se destinan a procesamiento en CPU. En la práctica, el algoritmo paralelo maneja solo g+1 hilos, usando g para control de cada GPGPU y el restante para la invocación de la rutina de BLAS necesaria. Dado que se usan bibliotecas optimizadas para equipos multicore, el llamado a la rutina lleva un parámetro de valor c − g, destinado a utilizar ese número de hilos para cómputo. Se investigará tanto en la avance desigual de tareas como en la división desigual de datos. Para la multiplicación de matrices, se buscará determinar la división desigual de datos que permita minimizar el costo de sincronización del procesamiento conjunto GPGPU y CPU. Para el caso de factorización de matrices, algoritmos de Cholesky, LU y QR, se investigará tanto para la división desigual de datos, como el avance desigual de tareas. A priori, ambas estrategias presentan problemas. La división desigual de datos impone restricciones en cuanto al tamaño de los bloques a dividir los datos, ya que en estos algoritmos, existe un update de datos con resultados parciales que utiliza suma y multiplicaciones de matrices, por cuanto la dimensión de los bloques deben respetar la regla de la dimensiones de ambas operaciones. La estrategia de avance desigual tiene el problema de la coordinación del avance de las tareas. Para ello se experimentará modelando los algoritmos con de Redes de Petri Coloreadas (Coloured Petri Nets - CPN)[JK09]. Una Red de Petri es generalmente definida como un grafo dirigido bipartito, cuyos nodos representan lugares o transiciones (nodos places y nodos transitions) y sus arcos conectan un nodo de un tipo con un nodo de otro tipo, por lo que un arco que tiene como origen a un nodo place tendrá obligatoriamente como destino un nodo transition o viceversa [Dia09]. Existen tokens que representan hechos o eventos y están contenidos solo en nodos de tipo place. En general, los places representan estados y las transition representan acciones [Jen93]. WICC 2012 789 2012 XIV Workshop de Investigadores en Ciencias de la Computación Los tokens pasan de un place a otro cuando una transition se dispara. Cuando esto sucede, se genera un movimiento de tokens, ya que se eliminan tokens de los places que tiene como origen y se inyectan tokens sobre los places que tiene como destino. Esta modelo se ajusta a la ejecución de un algoritmo paralelo: los estados, o places, representan el avance del procesamiento y las transitions el cómputo. La concurrencia es inherente al modelo, ya que nada impide que varias transitions puedan dispararse en simultáneamente, si hay suficientes tokens en los respectivos places de entrada. Las Redes de Petri Coloreadas son una variedad de las Redes de Petri donde los tokens toman identidad mediante su asociación a un “color”, permitiendo representar distintos hechos [JKW07]. Esto es apropiado para nuestro estudio, ya que se identificará cada bloque de datos con un “color”. Se experimentará el uso de las CPN no solo para modelar el procesamiento paralelo, sino también como scheduler de tareas con avance desigual. Los places serán usados para representar el avance del procesamiento, y las transitions, la rutina a ejecutar. Los bloques de datos estarán representados por tokens de distinto “color“. De esta forma, cuando los places tengan disponibles los tokens necesarios para la ejecución de una rutina, esta será lanzada en algún procesador inactivo bajo una determinada política. Este scheduling tiene la ventaja de prescindir de la necesidad de explicitar los puntos de sincronización en el algoritmo paralelo y por lo tanto, cada tarea de procesamiento es lanzada con el solo requisito de que los datos y un procesador este disponible. Nuestra hipótesis es que utilizando las CPN como modelo y scheduler del algoritmo paralelo, los procesadores quedarán inactivos solo cuando no se disponga de datos para procesar, y no por esperar sincronización con otros procesadores, lo cual generará una mejora en la performance. En términos de Redes de Petri, estarán inactivos cuando los tokens necesarios aun no estén disponibles en los places que habilitan una transition. La correctitud del resultado de la ejecución del algoritmo depende del correcto modelado en la Red de Petri del algoritmo. 4. Resultados preliminares Se realizaron experimentos con el algoritmo de multiplicación de matrices y el de factorización de Cholesky. Ambos utilizando un computador equipado con un procesador AMD-Phenom II de 6 cores, 3.2 Ghz de velocidad, 8 GB de RAM y dos placas GPU NVIDIA GTX 470, con 448 stream processors y 1.2 GB de memoria en cada una de ellas. Los experimentos fueron realizados utilizando el compilador icc 11.1 de Intel y las implementaciones de BLAS y LAPACK, MKL de Intel para CPU y CUBLAS 4.0 de NVIDIA para las GPGPU’s. Los tests se realizaron utilizando 1 o 2 GPGPU’s. En cada caso, del total de 6 cores disponibles, se utilizaron 1 o 2 cores para el control de cada GPGPU y los restantes 5 o 4, para cómputo, respectivamente. 4.1. Multiplicación de Matrices El algoritmo implementado resuelve el producto de C = A × B, donde A, B y C son matrices cuadradas, de rango n, el cual se elige suficientemente grande para que el requerimiento de memoria de las matrices exceda los 1.2 GB de memoria que dispone cada placa GPGPU, pero sea inferior a los 8 GB de la memoria de la CPU. La división de datos se realizó bajo el criterio de la división desigual. Para evitar el problema de dimensiones de bloques incompatibles para realizar la multiplicación, se optó por dividir en bloques de igual tamaño, asignándole varios de estos bloques a las GPGPU’s, de forma tal que el bloque de mayor tamaño propio de las GPGPU’s es un factor del bloque asignado a cada core de la CPU. Con el objetivo de reducir el tráfico de datos entre la memoria principal y la memoria de cada GPGPU, el cómputo realizado en cada una de estas, es el producto de toda la matriz A por bandas columna de B. Si suponemos que el número de bandas es nb, las bandas columna de B son de dimensión n x (n/nb). Esto define que el producto de A por una banda columna Bi, de por resultado un bloque de la matriz resultante Ci, de rango n x (n/nb). Para asignar tareas se definió que cada core destinado a cómputo multiplique A por un bloque columna de B. Para las GPGPU, se definió un factor f > 1, tal que la tarea de cada GPGPU es la multiplicación de A por f bloques columna de B, obteniendo por resultado f bloques de C. Este esquema de división de datos presenta un obstáculo para rangos de matrices superiores a n = 16000, single presicion, debido a que toda la matriz A más las bandas de B y de C no quepan en la memoria de cada GPGPU. Con el fin de eludir este limitante, se dividió A en un número na de bandas filas, de forma tal que cada GPGPU tenga una parte Fig. 1 DIVISIÓN DE DATOS, CON VALORES nb = 16, na = 2 Y f = 6. PUEDE VERSE QUE PARTE DE B ES DESTINADA A CADA GPU Y A LOS cores. WICC 2012 790 2012 XIV Workshop de Investigadores en Ciencias de la Computación de A en lugar de su totalidad, lo que implica una importante reducción en la memoria ocupada. Un ejemplo de esta división de datos y tareas, es definir n = 16000, nb = 16, f = 6 y na = 2, usando 2 GPGPU’s. Así, los datos de B están divididos en 16 bloques columna. La división desigual se implementa asignando 6 bloques a cada GPGPU y los restantes 4 bloques, a los 4 cores destinados a cómputo de la CPU, como se grafica en Fig.(1). size (n) cpu gpu f nb na mem cpu tmpo cpu (seg) tmpo gpu (seg) flops (G) 16800 6 0 1 1 1 3144 72.3 0.0 131.1 16800 1 1 1 40 1 3144 0.0 20.0 473.0 16800 6 1 15 20 20 3144 22.1 14.5 429.1 16800 6 1 25 30 4 3144 14.7 14.3 645.2 16800 6 2 8 20 4 3144 21.9 7.6 433.0 16800 6 2 46 96 2 3144 9.0 8.6 1053.0 21400 6 0 1 1 1 5004 149.0 0.0 131.5 21400 6 1 5 10 4 5004 90.8 17.5 215.9 21400 6 1 25 30 4 5004 31.5 30.4 622.2 21400 6 2 2 8 4 5004 117.0 10.7 167.5 21400 6 2 23 50 4 5004 19.3 19.3 1015.5 24000 6 0 1 1 1 6468 209.9 0.0 131.7 24000 6 1 15 20 4 6468 65.6 37.0 421.5 24000 6 1 25 30 8 6468 42.4 42.1 652.1 24000 6 2 8 20 8 6468 92.9 66.0 277.0 24000 6 2 23 50 4 6468 25.9 25.8 1067.5 Table 1 TIEMPOS PARA DIFERENTES COMBINACIONES DE CPU’S, GPU’S, RANGO DE MATRICES Y PARTICIONES DE DATOS, single presicion. LA ÚLTIMA COLUMNA TIENE EL CÁLCULO DE LOS FLOPS OBTENIDOS. Resultados preliminares de estos experimentos se muestran en la Tabla 1. Allí son presentados valores para distintas combinaciones de rango de matriz, de GPGPU’s usados y de esquemas de división de datos. También se presentan los flops obtenidos como el resultado del total de operaciones realizadas bajo la fórmula flops = (2 ∗ n3 − n2)/tiempo y expresado en términos de Giga-flops. Solo algunos resultados seleccionados son incluidos, entre ellos el mejor valor de flops obtenido para los recursos utilizados, y otros donde el resultado no alcanza la misma eficiencia, a pesar de utilizar iguales recursos, pero con distinta división de datos. Una conclusión preliminar es que el máximo rendimiento se obtiene cuando se igualan los tiempos insumidos por cada tipo de procesador. Sin embargo, determinar cual es la división de datos que permite obtener dichos tiempos, es netamente experimental y dependiente del número de cores de CPU y de GPGPU’s destinados a cómputo, de la potencia de cómputo relativa entre ambos tipos de procesadores, y de lo optimizada que esté programada la rutina a ejecutar. El siguiente paso consiste en modelizar el scheduling con Redes de Petri para mejorar el balance de carga. 4.2. Factorización de Cholesky La factorización o descomposición de Cholesky es un método que resuelve un problema clásico del álgebra lineal, donde una matriz cuadrada simétrica y definida positiva A, de rango n, es factorizada como A = L ∗LT , siendo L una matriz triangular. Los valores de L se obtienen siguiendo las fórmulas: lij = ( aij − j−1 ∑ k=1 lik ∗ ljk ) /ljj 1 ≤ j < i ≤ n (1) lii = √ √ √ √aii − i−1 ∑ k=1 l2ik 1 ≤ i ≤ n (2) Las ecuaciones (1) y (2) imponen una fuerte dependencia de datos, ya que para calcular los elementos de una fila i, primero deben computarse los valores lij de dicha fila, con j < i y luego el elemento de la diagonal principal lii. A su vez, para computar cada lij deben primero estar calculados todos los valores de la fila i y de la fila j, hasta la columna j − 1 y luego realizar la división por ljj . El algoritmo de test es desarrollado dentro del contexto de realizar los cómputos sobre bloques de matrices y utilizar rutinas de álgebra lineal previamente desarrolladas. Supongamos que la matriz a factorizar es dividida en q × q bloques cuadrados. El cómputo de los bloques de la diagonal principal se realiza usando las rutinas xpotrf y xsyrk, y los restantes bloques con xgemm y xtsrm, donde la x = {d|s}, según se use double o single presicion. El algoritmo es expuesto en la siguiente tabla y el avance es graficado en Fig.(2), para el supuesto que q = 5. 1 i = 1 2 mientras (i ≤ q) 3 calcular el bloque diagonal liiinvocando xsyrk y luego xpotrf 4 calcular los bloques lji, j > iinvocando a xgemm y luego xtsrm 5 i = i + 1 6 volver a 2 Fig. 2 AVANCE DE TAREAS PARA EL ALGORITMO DE CHOLESKY PARA q = 5 La división de datos y distribución de las tareas es más compleja que para multiplicación de matrices. Ha de tenerse en cuenta, que de las 4 rutinas necesarias, 3 están definidas WICC 2012 791 2012 XIV Workshop de Investigadores en Ciencias de la Computación en BLAS, y una, xpotrf, está definida en LAPACK, y que la biblioteca CUBLAS solo implementa BLAS, y no LAPACK, por lo que xpotrf debe ejecutarse en CPU 1. El reporte LAWN 223 [LTN+09] trata específicamente sobre la ejecución del algoritmo en cuestión en un entorno híbrido multicore - multigpu. Plantea una división de datos homogénea (bloques cuadrados de igual tamaño), asignando tareas a las GPGPU‘s, salvo la factorización de los bloques de la diagonal principal, es decir, la llamada a xpotrf, la cual es siempre ejecutada en CPU. La dependencia de datos es respetada siguiendo un grafo de dependencia, el cual es definido específicamente para el algoritmo y el número de bloques en que sea dividida la matriz. Uno de los desafíos que el reporte señala es determinar el número de bloques a dividir la matriz para lograr un balance de carga. No hay referencia a como es armado el grafo de dependencias, lo cual de por si solo es complejo, ya que depende del algoritmo y del número de bloques. Además, no plantea la posibilidad de que la CPU realice alguna de las tareas que por defecto están asignadas a las GPGPU’s para lograr un mejor balance de carga. La utilización de una Red de Petri para guiar la ejecución del programa está orientada a eludir dichos inconvenientes, adecuándose al esquema de avance desigual de tareas. La definición de la Red puede hacerse con un meta-lenguaje, XML por ejemplo, lo cual facilita el uso del scheduler para distintos algoritmos. Si bien XML también puede utilizarse para definir un grafo de dependencias, el cambio del número de bloques en que se divida la matriz afecta el número de componentes del grafo, no así para la Red de Petri, facilitando la determinación del tamaño de bloque óptimo. Otra ventaja del uso de una Red de Petri es la flexibilidad que la misma brinda en lo referente a la asignación de tareas. La ejecución siguiendo un grafo de dependencia es relativamente estática y predeterminada. Utilizar una Red de Petri facilita la asignación dinámica de tareas. Un primer test se realizó usando dos GPGPU’s y dividiendo una matriz de rango 24000 en 6 bloques cuadrados, con datos single precision. La única restricción en cuanto a la ejecución es que la rutina xpotrf es ejecutada solamente en CPU por los motivos antes descriptos. La métrica de bondad utilizada tiene dos elementos. En primer lugar, los flops observados, usando flops = (n3/3 + n2/2 + n/6)/tiempo [Wat04] y en segundo término, el porcentaje de tiempo que cada tipo de procesador (GPGPU’s y cores) queda inactivo a la espera de datos, a los fines de determinar la tasa de uso de los procesadores. El resultado puede verse en Tabla 2. Como conclusión de este primer test, se obtuvo un buen valor de flops, y una reducida inactividad de las GPU’s. Se debe mejorar la participación de la CPU en el cómputo total ya que esta tiene una alta tasa de inactividad. De lograrse, 1Existe una versión de LAPACK para placas NVIDIA desarrollada por tercera parte, incompleta para uso libre. impactará positivamente en la reducción del tiempo global y en el valor observado de flops. size (n) blqs tmpo cpu (s) tmpo gpu1 (s) tmpo gpu2 (s) tmpo máx (s) cpu idle ( %) gpu1 idle ( %) gpu2 idle ( %) flops (G) 24000 6 10.67 9.80 9.97 10.67 62.9 11.2 13.9 431.9 Table 2 TIEMPO PARA LA FACTORIZACIÓN DE CHOLESKY CON scheduler DE PETRI, 2 GPU’S. 5. Formación de Recursos Humanos El proyecto de investigación se enmarca dentro del plan de Doctorado en Ciencias Informáticas en la Universidad Nacional de La Plata del autor. Cuatro alumnos de la carrera de Ingeniería en Computación de la UNC realizan sus tesinas de graduación en temas de este proyecto. Además, el grupo de investigadores está constituido por seis docentes del Departamento de Computación, FCEFyN, UNC. References [BLKD07] Alfredo Buttari, Julien Langou, Jakub Kurzak, and Jack J. Dongarra. A class of parallel tiled linear algebra algorithms for multicore architectures. LAPACK Working Note 191, September 2007. [Dia09] Michel Diaz. Chapter 1 - basic semantics. In Michel Diaz, editor, Petri Nets: Fundamental Models, Verification and Applications. Wiley-ISTE, 2009. [Enc] Wikipedia Free Encyclopedia. Nvidia geforce 400 series. http://en.wikipedia.org/wiki/GeForce 400 Series. [Fog11] Agner Fog. The microarchitecture of Intel, AMD and VIA CPUs. Copenhagen University College of Engineering, 2011. [Jen93] Kurt Jensen. An introduction to the theoretical aspects of coloured petri nets. In REX School/Symposium, pages 230–272, 1993. [JK09] Kurt Jensen and Lars Michael Kristensen. Coloured Petri Nets - Modelling and Validation of Concurrent Systems. Springer, 2009. [JKW07] Kurt Jensen, Lars Michael Kristensen, and Lisa Wells. Coloured petri nets and cpn tools for modelling and validation of concurrent systems. STTT, 9(3-4):213–254, 2007. [KLDB09] Jakub Kurzak, Hatem Ltaief, Jack Dongarra, and Rosa M. Badia. Scheduling linear algebra operations on multicore processors. LAPACK Working Note 213, February 2009. [LTN+09] Hatem Ltaief, Stanimire Tomov, Rajib Nath, Peng Du, , and Jack Dongarra. A scalable high performant cholesky factorization for multicore with gpu accelerators. Technical Report 223, LAPACK Working Note, November 2009. [oTICL] University of Tennessee Innovative Computing Laboratory. Plasma - parallel linear algebra for scalable multi-core architectures. http://icl.cs.utk.edu/plasma/index.html. [STD11] Fengguang Song, Stanimire Tomov, and Jack Dongarra. Efficient support for matrix computations on heterogeneous multicore and multi-GPU architectures. LAPACK Working Note 250, June 2011. UT-CS-11-668. [Wat04] D.S. Watkins. Fundamentals of Matrix Computations. Pure and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts. John Wiley & Sons, 2004. WICC 2012 792 2012 XIV Workshop de Investigadores en Ciencias de la Computación	﻿cómputo científicos , procesamiento paralelo , procesamiento híbrido multicore GPGPU , GPGPUs	es	19393
10	Indexación y consultas para bases de datos no convencionales	﻿ La constante aparición de datos en forma digital de diferentes tipos y tamaños ha dado lugar a la aparición de depósitos no estructurados de información, Bases de Datos Multimedia, donde se consultan nuevos tipos de datos (texto libre, imágenes, audio, vídeo, etc.). Esto requiere un modelo más general tal como las Bases de Datos Métricas, que además alcance un nivel de madurez similar al de las bases de datos tradicionales. Por otro lado, la creciente cantidad de estos datos exige dispositivos de almacenamiento capaces de mantenerlos y de proveer un acceso eficiente a los mismos. Dado que la brecha entre los tiempos de CPU y los de I/O se ha mantenido creciente, se hace necesario considerar memorias con mayor capacidad y más rápidas. Este panorama ha promovido la aparición estructuras de datos especializadas que tienen en cuenta estas arquitecturas como las Estructuras de datos compactas y las Estructuras de datos con I/O eficiente. Nuestra investigación apunta a contribuir a la madurez de estas nuevas bases de datos. Palabras Claves: bases de datos no convencionales, lenguajes de consulta, índices, expresividad. Contexto Esta línea de investigación pertenece al Proyecto Tecnologías Avanzadas de Bases de Datos. En el marco de este proyecto se vienen desarrollando actividades vinculadas al tratamiento de objetos de diversos tipos, estructurados y no estructurados que son de utilidad en diversos campos de aplicación, por ejemplo, robótica, visión artificial, computación gráfica, sistemas de información geográfica, computación móvil, diseño asistido por computadora, motores de búsqueda en internet, entre otras, y que se relacionan en tales bases de datos. Este proyecto pertenece a la Universidad Nacional de San Luis y se encuentra dentro del Programa de Incentivos a la Investigación. Las actividades centrales de esta línea están relacionadas con la investigación de aspectos teóricos, empíricos y aplicativos del problema general de administrar una base de datos capaz de manipular tipos de datos no convencionales. Esto incluye analizar distintos tipos de bases de datos, la expresividad de los lenguajes de consulta, los operadores necesarios para responder consultas de interés, como así también las estructuras y operaciones necesarias para responderlas eficientemente. Además nuestras investigaciones se encuadran en el marco de un proyecto dentro del Programa de Promoción de la Universidad Argentina para el Fortalecimiento de Redes Interuniversitarias III en los que participa nuestra universidad junto con las universidades de: Chile y de La Coruña (España). 1. Introducción y Motivación La brecha entre los tiempos de CPU y los de I/O se ha mantenido creciente durante las últimas décadas. Asimismo han aparecido nuevos niveles en la jerarquía de memoria (caches de tamaño cada vez más considerable). Por ello, se ha hecho cada vez más atractivo el uso de estructuras de datos que ocupen poco espacio, incluso a veces comprimiendo la información sobre la que actúan. Si bien trabajar sobre esta información compacta es más laborioso, el hecho de poder mantenerla en una memoria de órdenes de magnitud más rápida la convierte en una alternativa muy conveniente a las implementaciones clásicas. Además, la transferencia de los datos sobre una red local cuesta casi lo mismo que la transferencia a disco, por lo cual ésta también se ve favorecida con la compresión. Este escenario ha originado líneas de investigación que consideran estas diferencias de costos de operaciones, y diseñan estructuras                       231WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación de datos más eficientes para memorias jerárquicas, utilizando la compacticidad o la I/O eficiente. Particularmente nos centraremos en las estructuras de datos capaces de manipular los siguientes tipos de datos: secuencias, textos, árboles, grafos, y espacios métricos, entre otros, sobre los cuales carece de sentido realizar búsquedas exactas. Por lo tanto, se hace necesario un modelo más general tal como el de espacios métricos donde las búsquedas por similitud, más naturales sobre estos tipos de datos, son posibles. Además de diseñar estructuras de datos estáticas, planeamos investigar otros aspectos tales como la construcción eficiente (en espacio o en términos de la I/O u otras medidas de eficiencia), el dinamismo (es decir actualizaciones eficientes), operaciones de búsqueda complejas (más allá de las básicas soportadas por las estructuras de datos clásicas), tratar de obtener una mayor expresividad en los lenguajes que permitan expresarlas y caracterizar la clase de consultas computables. En los espacios de vectores, representación más común para datos multimedia, la “maldición de la dimensionalidad” describe el fenómeno por el cual el desempeño de todos los índices existentes se deteriora exponencialmente con la dimensión. Aunque los espacios vectoriales son un caso particular de espacios métricos, aún no está determinado completamente cómo afecta la dimensión a los índices para espacios métricos. El estudio de distintas maneras de optimar algunas de las estructuras para búsquedas por similitud en espacios métricos se debe a que, de las numerosas estructuras que existen, sólo unas pocas trabajan eficientemente en espacios de alta o mediana dimensión, y la mayoría no admiten dinamismo, ni están diseñadas para trabajar sobre grandes volúmenes de datos; es decir, en memoria secundaria. 2. Lenguajes de Consulta Como es deseable obtener información de una base de datos, es necesario contar con un lenguaje de consulta. Algunos de estos lenguajes son equivalentes, en su poder expresivo a la Lógica de Primer Orden (FO). Sin embargo no todas las consultas pueden ser expresadas en FO, esto ha llevado a la búsqueda de un mayor poder expresivo por medio de extensiones a la misma. Se logró incrementar la expresividad de esta lógica pero todavía aparece como incompleta, entonces se enfatizó el estudio de la expresividad de la Lógica de Segundo Orden (SO). En los últimos años se han realizado importantes investigaciones sobre la relación entre la teoría de modelos finitos y la teoría de complejidad computacional. Hay una relación cercana entre la complejidad computacional, es decir la cantidad de recursos necesarios para resolver un problema sobre algún modelo de máquina computacional, y la complejidad descriptiva, o sea el orden de la lógica que se necesita para describir el problema. La consecuencia más importante de esta relación es el resultado de Fagin [5]. Allí se establece que las propiedades de las estructuras finitas que son definidas por sentencias existenciales de segundo orden coinciden con las propiedades que pertenecen a la clase de complejidad NP, lo cual fue extendido por Stockmeyer [12] estableciendo una relación cercana entre la lógica de segundo orden y la jerarquía polinomial. Hay muchos resultados igualando la expresividad lógica a la complejidad computacional, pero ellos requieren estructuras ordenadas (ver [6], [7]). En nuestra investigación hemos introducido una restricción de SO para la estructura finita, SOF . En esta restricción los cuantificadores se extienden sobre la relación cerrada por la relación de equivalencia ≡FO. En esta relación de equivalencia las clases de equivalencia están formadas por k−tuplas cuyo FO type es el mismo, para algún entero k ≥ 1. Esta lógica es una extensión de la lógica SOω definida por Dawar [3]. Además SOω está estrictamente incluida en SOF . En el fragmento existencial de SOF , Σ1,F1 , podemos expresar cierto tipo de consultas booleanas que no puede ser expresadas en SOω. También definimos la clase de complejidad NPF y demostramos que esta clase de complejidad es capturada por el fragmento existencial Σ1,F1 . Esta temática se está desarrollando como parte de las tesis de maestría y doctorado de dos investigadores de la línea. 3. Métodos de Acceso Métricos Tomando como modelo para las bases de datos no convencionales a los espacios métricos, surge la necesidad de responder consultas por similitud eficientemente haciendo uso de métodos de acceso métricos (MAMs). En espacios métricos generales la complejidad se mide como el número de cálculos de distancias realizados. Por ello, se analizan aquellos MAMs que han mostrado buen desempeño en las búsquedas, con el fin de optimizarlos aún más, teniendo en cuenta la jerarquía de memorias.                       232WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación 3.1. Dimensión Intrínseca En los espacios vectoriales existe una clara relación entre la dimensión (intrínseca) del espacio y la dificultad de buscar. Se habla de “intrínseca”, como opuesta a “representacional”. Los algoritmos más ingeniosos se comportan más de acuerdo a la dimensión intrínseca. Hay varios intentos de medir la dimensión intrínseca en espacios de vectores, como la transformada de Karhunen-Loève (KL) y otras medidas como Fastmap y, para espacios no uniformemente distribuidos, la dimensión fractal [1]. Existen sólo unas pocas propuestas diferentes sobre cómo estimar la dimensión intrínseca de un espacio métrico tales como el exponente de la distancia [13], y la medida de dimensión intrínseca como una medida cuantitativa basada en el histograma de distancias [2]. Aunque, también parece posible adaptar algunos de los estimadores de distancia en espacios de vectores para aplicarlos a espacios métricos generales, como por ejemplo Fastmap y dimensión fractal. Muchos autores [2] han propuesto usar histogramas de distancia para caracterizar la dificultad de las búsquedas en espacios métricos arbitrarios. Existe al menos una medida cuantitativa [2], pero ella no refleja fielmente la facilidad o dificultad de buscar en un espacio métrico dado. En aplicaciones reales de búsqueda en espacios métricos, sería muy importante contar con un buen estimador de la dimensión intrínseca porque nos permitiría decidir el índice adecuado a utilizar en función de la dimensión del espacio. Además, tener una buena estimación de la dimensión nos permitiría, en algunas ocasiones, elegir la función de distancia de manera tal que se obtenga una menor dimensión. Este tema ha dado lugar a un trabajo final de la Lic. en Ciencias de la Computación, que está en desarrollo. 3.2. Árbol de Aproximación Espacial Dinámico El estudio del Árbol de Aproximación Espacial [9], uno de los MAMs que había mostrado un muy buen desempeño en espacios de mediana a alta dimensión, pero que era totalmente estático, nos permitió el desarrollo de un nuevo índice llamadoÁrbol de Aproximación Espacial Dinámico (SATD) [10] que permite realizar inserciones y eliminaciones, conservando el buen desempeño en las búsquedas. Este logro ha sido importante ya que muy pocas estructuras para espacios métricos son completamente dinámicas. El SATD es una estructura que realiza una partición del espacio considerando la proximidad espacial; pero, si el árbol agrupara los elementos que se encuentran muy cercanos entre sí, lograría mejorar las búsquedas al evitar recorrerlo para alcanzarlos. Podemos pensar entonces que construimos un SATD, en el que cada nodo representa un grupo de elementos muy cercanos (“clusters”) y relacionamos los clusters por su proximidad en el espacio. La idea sería que en cada nodo se mantenga el centro del cluster correspondiente, y se almacenen los k elementos más cercanos a él; cualquier elemento a mayor distancia del centro que los k almacenados, pasa a formar parte de otro nodo en el árbol. Esperamos así obtener una estructura más eficiente en espacios donde de antemano se sabe que pueden existir “clusters” de elementos y que aprovechando la existencia de los mismos mejore las búsquedas. Otro aspecto a analizar es cuán bueno es el agrupamiento o “clustering”que lograría esta estructura, lo cual podría estudiarse haciendo uso de nuevas estrategias de optimización de funciones a través de heurísticas bioinspiradas, las cuales han mostrado ser útiles en detección de clusters. Estos temas han promovido un trabajo final de la Lic. en Ciencias de la Computación, en desarrollo actualmente. 3.3. Join Métricos El modelo de espacios métricos permite cubrir muchos problemas de búsqueda por similitud o proximidad, aunque en general se deja fuera de consideración al ensamble o “join”por similitud, otra primitiva extremadamente importante [4]. De hecho, a pesar de la atención que esta primitiva ha recibido en las bases de datos tradicionales y aún en las multidimensionales, no han habido grandes avances para espacios métricos generales. Nos hemos planteado resolver algunas variantes del problema de join por similitud: (1) join por rango: dadas dos bases de datos de un espacio métrico y un radio r, encontrar todos los pares de objetos (uno desde cada base de datos) a distancia a lo sumo r, (2) k–pares más cercanos: encontrar los k pares de objetos más cercanos entre sí (uno desde cada base de datos). Para resolver estas operaciones de manera eficiente hemos diseñado un nuevo índice métrico, llamado Lista de Clusters Gemelos (LTC) [11],                       233WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación éste se construye sobre ambas bases de datos conjuntamente, en lugar de indexar una o ambas bases de datos independientemente. Esta nueva estructura permite además resolver las consultas por similitud clásicas en espacios métricos sobre cada una de las bases de datos independientemente. A pesar de que esta estructura ha mostrado ser competitiva y obtener buen desempeño en relación a las alternativas más comunes para resolver las operaciones de join, aún queda mucho por mejorar para que se vuelva una estructura práctica y mucho más eficiente para trabajar con grandes bases de datos métricas. De esta manera sería posible pensar en extender apropiadamente álgebra relacional como lenguaje de consulta y diseñar soluciones eficientes para nuevas operaciones, teniendo en cuenta aspectos no sólo de memoria secundaria, sino también de concurrencia, confiabilidad, etc. Algunos de estos problemas ya poseen solución en las bases de datos espaciales, pero no en el ámbito de los espacios métricos. Este tema forma parte de la tesis doctoral de uno de los investigadores de la línea. 4. Índices Compactos para Texto Un tipo de dato no convencional es el texto, en él se necesita también realizar búsquedas por similitud; donde el problema de la búsqueda aproximada de un patrón en una secuencia puede verse como: sea T = T [1, n] un texto, y P = P [1,m] un patrón sobre el alfabeto Σ (con m << n) y un entero k, se desea encontrar y devolver todos los substrings en el texto T que sean una ocurrencia aproximada de P , con a lo más k diferencias. La diferencia entre dos strings α y β se obtiene con la distancia de edición d; d(α,β ) es el mínimo número de inserciones, eliminaciones y/o sustituciones de caracteres que se deben realizar para convertir β en α. Este tipo de búsqueda tiene aplicaciones tales como la recuperación de errores (en reconocimiento óptico de caracteres, spelling), biología computacional, comunicaciones de datos, data mining, bases de datos textuales, entre otras. Una variante de los índices clásicos son los llamados índices compactos que suelen aprovechar la existencia de la jerarquía de memorias. Éstos funcionan en espacio reducido y se pueden dividir en dos grupos: sucintos y comprimidos. Una medida popular de compresibilidad de secuencias es la entropía de orden k-ésimo (Hk), según lo definido por Manzini [8]. Siendo s el alfabeto de una secuencia de símbolos S, n(i) el número de ocurrencias del i-ésimo símbolo, y n la longitud de S, otra medida popular es H0(S) = Σ(n(i) log(n/n(i)). Sea w una secuencia de longitud k y w(S) la subsecuencia de símbolos de S que siguen a w, entonces Hk(S) = 1/nΣ(|w|H0(w)) sobre todos los posibles w. Se usa en aquellos índices comprimidos que codifican un texto T de n símbolos sobre un alfabeto s usando espacio de Hk(T ) + o(n log s) bits y además pueden buscar eficientemente patrones en el texto. Observar que la codificación llana del texto toma n log s bits. La idea de los índices compactos se diferencia de la compresión pura en su capacidad de manipular los datos en forma comprimida, sin tener que descomprimirlos primero. En la actualidad los índices compactos pueden manipular secuencias de bits o de símbolos generales, árboles en general, grafos, colecciones de texto, permutaciones y mapping, sumas parciales, búsqueda por rango en una y más dimensiones, etc. Indexación con q-gramas. Un q-grama es una subsecuencia de tamaño q de un texto. Un índice de qgramas es una estructura de datos que permite encontrar rápidamente en el texto todas las ocurrencias de un q-grama dado. Existen distintas implementaciones de índices para q-gramas. La básica es un arreglo de punteros de tamaño |Σ|q . Cada posición del arreglo referencia a la lista de ocurrencias en el texto del q-grama correspondiente. Una mejora a este esquema de indexación es reducir el tamaño del arreglo de punteros por medio de un hashing eficiente, sin una demora significativa en las búsquedas. Otro enfoque usa una estructura de trie construído con los distintos q-gramas que aparecen en el texto. Cada hoja del trie contiene un puntero a la lista de ocurrencias del correspondiente q-grama en el texto. Las implementaciones anteriores encuentran todas las ocurrencias de un q-grama dado en tiempo óptimo en función del número de ocurrencias encontradas. Pero todas sufren el mismo inconveniente: el tamaño del índice se vuelve impráctico al crecer la longitud del texto. Existe un índice para q-gramas que reemplaza las listas de ocurrencias con una estructura de datos más compacta, es el Lempel-Ziv (LZ). En él, la estructura (hashing o trie) para los distintos q-gramas, ahora llamada índice primario, es aún necesaria para proveer un punto de comienzo para las búsquedas. La representación compacta de las listas de ocurrencias toma ventaja de las repeti                      234WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación ciones en el texto. La primera ocurrencia del string será llamada definición y las siguientes se llamarán frases. Luego, cada ocurrencia de un q-grama es o el primero de su clase o parte de alguna frase. La primera ocurrencia se almacenará en el índice primario y las demás se encontrarán usando un parsing de Lempel-Ziv[14] que usa la información sobre repeticiones. El índice LZ encontrará todas las ocurrencias de un q-grama en igual tiempo que los índices tradicionales. Esta temática se investiga en el marco de la tesis de maestría de un investigador de la línea. 5. Conclusiones y Trabajos Futuros Como trabajo futuro de esta línea de investigación se consideran varios aspectos relacionados al diseño de estructuras de datos que, concientes de que existe una jerarquía de memorias y de las características particulares de los datos a ser indexados, saquen el mejor partido haciendólas eficientes tanto en espacio como en tiempo. Se trabajará en particular con estructuras de datos compactas para textos, implementando un índice LZ para q-gramas y estudiando su comportamiento en comparación con los índices tradicionales. En el caso de espacios métricos, se intentará que las estructuras se adapten mejor al espacio métrico particular considerado, gracias a la determinación de su dimensión intrínseca, y también al nivel de la jerarquía de memorias en que se deba almacenar. Es importante destacar que estos estudios sobre espacios métricos y sobre algunas estructuras de datos particulares (como el SATD) permitirán no sólo mejorar el desempeño de las mismas sino también aplicar, eventualmente, muchos de los resultados que se obtengan a otras estructuras para espacios métricos. Respecto de los lenguajes de consulta se continuará analizando la expresividad de distintas extensiones de FO y de posibles restricciones SO, con el propósito de lograr caracterizar la clase de las consultas computables sobre bases de datos no convencionales.	﻿bases de datos no convencionales , lenguajes de consulta , índices , expresividad	es	19480
11	Datos no estructurados no textuales desarrollo de nuevas tecnologías	﻿ Cuando una persona recibe estímulos sensoriales de tipo visual o auditivo reacciona realizando una asociación y reconocimiento en forma natural, como consecuencia de la información que los estímulos le brindan. Durante los últimos años, el avance de los medios digitales y la proliferación de su uso ha generado la necesidad del desarrollo de herramientas que permitan la eficiente representación, procesamiento y administración (acceso y recuperación) de información de contenido multimedial. En este contexto, la información almacenada principalmente en forma de audio, imagen y video se ha convertido en la principal materia prima utilizada por los sistemas computacionales para la transmisión de información en forma rápida y eficiente, principialmente aquella relacionada con la toma de decisiones y la resolución de problemas de índole general. Dadas sus particularidades, dicha información es catalogada como información no estructurada y su administración y manipulación requiere de la definición de nuevos procesos y métodos que faciliten y agilicen el uso de la misma. Esta propuesta de trabajo establece los lineamientos a seguir con la intención de redefinir nuevas tecnologías para el procesamiento de información no estructurada que permita la incorporación de la misma en procesos generales de resolución de problemas o toma de decisiones. Palabras Claves: Procesamiento de Señales, Procesamiento de Imágenes, Visión por Computadora, Computación Gráfica, Vector Característica, Computación Paralela. Contexto Esta propuesta de trabajo se lleva a cabo dentro de la línea de Investigación “Procesamiento de Información Multimedia” del proyecto “Nuevas Tecnologías para un tratamiento integral de Datos Multimedia”. Este proyecto es desarrollado en el ámbito del Laboratorio de Investigación y Desarrollo en Inteligengia Computacional (LIDIC) de la Universidad Nacional de San Luis. 1. Introducción La administración de datos no estructurados es uno de los mayores problemas aún no resueltos en la industria de la tecnología de la información. La principal razón radica en que las herramientas y técnicas existentes han sido desarrolladas para el tratamiento de información estructurada pero fallan al momento de procesar información no estructurada. 1                       330WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación No es un secreto que gran cantidad de la información que manejan actualmente las empresas se encuentra organizada en archivos y documentos no estructurados. Debido a ello, un gran número de organizaciones han tomado conciencia de que la consolidación, acceso y procesamiento de datos no estructurados es un factor importante para la optimización y el análisis de los procesos empresariales. Cuando se hace mención a datos no estructurados o información no estructurada se hace referencia a información de computadora que no tiene un modelo formal de dato que la represente o que, en caso de poseer uno, no es fácilmente utilizable por un programa de computadora. El término distingue este tipo de información de aquella que se encuentra almacenada en bases de datos y organizada a través de campos. En particular, el entorno de la Web ha propiciado la ocurrencia de situaciones en donde es necesario el tratamiento de información, la cual se encuentra compuesta por datos que: Poseen una estructura formal definida, no obstante su estructura no es útil para el desarrollo de ciertas tareas de procesamiento por lo que deberían ser categorizados como no estructurados. Si bien no tienen una estructura formalmente definida, ésta se encuentra implícita. Desde el punto de vista humano, dicha estructura puede ser inferida a partir de la simple observación (imagen o video), la escucha (relato o canción), o la lectura (texto) reconociendo interrelaciones, morfologías, sintáxis, etc.. En forma individual no poseen una estructura definida, sin embargo ellos suelen encontrarse empaquetados en objetos tales como archivos o documentos los cuales tienen una estructura explícita (documentos multimediales, páginas web, etc.). En consecuencia, todo software que pretenda automatizar la inferencia de información debería crear estructuras aptas para ser procesadas en forma automática por una máquina, donde se explote la estructura linguistica, auditiva y visual inherente a todas las formas de comunicación humana. Es decir, debe aumentarse la flexibilidad de los sistemas los cuales deberían extraer significado de los datos no estructurados, para luego utilizarlo en la identificación de interrelaciones y la administración de los mismos. En la actualidad existen algoritmos y bibliotecas que intentan extraer información de los datos estructurados, yendo desde consultas XPath y analizadores de protocolos hasta el procesamiento de lenguaje natural (Natural Language Processing) y Visión por Computadora. Dependiendo del análisis final a realizar a los datos será el tipo de algoritmos utilizados, no obstante al momento de definir un sistema, todos comparten las mismas características. Un sistema para el procesamiento de datos no estructurados de tiempo real requiere estar conformado básicamente por cuatro partes: Objetos de Datos no Estructurados: El procesamiento de los datos comienza con la representación de los mismos. Una plataforma debería poseer un mecanismo eficiente y robusto de representación de los objetos, en conjunto con, entre otros, la administración de memoria. Lenguaje Extensible: La mayor parte del procesamiento de los datos no estructurados involucra algoritmos especializados. El lenguaje debe ser extensible de modo que algoritmos con dominio específico puedan coexistir con aquellos de funcionalidad genérica. Dado que los algoritmos se encuentran mayormente disponbiles a través de bibliotecas, es importante que las API soporten lenguajes de uso corriente. Herramientas de autoría que consideren el uso de datos no estructurados: Las herramientas de autoría son una parte crítica de cualquier sistema moderno, y más aún es importante que dichas herramientas sean diseñadas para el procesamiento datos no estructurados en tiempo real.                       331WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación Capacidades de Clustering: Toda aplicación de procesamiento de datos no estructurado implica el uso intensivo de recursos. La distribución de la carga entre un gran número de servidores es una técnica de escalado crítica así como también el uso de multi-threads en un único servidor. En función a lo planteado, las investigaciones para el tratamiento de la problemática a abordar podrían organizarse acorde con cuatro grandes objetivos: La obtención de una representación robusta para cada objeto no estructurado. Intentando el reconocimiento y clasificación en forma automática para la resolución de problemas tradicionalmente complejos tales como la reconstrucción de escenarios 3D y el seguimiento de objetos en movimiento, entre otros [3, 15, 1, 2, 7, 9, 12, 13, 14, 16, 23]. Así como también la definición de nuevas estrategias de almacenamiento y recuperación desde grandes repositorios de almacenamiento [41, 42]. La transferencia de los logros obtenidos con los diferentes datos no estructurados. Intentando utilizar los métodos o técnicas desarrollados con las nuevas representaciones en otros tipos de objetos [4, 6, 10, 11, 20, 21, 22, 24, 27, 28, 29, 33, 34, 35, 37, 38, 39]. El abordaje de problemas multimedia. Donde la especificación de una única representación general para todos los tipos de datos no estructurados habilita el tratamiento de información multimedia en forma sistematizada con mayor probabilidad de éxito que los métodos actuales [5, 36, 40, 43, 44, 45]. El uso de técnicas computacionales de alto desempeño, aplicadas al proceso de Adquisición, pre-procesamiento y análisis de datos no estructurados [7, 17, 18, 19, 25, 26, 30, 31, 32]. En la siguiente sección se detallan algunas líneas de trabajo específicas a abordar según los cuatro objetivos generales detallados. 2. Líneas de Investigación y Desarrollo Se propone profundizar los estudios asociados con la adquisición, pre-procesamiento y análisis de datos no estructurales no textuales, como así también tratamiento de tendientes a mejorar la transmisión de información mediante el análisis y/o optimización del proceso de producción/obtención de información de contenido a partir de datos no estructurados (audio, imágenes, video, gráficas y texto) mediante el uso de métodos no convencionales. El trabajo con datos no estructurados para la transmisión de información involucra tres áreas claramente definidas: Procesamiento de Señales, Visión por Computadora y Computación Gráfica; así como también de la interacción de éstas con otras áreas, tales como la Minería de Datos, Recuperación de Información, Computación de alto rendimiento, entre otras. En consecuencia, las líneas específicas a seguir son: El tratamiento de imágenes para la segmentación bajo criterios perceptuales. Esta línea propone determinar el impacto de la segmentación de imágenes, acorde con criterios perceptuales, en la categorización de las imágenes en clases semánticas y/o geométricas. El reconocimiento y reconstrucción de escenarios 3D. Como consecuencia de la línea anterior, a partir de la correcta identificación y categorización de los objetos interactuantes en una imagen así como de sus interrelaciones, es deseable la reconstrucción del escenario completo capturado en una imagen o colección de ellas. El tratamiento de Streamings de Video para el seguimiento de objetos en                       332WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación movimiento. Dada la gran importancia del procesamiento de transmisiones de canales de TV, bases de datos de video compartido (Google, You Tube), seguridad a través de circuitos cerrados de video, derechos de autor, entre otros, se pretende concentrar el trabajo en el análisis y procesamiento de video en tiempo real. La determinación de Huellas Digitales Robustas de Señales. El objetivo consiste en desarrollar un método robusto para la determinación de huellas digitales de señales (imagen, streamings de audio o video), que permitan identificar e individualizar una señal a pesar de las distorsiones naturales (compresión, codificación analógica, entre otros) y ataques maliciosos (adición de logo, distorsión geométrica, cortes en la señal, entre otros) en forma eficiente. El tratamiento de Streamings de Sonido para el reconocimiento robusto de segmentos de audio. La identificación de objetos de audio a partir de segmentos es necesario en diferentes áreas tales como el monitoreo de canales de sonido, detección de duplicaciones, plagios, rotulado automático (MP3 modernos), consulta por ejemplos y filtrados en redes p2p, entre otros. La adaptación de algoritmos efectivos para clustering de documentos en problemas de clustering y segmentación de imágenes. Esta línea se focalizará en la transferencia de la experiencia lograda en el clustering de documentos cortos a tareas similares con otros tipos de datos no estructurados como por ejemplo clustering de imágenes y segmentación de imágenes. Como puede observarse, desde un punto de vista computacional, los datos no estructurados y la información contenida en ellos son un nexo que habilita a la resolución de una variedad de problemas en forma colaborativa entre áreas, permitiendo una realimentación de conocimiento y enriquecimiento hacia nuevas líneas de trabajo. 3. Resultados obtenidos / esperados Dentro de los trabajos desarrollados en el ámbito de las distintas líneas de investigación propuestas se pueden destacar los relacionados al desarrollo de un Sistema de Minería de Imágenes (SMI), principalmente a la etapa de pre-procesamiento, transformación y extracción de características relacionadas al subsistema Qué? asociado al Sistema Visual Humano (Color, Forma y Textura) y al subsistema Dónde? (Punto de Fuga y Gradiente de Texturas). Dado que un SMI demanda un alto costo de recursos y procesamiento, es necesario la búsqueda de técnicas alternativas que permitan reducir los mencionados costos, se propusieron diferentes optimizaciones aplicando modelos de computación paralela propuestas en la tesis doctoral de una integrante del grupo responsable. La generación de vectores de características robustos depende en gran medida de la correcta selección de la información esencial de los objetos (audio, imagen, video), a ser resumida en dichos descriptores. El área de No Fotorealismo es un área con gran riqueza de métodos y técnicas que contribuyen en la determinación de identificadores robustos, considerando no sólo las características propias del objeto multimedia evaluado (como color, forma, etc.), sino también las posibles distorciones (geométricas, de luz, de calidad, entre otras) a las que pueda ser sometido. Los trabajos desarrollados han sido orientados no sólo en esta dirección, sino que además se pretende lograr la universalidad de la representación (independencia del tipo de objeto multimedia). En la actualidad, los conocimientos adquiridos han sido de gran aporte para el abordaje de nuevas investigaciones relacionadas a la extracción de características alternativas de la información de audio e imágenes sobre arquitecturas GPU                      333WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación CPU. 4. Formación de Recursos Humanos Como resultado de las investigaciones se cuenta con una tesis de maestría concluida y dos tesis doctorales y dos de maestría en desarrollo; así como también varios trabajos de fin de carrera de la Licenciatura en Ciencias de la Computación. Además las investigaciones se encuadran en el marco de un proyecto dentro del Programa de Promoción de la Universidad Argentina para el Fortalecimiento de Redes Interuniversitarias III en los que participa nuestra universidad junto con las universidades Michoacana (México) y de Zaragoza (España). Referencias [1] Blaschke, T. and Hay, G., “Object-oriented image analysis and scale-space: Theory and methods for modeling and evaluating multiscale landscape structure”. International Archives of Photogrammetry and Remote Sensing 34: 22-29 (2001). [2] A.L. Callahan and Dmitry and B. Goldgof and Ph. D and Ph. D and Thomas A. Sanocki and Melanie A. Sutton, “Function from visual analysis and physical interaction: a methodology for recognition of generic classes of objects”, Journal on Image and Vision Computing, Vol 16, pp 745-763, 1998. [3] A. Camarena-Ibarrola, E. Chavez, “On Musical Performances Identification, Entropy and String Matching”, MICAI 2006. [4] P. Carbonetto, N. de Freitas, and K. Barnard, “A statistical model for general contextual object recognition”, in Proc. ECCV, 2004. [5] (CUDA)-IEEE International Conference on Multimedia and Expo 2008 ? Pp 697:700 ? April 2008. [6] Crockett, et al.,“A Method for Characterizing and Identifying Audio Based on Auditory Scene Analysis”, AES Convention Paper 6416, presentedat the 118.sup.th Convention May 28-32, 2005, Barcelona, Spain. cited by other. [7] Dixon, S.: Live tracking of musical performances using on-line time warping. Proc of the 8th Int Conf on Digital Audio Effects (DAFx?05) (2005) [8] A. Ess, B. Leibe, and L. Van Gool. “Depth and appearance for mobile scene analysis”. In ICCV, 2007. [9] R. Fergus, P. Perona, and A. Zisserman. “Object class recognition by unsupervised scaleinvariant learning”. In IEEE Conference on Computer Vision and Pattern Recognition, 2003. [10] M.A. Fischler and R.C. Bolles. “Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography”. Communications of the ACM, 24(6):381?395, June 1981. [11] G. Kim, C. Faloutsos, and M. Hebert, “Unsupervised Modeling and Recognition of Object Categories with Combination of Visual Contents and Geometric Similarity Links”, ACM International Conference on Multimedia Information Retrieval (ACM MIR), October, 2008. [12] Hay, G.J., Marceau, D. J., Dube, P., and Bouchard, A. “A Multiscale Framework for Landscape Analysis: Object-Specific Analysis and Upscaling”. Landscape Ecology 16 (6): 471-490 (2001). [13] A. Hoiem, A. Efros, and M. Hebert, “Geometric context from a single image”. In ICCV, 2005. [14] Hoiem, A.A. Efros, and M. Hebert, “Closing the Loop on Scene Interpretation”, In CVPR 2008. [15] Ibarrola, A.C., Chavez, E.. “A robust, entropy-based audiofingerprint”. IEEE, July 2006.                       334WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación [16] B. Leibe, N. Cornelis, K. Cornelis, and L. Van Gool. “Dynamic 3d scene analysis from a moving vehicle”. In CVPR, 2007. [17] Liu Yang, Rong Jin, Caroline Pantofaru, and Rahul Sukthankar, “Discriminative Cluster Refinement: Improving Object Category Recognition Given Limited Training Data”, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June, 2007. [18] B. Moghaddam, “Principal Manifolds and Probabilistic Subspaces for Visual Recognition”, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 24, Issue 6, June 2002, pp. 780-788. [19] Pech-Pacheco, José L., álvarez-Borrego, Josué, Cristóbal, Gabriel, Keil, Matthias S., “Automatic object identification irrespective of geometric changes”, International Society for Optical Engineering (SPIE), Vol 42(2): 551-559 (2003). [20] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora, and S. Belongie. “Objects in context”. In ICCV, 2007. [21] A. Saxena, M. Sun, and A. Y. Ng. “Learning 3-d scene structure from a single still image”. In ICCV 3dRR-07, 2007. [22] H. Schneiderman, “Learning a restricted bayesian network for object detection,” in Proc. CVPR, 2004. [23] Torres M., “Is there any hope for face recognition?”, Proc. of the 5th International Workshop on Image Analysis for Multimedia Interactive Services, WIAMIS 2004, 2123 April 2004, Lisboa, Portugal. [24] R. Unnikrishnan, C. Pantofaru, and M. Hebert, “Toward Objective Evaluation of Image Segmentation Algorithms”, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 29, No. 6, June, 2007, pp. 929944. [25] P. Viola, M. J. Jones, and D. Snow, “Detecting pedestrians using patterns of motion and appearance,” in Proc. ICCV, 2003. [26] P. Viola and M. J. Jones, “Robust real-time face detection,” IJCV, vol. 57, no. 2, 2004. [27] Xiaoxing Li; Mori, G.; Hao Zhang, Expression-Invariant Face Recognition with Expression Classification, The 3rd Canadian Conference on Computer and Robot Vision, Volume , Issue , 07-09 June 2006 Page(s): 77 ? 77, Digital Object Identifier:10.1FER03/CRV.2006.34. 2006. [28] W. Zhang, B. Yu, D. Samaras, and G. Zelinsky. “Object class recognition using multiple layer boosting with heterogeneous features”. In IEEE Conference on Computer Vision and Pattern Recognition, 2005. [29] W.Y. Zhao, R. Chellappa, “Image-based Face Recognition: Issues and Methods, Image Recognition and Classification”, Ed. B. Javidi, M. Dekker, 2002, pp. 375-402. [30] J Fernández, N. Miranda, R. Guerrero, F. Piccoli, “Image identification thru Multilevels parallelism”, In 8th International Information and Telecommunication Technologies Symposium (I2TS2009), IEEE. Pp. 314-328, ISBN: 978-85-89264-11-2, Florianopolis, DF, Brazil, December 2009. [31] Fernández J., Miranda N., Guerrero R., Piccoli F., “Multi-level Paralelism in Image Identification”, In XVIII Congreso sobre Métodos Numéricos y sus Aplicaciones, (ENIEF 2009) Vol 28. Pp: 227-240. ISSN 1666-6070. Univ. Del Centro de la Prov. de Buenos Aires. Tandil ? Bs. As. Noviembre 2009. [32] Fernández J., Miranda N., Guerrero R., Piccoli F. “A Distributed Computing for an Image Processing Function Set”. In XVII Congreso sobre Métodos Numéricos y sus Aplicaciones, (ENIEF 2008), Pp: 2895-2906. ISSN 1666-6070. Univ. Nac. de San Luis. San Luis. Noviembre 10, 2008. [33] Fernández J., Miranda N., Guerrero R., Piccoli F. “Driving to a Fast IMS Feature Vector Computing”. In 14to Congreso Argentino de Ciencias de la Computación (CACIC 2008), Pp 1993-2004, ISBN 978-987-24611-02, Univ. Nac. de Chilecito, La Rioja, Argentina. Octubre 1, 2008.                       335WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación [34] Adachi M. and Shibata T., “Image representation algorithm featuring human perception of similarity for hardware recognition systems”, In Proc. of the Int. Conf. On Artificial Intelligence (IC-AI?2001), volume 1, pages 229?234. CSREA Press, Las Vegas, Nevada, USA, 2001. ISSN 1-892512-78-5. [35] Cantoni V., Cantoni V., Lombardi L., Porta M., and Sicard N., “Vanishing point detection: Repre sentation analysis and new approaches”. In Proceedings of the 11 th International Conference on Image Analysis & Processing, pages 26?28. 2001. [36] Dong W., Zhou N., and Paul J.C., “Perspective-aware texture analysis and synthesis”, In Vis. Comput., 24(7):515?523, 2008. ISSN 0178-2789. [37] Ilonen J., Kamarainen J., Paalanen P., Hamouz M., Kittler J., and Kalviainen H. “Image feature localization by multiple hypothesis testing of gabor features”. 17(3):311?325, 2008. [38] Pan Q., Min-Gui Z., De-Long Z., Yong-Mei C., and Hong-Cai Z, “Face recognition based on singular-value feature vectors”. In Optical engineering, volume 42, pages 2368?2374. Society of Photo-Optical Instrumentation Engineers, Bellingham, 2003. ISSN 0091-3286. [39] Seo K.S., Lee J.H., and Choi H.M. “An efficient detection of vanishing points using inverted coordinates image space”. Pattern Recogn. Lett., 27(2):102?108, 2006. ISSN 0167-8655. doi:http://dx.doi.org/10.1016/j.patrec.2005.07.011. [40] Serre T., Wolf L., and Poggio T. “Object recognition with features inspired by visual cortex”. In IEEE CSC on CVPR. 2005. [41] Lee K. and Street W., “Automatic feature mining for personalized digital image retrieval”, In ACM, editor, Proceedings of the International Workshop on Multimedia Data Mining (MDM/KDD 2001), pages 38?43. ACM, San Francisco, USA, 2001. [42] Tan K., Ooi B., and Yee C., “An evaluation of color-spatial retrieval techniques for large image databases”. Multimedia Tools and Applications, 14(1):55?78, 2001. [43] Marquis-Bolduc M., Descheˆnes F., and Pan W., “Combining apparent motion and perspective as visual cues for content-based camera motion indexing”. Pattern Recogn., 41(2):445?457, 2008. ISSN 0031-3203. doi:http://dx.doi.org/10.1016/j.patcog.2007.06.021. [44] Rosin P.L., “Training cellular automata for imageprocessing”. In IEEE Transactions on Image Processing, volume 15, pages 2076?2087. 2006. [45] Wang C., Yu-bin Y., Wu-jun L., and Shifu C., “Image texture representation and retrieval based on power spectral histograms”. In Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004). 2004.                       336WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación	﻿Visión por computadoras , Computación Gráfica , computación paralela , Procesamiento de Señales , Procesamiento de Imágenes , Vector Característica	es	19506
12	Usando sistemas dedicados para computación paralela de propósito general	﻿ Esta propuesta de trabajo se lleva a cabo dentro de la línea de Investigación “ Sistemas Paralelos” del proyecto “Nuevas Tecnologías para un tratamiento integral de Datos Multimedia”. Este proyecto es desarrollado en el ámbito del Laboratorio de Investigación y Desarrollo en Inteligengia Computacional (LIDIC) de la Universidad Nacional de San Luis. 2. Resumen Los sistemas diseñados para resolver problemas específicos como los procesadores gráficos(GPU), tienen características (bajo precio en relación a su potencia de cálculo, gran paralelismo, optimización para cálculos en coma flotante) muy atractivas para su uso en aplicaciones de propósito general, en problemas relacionados al ámbito científico, de simulación, ingeniería, entre otros. Esto llevó al desarrollo de herramientas y técnicas para facilitar su utilización y transformarlos en una alternativa válida y casera para resolver la mayor cantidad de problemas. En este trabajo se presentan las características básicas de las GPU y las distintas líneas de trabajo a seguir. Estas líneas tienen en común la consideración de la GPU como computadora masivamente paralela. Los problemas a tratar están relacionados a las Redes de Computadoras y las Bases de Datos. 3. Introducción El poder computacional asociado a las tecnologías dedicadas a fines específicos y la posibilidad que ofrecen de mejora constante y bajo costo, han constituido una alternativa válida a las supercomputadoras paralelas. El ejemplo más popular de las tecnologías dedicadas son las GPU (Unidad de Procesamiento Gráfico)[4, 22]. Una tarjeta de video puede proporcionar hasta 50 veces más poder de cómputo que la computadora huésped en algunas aplicaciones[21]. Por muchos años la GPU fue utilizada exclusivamente para acelerar el cálculo de ciertas aplicaciones relacionadas directamente con el procesamiento de imágenes, en aplicaciones como videojuegos o 3D interactivas, por ejemplo. Su buen desempeño en este ámbito, junto a su constante y rápida evolución (comparada con los microprocesadores de propósito general), un número de instrucciones menor, y sin aritmética de doble precisión [20, 22], ha permitido desarrollar un modelo de supercómputo casero en donde, con menos recursos económicos de los requeridos para comprar una PC, es posible resolver cierto tipo de problemas aplicando un modelo de paralelismo masivo sobre una arquitectura de procesadores con varios núcleos, memoria compartida y soporte multihilos. Existen alternativas para procesamiento en GPU, la más ampliamente utilizada es la tarjeta Nvidia, para la cual se ha desarrollado un kit de programación en C, con un modelo de comunicación de datos y de control de hilos proporcionado por un driver, el cual provee una interfaz GPU-CPU [19]. Este ambiente de desarrollo llamado Compute Unified Device Architecture (CUDA) ha sido diseñado para simplificar el trabajo de sincronización de hilos y la comunicación con la GPU [5, 23] propone un modelo de programación SIMD(Simple Instrucción, Múltiples Datos) con 1                       622WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación funcionalidades de procesamiento de vector. Dadas las ventajas de poder computacional, bajo costo, continua evolución, bandwidth de memoria, flexibilidad y programabilidad de la GPU y a pesar de su limitación y dificultad para resolver cualquier tipo de aplicaciones siguiendo un modelo de programación no usual, se intenta aprovechar la gran potencia de cálculo de las GPU para aplicaciones no relacionadas con los gráficos, en lo que se conoce como GPGPU(General Purpose GPU)[24]. La línea de investigación que se propone seguir pretende evaluar la factibilidad de utilizar la GPU como computadora masivamente paralela para obtener soluciones de alto desempeño a problemas de propósito general, tal como los derivados de la administración de redes de computadoras y de base de datos. 4. Líneas de Investigación y Desarrollos Utilizar arquiteturas dedicadas, como la GPU, para resolver computacionalmente problemas de naturaleza distinta a la de ellas, implica plantear soluciones paralelas a los problemas considerando el modelo de programación propio de sus interfaces. Entre las líneas de investigación y desarrollo que actualmente se siguen se encuentran: Seguridad en Redes de Computadoras La seguridad en sistemas de computación es un tema de gran interes, por su amplitud en las áreas que abarca y su constante evolución. Las redes de computadoras constituyen un ámbito natural para su aplicación[7, 27]. Si bien es cierto que existen numerosas tecnologías para hacer segura una red de computadoras, la Detección de Intrusiones(IDS) constituye una de las más populares[10, 17]. Su objetivo es monitorear las actividades en los sistemas de computación a fin de encontrar violaciones de seguridad. Los IDS junto con otras herramientas procuran detectar mediante el monitoreo del tráfico entrante y saliente de un nodo, los intentos de ingresar a la red sin autorización. La mayoría de los IDS, en las redes actuales, basan su seguridad en un conjunto de reglas, las cuales sirven para establecer la autenticidad o no del mensaje. La técnica más común es realizar una búsqueda de firmas (Signatures) en cada uno de los paquetes circulando en la red [9, 13, 26]. Este proceso implica un alto costo computacional, no sólo por el tiempo involucrado en obtener la solución, sino también por la gran cantidad de recursos del sistema. Existen numerosas propuestas para optimizar o reducir el costo de los IDS a través de la aplicación de técnicas de computación de alto desempeño: técnicas de computación paralela o utilización de hardware especializado [2, 6, 12, 25]. Si bien los resultados obtenidos muestran eficiencia, las soluciones son complejas y poco flexibles. Investigaciones recientes están considerando las Unidades de Procesamiento Gráfico(GPUs) como una posibilidad a la hora de acelerar las desiciones de un IDS y de otras aplicaciones en redes [8, 18]. Base de Datos Una Base de Datos(BD) es una colección de datos organizados y relacionados entre sí, los cuales son recolectados y analizados, de forma rápida y ordenada, por los sistemas de información de una empresa o negocio en particular [11, 16]. Actualmente, las BD almacenan información no necesariamente estructurada, cualquier conjunto de datos pertenecientes a un mismo contexto y almacenados sistemáticamente para su posterior uso constituye una BD. Casi todas las bases de datos actuales son Sistema de Gestión de Bases de Datos (DBMS)[11]. Un DBMS establece que una BD no es simplemente un conjunto de archivos, sino además una serie de herramientas para manipular la información almacenada. Es esta característica la que permite, entre otras cosas, hacer consultas y resolverlas obteniendo el conjunto de datos que las satisfagan. Entre las herramientas incormparadas se encuentran los índices (o índice inverso), listas ordenadas de términos (en cualquier formato), los cuales representan a uno o varios datos de la BD. Las consultas a bases de datos de gran escala como Google o eBay significan encontrar un                       623WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación pequeño grupo de entradas específicas entre unas decenas de millones de posibilidades en el menor tiempo posible. Esto implica que un procesador debe resolver consultas de un usuario, o miles, en la misma BD, sobre dominios diferentes y al mismo tiempo, en unos pocos milisegundos. Los índices facilitan dichas consultas; a través de ellos se delimita el acceso a un subconjunto de los datos pertenecientes al universo, y permite dar respuesta a las consultas realizadas. Proveer métodos eficientes para índices y para resolver consultas es primordial, algunos ya lo están intentando [14, 20]. Sistema de Información Geográfica Un Sistema de Información Geográfica (GIS) se lo define como un conjunto de equipos informáticos, de programas, de datos geográficos y de técnicas organizadas para recoger, almacenar, actualizar, manipular, analizar y presentar eficientemente todas las formas de información georeferenciada[1, 29]. Los GIS permiten responder preguntas de: Localización: ¿Qué hay en...?; Condición: ¿Dónde se encuentra?; Tendencia: ¿Qué ha cambiado desde...?; Distribución: ¿Qué patrones de distribución espacial existen?; y Modelización: ¿Qué sucede si...?.. La información geográfica administrada por un GIS contiene una referencia territorial explicita como latitud y longitud o una referencia implícita como domicilio o código postal. Ésta es el elemento diferenciador de un SIG frente a otro tipo de Sistemas de Información. Generalmente la información geográfica es adminstrada en una BD espacial. Una BD espacial es un caso especial de BD. Ésta se carateríza por optimizar el almacenamiento y las consultas de objetos en el espacio, incluyendo puntos, líneas y polígonos. Un Sistema de administración de Bases de Datos espaciales (SGBDE) consiste en una colección de tipos de datos espaciales, operadores, índices y estrategias de procesamiento[29]. Sus componentes incluyen: modelo de dato espacial, lenguaje de consulta, procesamiento de consultas, organización de archivos e índices y optimización de consultas. Un dato espacial es una variable asociada a una localización del espacio, entre otros. Los datos espaciales refieren a entidades o fenómenos que cumplen con los principios básicos de tener: Posición absoluta en un sistema de coordenadas (x,y,z), Posición relativa respecto a otros elementos del paisaje, Una figura geométrica que las representan, y Atributos que lo describen. Las imágenes satelitales son ejemplos de datos espaciales, su procesamiento debe hacerse respecto a un marco de referencia espacial, posiblemente la superficie de la tierra. La utilidad de los GIS y las bases de datos espaciales es muy amplio, entre alguno de sus usos están los Mapas de población, Mapas de densidades, Cálculo de distancias, Cartografía. Sus múltiples usos y el incremento de la complejidad de los datos (producto de los nuevos medios de obtención de datos: sensores, satélites, entre otros; y los requerimientos de mayor detalle de análisis) hacen que obtener mejores tiempos de respuesta sea un desafío constante[30]. El común denominador de los tres problemas planteados anteriormente es el rápido crecimiento del volumen de datos a tratar en cada uno y la necesidad de resolverlos en forma rápida, eficiente y precisa. Analizar las ventajas y desventajas de utilizar la tecnología de GPU y su modelo de programación asociado constituye una imnovadora línea de investigación. 5. Resultados obtenidos / esperados El principal aporte de esta línea de investigación es analizar la factibilidad de utilizar la arquitectura GPU y su modelo de programación asociado para desarrollar soluciones de alto desempeño a problemas de propósito general, estableciendo los límites del modelo GPU-CPU para las soluciones eficientes y eficaces de cada uno de los problemas planteados. Actualmente se está trabajando en las dos primeras líneas de investigación. 6. Formación de Recursos Humanos Los resultados esperados respecto a la formación de recursos humanos son hasta el momento                       624WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación una tesis de maestría y un doctorado en desarrollo; así como también varios trabajos de fin de carrera de la Licenciatura en Ciencias de la Computación. Acualmente se ha obtenido una beca otorgada por la Facultad de Ciencias Físico, Matemáticas y Naturales de la Universidad Nacional de San Luis, categoría postgrado. Referencias [1] O. O. Ayeni, D. N. Saka and G. Ikwuemesi - Developing a multimedia GIS database for tourism industry in NIGERIA [2] Z. K. Baker and V. K. Prasanna -Time and area eficient pattern matching on FPGAs - In Proceedings of the 2004 ACM/SIGDA 12th International Symposium on Field Programmable Gate Arrays (FPGA 04) - Pp 223232 - New York, NY, USA. 2004. [3] P. Bakkum, K. Skadron - Accelerating SQL database operations on a GPU with CUDA. Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units. ACM International Conference Proceeding Series; Vol. 425. Pp: 94-103. Pittsburgh, Pennsylvania. ISBN:978-1-60558935-0. 2010. [4] Buck, I. - GPU computing with NVIDIA CUDA - SIGGRAPH ’07: ACM SIGGRAPH 2007 courses ACM, New York, NY, USA. 2007. [5] Chen, W. , Hang, H. - H.264/AVC motion estimation implementation on Compute Unified Device Architecture (CUDA) - IEEE International Conference on Multimedia and Expo 2008 - Pp 697:700 - April 2008. [6] C. Clark, W. Lee, D. Schimmel, D. Contis, M. Kone, and A. Thomas - A hardware platform for network intrusion detection and prevention - In Proceedings of the 3rd Workshop on Network Processors and Aplications (NP3) 2004. [7] D. E. Comer - Computer Networks and Internets - ISBN 0136061273 - Prentice Hall, 2008. [8] D. L. Cook, J. Ioannidis, A. D. Keromytis, and J. Luck - Cryptographics: Secret key cryptography using graphics cards - In Proceedings of RSA Conference, Cryptographer’s Track (CT-RSA) - Pp 334-350, 2005. [9] G. Cretu, A. Stavrou, S. Stolfo, A. Keromytis - Data Sanitization: Improving the Forensic Utility of Anomaly Detection Systems - In the Proceedings of the Third Workshop on Hot Topics in System Dependability - Edinburgh, UK - June 2007 [10] T. Crothers - Implementing Intrusion Detection Systems - Wiley - 2003. [11] C. J. Date - Database in depth: relational theory for practitioners. O’Reilly Media. ISBN 0596100124, 9780596100124. 2005. [12] S. Dharmapurikar and J. Lockwood - Fast and scalable pattern matching for content Filtering - In Proceedings of the 2005 ACM symposium on Architecture for networking and communications systems (ANCS 05), Pp 183192, New York, NY, USA, 2005. ctober 2004. [13] V. Frias-Martinez, S. Stolfo, A. Keromytis - Behavior-Profile Clustering for False Alert Reduction in Anomaly Detection Sensors In the Proceedings of the Annual Computer Security Applications Conference (ACSAC) 2008. [14] N.K. Govindaraju, N. Raghuvanshi, M. Henson, D. Tuft and Dinesh Manoch - A cacheefficient sorting algorithm for database and data mining computations using graphics processors - 2005. [15] B.He, K. Yang, R. Fang, M. Lu, N. Govindaraju, Q. Luo, P. Sander - Relational joins on graphics processors. International Conference on Management of Data. Proceedings of the 2008 ACM SIGMOD international conference on Management. Vancouver, Canada Pp: 511-524. ISBN:978-1-60558-102-6. 2008. [16] J.A. Hoffer, M. Prescott, H. Topi - Modern Database Management. Prentice Hall. ISBN 0136003915, 9780136003915. 2008. [17] P. Inella, O. McMillan - An Introduction to Intrusion Detection Systems - SecurityFocus.com                       625WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación [18] N. Jacob and C. Brodley - O?oading IDS computation to the GPU - In Proceedings of the 22nd Annual Computer Security Aplications Conference on Annual Computer Security Aplications Conference (ACSAC 06), Pp 371-380, Washington, DC, USA, 2006. [19] Joselli, M., Zamith, M., Clua, E., Montenegro, A., Conci, A., Leal-Toledo, R. Valente, L., Feijo, B., dórnellas, M., Pozzer, C - Automatic Dynamic Task Distribution between CPU and GPU for Real-Time Systems - . 11th IEEE International Conference on Computational Science and Engineering, 2008 ( CSE ’08) - Pp 48:55 - July 2008. [20] Lieberman, M.D. and Sankaranarayanan, J. and Samet, H. - A Fast Similarity Join Algorithm Using Graphics Processing Units ICDE 2008. IEEE 24th International Conference on Data Engineering 2008 - Pp 1111:1120 - April 2008. [21] Lloyd, D., Boyd, C., Govindaraju, N. - Fast computation of general Fourier Transforms on GPUS - IEEE International Conference on Multimedia and Expo - Pp 5:8 - April 2008. [22] Luebke, D., Humphreys, G. - How GPUs Work Computer - vol 40, N 2 - Pp 96:100 - ISSN 0018-9162 - Feb. 2007. [23] Luebke, D. - CUDA: Scalable parallel programming for high-performance scientific computing - 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro. ISBI 2008. Pp 836:838 - May 2008. [24] J.D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Krüger, A.E. Lefohn and T. Purcell - Survey of General-Purpose Computation on Graphics Hardware - Eurographics 2005, State of the Art Reports. Pp. 21-51. September 2005 [25] V. Paxson, R. Sommer, and N. Weaver. An architecture for exploiting multi-core processors to parallelize network intrusion prevention. In Proceedings of the IEEE Sarno? Symposium, May 2007. [26] Y. Song, A. D. Keromytis and S. J. Stolfo - Spectrogram: A Mixture-of-Markov-Chains Model for Anomaly Detection in Web Traffic - In the Proceedings of the 16th Annual Network & Distributed System Security Symposium (NDSS). San Diego, CA, USA. February 2009. [27] A. Tanenbaum - Computer networks - ISBN 8120321758 - Prentice-Hall - 2007 [28] G. Vasiliadis, S. Antonatos, M.Polychronakis, E. Markatos and S. Ioannidis - Gnort: High Performance Network Intrusion Detection Using Graphics Processors - RAID - Pp 116:134 - 2008. [29] Shashi Shekhar and Sanjay Chawla - Spatial Databases: A Tour - Prentice Hall, 2003 (ISBN 013-017480-7) [30] Y. Wu, Y. Ge, W. Yan, X. Li - Improving the performance of spatial raster analysis in GIS using GPU. Geoinformatics 2007: Geospatial Information Technology and Applications. Edited by Gong, Peng; Liu, Yongxue. Proceedings of the SPIE, Volume 6754, pp. 67540P. 2007.                       626WICC 2010 - XII Workshop de Investigadores en Ciencias de la Computación	﻿computación paralela , sistemas , procesadores gráficos	es	19594
13	Estereotipos de clases entidad y clustering de objetos en sistemas de gestión	"﻿En las metodologías de desarrollo de software de gestión con objetos, los modelos de dominio  se realizan en la etapa de elaboración o de análisis, y a medida que se avanza en el desarrollo las clases entidad  de  estos  modelos  se  insertan  en  distintos  diagramas  de  diseño  correspondientes  a  la realización de casos de uso que responden a modelos de ejecución y no de persistencia, y el modelo integral de dominio se va refinando según esquemas de navegación que surgen de los diagramas de diseño  [3].  El  rendimiento  de  los  manejadores  de  bases  de  objetos  se  puede  optimizar  si  se consideran estereotipos de clases entidad propios de los sistemas de gestión que permitan clusterizar objetos en función de los patrones de acceso característicos. Los  propósitos  de  la  línea  de  investigación  y  desarrollo  que  se  presenta  en  este  trabajo  son establecer estereotipos de clases entidad para sistemas de gestión que faciliten el diseño de bases de objetos  y  estudiar  y  ensayar  técnicas  de  clustering  para  manejadores  de  bases  de  objetos  que permitan optimizar la recuperación. PALABRAS CLAVE bases, objetos, clusterización, estereotipos, clases, gestión I TRODUCCIO Las clases de objetos en un sistema de gestión se pueden clasificar o estereotipar según la clase de datos que clasifiquen. En general pueden ser  • De  Datos  Maestros:  objetos  de  un  sistema  de  información  que  representan  entidades  de existencia  real  o  ideal,  por  ejemplo  productos  o  servicios,  o  valores  de  referencia  para determinar  características  o  atributos  de  otros  datos  (dominios  de  atributos  definidos  por extensión). • De Datos Transaccionales: registros de hechos o eventos relacionados con datos maestros, por ejemplo de ventas de productos o de prestaciones de servicios. Los  datos  transaccionales  a  su  vez  pueden  diferenciarse  según  la  posibilidad  de  que  las transacciones que representan sean actualizables o no: • Las  transacciones  actualizables son  aquellas  de  las  cuales  se  registra  su  previsión  o programación en el tiempo, antes de que la transacción se produzca, y una vez producida se completan sus características (por ejemplo turnos para servicios que devienen en registros de servicios).  • Las transacciones no actualizables son aquellas que se registran luego de producidas, como si fuera en una bitácora, y luego no se actualizan más (por ejemplo registros de facturación o de operaciones bancarias). Las  operaciones  con  objetos pueden  ser  de  navegación  o  de  consulta.  Las  operaciones  de navegación  implican  accesos  por  identificador  automático  o  del  manejador,  y  las  de  consulta, expresadas en OQL (Object Query Language), por el índice de identificación de los objetos en el sistema de gestión o por otros criterios, que pueden sustentarse en otros índices. En las bases de objetos los identificadores de objetos son automáticos y transparentes al usuario, por lo que no garantizan la unicidad de los objetos (es posible instanciar más de una vez un mismo objeto para el sistema de información pero con distintos identificadores): para garantizar la unicidad de  los  objetos  se  impone  definir  un  criterio  de  identificación  que  sea  propio  del  sistema  de información. En el manejador que se está desarrollando se exige que para toda clase se precise qué atributo o conjunto de atributos identifica unívocamente a cada objeto, como es característico en el diseño conceptual de bases de datos. LI EAS DE I VESTIGACIO  Y DESARROLLO En  aras  de  la  flexibilidad  y  en  atención  a  que  en  los  sistemas  de  información  los  objetos transaccionales se suelen identificar naturalmente a partir del identificador del objeto maestro con el que se relaciona la transacción más un indicador de tiempo, en el manejador de bases de objetos de este  proyecto  se  admiten  identificadores  mixtos  y  externos  [4],  es  decir,  compuestos  por identificadores  de  objetos  maestros  más  un  discriminante  propio,  en  el  caso  de  los  mixtos,  o compuestos exclusivamente por identificadores de objetos maestros, en el caso de los externos. Para  la  composición  de  los  identificadores  mixtos  y  externos,  en  el  ODL  (Object  Definition Language) se usa el nombre de las clases de objetos maestros, que en los objetos se traduce al identificador  automático  o  del  sistema.  Por  ejemplo,  para  definir  objetos  transaccionales  que representen operaciones de una cuenta bancaria se define una clase Operación, dependiente de una clase Cuenta: Caja de  Ahorro Cuenta  Corriente Por Caja Por Cajero  Automático Por TeléfonoPor Web Por Administración Sucursal Cliente Operación Cuenta 0..n 1 0..n 1..n +cuentas +dueños 0..n 1 +movimientos La definición en XML (como estándar de represenación) quedaría: <clase nombre=""Operación"" tipo=""TNA"" instanciable=""no""> <atr nombre=""momento"" tipo=""tiempo""/> <atr nombre=""movimiento"" tipo=""(DE|CR)""/> <atr nombre=""tipo"" tipo=""Tipo Operación""/> <atr nombre=""monto"" tipo=""fracc""/> <id tipo=""mixto""> <comp tipo=""ext"" pos=""1"" clase=""Cuenta""/> <comp tipo=""int"" pos=""2"" atr=""momento"" orden=""desc""/> </id> </clase> En  el  fragmento  de  XML precedente  se  define  la  clase  Operación,  de  tipo  Transaccional  No Actualizable y no instanciable (abstracta). Los atributos de esta clase son momento, que registra fecha y hora de la operación; movimiento, que registra si es un DE(bito) o un CR(édito); el tipo (definido  por  el  administrador  por  extensión  en  la  clase  Tipo  de  Operación)  y  el  monto.  El identificador de la clase en el sistema de información es mixto, y su primer componente, externo, es el identificador automático de la cuenta a la cual afecta la operación, y el segundo componente, interno, es la fecha y hora de la operación en orden descendente. Cada operación se asocia con una (multiplicidad mínima) y sólo una (multiplicidad máxima) cuenta, que sólo debe definirse en caso de que se pretenda navegar desde una operación hacia una cuenta. La posibilidad de definir identificadores mixtos o externos representa una innovación en las bases de objetos, ya que al no requerirse explícitamente navegación desde una operación hacia la cuenta a la que pertenece, en una base de objetos tradicional no se caracterizaría a los objetos de Operación con un identificador de la cuenta y en el  manejador de este trabajo sí.  La introducción de este identificador posibilita una clusterización de las operaciones por la cuenta a la que afectan y en orden cronológico inverso, optimizándose así las recuperaciones de navegación. La  implementación  de  las  asociaciones  entre  una  clase  de  objetos  maestros  y  una  de  objetos transaccionales con identificador mixto no se realiza mediante una colección de identificadores en los  objetos  maestros,  que  sería  muy voluminosa,  sino  que,  a  la  manera  relacional,  se  maneja accediendo a los objetos transaccionales asociados por el identificador definido. Como la clase Operación es transaccional no actualizable, los patrones de acceso característicos son para inserción o recuperación (no hay bajas ni actualización de objetos). Si la identificación de objetos en el sistema de información fuera por un número de operación independiente de la cuenta de pertenencia,  una organización adecuada para la  persistencia sería  la  secuencial  indexada sin agrupar registros en bloques: todo nuevo registro se agrega al final del archivo, y el criterio de indexación de secuencias podría ser por umbrales horarios (por ejemplo se indexarían secuencias de operaciones por cada hora del día, por cada x cantidad de horas de cada día, o por cada día a partir de una hora determinada -por ejemplo la hora de clearing). Pero  en  el  caso  de  ejemplo,  como  la  identificación  depende  de  la  cuenta  a  la  que  afecta,  la agrupación o clusterización de operaciones se realiza según la cuenta, y las operaciones en cada cuenta se ordenan por cronología inversa; entonces la inserción de registros no puede realizarse siempre al  final  del archivo, sino al  inicio de cada secuencia,  siendo que cada secuencia es de operaciones en una cuenta en orden cronológico inverso. Por lo tanto la organización que se escoge es  la  secuencial  indexada  con  registros  organizados  en  bloques.  Particularmente  para  esta combinación de clase o estereotipo de clase y definición de identificador se emplea árboles B# (árbol B+ con restricciones  de contenido mínimo en nodos propias  de los árboles  B*) con los objetos organizados por el identificador del sistema de información. El fundamento de esta elección es que es común que las aplicaciones pidan iteradores sobre objetos por la clave de identificación del sistema de información, ya sea para listados o para búsquedas aproximadas, cuando se trata de objetos maestros, y para objetos transaccionales también, para realizar operaciones de totalización o síntesis (cortes de control); entonces se piensa que tener los objetos almacenados según el criterio de recuperación más probable importa una mejora del rendimiento general. En  cuanto  a  los  identificadores  automáticos  o  del  manejador,  los  define  automáticamente  el manejador,  y  para  los  identificadores  mixtos  y  externos  los  componentes  externos  son  los identificadores automáticos de las clases principales.  Para toda clase se crea por defecto un índice para el identificador automático con organización directa extensible y función de dispersión basada en los bits menos significativos del identificador de los objetos (resto de la división del identificador entre el tamaño de la tabla de dispersión). En cuanto a la organización de los objetos secuencial indexada sin bloques, el índice se organiza con estructura B# y como referencias se usa el desplazamiento en bytes a cada objeto a partir del inicio del archivo.  Para otros índices también se usa árboles B#. Se admite la definición de índices que se clasifican según la multiplicidad de objetos referidos por  cada clave,  como  índices de identificación (un objeto por clave) o índices de clasificación (muchos objetos por clave). Como referencias a objetos correspondientes a cada clave de recuperación (sea de identificación o de clasificación), se usa la clave de organización: el identificador del sistema de información. En general, las  organizaciones de archivo que se emplean para la persistencia de los objetos se detallan en el siguiente cuadro: Datos Maestros Actualizables Datos Maestros  o Actualizables (Dominios) Transacciones Actualizables Transacciones  o Actualizables Identificador Interno B# Sec. Ind. s/bloques B# Sec. Ind. s/bloques Identificador Mixto o Externo B# B# B# B# Para almacenar jerarquías de objetos se usa un único archivo, correspondiente a la clase raíz, sin importar que ésta sea abstracta o concreta. No se admite herencia múltiple, y si se desea identificar objetos de alguna subclase con un identificador propio se debe definir un índice de identificación, pero  el  identificador  formal  siempre  es  el  de  la  clase  raíz  (no  se  admite  identificadores  en subclases). Los índices de subclases, tanto para identificadores automáticos como los definidos para recuperación se implementan como índices selectivos: indexan sólo los objetos de la subclase sobre la que están definidos. RESULTADOS OBTE IDOS/ESPERADOS En la implementación del manejador se usan técnicas relacionales, dada su probada eficacia, pero la mayor eficiencia del manejador en los casos planteados queda supeditada a la definición de la base, que si se efectúa sin usar identificadores mixtos o externos no se logra. En el caso de ejemplo, si las operaciones de identificaran con un número de operación propio del sistema de información, la forma de implementar la asociación con la clase cuenta sería tener en cada objeto de cuenta una colección de identificadores automáticos de operación. Todavía no se cuenta con resultados experimentales porque el manejador del proyecto se encuentra en una estapa de pruebas. Cuando el producto se encuentre estabilizado se estará en condiciones de comparar parámetros de rendimiento con productos similares. Se  está  implementando  paralelamente  una  versión  del  módulo  de  almacenamiento  para  el manejador de bases de objetos que implementa las asociaciones al estilo relacional, para comparar rendimientos con la propuesta de este trabajo. FORMACIO  DE RECURSOS HUMA OS En el proyecto de investigación y desarrollo de referencia participan docentes auxiliares y alumnos de  grado  de  la  carrera  de  Ingeniería  en  Informática  en  el  marco  de  las  materias  Taller  de Programación  II  (desarrollo  de  módulos  experimentales  del  manejador  de  bases  de  objetos), Organización  de  Datos  (implementación  de  técnicas  de  clusterización  y  almacenamiento  de objetos), Trabajo Profesional, y Tesis; también participan alumnos de la carrera de Licenciatura en Análisis de Sistemas en el marco de la materia Aplicaciones Informáticas. En el proyecto de referencia también participan investigadores del Laboratorio de Informática de Gestión."	﻿gestión , bases , objetos , clusterización , estereotipos , clases	es	19730
14	Desarrollo de un framework para la gestión de la calidad en ingeniería reversa	﻿ Desde la cátedra de Gestión de la Calidad se  dio origen, en el año 2006, al Grupo de  Investigación y Desarrollo en Ingeniería y  Calidad del Software (GIDICALSO).  Inicialmente el objeto de investigación y  desarrollo se focalizó en los aspectos de  ingeniería y calidad en la producción de  software.  Este grupo, ante el requerimiento del  Dpto. de Sistemas de esta Facultad Regional, y  la Cátedra antes mencionados propusieron la  creación de las asignaturas Métricas del  Software y Calidad en los Servicios del  Software.  Con respecto a esta última asignatura -cuyo  foco principal de la asignatura está en el  Mantenimiento de Software y dentro de éste en  la Ingeniería Reversa- su propuesta se  fundamentó en dos cuestiones principales: a) la  reciente promulgación de la Ley de Promoción  Industrial del Software a nivel nacionali; y b)  La carencia de contenidos curriculares  específicos vinculados al Mantenimiento de  software en la currícula de Ingeniería en  Sistemas de Información de la mencionada  Facultad Regional Córdoba.  El planteamiento de este proyecto surge como  consecuencia de los resultados obtenidos  durante el dictado de la asignatura antes  mencionada en el ciclo lectivo 2008. Estos  resultados plantearon la necesidad de  profundizar los conocimientos sobre el estado  actual del arte en materia de mantenimiento de  software e ingeniería reversa y la aplicación de  prácticas de Gestión de la Calidad sobre estos  servicios.        RESUMEN  En el contexto de la cátedra de Calidad en Los  Servicios del Software y del GIDICALSO, se  visualiza la necesidad y oportunidad de  avanzar con programas de Investigación y  Desarrollo en lo referente a la Gestión de la  Calidad en la temática del Mantenimiento del  Bruno, Juan Pablo  jp.bruno@gmail.com  Cuevas, Juan Carlos  juancarloscue@gmail.com  Gimenez Zens, Inés  igimenezzens@gmail.com  Mana, Franco  manafranco@gmail.com  Peralta, Roque Eduardo  roqueper@gmail.com  Software en General y la Ingeniería Reversa en  particular.  Por lo expuesto, el presente proyecto de  investigación tiene por objetivo el Desarrollo  de un Marco de Trabajo destinado al desarrollo  conjunto de Universidad-Empresa de un  framework específico de Gestión de  Calidad  para ingeniería reversa orientado a incrementar  la eficiencia y productividad en esta actividad  tan específica.       PALABRAS CLAVES  Calidad, Mantenimiento de Software,  Ingeniería reversa, Framework.    INTRODUCCION  El mantenimiento de software es la disciplina  referida a los cambios relativos a un sistema  software después de su puesta en producción.  En los últimos años como consecuencia del  gran crecimiento de la industria del software a  nivel nacional e internacional, la producción de  productos software –que han invadido nuestra  vida cotidiana- se ha incrementado y por ende  la necesidad de “mantener” dicho software  operativo después de su puesta en producción,  es decir que tales sistemas cumplan con sus  objetivos, en forma eficaz cuando no  eficientemente, es decir que sean útiles. En la  actualidad, el funcionamiento y uso correcto de  un sistema software puede ser una cuestión de  vida o muerte. También es importante destacar  que, en el ciclo de vida de un sistema software,  el mantenimiento supera en mucho más de lo  deseable el 70%  de su ciclo de vida y sus  costos varían de un 40% a más del 70% del  costo total en dicho ciclo, lo cual implica la  urgente necesidad de encontrar formas de  eliminar o reducir los problemas relativos a  esta etapa de dicho ciclo.[Grubb-Takang:2005]  Por otro lado, cualquier tipo de cambio debe  ser precedido por una comprensión del sistema  software. Este proceso involucra una gran  proporción del tiempo invertido en llevar a  cabo el cambio. Entre las principales razones  para ello se puede mencionar: documentación  no existente, desactualizada o incorrecta, la  complejidad del sistema, la indisponibilidad de  los desarrolladores originales del sistema  software o la carencia del conocimiento  suficiente del dominio por parte del encargado  de mantenimiento. Una forma de abordar y  aliviar estos problemas es la de realizar  abstracciones a partir de información relevante  del código fuente acerca del sistema. Esto se  denomina Ingeniería Reversa.  Existen diferentes abordajes sobre la Ingeniería  Reversa [Grubb-Takang:2005], [TonellaPotrich:2005] y .[Eilam:2005], sin profundizar  demasiado en la Gestión de la Calidad de  dicho proceso en forma integradora y  completa, es decir en forma sistémica.  Otros trabajos abordan la calidad sobre algunos  aspectos particulares del proceso  la Ingeniería  Reversa, como por ejemplo: atributos de la  calidad –aplicabilidad, extensibilidad y  escalabilidad-  [CMU/SEI-98-TR-005], Métricas  para Ingeniería  Reversa [Zhou et al:1999],   cuestiones de Calidad para Ingeniería Reversa  en Base de Datos: el método, adquisición de la  semántica del dominio en forma minuciosa, y  criterio de evaluación de performance [ChiangBarron:1995] e Ingeniería de Reversa para  Requerimientos destacando aspectos  vinculados a simplificar la complejidad y  mejora de la calidad del software, entre otros.   Lo anteriormente puntualizado  genera la  necesidad de profundizar el conocimiento del  estado del arte en lo referente a Ingeniería  Reversa en general y las prácticas de Gestión  de la Calidad en dicho ámbito en particular.  Seguidamente, se preveé abordar una  investigación sobre la aplicación de dichas  prácticas en la Industria del software,  inicialmente, a nivel local. Posteriormente, se  deberá, en forma conjunta con la industria,  identificar y analizar las buenas prácticas  utilizadas en la actualidad, su eventual  mejoramiento e identificación  de nuevas  prácticas requeridas para completar un marco  de trabajo  orientado a la Gestión integral de la  Calidad en la Ingeniería Reversa.    LINEAS DE INVESTIGACION Y  DESARROLLO  Profundización del estudio del estado del Arte  en Mantenimiento de Software  Profundización del estudio del Estado del arte  en Ingeniería Reversa  Aplicación de los principios de Gestión de la  Calidad a la Ingeniería Reversa  Gestión de Riesgos en la Ingeniería reversa,  como herramienta de prevención, aspecto este  de vital importancia en los sistemas de Gestión  de la Calidad.  Investigación y desarrollo de buenas prácticas  para Ingeniería Reversa  Desarrollo de un Marco de Trabajo para la  gestión de la calidad en Ingeniería reversa    RESULTADOS  OBTENIDOS/ESPERADOS  Profundización del conocimiento en el  dominio del conocimiento vinculado a la  Ingeniería reversa  Transferencia a la cátedra de Calidad en los  Servicios del Software de los conocimientos  adquiridos y profundizados.  Trabajo conjunto del GIDICALSO con  empresas de la Industria del Software de la  provincia de Córdoba a los fines de desarrollar  y especificar buenas prácticas de aplicación en  proyectos de Ingeniería Reversa.  Desarrollo de un marco de trabajo orientado a  incrementar la eficiencia y la productividad  en  los proyectos de ingeniería reversa de  productos software sobre la base de la  aplicación de un Sistema de gestión de la  Calidad.  Transferencia de los resultados obtenidos en  materia de investigación y desarrollo a  Empresas vinculadas a la industria del software  a nivel local, nacional e internacional,  mediante cursos, talleres, seminarios, y  publicaciones en congresos y revistas  especializadas.    FORMACION DE RECURSOS  HUMANOS  Este aspecto es fundamental ya que los  resultados de las investigaciones y desarrollos  obtenidos redundarán en una mayor  cualificación profesional para quienes  participen en ellos, como así también los  destinatarios de las eventuales transferencias  que se realicen a estudiantes, docentes,  profesionales, colegios profesionales, empresas  y el sector gobierno. Además de los integrantes  de la cátedra de Calidad en los Servicios del  Software, se pretende incorporar –de hecho el  GIDISCALSO permanentemente lo hace- a  estudiantes, graduados, docentes y  profesionales para participar en los proyectos  del Programa de Investigación que se  encuentra en pleno desarrollo y vinculado a la  Ingeniería Reversa.  Propender a la formación de nuevos  investigadores en el GIDICALSO.  Proveer de temas y de guía u orientación para  la realización de tesis para quienes cursan la  Maestría en Sistemas de Información que se  dicta en la Facultad Regional Córdoba y otras  regionales de la Universidad Tecnológica  Nacional.   Incorporar, mediante convenios de  colaboración, a profesionales de la Industria a  los fines de especificar buenas prácticas que  sean la base para el desarrollo de un marco de  trabajo referido a la aplicación de un Sistema  de Gestión de la Calidad orientado a la  ingeniería reversa.	﻿Ingeniería reversa , Calidad , Mantenimiento de Software , Framework	es	19746
15	Diseño y clasificación de los Sistemas de Descubrimiento de recursos Grid	﻿ En la actualidad, contamos con grandes redes de recursos computacionales interconectadas a través de Internet. La computación grid provee una plataforma a través de la cuál se puede acceder a la mayoría de estos recursos haciendo abstracción de su localización. Para lograr esto, es necesaria la implementación de sistemas de descubrimiento (Discovery) que permitan localizar los recursos necesarios para llevar a cabo una tarea dentro del entorno grid. En este artículo se exploran los aspectos a tener en cuenta a la hora de diseñar un sistema de descubrimiento de recursos, así como también se observan como estos aspectos fueron considerados en el diseño de los sistemas existentes. Palabras Clave: Computación Grid, Descubrimiento de recursos, aspectos de diseño. 1. Introducción La disponibilidad de computadoras poderosas conectadas a Internet, así como también redes de alta velocidad, está cambiando la forma de hacer computación y de utilizar las computadoras en el presente [1]. También está creciendo el interés en agrupar recursos computacionales que se encuentran geográficamente distribuidos para resolver problemas de gran escala. Esta filosofía es en la que se basa el concepto de Computación Grid. En este contexto, una amplia variedad de recursos de procesamiento, dispositivos de visualización, instrumentos especiales de tipo científico, sistemas de almacenamiento y bases de datos son agrupados lógicamente y presentados como un único recurso integrado al usuario. Sin embargo, a medida que el sistema crece, la variedad de recursos se hace más extensa dificultando la tarea de localizar los recursos necesarios para la ejecución de aplicaciones. Por ello es de vital importancia que el sistema cuente con mecanismos para encontrar esos recursos. El descubrimiento de recursos constituye la habilidad de localizar los recursos que cumplen *Becario de la Comisión de Investigaciones Científicas (CIC) de la Provincia de Buenos Aires, Argentina con un conjunto de requerimientos establecidos en una consulta. Las entidades involucradas en esta capacidad son tres: el proveedor del recurso, el usuario del recurso y el servicio de descubrimiento. El proveedor del recurso es aquella entidad que permite el uso del recurso en cuestión. En computación grid, la administración de recursos se suele llevar a cabo a través de consorcios de individuos y/o organizaciones conocidos como Organizaciones Virtuales (VOs por su sigla en inglés) [2, 3]. El usuario del recurso es la entidad que se encuentra conectada a la red y que hace uso de los recursos compartidos. Se debe tener en cuenta que esta entidad podría ser un usuario humano así como también una aplicación que requiere de determinados recursos para llevarse a cabo. Por último, el servicio de descubrimiento es el servicio que devuelve la localización de los recursos que concuerdan con los requerimientos especificados en la consulta realizada por el usuario. En cuanto a este servicio, podría constituir un módulo como parte del usuario o el proveedor del recurso o ser una entidad independiente de estos. A la hora de implementar un servicio de descubrimiento de recursos es necesario considerar ciertos aspectos relativos al entorno en el que funcionará, como por ejemplo su arquitectura subyacente, la forma en que se manejan los recursos, entre otros. Las decisiones tomadas en cada uno de estos aspectos determinan cómo debe ser diseñado el sistema de descubrimiento y constituirán las características que harán distintivo al servicio implementado con respecto al resto de los servicios existentes. En la sección 2 se exponen estos aspectos de diseño mencionados, así como también las distintas alternativas que se pueden tomar en cada uno de los aspectos. Luego, en la sección 3 se muestra como estos aspectos son considerados en los sistemas de descubrimiento actuales. Por último, en la sección 4 se exponen las conclusiones y las posibles líneas de investigación relacionadas con este tópico. 2. Diseño del sistema de descubrimiento Como se mencionaba en la sección 1, los aspectos de diseño establecen una división espacial de los distintos tipos de servicios de descubrimiento dependiendo de las decisiones que se hayan tomado para cada uno de estos aspectos. Estas decisiones están influenciadas por el tipo de plataforma o qué tan distribuido es el sistema, por lo que no siempre la elección de una alternativa constituye una solución que pueda ser catalogada como la mejor alternativa ya que depende del entorno y del tipo de aplicaciones que solicitarán el servicio. En las siguientes subsecciones se mencionan algunos de los aspectos de diseño que fueron contemplados en [8] y [7], así como también las diversas alternativas para cada uno de estos aspectos. 2.1. Localización del proveedor del servicio Una de las primeras decisiones de diseño es establecer quién va a proveer el servicio de descubrimiento. Como se mencionaba en la sección 1, un caso bastante común en los sistemas es que el servicio quede a cargo de una entidad independiente del proveedor del recurso o de la entidad que va a hacer uso del mismo. Esta entidad generalmente constituye un servidor o conjunto de servidores que se encarga de recolectar la información sobre los recursos disponibles con el fin de responder las consultas de los usuarios. La alternativa a esto sería contar con un servicio que sea genuinamente distribuido, esto es, que el servicio de descubrimiento de recursos se encuentre distribuido entre todos los proveedores y usuarios de recursos, sin ningún componente central que esté coordinando. 2.2. Configuración de la red solapada Los sistemas distribuidos construyen redes solapadas a la red física sobre la que se encuentran. Esta red solapada es un grafo dirigido: los vértices representan los nodos de la red y los arcos representan el conocimiento de un nodo acerca de otro nodo. Existen dos alternativas para la construcción de esa red solapada: puede ser configurada manualmente o que se organice de manera automática. • Redes configuradas de forma manual: Un administrador es responsable de proveer a cada servidor de la información sobre el resto de los componentes, y los usuarios y proveedores de recursos deben saber las direcciones físicas de estos servidores. Esto es, el administrador debe construir el grafo dirigido. Mientras que esta alternativa provee de mayor control, es más complicado para escalarlo. • Redes configuradas de forma automática: Constituyen una alternativa interesante cuando las redes empiezan a hacerse grandes y las configuraciones manuales se hacen caras o no hay una autoridad central para coordinar las configuraciones. La desventaja es el incremento en el tráfico de la red con datos extras que únicamente sirven para mantener configurada la red solapada. • Híbridas: También es posible utilizar las ventajas de ambos enfoques en una alternativa híbrida. Por ejemplo, algunas redes utilizan multicast o broadcast para organizarse a nivel de LAN y tienen una configuración manual para interconectar estos subgrafos a nivel de WAN. 2.3. Identificación de los recursos Como se decía anteriormente, existe una gran variedad de recursos dentro de un grid. Por lo tanto, es necesario poder identificarlos para poder realizar consultas sobre estos y que el sistema de descubrimiento pueda encontrarlos. A la hora de especificar el nombre de un recurso, existen tres alternativas: • Identificadores hash : Se aplica una función de dispersión (hash) sobre alguna propiedad (o conjunto de propiedades) del recurso predeterminada para todos los recursos del grid. Esto da por un resultado un identificador único para el recurso. • Texto plano: Se identifican los recursos por una cadena de caracteres significativa. Esta alternativa permite realizar consultas más complejas sobre el nombre que no podrían ser realizadas sobre los hashes ya que no guardan relación sobre los mismos (salvo al momento de calcularlos). • Atributos: Los recursos son descriptos por medio de un número de atributos predeterminados que toman un valor. Este mecanismo es muy utilizado en los sistemas de descubrimiento ya que facilita el mapeo entre los requerimientos de usuarios y las propiedades de los recursos disponibles. 2.4. Registración de los recursos Antes de que se puedan realizar solicitudes sobre los recursos, es necesario generar y almacenar una referencia a los mismos en algún lugar donde estas referencias puedan ser accedidas por los usuarios. Este proceso se denomina registración y existen diferentes técnicas que se pueden usar para realizarla: • Registración local: Solamente el proveedor del recurso mantiene contacto directo con los recursos a compartir, y el resto puede localizarlos consultando al proveedor. • Referencias Hash : Las referencias a los recursos son almacenadas en los nodos cuyos identificadores hash estén más próximos al identificador hash del recurso en cuestión. Con este método se evita la centralización y la ubicación de las referencias se obtiene de manera intuitiva. • Registración en un servidor local: Los proveedores de recursos anuncian sus recursos al servidor local independiente de los mismos. 3. Ejemplos de sistemas concretos A continuación, se verán algunos sistemas de descubrimiento de recursos y se explorarán sus características teniendo en cuenta los aspectos de diseño analizados en la sección anterior. En primer lugar, uno de los sistemas que se pueden destacar es el MDS (Monitoring and Discovery System) que constituye el módulo de descubrimiento de Globus Toolkit [4]. Si bien su implementación ha variado desde su primera versión, mantiene la mayoría de sus aspectos de diseño. La provisión del servicio de descubrimiento está a cargo de un módulo denominado index formando una entidad separada del proveedor y del usuario del recurso como se mencionaba en la sección 2.1. Este mismo módulo constituye el servidor local para la registración que era una de las alternativas mostradas en la sección 2.4. Los mensajes de registración por parte del proveedor de recurso se envian de forma periódica hacia la dirección del servidor local que haya sido especificada de forma manual (sección 2.2) por el administrador del sistema. La identificación de los recursos se hace por medio de los atributos (sección 2.3) de los mismos, lo cual facilita a la hora de localizarlos. Otros sistemas importantes para mencionar son aquellos que hacen uso de técnicas peer-topeer (P2P) para soportar el servicio de descubrimiento de recursos. En [6] se presenta SDRD (Super node and Dht approach to Resource Discovery). En este modelo, el grid es dividido en VOs dentro de cada cual se utilizan DHTs (Distributed Hash Tables) para mejorar la eficiencia en el ruteo local. Además cada VO cuenta con uno o más super nodos que se encargan de transferir los pedidos a otros super nodos en otras VOs. Este caso, sería similar a una construcción híbrida como la planteada en la sección 2.2, ya que a nivel de VO (una red acotada, como sucede en una LAN) se utilizan tablas hash mejoradas como forma de configuración automática en la que se hace intercambio de mensajes con el fin de actualizar las tablas. Luego, a nivel de comunicación entre las VOs, se utilizan los super nodos, donde cada uno de ellos estarán configurados de forma manual para poder hacer transferencia de los pedidos al super nodo que corresponda. Otro sistema P2P es el presentado en [5], el cuál a diferencia del anterior, posee una configuración automática en la cuál tanto los requerimientos de recursos como los nodos que poseen los mismos son identificados por referencias hash (como se explicaba en la sección 2.4). 4. Conclusiones y trabajos futuros En el presente artículo se analizaron algunos aspectos a tener en cuenta a la hora de diseñar un servicio de descubrimiento de recursos y se mostró cómo estos aspectos fueron considerados en el diseño de sistemas que se utilizan en la actualidad. Si bien detalles como la forma de identificar los recursos, cómo llevar a cabo la registración de los mismos y las posteriores consultas son cuestiones de gran importancia, existen otros aspectos que deberían ser explorados en el futuro que en conjunto con los antes mencionados conformarán los ejes de un criterio no solamente destinado a diseñar nuevos sistemas de descubrimiento sino también para clasificar los sistemas existentes, como se menciona brevemente en la sección 3. Se plantea como línea de investigación, la exploración de estas características adicionales y la posible implementación de un esquema de clasificación de los sistemas de descubrimiento de recursos.	﻿Descubrimiento de recursos , Computación Grid , aspectos de diseño	es	19817
16	Seguridad en entornos virtuales	﻿ En un entorno virtual, un monitor de máquina virtual (VMM) controla múltiples VMs mediante una abstracción de software del hardware subyacente. Esta arquitectura provee algunas ventajas con respecto a la seguridad pero también introduce desafíos únicos. Irónicamente los avances en la potencia de cómputo y la disminución de los costos del hardware fueron los factores que dieron origen a la pérdida de interés en la virtualización, hoy principales contribuyentes de su renacimiento. La virtualización surgió a finales de la década de 1960, con el objetivo de multiplexar las aplicaciones sobre mainframes de forma tal de poder repartir los escasos y costosos recursos de cómputo entre múltiples procesos. La creación de las VMs hizo posible que múltiples aplicaciones coexistiesen sobre una máquina única. Esta línea de investigación busca desarrollar nuevas tecnologías centradas en la seguridad de los entornos virtuales, especialmente a nivel del VMM. Palabras Clave: Virtualización, Detección de Intrusos, Seguridad en Redes, Políticas de Seguridad, Sistemas Distribuidos, Computación Colaborativa, Automatización. 1. Introducción Cuarenta años después del surgimiento de la virtualización, modernos sistemas operativos (SOs), altas velocidades en los procesadores y bajos costos de hardware resolvieron los problemas para los que se inventaron las VMs. Fue posible entonces instalar nuevas aplicaciones tan fácilmente y barato como instalarlas en un servidor dedicado con su propio procesador, memoria y almacenamiento. Sin embargo, esta tendencia dio origen a nuevos problemas: el hardware barato hizo que proliferasen las máquinas subutilizadas, las cuales a su vez ocuparon espacios significativos y aumentaron el overhead en su administración. Mantener las aplicaciones actualizadas y los patches de los sistemas operativos en cada server se volvió una tarea pesada. Lograr que éstas máquinas fuesen seguras implicó que se determinasen claramente las responsabilidades de las organizaciones sobre cada server. Por ende, mover aplicaciones nuevamente a las VMs dentro de unas pocas máquinas físicas y administrándolas mediante monitores de VMs (VMMs) se volvió una solución factible. Un VMM es una capa de software que usualmente corre directamente sobre el hardware. En los sistemas actuales el VMM puede correr junto a un sistema operativo host. Las VMMs exportan una abastracción de máquina virtual que se asemeja al hardware subyacente. Cada abstracción de máquina virtual es un guest que encapsula el estado completo del SO que corre dentro de ella. El sistema operativo guest interactúa con la abstracción virtual del hardware, la cual es a su vez manejada por el VMM como si fuese hardware real. La VMM es entonces un “sistema operativo de sistemas operativos”. La VMM usualmente corre en modo privilegiado mientras que el sistema operativo guest lo hace en modo usuario. *Becario CONICET, Argentina. 2. Máquinas virtuales Básicamente existen dos tipos de entornos de máquinas virtuales. En el primero, el VMM se conoce con el nombre de hypervisor y se implementa directamente entre el hardware y los sistemas guest. El ejemplo clásico de este tipo de máquinas virtuales es el hypervisor de Xen (http://www.xen.org). Este tipo de virtualización es especialmente aplicable a entornos de servidores tipo UNIX. En el otro tipo de monitor de máquina virtual, el VMM corre como una aplicación más dentro del SO guest y depende de él para prover drivers de I/O y código bootstrap en lugar de implementarlo directamente desde cero. Este tipo de VMM se popularizó en PCs comunes (siendo posiblemente VMware (http://www.vmware.com) el más difundido), cuya plataforma x86 no fue originalmente diseñada para soportar completamente la virtualización por hardware. Este tipo de entorno suele ser considerado menos seguro que el anterior debido a que el VMM depende de los servicios provistos por el sistema operativo host. La virtualización actual se ha desviado de sus raíces originales como una herramienta de multiplexado para convertirse actualmente en una solución para hacer frente a los problemas de seguridad, confiabilidad y administración. Este hecho presenta consecuencias positivas y negativas, pues por un lado la virtualización junto con sus técnicas asociadas ayudan a resolver múltiples problemas de seguridad, especialmente porque las máquinas virtuales corren sobre un único sistema, el cual puede implementar un sistema seguro multinivel con sistemas virtuales separados en cada nivel. Por otro lado la virtualización fabrica espacios para nuevas vulnerabilidades, donde los mecanismos de seguridad tradicionales no están preparados para resolver estos problemas. Esta línea de investigación se suma entonces a la búsqueda de nuevos mecanismos para combatir estos nuevos vectores de ataque. 2.1. Monitores de máquinas virtuales (VMMs) Los VMMs soportan tres atributos en entornos virtuales [1]: aislamiento, interposición e inspección. Los VMMs propician el aislamiento debido a que las VMs no comparten memoria física. Gracias a la abstracción de la memoria virtual el VMM puede crear la ilusión de que cada VM tiene su propio espacio de direcciones. De esta forma cada VM corre sin saber de la existencia de otras VMs, pues todas sus acciones están confinadas a su propio espacio de direcciones. Para soportar interposición los VMMs gestionan todas las operaciones privilegiadas a nivel del hardware físico. Los SOs guest transfieren todos los traps e interrupciones al VMM para procesar los eventos. El VMM intercepta todos los pedidos de I/O provenientes de los dispositivos virtuales de las VMs y los mapea al dispositivo físico de I/O correspondiente. Gracias a esta abstracción el VMM gestiona y planifica todas las VMs simultáneamente. Los sistemas operativos guest no saben de la existencia del propio VMM, ni saben que se encuentran compartiendo recursos con otras VMs; por el contrario la abstracción les presenta la ilusión de estar interactuando directamente con los dispositivos físicos. Por último los VMMs tienen acceso a todos los estados de una VM, incluyendo el estado del CPU, de la memoria y de los dispositivos. Esta visión provee las capacidades de inspección, propiciando checkpoints, rollbacks y replays. Adicionalmente estos atributos se combinan para brindar otra característica deseable: portabilidad. Los administradores pueden guardar, copiar, mover e instanciar entornos con estados encapsulados desde una máquina a otra. Por todo ello decimos que el VMM es en sí mismo un pequeño sistema operativo que atrapa los eventos que surgen en los sistemas operativos guest, gestionando sus I/Os y mapeando datos a memoria y discos virtuales. Al mismo tiempo la implementación física está separada y oculta de los SOs guest pero colectivamente encapsulada permitiendo replicación y migración eficiente. 2.2. Virtualización: lo bueno Las características inherentes de la virtualización y los atributos de las VMMs que arriba resumimos simplifican la gestión de los recursos de cómputo. La reducción del overhead administrativo facilita el proceso de hacer seguro un sistema. 2.2.1. Pool de hardware Los VMMs brindan una vista uniforme del hardware subyacente y por ello una plataforma de hardware puede contener múltiples entornos virtuales. Por lo tanto los administradores pueden ver a una computadora como parte de un pool de recursos de hardware genéricos [2]. Esta característica permite disminuir costos de hardware y bajar los requerimientos de espacio. Adicionalmente, dado que el VMM puede mapear máquinas a recursos de hardware disponibles se simplifica el balance de carga y la escalabilidad, volviendo triviales las fallas de hardware. 2.2.2. Encapsulamiento La propiedad de encapsulamiento mejora la seguridad desde varios frentes. Las VMs son fácilmente encapsulables y replicables, por lo tanto los administradores pueden sistemáticamente agregar en tiempo de ejecución nuevos servicios y aplicaciones al entorno replicado. Cada nuevo servicio o aplicación puede correr independientemente sin el riesgo de corromper o interferir con otros. Si un sistema cae (crash) u ocurre un ataque, los administradores pueden suspender a la VM afectada, hacer un rollback a un estado de ejecución estable y recomenzar. Por último, analizar mediante replay puede exponer configuraciones defectuosas o proveer información acerca del método de ataque, vulnerabilidades, etc. 2.2.3. Logs seguros Los servicios de seguridad, como los logs seguros y la protección de intrusos a nivel del SO puede implementarse también en el VMM. El hecho de que estos servicios se ejecuten separados de todos los procesos en un entorno virtual mejora sus capacidades. La implementación de logs seguros a nivel del SO tiene la desventaja de que un atacante puede deshabilitar o modificar los logs una vez comprometido el sistema y por ello tampoco los logs proveen información útil para un efectivo análisis de seguridad forense. Estos problemas se solucionan con la implementación a nivel del VMM, pues el atacante no puede modificar los logs y con ello se vuelven una herramienta útil para el análisis forense. Además la virtualización mejora la prevención y detección de intrusos mediante el uso de clones. Los sistemas de detección de intrusos suelen basarse en firmas de patrones de ataques conocidos, tornándolas prácticamente inútiles frente a nuevos eventos sospechosos. Por otro lado los IDSs basados en anomalías corren el riesgo de marcar como ataques acciones legítimas; o peor aún pueden aceptar eventos maliciosos repetidos como actividad normal. Los clones por otra parte pueden correr eventos sospechosos capturados en una copia virtual del sistema real y observar los cambios sin comprometer el sistema real. Por último, la protección a intrusos en el nivel virtual permite detecciones imposibles para los sistemas tradicionales, pues permiten monitorear todos los eventos que ocurren en el entorno virtual y por lo tanto hacer cumplir una política preestablecida detectando por ejemplo cracking de passwords a partir de una lectura de bloques de discos que contienen un archivo con passwords seguido de una gran actividad de CPU. 2.2.4. VMs en cuarentena La virtualización permite que los administradores puedan poner una o más VMs en cuarentena (fuera de la red) y buscar vulnerabilidades, evidencias de ataques, prevención de diseminación de código malicioso hacia otros nodos, etc. 2.2.5. Distribución de software Para la mayoría de los sistemas complejos, las posibles combinaciones de hardware, versiones de sistemas operativos y librerías vuelven impráctico que los desarrolladores de software contemplen cada posible combinación. La virtualización alivia estos problemas permitiendo que los desarrolladores distribuyan máquinas virtuales completas que contengan su software. Los dispositivos portables de almacenamiento flash, por ejemplo pen drives, permiten extender aún más este concepto. Los usuarios suelen emplear estos dispositivos para llevar consigo documentos, imágenes, etc. Con la virtualización es posible que un usuario lleve consigo una copia de su VM junto a sus archivos de trabajo y llevar consigo su máquina completa en un bolsillo. 2.3. Virtualización: lo malo En la subsección anterior vimos ventajas de seguridad a partir del uso de entornos VM: infraestructuras que automáticamente realizan balance de carga, detectan fallas independientes de hardware que hacen que las VMs migren, se creen y se destruyan según la demanda de servicios particulares. Sin embargo, algunas propiedades de la virtualización hacen que lograr un nivel de seguridad aceptable en entornos VM sea más difícil. 2.3.1. Proliferación de VMs La escalabilidad constituyó uno de los puntos fuertes desde la creación de los entornos virtuales, pues crear una nueva VM es tan simple como encapsular y generar mediante copia una nueva instancia. Sin embargo ésto no siempre es bueno, pues los usuarios pueden tener demasiadas VMs de propósitos especiales, e.g., una para testing, otra para demostraciones, otra como sandbox para probar nuevas aplicaciones, y otra por cada sistema operativo. Esta situación torna inmanejable el tema desde el punto de vista de la performance y desde el gasto considerable de memoria física. Además, desde el punto de vista de la administración la situación se vuelve compleja, pues las actualizaciones, configuraciones, etc. deben hacerse en cada máquina. Desde la perspectiva de la seguridad, la proliferación de VMs puede abrumar a los administradores, haciendo inseguras a algunas VMs (exponiendo a amenazas a la organización). Finalmente la virtualización imposibilita algunos procedimientos tradicionales de seguridad, por ejemplo dado que múltiples VMs corren en el mismo host físico, deshabilitar un port en la máquina host para hacer segura una aplicación especial en una VM específica presenta el indeseable efecto de deshabilitarlo también para el resto de las VMs que podrían necesitarlo. 2.3.2. Comportamiento transitorio El beneficio de la portabilidad es a la vez un problema grave de seguridad. Dado que las VMs se replican fácilmente puede aparecer una legión de “máquinas transitorias” que aparecen y desaparecen de la red. Esto puede llevar a que vulnerabilidades que no existían ahora aparezcan brevemente, infecten otras máquinas y luego desaparezcan antes de detectarlas, complicando en gran medida el trabajo de los administradores de la seguridad, no sólo respecto a la gestión de patches sino también dificultando los penetration tests. Si bien los checkpoints, rollbacks y replays de las VMS ayudan a recuperar fallas en entornos virtuales también presentan el problema de la reexposición a vulnerabilidades, reactivación de servicios riesgosos, rehabilitación de cuentas desactivadas, etc. 2.3.3. Aspectos sociales Los aspectos sociales relacionados con la virtualización son sociales por naturaleza. La facilidad de instanciación de las VMs puede llevar a que los administradores simplemente remuevan y reinstalen una VM comprometida en lugar de que analicen lo sucedido en un ataque. 2.3.4. Nuevos riesgos Los entornos virtuales traen aparejados nuevos riesgos, como una máquina virtual robada que alguien llevaba en un pen drive. Éste es un riesgo análogo al de una laptop robada pero en este caso con un dispositivo de menor tamaño y por ende más fácil de sustraer. Adicionalmente un cracker podría tratar de robar máquinas virtuales completas atacando file servers. 3. Línea de investigación En el presente artículo se analizaron brevemente algunos aspectos a tener en cuenta a la hora de emplear virtualización como alternativa segura a las máquinas físicas. Vimos que no todas son ventajas, y por ello nos proponemos investigar en profundidad en este campo, área que resurgió a partir de las promesas de una seguridad mejorada, confiabilidad y ventajas administrativas. En particular nuestro interés se centra en la implementación de servicios de seguridad avanzados a nivel del VMM, explotando vistas que le son imposibles a un sistema de seguridad a nivel del SO guest. Las promesas de la virtualización arriba mencionadas son viables pero debemos ser concientes de los riesgos inherentes que plantea esta tecnología, pues si bien es cierto que se disminuyen overheads, se facilita la administración y se combaten las vulnerabilidades de seguridad a nivel del SO, no es menos real que se introducen nuevos riesgos que van en detrimento de la seguridad del sistema.	﻿Sistemas Distribuidos , Virtualización , Detección de intrusos , Seguridad en Redes , Políticas de seguridad , Computación colaborativa , Automatización	es	19828
17	Análisis de opinión como un sistema multiagente distribuido	﻿ El objetivo fundamental de este proyecto es el desarrollo de conocimiento especializado en el área de Inteligencia Artificial Distribuida, estudiando técnicas de representación del conocimiento y razonamiento, junto con métodos de planificación y tecnologías del lenguaje natural aplicadas al desarrollo de sistemas multiagentes. Esta línea de investigación se centra en el desarrollo de una aplicación destinada al estudio y seguimiento de la opinión pública sobre un tema determinado. El crecimiento de internet junto con el desarrollo de la Web 2.0 (Web Social) posibilita que personas de todo el mundo compartan información global. Millones de mensajes aparecen diariamente en los sitios más populares de microblogging, comentarios de noticias en diarios web, blogs, etc. Los autores de estos mensajes escriben acerca de sus vidas, comparten sus opiniones sobre una variedad de temas y discuten sobre estos. Toda esta información que los usuarios generan en las publicaciones acerca de los productos que utilizan o visión política y religiosa, se vuelve un recurso de gran valor para el análisis de opiniones y sentimientos de la opinión pública. El estudio de estos temas mediante un seguimiento continuo junto con la determinación sobre los acontecimientos o hechos causales de variaciones en la opinión pública son cruciales a la hora de tomar una decisión. Tanto a nivel de consumidor como de proveedor esta información tiene un gran valor estratégico, que les brinda una tendencia y/o comparativa del valor mundial a través del tiempo. Palabras Clave: Agentes inteligentes, Sistemas multiagentes, Procesamiento del lenguaje natural, Opinion Mining, Análisis de sentimientos. Contexto Este trabajo está parcialmente financiado por la Universidad Nacional del Comahue, en el contexto del proyecto de investigación Sistemas Multiagentes en Ambientes Dinámicos: Planificación, Razonamiento y Tecnologías del Lenguaje Natural. El proyecto de investigación tiene prevista una duración de tres años, ha comenzado en enero del 2010 y finaliza en diciembre de 2012. 1. Introducción La información textual disponible en la web podría ser categorizada en expresiones de hecho o de opinión. Las expresiones de hechos están relaciondas a entidades, eventos y sus propiendades. Por otro lado, las de opinión son usualmente expresiones subjetivas que describen algún sentimiento o valoración sobre las personas, entidades, eventos y sus propiedades [6]. Junto con el desarrollo de la tecnología y el creciente acceso a la información, hemos sido testigos del nacimiento de un nuevo tipo de sociedad: la sociedad de la interactividad y comunicación [9]. En este nuevo contexto, el papel de los sentimientos expresados en la web se han vuelto crucial. Las personas expresan las emociones que determinan ciertos hechos. Por otra parte, otras personas que tienen acceso a estas expresiones, las transforman bajo sus propias influencias en otras expresiones. El crecimiento de lugares on-line en donde las personas pueden expresar sus opiniones abre un nuevo campo de investigación: la minería de opinión (Opinion Mining -OM-). La investigación en OM es una disciplina reciente concerniente a la recuperación de opiniones expresadas en un documento y no sobre el tema del mismo como es el caso de la recuperación de información. Más específicamente está relacionado con la opinión de un autor expresada en un documento o texto, como ser blogs, micro-blogs, noticias, comentarios, etc. [3]. Este análisis de sentimientos conlleva algunos desafíos, entre ellos, determinar si cada segmento de texto (sentencia, párrafo o sección) es una opinión o no; identificar quién expresa la opinión (una persona, organización, etc.) y determinar si la opinión es positiva, negativa o neutra, o de acuerdo a una cierta taxonomía preestablecida. Teniendo en cuenta la riqueza del lenguaje humano y su gran poder expresivo y ambigüedad inherente al mismo, el problema de la clasificación de sentimientos no es trivial. Las aplicaciones en donde puede ser crucial el análisis de los sentimientos pueden ser: Resumenes de opiniones [2]. Opiniones de libros o películas [4, 5] Análisis de opiniones políticas: Análisis de candidatos políticos [1, 7], e-goverment [8] para analizar impacto de desiciones. Análisis del impacto de productos y marcas [1]. Los recursos lingü´ısticos para OM definen algunas propiedades relacionadas a los sentimientos. Los avances sobre este tópico tratan con tres tareas principales: Determinación de la orientación del término: positivo, negativo, neutro. Determinación de la subjetividad de un término, si un término tiene una naturaleza subjetiva u objetiva: verde, alto, líquido, etc. Determinación de la fuerza de la determinación del término (orientación o subjetividad), como el grado de positividad o negatividad del término. Las investigaciones sobre OM han tomado tres líneas de investigación interrelacionadas [6] Desarrollo de recursos lingü´ısticos para el análisis de sentimientos tal como corpus léxico anotado manualmente; Implementación de diferentes algoritmos para el análisis del texto y clasificación de acuerdo a su orientación semántica y subjetiva; Extracción de opiniones del texto, incluyendo diferentes tipos de relaciones con contenido asociado. En este trabajo se pone principal énfasis en el seguimiento continuo de una temática en la web, junto con la determinación de los acontecimientos o hechos causales de variaciones en la opinión pública, siendo esto crucial a la hora de la toma de decisión. Brindando una información de gran valor estratégico que nos muestra una tendencia y/o comparativa de su valor mundial a través del tiempo. 2. Líneas de investigación y desarrollo El proyecto de investigación Sistemas Multiagentes en Ambientes Dinámicos: Planificación, Razonamiento y Tecnologías del Lenguaje Natural tiene varios objetivos generales. Por un lado, el de desarrollar conocimiento especializado en el área de Inteligencia Artificial Distribuida. Además, se estudian técnicas de representación de conocimiento y razonamiento, junto con métodos de planificación y tecnologías del lenguaje natural aplicadas al desarrollo de sistemas multiagentes. Específicamenete, esta línea se centra en el estudio de un sistema multiagente en ambientes dinámicos para el seguimiento continuo de la opinión pública sobre un determinado tema de interés. Las encuestas fueron tradicionalmente, la forma de obtener información acerca de la opinión pública, siendo estas estáticas en un tiempo discreto. A diferencia de este tipo de encuestas, este trabajo está enfocado en realizar un seguimiento continuo de la opinión pública. Está opinión está expresada públicamente en diferentes sitios de la web. Teniendo este corpus a disposición el proceso continua realizando una clasificación de la información obtenida acerca de la temática a analizar. Por ejemplo si la temática a analizar es el “asignación universal por hijo” se pueden buscar los comentarios de las noticias relacionadas con este tema, y clasificarlos si están a favor o encontra. El objetivo de esta investigación es desarrollar una herramienta para hacer este proceso de forma cuasi-automática. La Figura 1 muestra la arquiterctura básica del sistema multiagentes destinada al análisis de opinión. La misma está dividida en cuatro agentes principales: Agente Buscador, Agente Filtrador, Agente Analizador y Agente Compositor. El primer agente que entra en juego es el Agente Buscador, el cual tiene tres tareas principales: análisis de la entrada o consulta, búsqueda sobre la web, y finalmente almacenar lo buscado en una base de datos.    Agente Buscador Web Agente Compositor Agente Filtrador Agente Analizador Resultados Continuos Frasesde opinión Opinionesclasificadas Expertos Marca,PersonajeFraseProductoEtc. Base de Datos Figura 1: Arquitectura de agentes para análisis de opinión Se producen además dos procesos. En el primero, se realiza una desambigüación de la entrada en el caso de ser necesario. Por ejemplo si se está buscando a Riquelme, se producirá una desambigüación entre el jugador de fútbol argentino “Juan Román Riquelme”; la modelo paraguaya “Larissa Riquelme”; el niño holandés cuyo nombre es “Riquelme Van Gool”, en homenaje al jugador de fútbol; entre otros. El segundo proceso, trata también con la consulta pero en el sentido de producir la relajación de la entrada, esto quiere decir que si estamos buscando a “Cristina Fernandez de Kirchner” también se considere, por ejemplo, al término “presidenta de la Argentina” como equivalente. En este sentido debemos identificar un algoritmo óptimo para la construcción de la entrada y sus limitaciones. Usando las consultas correctas se podrá encontrar las sentencias adecuadas en el proceso de recuperación. El proceso de búsqueda es realizado en diferentes ámbitos: Búsqueda en microblogs conocidos: Los microblogs se han convertido en una herramienta muy popular entre los usuarios de internet. Millones de mensajes aparecen diariamente en los sitios más populares de microblogging como Twitter, Facebook, Tumblr, etc. Los autores de estos mensajes escriben acerca de su vida, comparten sus opiniones sobre una variedad de temas y discuten sobre estos. Como el formato de los mensajes es libre y de fácil acceso, los usuarios tienden a modificar su forma de comunicación de blogs y listas de correos tradicionales a servicios de microblogging. Dado el gran crecimiento de las publicaciones por parte de los usuarios acerca de los productos que utilizan o visión política y religiosa, los sitios de microbloggin se vuelven un recurso de valor para las opiniones y sentimientos de la opinión pública. Estos sitios brindan herramientas de búsqueda a través de un web service por lo cual hace que esta tarea sea bastante simple. Comentarios de noticias de diarios (La Nación, Clarín, etc.): La proliferación de los diaros en su versión on-line, posibilitan a los lectores la opción de comentar las noticias, con el objetivo de hacer al diario más interactivo. Esta fuente de información es muy rica en contenido y en opinión. La búsqueda sobre los comentarios no es tan trivial como la anterior. Esta se realiza a través de un robot web que va navegando las noticias relacionadas con el tema y almacenando los comentarios. Búsqueda en la web a través de buscadores: aprovechando el resultado que arrojan los buscadores se realiza un robot web que navega los links y devuelve resultados de blogs, listas de correos públicos y noticias de sitios poco conocidos. Toda la información obtenida se almacena en una base de datos con toda la información que se puede obtener de la persona que publica su opinión. El Agente Filtrador se encarga de descartar todos los datos del corpus que no sirven, como por ejemplo entradas duplicadas, entradas que no demuestran sentimientos, etc.. El Agente Analizador se encarga de clasificar las entradas del corpus en sentimientos. Inicialmente comenzaremos a trabajar con una ontología de dos sentimientos: “amor” y “odio”. Este agente es el encargado de realizar un proceso de entrenamiento sobre análisis de sentimientos. Esta tarea es realizada con la herramienta Weka1 e inicialmente utilizado el clasificador Support Vector Machine (SVM) dado su relativo éxito en el tratamiento del lenguaje natural. Posteriormente se realizará un estudio comparativo más profundo sobre otros clasificadores. Finalmente, el Agente Compositor es el encargado de componer los resultados obtenidos por el agente analizador en un lapso de tiempo determinado. El factor tiempo en conjunto con los resultados son los puntos mas importantes a analizar. Los resultados obtenidos podrían modificar el comportamiento del agente buscador antes de comenzar un nuevo ciclo. 3. Resultados esperados El objetivo de este sistema es lograr una herramienta web accesible. De esta manera, el usuario puede proponer una temática para analizar el comportamiento de la opinión pública sobre dicho tema. Actualmente, se está trabajando en producir resultados que sirvan de base de comparación a futuros análisis y mejoras. En este sentido, se pretende analizar diferentes fuentes de búsqueda, algoritmos de clasificación, herramientas lingü´ısticas, etc. para un mejor desempeño del sistema.	﻿Sistemas multiagentes , Agentes inteligentes , Procesamiento del lenguaje natural , Opinion Mining , Análisis de sentimientos	es	19999
18	Indexación y recuperación de información multimedia	﻿ En general, es difícil tanto para los usuarios que intentan recuperar información multimedia poder especificar claramente sus intereses y a través de una consulta bien definida, como para aquéllos que diseñan el sistema decidir cuáles de las características de los objetos multimedia pueden resultar relevantes. La forma en que los datos multimedia se representan, como ellos se almacenan y el costo de transferirlos, entre distintos niveles de la jerarquía de memoria o sobre una red, afectan directamente las respuestas del sistema. Dada una consulta al sistema, el objetivo clave de un sistema de recuperación de información es recuperar la información que podría ser útil o relevante para el usuario, en general haciendo uso de un índice especialmente diseñado para responder a las consultas de manera eficiente. Así, nuestra línea de investigación está enfocada en lograr herramientas eficientes para recuperación de información multimedia, desarrollando nuevas técnicas capaces de soportar la interacción con el usuario, diseñando nuevas estructuras de datos ( índices) capaces de manipular eficientemente datos multimedia y buscando representaciones para los datos que reflejen más adecuadamente las características de interés de los objetos multimedia. Palabras Claves: Recuperación de Información, Bases de Datos Multimedia, Indexación. Contexto Esta línea de investigación se encuentra enmarcada dentro del Proyecto Consolidado 30310 de la Universidad Nacional de San Luis y en el Programa de Incentivos (código 22/F034): “Nuevas Tecnologías para el Tratamiento Integral de Datos Multimedia”. Este proyecto es desarrollado en el ámbito del Laboratorio de Investigación y Desarrollo en Inteligencia Computacional (LIDIC) de la UNSL y se desarrollaron además dentro del Proyecto “Cooperación interuniversitaria para fortalecer el grado y postgrado en Procesamiento y Recuperación de Señales Digitales”, dentro del Programa de Promoción de la Universidad Argentina para el Fortalecimiento de Redes Interuniversitarias III. Nuestra universidad participó junto con la Universidad de Zaragoza (España) y la Universidad Michoacana de San Nicolás de Hidalgo (México) (finalizado en 2010). En este proyecto de investigación se pretende avanzar en la integración de las investigaciones sobre adquisición, preprocesamiento y análisis de datos no estructurados y su aplicación en dominios no convencionales. Se espera que el principal aporte de esta propuesta sea la incorporación de información no estructurada en los procesos de toma de decisiones y resolución de problemas que queda fuera de consideración en los enfoques clásicos. Dentro de este contexto nuestra línea se dedica principalmente al diseño de índices que sirvan de apoyo a sistemas de recuperación de información orientados a datos no estructurados. Se espera así contribuir a estos sistemas obteniendo índices más eficientes para memorias jerárquicas, gracias a la compacticidad y/o con I/O eficiente. Se propone analizar las estructuras de datos existentes, y proponer nuevas u optimizaciones a las mismas, para manipular y recuperar algunos de los tipos de datos no estructurados que aparecen en entornos multimedia. 1. Introducción y Motivación Es común en nuestros días que los sistemas de computación hagan uso intensivo de información estructurada, es decir datos elementales o estructuras, generadas con un formato específico por un programa determinado. Una característica principal en estos casos, es que la estructura o formato de esta información puede ser fácilmente interpretada y directamente utilizada por un programa de computadora. Si bien la información estructurada ha sido la principal materia prima utilizada en los sistemas computacionales hasta la fecha el hecho de restringirse al uso de este tipo de información conduce, muchas veces, a representar una visión parcial del problema y dejar fuera de consideración información que podría ser de gran importancia para la resolución efectiva del mismo. En este contexto gran parte de la información que se requiere para la toma de decisiones y la resolución de problemas de índole general proviene de información no estructurada, principalmente aquélla almacenada en forma de texto, audio, imagen y video. Para responder eficientemente consultas sobre una base de datos multimedia se usan los métodos de acceso (índices) [12, 3, 10]. En general, en recuperación de información, se usan índices debido al volumen de datos con el que se trabaja. Una forma de implementar la búsqueda por similitud en bases de datos multimedia es usando información de anotaciones que describan el contenido de los objetos multimedia. Sin embargo, este modo no es práctico en grandes repositorios multimedia porque las descripciones textuales deben generarse a mano, ya que son difíciles de obtener automáticamente. Además, en general, son subjetivas y en la mayoría de los casos no pueden caracterizar toda la información disponible del objeto multimedia. Más aún, otra restricción de este enfoque es que el procesamiento del índice debe “adivinar” cuáles clases de consultas se podrán hacer sobre los datos. Un enfoque más prometedor para implementar sistemas de recuperación usando búsqueda por similitud es una búsqueda basada en contenidos, la cual usa el dato multimedia mismo. Para calcular la similitud entre dos objetos multimedia, se debe definir una función de distancia. Dicha función mide la similitud, o más bien la disimilitud, entre dos objetos. El concepto de búsqueda por similitud se puede definir a partir del concepto de espacios métricos, lo cual da un marco formal que es independiente del dominio de aplicación. Un espacio métrico está compuesto por un universo U de objetos y una función de distancia d : U × U −→ R+, que satisface las propiedades que hacen de ella una métrica. Las consultas por similitud, sobre una base de datos S ⊆ U , son usualmente de dos tipos: Búsqueda por rango: recuperar todos los elementos de S a distancia r de un elemento q dado. Búsqueda de los k vecinos más cercanos: dado q, recuperar los k elementos más cercanos a q en S . Si la base de datos S posee n objetos, las consultas pueden ser trivialmente respondidas llevando a cabo n evaluaciones de distancia. Sin embargo, la mayoría de las aplicaciones requieren distancias costosas de computar, por ejemplo, la comparación de huellas digitales, búsquedas por contenido en bases de datos multimedia, etc. Por lo tanto, la búsqueda secuencial no escala para problemas de tamaño medio o grande, que son los tamaños más habituales de las bases de datos multimedia. Así el objetivo es preprocesar la base de datos, construyendo un índice, para que las consultas puedan ser respondidas con la menor cantidad de cálculos de distancia. Un caso particular de dato multimedia son las imágenes. Una imagen es un arreglo de píxeles que resultan de la convolución de una función señal y de una función de “rendering”. Dos imágenes son consideradas copia una de otra si tienen la misma señal, aunque tengan diferente función de “rendering”. El estudio del conjunto de características para una recuperación exitosa de imágenes basadas en su contenido representa un aspecto fundamental en muchas aplicaciones tales como reconocimiento de objetos, recuperación de imágenes, reconocimiento de texturas entre otras. El color, la forma o la textura son características visuales de la imagen importantes para analizar, representar e indexar la imagen [5, 6, 7]. Una imagen puede sufrir distorsiones geométricas como RST (rotación, escalado y traslación), “cropping”, perspectiva; distorsiones de iluminación (“light distortion”, “sunburnt distortion”), así como también distorsiones de calidad (“blurring” o “halftone scan-line”). En sistemas típicos los contenidos visuales de las imágenes son extraídos y descriptos mediante vectores característicos multidimensionales. Los vectores deben ser unívocos y robustos a las transformaciones a las que puede estar sometida una imagen. Existe una extensa literatura proponiendo diferentes vectores o descriptores y comparando la performance entre ellos. Dependiendo de la aplicación, algunos descriptores son mas apropiados que otros. Por lo tanto, nuestra propuesta se enfoca en tratar de mejorar las herramientas de recuperación desarrollando nuevas técnicas que soporten la interacción con el usuario, diseñando nuevas estructuras de datos (índices) capaces de manipular eficientemente datos multimedia y, dado que es fundamental la etapa de extracción automática de características de interés, definiendo representaciones que reflejen más adecuadamente los objetos multimedia. 2. Líneas de Investigación Como se ha mencionado previamente se pretende investigar respecto de dos aspectos importantes para los sistemas de recuperación de información multimedia: diseñar nuevos índices capaces de manipular eficientemente datos multimedia y definir representaciones que reflejen las características de interés de los objetos multimedia, gracias a la obtención de un descriptor que sea robusto frente a operaciones (por ejemplo el cizallado). Búsqueda en Texto Comprimido Autoindexado Los tiempos de respuesta de las consultas dentro de una fracción de segundo en los motores de búsqueda Web son factibles debido a la utilización de las técnicas de indexación y el almacenamiento en caché, que están pensados para grandes colecciones de textos particionado y replicado en un conjunto de procesadores de memoria distribuida. En este contexto se estudian métodos de procesamiento de consultas alternativas para esta configuración, que se basa en una combinación de texto comprimido autoindexado y listas de posteo (posting lists). Un texto auto-indexado (es decir, un índice que comprime el texto y es capaz de extraer las piezas arbitrarias del mismo) puede ser competitivo con un índice invertido si tenemos en cuenta el proceso de consulta completo, que incluye la descompresión del índice, el ranking y el tiempo de extracción de snnipets. La ventaja es que en el espacio de la colección de documentos comprimidos, se puede llevar a cabo la generación de listas de posteo, el ranking de documentos y extracción de snnipets. Esto reduce significativamente el número total de procesadores que participan en la solución de las consultas. Diseño de Índices Un catálogo importante de índices para espacios métricos aparece en [10, 3, 12]. La mayoría de los índices usan la desigualdad triangular para evitar el análisis secuencial de la base de datos. La distancia entre la consulta y los objetos de la base de datos puede ser estimada calculando algunas distancias de antemano hacia unos objetos distinguidos llamados pivotes y sin calcular las distancias reales desde el objeto de consulta a los objetos de la base de datos durante una búsqueda. Otra técnica común es indexar una partición del espacio en regiones denominadas particiones compactas. Entre todas las técnicas para indexación en espacios métricos nos interesan las estructuras de datos dinámicas, donde la base de datos no se conoce de antemano y además tanto los objetos como las consultas arriban al azar. En cambio, las estructuras estáticas se benefician desde el conocimiento de la base de datos seleccionando los mejores puntos de referencia para una estructura de datos determinada. En particular es de interés mejorar el desempeño de índices dinámicos jerárquicos (árboles), que es el caso de algunos de los índices para espacios métricos. Estos índices dinámicos, en general, se construyen incrementalmente vía inserciones. De tal manera, la raíz del árbol es el primer objeto que llega, y esto se repite recursivamente en cada nivel del árbol. En esta línea se ha propuesto una técnica donde el “buffering” logra un buen compromiso entre una estructura estática construída con toda la información necesaria y una dinámica con conocimiento local de los datos. Entonces, en lugar de elegir al primer elemento como la raíz, se demora la selección hasta que hayan arribado suficientes elementos para estar en condiciones de realizar dicha selección, y de esta manera se toma una decisión en base a más información. Dado que las consultas arriban a un ritmo desconocido, para mantener el dinamismo es necesario contar con un índice que responda a las consultas con mejor desempeño que la técnica de fuerza bruta. La idea ha sido, entonces, dar una estructura propia al “buffer” de manera que fuera capaz de responder consultas. Es por ello que el índice del “buffer” debería ser rápido y eficiente. En este sentido, se han analizado dos elecciones posibles: la primera fue usar un índice del mismo tipo como estructura del “buffer”, reconstruyendo una vez que el “buffer” estuviera completo. La segunda alternativa fue usar otra estructura de datos, como AESA [11], donde se asume el conocimiento completo de las distancias entre los elementos del “buffer”. Así, se han analizado distintas estrategias de selección de la raíz. Esta técnica provee un marco adecuado para diseñar estructuras de datos dinámicas estables. Por lo tanto, tener un “buffer” en todos los niveles de una estructura jerárquica debería ser útil cuando se diseñan estrategias de ruteo para guiar las búsquedas, lo cual resulta un área promisoria de investigación. Resultados preliminares fueron publicados en [4]. En muchos casos los volúmenes de información con los que se debe trabajar (millones de imágenes en la Web), hacen necesario que los índices sean almacenados en memoria secundaria. En este caso, para hacerlos eficientes, no sólo se debe considerar que durante las búsquedas se realice el menor número de cálculos de distancia sino también, dado el costo de las operaciones sobre disco, se efectúe la menor cantidad posible de operaciones de E/S. Por ello, en esta línea nos hemos dedicado a diseñar índices especialmente adaptados para trabajar en memoria secundaria, logrando un buen desempeño de los mismos, principalmente en las búsquedas. Hemos diseñado e implementado las siguientes estructuras DSACL*-tree y el DSACL+-tree [2], las cuáles son optimizaciones para memoria secundaria de la estructura propuesta en [1] y demostraron ser competitivas frente a otras de las estructuras para memoria secundaria conocidas tales como el M-tree y DSA*-tree y DSA+-tree [8]. Nos proponemos optimizarlas aún más, gracias a la aplicación de técnicas de computación de alto desempeño. Representación de Objetos Multimedia La performance es cada vez más importante en visión por computadora. Para implementar sistemas de recuperación efectivos usando búsqueda por similitud se realiza una búsqueda basada en contenidos, la cual usa el dato multimedia mismo y se define una función de distancia para calcular la similitud entre dos objetos multimedia. En muchos casos para modelar la similitud de objetos multimedia se transforman los objetos en puntos de un espacio vectorial, el cual es un tipo particular de espacio métrico. Luego de definir ciertas características de interés para los objetos, se extraen los valores numéricos que los representa y se construye el vector de características o descriptor, generalmente de alta dimensionalidad. Sobre espacios vectoriales se han definido numerosas funciones de distancia; por ejemplo: la distancia Euclidiana. El tipo de aplicación, las características a explotar o la dimensionalidad son aspectos fundamentales a considerar para definir la mejor función de distancia a utilizar. Por lo tanto, es necesario resolver un problema de optimización. En el caso de los espacios métricos, la función de similitud (la distancia) normalmente mide el mínimo esfuerzo (costo) necesario para transformar un objeto en otro. Aunque ésta es una manera formal de definir la similitud entre objetos, dependiendo de los tipos de datos multimedia reales la función de similitud puede ser muy compleja y puede no siempre satisfacer las propiedades de una métrica. Dependerá realmente de la aplicación final cuál de ambos modelos debería usarse. El enfoque más práctico, como ya se mencionó, es modelar los datos multimedia como un espacio vectorial. Sin embargo, el enfoque de espacios métricos puede ser también útil si la similitud satisface las propiedades de una métrica. Cuando los datos multimedia se han modelado como un espacio métrico o como un espacio vectorial, la búsqueda por similitud se reduce a una búsqueda de objetos o puntos cercanos en el espacio. 3. Resultados Se ha podido comprobar experimentalmente que las estrategias de “buffering” mejoran la performance de una estructura de datos dinámica [4]. Se seleccionó como caso de estudio el Árbol de Aproximación Espacial Dinámico (DSA-tree) [8] y se obtuvo una mejora sistemática en los costos de las consultas usando un “buffer” en el primer nivel del árbol. En particular, se ha podido verificar que esta estructura es mejor que su versión estática [8], porque deja como “vecinos” de un nodo algunos objetos alejados, permitiendo así avanzar en la exploración espacial con “pasos más grandes” a los vecinos de un nodo. Es por esto que ahora se pretende analizar el efecto de elegir como vecinos de un nodo una muestra de objetos cercanos y lejanos. Si se clasificaran los objetos por distancia a la raíz, usando la información del histograma de distancias a ella, se podría elegir con esa misma densidad a los vecinos: muchos donde el histograma sea denso y pocos donde el histograma sea ralo. Se espera que esta estrategia mejore, aún más, el desempeño de este índice y que esto pueda extenderse a otros índices jerárquicos. En este mismo sentido, se implementaron dos versiones de índices (DSACL*-tree y DSACL+-tree) que permiten trabajar con grandes volúmenes de datos, por haber sido diseñadas específicamente para trabajar en memoria secundaria y que mostraron ser competitivas contra otras estructuras igualmente diseñadas para tal fin [2]. Se espera lograr para estos índices una implementación paralela eficiente. Respecto a la representación de datos multimedia, en particular sobre imágenes, un problema abierto corresponde a la construcción de una distancia que permita identificar imágenes completas [9]. Es decir, el estado del arte ([5, 6, 7]) permite identificar puntos de interés que son semejantes; pero es sólo un paso para poder identificar imágenes completas que contienen muchos puntos de interés. Por lo tanto, para una imagen se desea llegar a determinar los puntos de interés de la misma, que sean invariantes a distorsiones. Entonces, estos puntos de interés para una imagen formarían su representación, donde los diferentes puntos de interés serían a su vez vectores. Luego, la recuperación basada en contenido considerará, como imágenes similares, a aquellas imágenes cuyos grafos que se forman con los respectivos conjuntos de puntos de interés sean similares. Así uno de los objetivos consistirá en determinar una función de distancia entre los respectivos grafos de puntos de interés. 4. Formación de Recursos Humanos Considerando la importancia de la formación, para contribuir al desarrollo de sistemas de recuperación de información multimedia, dentro de esta línea se están formando los siguientes docentes: Trabajo Final de Licenciatura en Ciencias de la Computación: un alumno desarrolló su trabajo final sobre un índice dinámico para búsquedas por similitud en espacios métricos especialmente diseñado para memoria secundaria, gracias a una Beca Estímulo de la Facultad de Ciencias Físico Matemáticas y Naturales de la UNSL. El mismo continuará sus estudios de Maestría profundizando en la misma temática. Tesis de Maestría en Ciencias de la Computación: uno de los integrantes está desarrollando su tesis de Maestría sobre el tema de recuperación de imágenes. Tesis de Doctorado en Ciencias de la Computación: uno de los integrantes se encuentra definiendo su plan de doctorado sobre temas de diseño y optimización de índices para realizar búsquedas por similitud, con miras a aplicaciones de minería de datos multimedia.	﻿Bases de Datos Multimedia , Recuperación de Información , Indexación	es	20049
19	Estudio preliminar sobre la producción científica en ingeniera de software en la argentina	﻿    Se realiza un estudio preliminar en el cual se  identifican y recopilan publicaciones de los  años 2009 y 2010 de las siguientes eventos:  Workshop de Investigadores en Ciencias de la  Computación (WICC), Congreso Argentino de  Ciencias de la Computación (CACIC),  Jornadas Argentinas de Informática (JAIIO)  relativos a la  investigación en Ingeniería de  Software en Argentina  y en la región. Se toma  como punto de partida la conceptualizacion de  Ingeniería de Software propuesta por  diferentes autores.    Palabras Clave  ingeniería de software     investigación en ingeniería de software en  Argentina      Contexto.    Este trabajo se desprende del proyecto “La  Investigación en Ingeniería de Software. Su  estado actual en nuestro país y la región.” con  código 02/F768 Resolución Rectoral N°  266/09 de la Universidad Nacional de  Catamarca – Secretaria de Ciencia y  Tecnología - Consejo de Investigación.    Introducción.    Una de las primeras definiciones de Ingeniería  de Software (IS) es la propuesta por Fritz  Bauer en la primera conferencia importante  dedicada al tema (Naur, 1969) como: “El  establecimiento y uso de principios de  ingeniería robustos, orientados a obtener  software económico y que funcione de manera  eficiente sobre máquinas reales” . Esto da una  idea de la poca madurez que comporta en  relación a otras disciplinas tales como la física  o la matemática.  Posteriormente se han propuesto muchas  definiciones destacando la importancia de base  teórica ingenieril para el desarrollo del  software. A continuación se exponen algunas  definiciones del prólogo de la cuarta edición  en español de “Ingeniería del Software: un  enfoque práctico” de Roger Pressman  (Pressman, 2005):  • Ingeniería del Software es el estudio de  los principios y metodologías para  desarrollo y mantenimiento de sistemas  de software. [Zelkovitz, 1978]  • Ingeniería del Software es la aplicación  practica del conocimiento científico en  el diseño y construcción de programas  de computadora y la documentación  necesaria requerida para desarrollar,  operar (funcionar) y mantenerlos”.  [Bohem,1976]  • Ingeniería del software trata del  establecimiento de los principios y  métodos de la ingeniería a fin de  obtener software de modo rentable que  sea fiable y trabaje en máquinas reales.  [Bauer, 1972]  • La aplicación de un enfoque  sistemático, disciplinado y  cuantificable al desarrollo, operación  (funcionamiento) y mantenimiento del  software; es decir, la aplicación de  ingeniería al software. [IEEE, 1993]  • Es una disciplina que comprende todos  los aspectos de la producción de  software desde las etapas iniciales de la  especificación del sistema, hasta el  mantenimiento de éste después de que  se utiliza. [Sommerville, 2001]  En el ámbito académico, por ejemplo, la  ingeniería de software se incorpora más tarde  como asignatura dentro del currículum, no  como eje principal de una ingeniería.   Los planes de estudio en informática en la  Argentina, han tomado como base el modelo  de Currículum de la Association for  Computing Machinery (ACM, 1968) y sus  actualizaciones.   Es así que existe un problema importante con  respecto a la definición de las características  de la IS, en parte atribuida por la poca  madurez de la disciplina.   La IS no ha alcanzado la capacidad de  identificar y explicar sus procesos de  investigación y tampoco ha adquirido la  capacidad de reconocer trabajos de excelencia.  Tales explicaciones, no sólo sirven de  referencia a los investigadores, sino al público  en general. La aceptación de sus resultados  está íntimamente relacionada con los métodos  de investigación utilizados. La IS no dispone  de tales capacidades, sus investigadores,  raramente describen sobre los paradigmas de  investigación utilizados o sobre estándares  para aceptar la calidad de sus resultados. Los  trabajos existentes en esta área son escasos.  En nuestro país y en los últimos años, es  evidente que la mayor parte de las  contribuciones importantes en IS son  realizados por empresas del medio y muy  pocas provienen del ámbito académico. Una  muestra de ello es la instalación de empresas  como Motorota e Intel.  ¿Cuál es el camino que sigue la investigación  en IS en nuestro país y la región?  ¿Qué  métodos emplea? ¿Cuál es el enfoque  investigador que le corresponde? ¿Cuál es su  sentido? Nos hemos planteado estos  interrogantes de investigación y es nuestra  intención poder darles alguna respuesta  concreta. Con ella podremos describir el  estado actual de la investigación en el campo  de la IS y así obtener una visión global y local  de la situación.  Se pretende centrar la atención en este campo  porque consideramos que aún está sin explorar  y haciendo un estudio de su situación ayudará  a los investigadores, presentando una  clasificación de la bibliografía, el patrón más  común en este tipo de publicaciones, así como  los medios más relevantes donde suele  publicarse.  Para llevar a cabo esta investigación, se ha  previsto cumplir las siguientes etapas:  1. Identificación y recopilación de  publicaciones para su análisis mediante el  empleo de distintos métodos, como los  sugeridos por  Kitchenham (Kitchenham,  2004) y otros autores. Dada la escasez de  publicaciones en este campo, trataremos de  incluir el mayor número de investigaciones  que hay hasta el momento.  2. Determinación de una serie de parámetros  relativos a la  investigación en IS en  Argentina  y en la región, enfocándonos  siempre en los fundamentos teóricos,  filosóficos, conceptuales explicitados:  Producción científica, esto es, cuánto  representa la producción argentina en IS en  el contexto de la región.  3. Búsqueda sobre el título, resumen y  palabras clave. Examinaremos el título y el  resumen de artículos encontrados para  verificar su relevancia. No tenemos  previsto tener en cuenta las publicaciones  en  libros, capítulos de libro e informes  técnicos, ya que para estudiar la  producción científica, es preferible acudir a  revistas y conferencias. Además, los libros,  capítulos de libro, otras publicaciones  periódicas e informes técnicos sólo  representan un porcentaje ínfimo del total  de publicaciones significativas.  4. Definición de conclusiones mediante la  relación entre los datos obtenidos con otros  indicadores socioeconómicos, como el  número de patentes y modelos de utilidad,  inversión en investigación y desarrollo,  importancia económica de las empresas de  tecnologías de la información, entre otros.  Prevemos realizar este tipo de análisis en  el futuro.  En el presente trabajo nos encontramos en la  primera etapa de la investigación.    Líneas de investigación y desar rollo.     Conceptualización de la Ingeniería de  Software: Definir la Ingeniería de Software  según los diferentes autores.    Búsqueda e identificación de un marco de  referencia para la caracterización de la  investigación en Ingeniería de Software:  Identificar un marco que provea una visión  global y unificada de las investigaciones en  Ingeniería de Software.       Identificación de los parámetros referentes a  investigación: Identificar las formas posibles  de analizar las publicaciones en IS.    Relevamiento de información en  publicaciones: Buscar publicaciones sobre el  tema IS y analizar resúmenes y palabras  claves.    Objetivos.    El objetivo general de este proyecto de  investigación es: Identificar si existen  conceptos, paradigmas, métodos y procesos de  investigación definidos en IS en la región,  nuestro país y en el ámbito de la FTyCA de la  UNCa  Para alcanzar el mismo se plantean los  siguientes objetivos específicos:  • Buscar diferentes marcos teóricos sobre el  tema.  • Analizar los modelos de evolución  tecnológica  • Caracterizar el proceso investigador en IS.  • Identificar los ámbitos de investigación en  IS en nuestro país en los que se estén  aplicando principios y conceptos que estén  referidos a los procesos investigativos.  • Comparar las estrategias de investigación a  nivel nacional con las estrategias de  investigación a nivel regional e inclusive,  internacional.  Para lograr estos objetivos de la investigación  en el presente trabajo se realiza un estudio  preliminar en el cual se identifican y recopilan  publicaciones de los años 2009 y 2010 de las  siguientes conferencias, jornadas y workshops:    • Workshop de Investigadores en Ciencias de  la Computación (WICC): El objetivo del  Workshop es crear un foro para el  intercambio de ideas entre investigadores  en Ciencias de la Computación,  fomentando la vinculación y potenciando  la posibilidad de acciones coordinadas de  Investigación y Desarrollo. Se encuentra  organizado en áreas, una de ellas es la  Ingeniería de Software.   • Congreso Argentino de Ciencias de la  Computación (CACIC): es el evento  académico anual más relevante de  Argentina en la disciplina, organizado por  la Red de Universidades Nacionales con  Carreras en Informática (RedUNCI).  Cubre todos los tópicos relevantes de  Informática a través de diferentes  Workshops, entre ellos el Workshop de  Ingeniería de Software.  • Jornadas Argentinas de Informática  (JAIIO): organizada por la SADIO, donde  en sesiones paralelas se presentan trabajos  que se publican en Anales, se discuten  resultados de investigaciones y actividades  sobre diferentes tópicos, desarrollándose  también conferencias y reuniones con la  asistencia de profesionales argentinos y  extranjeros. La JAIIO se organizan como  un conjunto de simposios separados, cada  uno dedicado a un tema específico, entre  ellos la Ingeniería de Software    Resultados.    El estudio ha tomado como punto de partida la  conceptualización de la IS de acuerdo a  diferentes autores y luego se relevó un  conjunto de 194 publicaciones en el  área de  IS, estas publicaciones pertenecen a las  conferencias, jornadas y workshops  mencionados en el punto anterior.  Las publicaciones fueron sometidas a los  siguientes filtros:  1°.- Identificación de los autores. Se  localizaron hasta el momento 84 publicaciones  de las que se extrajo autores, el título, resumen  y palabras clave.   2º.- Se restringió el número de publicaciones a  aquellas a donde al menos un autor pertenecía  a una universidad argentina. En este sentido  fueron descartadas 10 publicaciones.  3°.- Publicaciones de las que no se obtuvo  autores o resumen aún: 90 publicaciones.  La búsqueda de las publicaciones se realizó  utilizando los anales de las conferencias,  jornadas y workshops que pertenecían a algún  integrante del grupo de investigación y a  través de Internet para localizar aquellas  publicaciones de las que no se cuenta con los  anales.  En este trabajo se limitó la identificación a un  conjunto de publicaciones de los años 20092010. Se tratará de incluir más publicaciones  en los próximos pasos para dar una respuesta  más cabal a la línea de investigación  “Relevamiento de información en  publicaciones”.      Formación de Recursos Humanos.    Con este proyecto se prevé continuar con la  formación y fortalecimiento en investigación  de los integrantes del proyecto.   Se ha incorporado una becaria al proyecto y se  contempla también la incorporación de otros  becarios alumnos de la carrera Ingeniería en  Informática de la  Facultad de Tecnología y  Cs. Aplicadas (FTyCA) de la UNCa para su  iniciación en el proceso de investigación.  Dichos becarios desarrollarán tareas del  proyecto que la Directora les asigne.  Constituirá un avance significativo en el  conocimiento y comprensión de las  características de la investigación en IS, una  nueva forma de abordarla desde la propia  investigación por parte de sus integrantes y  becarios, que son docentes y estudiantes del  área de Informática / Sistemas de Información.  Permitirá comprender claramente las  características y tendencias de las  investigaciones en esta disciplina.  Se prevé el dictado de uno o dos  cursos/seminarios a cargo de integrantes del  grupo de investigación sobre las características  de la IS, la calidad en software y/o algún otro  tema relacionado destinados a la comunidad de  la FTyCA,.   Está previsto que dos integrantes del grupo  definan el anteproyecto y comiencen el   desarrollo de su Tesis de Maestría en  Ingeniería de Software dentro del marco de  esta investigación.	﻿investigación en ingeniería de software en Argentina , ingeniería de software	es	20100
20	Definición de un proceso de implantación de sistemas	﻿ La Implantación de sistemas es un tema  relevante en lo que se refiere al desarrollo de  software y de tecnologías de la información. A  pesar de ello, la ingeniería de software continúa  centrándose en abordar los problemas del  desarrollo desde la mejora de procesos pero sin  abordar de manera sistemática la Implantación  como un conjunto de temas específicos a ser  tratados.   Uno de los problemas detectados en gran parte  de los proyectos informáticos, en general está  dado por las dificultades en la Implantación de  los mismos en los diferentes entornos sociales y  tecnológicos,  siendo esta etapa, un atributo  fundamental para el éxito de la puesta en  marcha de los sistemas.   La investigación que se expone en el presente  artículo se propone enmarcar los límites de la  Implantación de sistemas, como parte un  proceso inherente a la definición del proceso  software, y que asimismo, debe ser definido  específicamente y a través de un conjunto de  principios básicos que permitan comprender y  abordar esta etapa como un área específica  dentro de la Ingeniería de Software o la  Ingeniería de Sistemas.  Palabras Clave: Implantación de Sistemas,  Proceso Software.     CONTEXTO  El Grupo de Ingeniería de Software “G.I.S.” se  encuentra trabajando en la línea de  investigación  de ingeniería de software que  aborda específicamente la Implantación de  sistemas de información y la sistematización de  las prácticas necesarias que son tratadas en los  estándares de proceso utilizados en la industria  del software. La línea de investigación que se  plantea en este artículo, está constituida como  una línea de transferencia de tecnología al  sector, basada en el desarrollo de una propuesta  metodológica para abordar la Implantación de  los sistemas.     INTRODUCCION  La correcta Implantación de un sistema  involucra diversos aspectos tecnológicos así  como de contexto social en cuanto a los actores  involucrados en las diferentes instancias.   Para los proyectos de TI existen diversos  modelos de proceso y de gestión que dividen en  sub-procesos cada una de las actividades que  deben llevarse a cabo en el desarrollo y la  puesta en marcha de los sistemas de  información.   El estándar PMBOK [1], reconocido  internacionalmente provee fundamentos para la  gestión de proyectos, aplicables a un amplio  rango de proyectos generalmente aceptados  como las mejores prácticas para la gestión.   Este estándar, dedica dos de sus capítulos a  presentar una guía básica de prácticas acerca de  cómo se deben gestionar los recursos humanos  y las comunicaciones dentro de un proyecto  El 'PMBOK' reconoce 5 grupos de procesos  básicos y 9 áreas de conocimiento comunes a  casi todos los proyectos.  El esquema de los grupos de procesos se  presenta en el siguiente gráfico 1:      Gráfico 1. Grupos de procesos definidos por el PMBOK  Los procesos se traslapan e interactúan a través  de un proyecto o fase. Los procesos son  descritos en términos de: Entradas  (documentos, planes, diseños, etc.),  Herramientas y Técnicas (mecanismos  aplicados a las entradas) y Salidas (documentos,  productos, etc.). Las nueve áreas del  conocimiento mencionadas en el PMBOK se  exponen en la siguiente figura:    Gráfico 2. Áreas de conocimiento del PMBOK  Al analizar el ITIL, Information Technology  Infrastructure Library, [2] se observa  que es un  Modelo conformado por 5 libros, basados en el  ciclo de vida del servicio:  1. Estrategia del Servicio  2. Diseño del Servicio  3. Transición del Servicio  4. Operación del Servicio  5. Mejora Continua del Servicio    El Modelo CMMI, Capability Maturity Model,  [3] propone mejorar la capacidad de los  procesos en las organizaciones, integrando la  ingeniería de software con la ingeniería de  sistemas en un único marco de referencia.   Está conformado por 4 categorías de proceso  Ingeniería, Soporte, Gestión de Procesos y  Gestión de Proyectos, dentro de las cuales se  definen 22 áreas clave de proceso, que se  presentan de la siguiente manera:  REQM – Gestión de Requerimientos  PP – Planeación de Proyectos  PMC – Monitoreo y Control de Proyectos  PPQA – Aseguramiento de calidad de proceso y  producto  MA – Medición y Análisis  SAM – Administración de Acuerdos con  Proveedores  CM – Administración de la configuración  DAR – Análisis y resolución de decisión  IPM – Gerencia integrada de proyectos  OPD – Definición del Proceso Organizacional  OPF – Foco en el Proceso Organizacional  PI – Integración de producto  RD – Desarrollo de Requerimientos  RSKM – Gestión de Riesgos  TS – Solución Técnica  VER – Verificación  VAL – Validación  OT – Entrenamiento organizacional  QPM – Gestión Cuantitativa de Proyectos  OPP – Desempeño del Proceso Organizacional  OID – Innovación y Despliegue Organizacional    Por otra parte, para referirnos específicamente  al desarrollo de software, debemos analizar el  estándar de IEEE 1074 [5] y la Norma ISO  12207 [4].  En cuanto a la norma ISO 12207- 1995 [4]  constituye un modelo prescriptivo que propone  una guía manual para la gestión y el desarrollo  del software. El proceso software que modela  divide las actividades que se pueden realizar  durante el ciclo de vida del software en cinco  procesos primarios, ocho procesos de soporte y  cuatro procesos organizativos.   Estos procesos se presentan en la tabla 1:    Areas de  Proceso  Procesos  Procesos  Principales del  Ciclo de Vida  Proceso de Adquisición  Proceso de Suministro  Proceso de Desarrollo  Proceso de Operación  Proceso de Mantenimiento  Procesos de  Soporte del  Ciclo de Vida  Proceso de Documentación  Proceso de Gestión de la  Configuración  Proceso de Garantía de la  Calidad  Proceso de Verificación  Proceso de Validación  Proceso de Revisión Conjunta  Proceso de Auditoría  Proceso de Resolución de  Problemas  Procesos  Organiza-tivos  del Ciclo de  Vida  Proceso de Gestión  Proceso de Mejora  Proceso de Infraestructura  Proceso de Formación   Proceso de Adaptación  Tabla1. Procesos de la ISO/IEC 12207-1995[4]  Según el estándar de IEEE 1074 [5] el proceso  del software se compone de 17 grupos de  actividad, cada uno de los que contiene  actividades que son responsables de satisfacer  sus requisitos asociados. El proceso se compone  de un total de 65 actividades. IEEE 1074  engloba las siguientes familias de actividades:  GRUPOS  ACTIVIDADES   Gestión del  Proyecto  Iniciación del  Proyecto   Planificación del  Proyecto  Control y Seguimiento    Pre-Desarrollo Exploración del Concepto  Asignación del Sistema  Importación del Software  Desarrollo Requisitos  Diseño  Implementación  Post Desarrollo Instalación  Operación y Soporte  Mantenimiento  Retiro  Actividades  Integrales  Evaluación  Gestión de la Configuración  Documentación   Formación  Tabla 2. Procesos de la IEEE 1074 [5]  En éste modelo existe un proceso de Instalación  pero cuya descripción esta acotada a “Verificar  que la configuración del software que ha sido  implementada es la correcta y termina con la  aceptación formal por parte del cliente del  software en cumplimiento de las  especificaciones de la Información del Plan de  Gestión del Proyecto de Software y la  realización con éxito de la prueba de aceptación  del usuario.” [5]  Con la excepción del modelo IEEE 1074, en el  estudio de estos modelos, la Implantación del  sistema no es tratada como un área específica   como un sub-proceso, sino que a lo largo del  proceso se incluyen algunas actividades que  involucran algunas definiciones para la puesta  en marcha de los sistemas.  En cuanto a las metodologías ágiles de  desarrollo de software, se puede destacar a  Scrum [6] que constituye una metodología de  desarrollo ágil que se aplica en proyectos en los  cuales se trabaja con requisitos inestables y que  requieren flexibilidad y rapidez en el desarrollo.   Scrum propone tres etapas en las cuales tanto  los desarrolladores como los usuarios juegan un  papel muy importante, desde la participación en  la planificación hasta la toma de decisiones en  cada instancia.  Por su parte, XP [7] propone una fuerte  interacción entre el equipo de desarrollo y  los  usuarios finales del sistema, llegando a   proponer que un representante del cliente  trabaje en forma conjunta con el equipo de  desarrollo. También propone la programación  de a pares y las reuniones frecuentes para  evaluar el estado y avance de las diferentes  actividades.  Desde esta perspectiva metodológica, se  observa que existen modelos y técnicas que   abordan el trabajo en equipo y la necesidad de  definir una dinámica ágil en los proyectos de  TI, pero desde un enfoque de la administración  de los recursos en general y de la  reorganización del proceso, pero no abordan los  puntos críticos de instancias como la  Implantación, que requiere de todos los  involucrados en diferentes roles para el logro  del objetivo y una definición específica de  actividades en esa instancia particular del  proceso.       LINEAS DE INVESTIGACION  y  DESARROLLO  La línea principal de la presente investigación  consiste en definir específicamente que se  entiende por Implantación de Sistemas o  Implantación de software, para poder de modo  tal que permita trabajar sobre el área como un  dominio específico y elaborar una propuesta  metodológica que incluya las  mejores prácticas  para la gestión de la Implantación de los  proyectos de TI que contenga recomendaciones  aplicables a las diferentes etapas del proyecto  en lo que respecta a esa fase de un proyecto.   Por lo tanto, se trabaja en la idea de conformar  una Ingeniería de Implantación que pueda  modelar diferentes metodologías, de acuerdo  con los diferentes contextos sociales y  tecnológicos de Implantación.  En este contexto, se ha realizado un trabajo  exploratorio entre estudiantes avanzados con  experiencia laboral en la Implantación de  sistemas y en un grupo de expertos. El estudio  exploratorio consiste en la realización de  entrevistas abiertas para detectar los elementos  centrales de la puesta en marcha de sistemas y a  partir de allí se ha definido una encuesta  estructurada con preguntas cerradas y otras  abiertas, de modo tal que permita indagar en  diferentes grupos de involucrados.   El grupo de estudiantes relevado ha sido de  diferentes universidades y está conformado por  22 alumnos de diversas materias del último año  de licenciatura en sistemas, con experiencia  laboral en el área. En cuanto al grupo de  expertos, se ha optado por entrevistar a 25  profesionales con vasta experiencia en gestión y  puesta en marcha de importantes proyectos  informáticos.     RESULTADOS y OBJETIVOS   Frente a esta evaluación exploratoria y  analizados los diferentes modelos de gestión de  IT, la presente investigación ha alcanzado una  especificación de las características básicas de  la Gestión en la implantación de sistemas de TI.  Como primer resultado, se puede destacar una  definición conceptual en la elección del término  “implantación” en lugar de implementación, al  referirnos específicamente a todo lo incluido en  la puesta en marcha de un sistema, abarcando  los aspectos de factor humano, de  infraestructura tecnológica y de producto  software. En cuanto al término  “implementación” se ha detectado que  habitualmente se utiliza en un sentido más  amplio, refiriéndose a todo el proceso de  desarrollo y/o construcción del software.   Respecto a la encuesta relevada, y se ha  detectado que la definición del área específica  de implantación no es lo suficientemente clara  para los diferentes involucrados en los  estándares y modelos más utilizados en la  industria del software y servicios informáticos.  En tanto que gran parte de los expertos define  académicamente la idea de Implantación, no  aparecen claramente definidos los límites de su  incumbencia.  Asimismo, se ha explorado sobre la necesidad  de definir un proceso específico para la puesta  en marcha de sistemas y en ambos grupos  explorados se ha remarcado la necesidad de  definir un modelo que ayude a la gestión en la  Implantación, así como se ha detectado la falta  de un conjunto de técnicas específicas que  soporten la puesta en marcha de los sistemas.  Por otra parte, de los resultados exploratorios se  detecta que, de los 22 estudiantes encuestados  el 86% considera que la implantación de  sistemas es una etapa específica del proceso, en  tanto que solo un 14% no lo considera  necesario.       Gráfico 3. Estudiantes Fuente: elaboración propia  En cuanto a la opinión de los expertos, sobre 25  encuestados, detectamos que el 92% la  considera como una etapa específica, mientras  que el 4% no responde y otro 4% no lo  considera pertinente, tal como se presenta en el  siguiente gráfico.     Gráfico 4. Expertos. Fuente: elaboración propia  Por otra parte, respecto a la pregunta  ¿Considera necesaria la definición de un  conjunto de actividades específicas para  abordar la Implantación?, el 99% de los  encuestados, incluidos estudiantes y expertos  respondieron afirmativamente.  Sobre el estudio exploratorio existe un conjunto  de elementos más específicos que han sido  relevados y su procesamiento y análisis será  producto de los avances de la investigación.   En este sentido, la siguiente etapa del proyecto  tiene como objetivo determinar los límites del  proceso de implantación de sistemas así como   realizar un relevamiento de las diferentes  prácticas y técnicas específicas que se utilizan  para la implantación en los proyectos de TI  definiendo el alcance y las limitaciones de cada  una de ellas.   A partir de estas definiciones, se consolidará  una propuesta metodológica de Implantación de  Sistemas que incluya mejores prácticas,  organizando el mismo de acuerdo a los  diferentes tipos de proyecto de TI.    Finalmente la última etapa a desarrollar consiste  en validar el modelo generado, a partir de la  puesta en práctica de casos de estudios y de  modo tal que permitan elaborar un análisis y  evaluación de la propuesta metodológica.    FORMACION DE RECURSOS HUMANOS  El Grupo de Ingeniería de Software (G.I.S.) es  un grupo ínter universidad, en esta línea de  investigación que aborda, en la Universidad  Caece, se están desarrollando dos tesis de  Maestría por parte de docentes-investigadores y  se han incorporado alumnos avanzados de la  carrera de sistemas.	﻿Implantación de Sistemas , Proceso Software	es	20124
21	Coordinación basada en argumentación en sistemas multi agente	﻿ Este artículo describe, en forma resumida, parte de los trabajos de investigación y desarrollo que se están llevando a cabo en la línea “Agentes y Sistemas Multi-agente” del LIDIC, en conjunto con investigadores del LIDIA. El objetivo de este trabajo es presentar las principales temáticas que están siendo abordadas actualmente en el área de agentes cognitivos, para posibilitar un intercambio de experiencias con otros investigadores participantes del Workshop, que trabajen en líneas de investigación afines. Uno de los objetivos principales de esta línea, es el estudio y desarrollo de modelos de coordinación para agentes que forman parte de un sistema multi-agente; actualmente, uno de los objetivos parciales del grupo de trabajo, es analizar la utilización de técnicas de argumentación en modelos de coordinación de alto nivel. Este estudio se abordará con un enfoque teórico/práctico abarcando modelos teóricos de sistemas multi-agente y su aplicación en problemas complejos del mundo real. En particular, el énfasis estará puesto en problemas que involucren coordinación de múltiples robots. 1Las investigaciones realizadas en el LIDIC son financiadas por la Universidad Nacional de San Luis y por la Agencia Nacional de Promoción Científica y Tecnológica (PICT 2002, Nro. 12600). 2Las investigaciones realizadas en el LIDIA son financiadas por la Universidad Nacional del Sur, por la Agencia Nacional de Promoción Científica y Tecnológica (PICT 2002, Nro. 13096) y por el Consejo Nacional de Investigaciones Científicas y Técnicas (PIP 5050). 1. Introducción Un agente es una entidad computacional autónoma, virtual (programa) o física (robot), que puede percibir su entorno a través de algún tipo de sensores y que es capaz de interactuar con ese entorno utilizando efectores. En un Sistema Multi-agente, un conjunto de agentes interactúa para conseguir algún objetivo o realizar alguna tarea. Usualmente, en este tipo de sistemas, cada agente posee información incompleta sobre su entorno y sobre sí mismo, teniendo además capacidades limitadas. El control del sistema es distribuido, los datos están descentralizados, y la computación es asincrónica. El entorno en el que los agentes se desenvuelven es dinámico y cambiante, viéndose afectado por las acciones de otros agentes y por su propia dinámica. Por esta razón, no puede predecirse con certeza el estado futuro de ese entorno. Todas estas características transforman a la tarea de coordinación de los agentes en un aspecto fundamental de un sistema multi-agente. El problema de coordinación puede definirse informalmente como “el manejo de las interdependencias entre las actividades de los agentes” [1]. Este problema es generado por la necesidad de tomar en cuenta las acciones de otros agentes, teniendo como objetivo mejorar la acción conjunta del grupo mediante la articulación de las acciones individuales de sus miembros. Las interdependencias entre los agentes pueden ser positivas o negativas. En el primer caso los agentes se benefician con la presencia de otros agentes, mientras que en el segundo, hablamos de la ocurrencia de distintos tipos de conflictos que surgen por el acceso a recursos escasos, objetivos en conflicto o distintos puntos de vista. En este sentido, la tarea de coordinación se dedica a aprovechar las interacciones positivas entre los agentes, y evitar, reducir o resolver las interacciones negativas (conflictos). La coordinación puede darse entre agentes que cooperan para realizar una tarea en conjunto, o también entre un conjunto de agentes individualistas que persiguen sus metas particulares y deben negociar por recursos compartidos o que son brindados por otros agentes del conjunto. En el caso de un sistema cooperativo, la coordinación consistirá en la “planificación” de la distribución de las tareas [2, 3], mientras que en el caso competitivo la coordinación consistirá de una “negociación” entre agentes [4, 5, 6, 7]. En ambos casos, la coordinación entre entidades autónomas requiere de cierta habilidad social [8]. Es importante mencionar, que el mecanismo específico de coordinación implementado en un agente determina su dinámica interna así como las propiedades externas de la sociedad donde se desenvuelve. Los mecanismos de coordinación usualmente varían dependiendo de los dominios de aplicación, de los agentes y de sus inclinaciones hacia la cooperación, del entorno y de su grado de predictibilidad. De acuerdo con lo expuesto por Ossowski [9], las organizaciones, la planificación multi-agente y la negociación, son tres de las clases más importantes de mecanismos de coordinación usados en Inteligencia Artificial Distribuida (IAD). Las organizaciones en los sistemas multi-agente artificiales son vistas como una metáfora sobre un conjunto de relaciones estructurales de largo plazo entre los roles que pueden desempeñar los agentes. Cuando un agente está de acuerdo con adoptar un cierto rol dentro de una organización debe subordinarse al comportamiento que el rol implica. Existen varias propuestas para clasificar las estructuras organizacionales [10]. Los casos extremos corresponderían a una organización jerárquica, donde para cada tarea a realizar hay un manager que controla y coordina su desarrollo, y a una organización lateral donde no hay managers únicos y cada tarea se logra mediante la cooperación entre pares. Por su parte, la planificación multi-agente, es un mecanismo de coordinación donde los agentes forman planes que especifican todas sus acciones e interacciones futuras con respecto a un objetivo particular. De esta forma, los agentes involucrados en la ejecución del plan multi-agente se comprometen a comportarse en concordancia con el mismo. La planificación multi-agente normalmente involucra la elaboración de los planes individuales de cada agente y su coordinación. Cada una de estas tareas podrá ser realizada con distintos niveles de distribución. Existen formas de representar los planes individuales que facilitan su integración y coordinación. En particular, los “Planes de Orden Parcial” (POP) [6] ofrecen mayor flexibilidad para integrar los planes individuales en un plan multi-agente coordinado. La negociación [11], es un mecanismo de coordinación basado en compromisos de término medio entre un grupo de agentes. Los procesos de negociación generan acuerdos dinámicamente, en los cuales los compromisos de los agentes no son permanentes como los compromisos a priori de las estructuras organizacionales. Además, los acuerdos pueden ser re-negociados y de esta manera pueden durar más que los compromisos de los planes multi-agente. Un punto central de un modelo de negociación es representar el objeto de negociación, el cual claramente es dependiente de la aplicación. Por último, es importante mencionar que un protocolo de negociación debe ser definido, de manera de determinar que secuencias de mensajes son legales durante la negociación. Este protocolo se asume público entre los agentes y se refiere tanto a las primitivas de negociación como a los objetos de negociación que pueden ser incluidos en los mensajes. 2. Tareas en Progreso y Trabajos Futuros Dadas las características de los mecanismos de negociación, donde propuestas y contra-propuestas son expuestas por las partes intervinientes, recientemente se han propuesto distintos enfoques que consideran que la argumentación puede ser naturalmente aplicada en este contexto [12, 13, 14]. Asimismo, la argumentación no sólo ha sido utilizada como un mecanismo para llegar a acuerdos entre agentes [15], sino también como un mecanismo de razonamiento interno donde el agente expone argumentos a favor y en contra acerca de aceptar o no una determinada proposición [12, 16, 17, 18, 19]. En el contexto general de negociación, distintos investigadores han comenzado a considerar diversas formas en cómo el proceso de negociación es afectado por las normas sociales e interdependencias entre los agentes. Por ejemplo, en [9], se considera de qué manera las relaciones de dependencia entre los agentes y normas prescriptivas aplicables a un grupo de agentes, puede conducir a la elaboración de planes conjuntos coordinados. Conte y Castelfranchi [20] por su parte, también proponen un modelo acerca del rol de las estructuras normativas en las sociedades de agentes, y consideran a las normas como una fuente externa de modificación de comportamiento. Shoham y Tennenholz [21] proponen la idea de restringir los comportamientos del agente mediante el concepto de leyes sociales aplicables a todo el sistema multi-agente. En particular, en nuestro trabajo se considerará cómo influyen distintos tipos de normas sociales e interdependencias entre agentes en las líneas de argumentación de agentes artificiales inteligentes cuando toman sus decisiones individuales, o en sus negociaciones con otros agentes. En este contexto, nuestro trabajo toma las ideas planteadas en [15], acerca del uso de un protocolo de argumentación flexible para llegar a acuerdos; sin embargo, no se utilizará el ordenamiento prioritario (por orden de severidad) propuesto para selección de argumentos en la negociación, sino que se adaptarán a un contexto argumentativo las ideas propuestas en [9], donde se considera la fuerza social de un agente en las negociaciones dándole un enfoque basado en utilidades y bargaining. Además, se verá cómo estas interdependencias influyen en el razonamiento interno del agente. En la investigación propuesta, en una primera etapa el sistema multi-agente estará formado por un grupo de agentes software (sofbots) mientras que a futuro se procura trabajar con robots móviles. En principio se prevé el uso de Defeasible Logic Programming (DeLP) [22] para representar el conocimiento de los agentes. Asimismo, se utilizará el mecanismo de inferencia provisto por este formalismo para argumentar a favor o en contra de las acciones en base a las cuales se dará la coordinación de los agentes. Sobre este último aspecto ya se han hecho avances, dado que se ha implementado un framework [23] que posibilitará a robots Khepera 2 reales [24] y simulados [25] razonar usando DeLP. En [26], se expone un ejemplo simple de aplicación de este framework para robots reales y simulados que realizan tareas de limpieza. Actualmente, ya se están llevando a cabo experimentos que extienden la labor presentada en [26], puesto que se está trabajando con ambientes donde más de un robot puede estar realizando tareas de limpieza al mismo tiempo [27]. De esta forma, las acciones que cada robot debe tomar están restringidas por el accionar de terceros, y en consecuencia, cada robot necesita modelar a los otros y a si mismo (usando programas DeLP). Además, se tiene como uno de los objetivos generales futuros, extender este framework con los modelos de coordinación que se desarrollen. 3. Consideraciones finales La experiencia adquirida al trabajar con este grupo de robots es doble para esta línea de investigación: por un lado provee de elementos de juicio a tener en cuenta para la coordinación de agentes en un entorno complejo, y por otro lado permitirá experimentar con los resultados teóricos que se obtengan, logrando de esta manera una retroalimentación acerca de qué simplificaciones hechas en el momento de modelar el mundo en una simulación, influyen de manera significativa (positivamente o negativamente) si no son tomadas en cuenta, cuando se realiza la correspondiente confrontación con el mundo real. Algunos de los aspectos principales que están siendo considerados, incluyen el análisis del impacto que tienen los distintos niveles de modelización que pueden realizar los robots sobre el mundo y sobre los otros robots. Asimismo, se están estudiando distintos marcos formales que abordan aquellos factores externos que influyen en el comportamiento de los agentes, tales como la existencia de leyes sociales, normas prescriptivas o simplemente la interdependencia que pudiese llegar a existir entre dos o más agentes.	﻿Sistemas Multi agente , Coordinación , Argumentación	es	20301
22	Explicaciones dialécticas	﻿Explicaciones Dialécticas Nicolás D. Rotstein Alejandro J. García Guillermo R. Simari e-mail: {ndr,ajg,grs}@cs.uns.edu.ar Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET) Laboratorio de Investigación y Desarrollo en Inteligencia Artificial Departamento de Ciencias e Ingeniería de la Computación Universidad Nacional del Sur Av. Alem 1253, (8000) Bahía Blanca, Argentina 1. Introducción Este artículo reporta el estudio realizado hasta el momento en la línea de investigación de explicaciones dialécticas [4] y propone direcciones para el trabajo a futuro. Dentro de varias áreas de la Inteligencia Artificial se ha puesto atención al rol de las explicaciones, en particular en sistemas expertos [7, 10, 6]. El objetivo de brindar explicaciones en sistemas expertos es brindar mayor confianza al usuario con respecto a las respuestas dadas por el sistema. No obstante, pocos han analizado el uso de explicaciones dentro de sistemas argumentativos [8]. En general, se habla de ‘argumento’ como una explicación para un literal dado; es decir, el punto siendo explicado es puesto en discusión, y a partir de ahí puede ser aceptado o no. Por ejemplo, el argumento “hoy está lloviendo, por lo cual me voy a quedar a dormir” puede ser una explicación para el literal “hoy no voy a trabajar”. Sin embargo, la razón provista para no ir a trabajar puede ser puesta en tela de juicio. Por otro lado, en revisión de creencias también se ha estudiado el rol de las explicaciones [3]. Éstas son utilizadas para resolver inconsistencias entre las creencias y las nuevas percepciones; aquel literal soportado por la explicación más fuerte es el que finalmente prevalecerá. Si bien reconocemos que un argumento en sí puede ser tomado como una explicación para un literal, en esta línea de trabajo estamos enfocados en otro tipo de explicaciones: aquellas que justifican las garantías brindadas por un sistema argumentativo. Nuestro trabajo puede analizarse desde un punto de vista abstracto, y los resultados se implementarán en un formalismo en particular: Defeasible Logic Programming (DeLP) [5]. Este formalismo utiliza programas lógicos rebatibles (de aquí en adelante, DeLP-programs) para representar el conocimiento, y argumentación rebatible para el razonamiento. El procedimiento de prueba utilizado es dialéctico, por lo cual estudiaremos un concepto al que denominaremos explicaciones dialécticas (de aquí en adelante, δ-Explanations). El procedimiento de prueba, ante una consulta (query), responde si puede o no creerse en dicho literal, es decir, si éste puede considerarse garantizado. El mecanismo para lograrlo es construir árboles de dialéctica enraizados en un argumento para el literal consultado o para su complemento. Para más detalles referirse a [5]. El conjunto completo de árboles de dialéctica relacionados con la consulta realizada será considerado la δ-Explanation de la respuesta para esa consulta. A lo largo del artículo mostraremos cómo las δ-Explanations son una herramienta poderosa para comprender y analizar las interacciones entre argumentos dentro de un árbol de dialéctica, y para asistir la codificación y el depurado de bases de conocimiento. Actualmente, hay un prototipo implementado en nuestro laboratorio que permite visualizar el conjunto de árboles de dialéctica que justifican la respuesta dada para una consulta. Los ejemplos mostrados en este artículo están generados a partir de este prototipo. En esta línea de investigación se propone el estudio de un área poco explorada: la de explicaciones en sistemas argumentativos. Las explicaciones que proveemos apuntan a revelar el contexto completo de una consulta. Los ejemplos dados en este artículo enfatizan este punto. En la sección que sigue describimos y ejemplificamos las explicaciones dialécticas tanto para consultas fijas como con variables, en la sección 3 discutimos y hacemos una comparación con un acercamiento que se relaciona al nuestro, y en la sección 4 delineamos el trabajo futuro sobre esta línea. 2. Explicaciones Dialécticas Una consulta DeLP (DeLP-query) es un literal fijo que DeLP intentará garantizar. Una consulta con al menos una variable será llamada consulta esquemática (schematic query) y representará el conjunto de DeLP-queries que unifican con ella. Ambos tipos de consultas requieren tratamientos diferentes, por lo cual primero ejemplificaremos las explicaciones para DeLP-queries, y luego, para schematic queries. Pueden existir varios argumentos para un mismo literal, y cada argumento generará un árbol de dialéctica distinto. Por esto, la respuesta devuelta ante una consulta es sólo la ‘punta del iceberg’ de un proceso que incluye la exploración del conjunto de árboles de dialéctica que soporta dicha respuesta. Por lo tanto, para entender realmente por qué una consulta tiene una respuesta en particular, es esencial considerar qué argumentos han sido generados y qué conexiones existen entre ellos. Una δ-Explanation para una DeLP-query Q es la unión de los árboles de dialéctica construibles a partir de cada argumento para Q y para el complemento de Q. Ejemplo 1 (DeLP-query) Consideremos un programa lógico rebatible (Π1,∆1): Π1 = {q, t} ∆1 = { (r –≺q), (∼r –≺q, s), (r –≺s), (∼r –≺t) } desde el cual se pueden construir los siguientes argumentos: 〈R1,∼r〉 = 〈{∼r –≺t},∼r〉 〈R2, r〉 = 〈{r –≺q}, r〉 A partir de este programa la respuesta para la DeLP-query r es UNDECIDED, cuya δ-Explanation puede verse en la Figura 1. Allí puede verse que ambos argumentos están en una situación de bloqueo; es decir, ninguno bloquea al otro. R2  r U R1D r R1 r U R2D  r Figura 1: δ-Explanation para la consulta r del Ejemplo 1 A continuación, el Ejemplo 2 muestra cómo la introducción de un único hecho en el programa del Ejemplo 1 no sólo modifica la respuesta para la consulta r, sino que hace una diferencia significativa en la explicación. Es en estos casos en los que recurrir a las δ-Explanations se torna necesario para el completo entendimiento de lo que ocurre dentro del sistema. Ejemplo 2 (Extiende al Ej. 1) Consideremos el DeLP-program (Π1 ∪ {s},∆1). Si realizamos nuevamente una DeLP-query por r, obtenemos NO como respuesta, y la δ-Explanation es la mostrada en la Figura 2. Esta explicación tiene dos árboles más que la mostrada en el ejemplo anterior, debido a dos nuevos argumentos que pueden extraerse del programa: 〈R3, r〉 = 〈{r –≺s}, r〉 〈R4,∼r〉 = 〈{∼r –≺q, s},∼r〉 R1 R2 R4  r  r U U R4  r U D R3 D rr R4 U  rR2 R1 r D U R4 U  rrrr R3 R1 r D U R4 U  rrrr Figura 2: δ-Explanation para el Ejemplo 2 A diferencia de las explicaciones dialécticas para DeLP-queries, las δ-Explanations para las schematic queries tienen que lidiar con las diferentes instanciaciones posibles de la consulta dada. Por esto, una δ-Explanation generalizada para una consulta esquemática Q es la unión de todas las δ-Explanations para cada instancia de Q. A continuación, vamos a ilustrar este concepto con un ejemplo. Ejemplo 3 Consideremos el siguiente DeLP-program: Π =    (bird(X)← chicken(X)) chicken(little) chicken(tina) bird(rob) scared(tina)    ∆ =    flies(X) –≺bird(X) flies(X) –≺chicken(X), scared(X) ∼flies(X) –≺chicken(X)    Supongamos que queremos saber, a partir de este programa, si se puede garantizar que algún individuo no vuela. Al consultar por ∼flies(X) la respuesta es YES, debido a que hay una instancia garantizada para esta consulta: ∼flies(little). El argumento que soporta esta garantía es: 〈B2,∼flies(little)〉 = 〈{∼flies(little) –≺chicken(little)},∼flies(little)〉, el cual derrota a: 〈B1, f lies(little)〉 = 〈{flies(little) –≺bird(little)}, f lies(little)〉, por ser más directo. Los árboles de dialéctica que componen la explicación generalizada se muestran en la Figura 3. Esta explicación también muestra que la instancia ∼flies(tina) no está garantizada. Por otro lado, veamos que la respuesta para la schematic query opuesta (i.e., flies(X)) es también YES. Por supuesto, el conjunto de instancias garantizadas es distinto esta vez: flies(tina) y flies(rob). Los argumentos que soportan estas garantías pueden verse en la Figura 3, y son: 〈A1, f lies(tina)〉 = 〈{flies(tina) –≺bird(tina)}, f lies(tina)〉 〈A2,∼flies(tina)〉 = 〈{∼flies(tina) –≺chicken(tina)},∼flies(tina)〉 〈A3, f lies(tina)〉 = 〈{flies(tina) –≺chicken(tina), scared(tina)}, f lies(tina)〉 〈C1, f lies(rob)〉 = 〈{flies(rob) –≺bird(rob)}, f lies(rob)〉 La δ-Explanation generalizada para flies(X) es la misma que aquella para ∼flies(X), dado que las explicaciones para un literal también incluyen el análisis para el complemento del mismo. B 2  flies(little) U C 1 flies( rob) UA 2 A 3  flies(tina) flies(tina) U DA 1 A 2 A 3  flies(tina) flies(tina) flies(tina) U U D A 3 flies(tina) U B 2  flies(little) U B 1 flies(little) D Figura 3: δ-Explanation generalizada para ∼flies(X) Las consultas esquemáticas nos dan la posibilidad de hacer preguntas más generales que las DeLPqueries. Con ellas no estamos preguntando si se puede creer en una dada pieza de conocimiento, sino que estamos preguntando si existe una instancia de la consulta (relacionada a un individuo) que puede ser garantizada por el sistema. Esto puede llevar a un razonamiento más profundo, ya que se puede plantear una consulta, obtener las instancias garantizadas y continuar con la discusión con esos individuos. Es decir, la discusión puede ser focalizada. 3. Trabajo Relacionado En el sitio web del proyecto ASPIC [2] se presenta una demostración de un engine argumentativo basado en el formalismo de Gerard Vreeswijk. En ella se brindan varios casos de ejemplo, dando la posibilidad de ejecutar consultas sobre esas bases de conocimiento, e incluso editarlas. También se puede crear una base de conocimiento nueva, y realizar consultas sobre ella. Es interesante la posibilidad de visualizar, mediante un grafo, el proceso argumentativo provocado por la consulta. Esto puede tomarse como una explicación para la respuesta dada. Los argumentos y las relaciones de ataque entre ellos se muestran de acuerdo al Argument Interchange Format (AIF) [1], y los nodos están adecuadamente coloreados para representar argumentos derrotados (rojo) y no derrotados (verde). Sin embargo, la visualización resulta confusa, ya que hay demasiados elementos en pantalla. Existen estilos de flecha de distintos colores utilizados para representar rebut, undercut, etc. y pueden hacerse muy difíciles de seguir, sobre todo por su longitud. Además, las estructuras internas de los argumentos no son simples y se muestran desde un principio en completo detalle. En cambio, la representación dada por nuestro sistema es estructuralmente más simple, ya que se trata de árboles, y los nodos (i.e., argumentos) se muestran en una versión simplificada, con la posibilidad de ver su contenido mediante un tooltip. Uno o varios nodos pueden ser expandidos y contraídos para revelar la derivación lógica que hizo a la construcción del argumento. También se puede hacer zoom in/out (útil en caso de árboles muy grandes) y un mapa de referencia facilita la ubicación del foco de la ventana de visualización dentro de la pantalla. Por otra parte, en [2] se brinda otra opción para obtener una explicación de la respuesta dada por el engine: una traza que muestra no sólo la construcción de argumentos, sino que también describe interferencias y defensa entre ellos y el status de cada uno: PRO o CON, es decir, a favor o en contra del argumento que soporta la consulta, respectivamente. Se detalla el grado de fortaleza de la interferencia, es decir, si un argumento logró derrotar a otro. No obstante, la traza tiene debilidades similares al grafo en cuanto a visualización: es compleja desde un principio, y no brinda interacción alguna, i.e., el usuario no puede contraer/expandir secciones de la traza. El detalle con el que se muestra la traza parece un tanto excesivo, lo cual afecta a la usabilidad. Nuestro sistema no cuenta con este tipo de facilidades para trazar el proceso argumentativo. 4. Conclusiones y Trabajo Futuro Esta nueva línea de trabajo en el área de argumentación está dedicada a estudiar una parte poco explorada: la de explicaciones en sistemas argumentativos, plasmando los resultados en el sistema DeLP. Este estudio abre nuevas posibilidades para implementar, por ejemplo, sistemas expertos basados en DeLP. La idea de este trabajo nació como una necesidad al hacer testeo y depurado de programas lógicos rebatibles, ya que se torna realmente difícil seguir la dinámica argumentativa del sistema. Colateralmente, esta línea abarca el análisis e implementación de nuevos sistemas de visualización que faciliten la tarea del usuario en el uso de las explicaciones. Este tipo de herramientas se torna necesario debido a que el número y tamaño de los árboles de dialéctica generados por una consulta pueden llegar a ser muy incómodos de manejar. Por esto, contar con la posibilidad de visualizar los árboles con las facilidades que mencionamos a lo largo del artículo hace que las explicaciones dialécticas cobren mayor sentido. Actualmente, existe un prototipo del sistema DeLP que ofrece una visualización interactiva de explicaciones dialécticas. Como próximo paso de investigación, nos dirigimos a la inclusión de nueva información dentro de las explicaciones.	﻿Explicaciones , Dialéctica	es	20305
23	Extensiones del sistema de búsqueda de respuesta AliQAn	﻿ Este trabajo describe las extensiones del sistema AliQAn para el español en dominio abierto. Presenta al sistema Cross-lingual BRILI y un mecanismo de inferencia aplicado al Sistema de Búsqueda de Respuestas monolingual. 1. INTRODUCTION El Procesamiento del Lenguaje Natural (PLN) es una parte esencial de la Inteligencia Artificial, el cual investiga y formula mecanismos computacionales que permiten el desarrollo de sistemas capaces de comprender el conocimiento expresado en textos de un lenguaje dado. La gran cantidad de información digital disponible ha impulsado la investigación en sistemas de información textual que faciliten la localización, acceso y tratamiento de toda esta ingente cantidad de datos. Aunque las investigaciones avanzan en buena dirección aún no existe ningún sistema operacional que localicen la información requerida, procese, integre y genere una respuesta acorde a los requerimientos expresados por el usuario en sus preguntas. Inicialmente, la comunidad científica concentró sus esfuerzos sistemas más fácilmente abordables como la Recuperación de Información (RI, Information Retrieval) y la Extracción de Información (EI, Information Extraction). Estas investigaciones facilitaron el tratamiento de grandes cantidades de información. Sin embargo, las características que definieron estas líneas de investigación presentaban serios inconvenientes a la hora de afrontar la obtención de respuestas concretas a preguntas muy precisas formuladas por los usuarios. Todos esto ocasionó un creciente interés en sistemas que afrontaran con éxito la tarea de localizar respuestas concretas en grandes volúmenes de información, dejando la puerta abierta a la aparición de nuevos campos de investigación tales como Búsqueda de Respuestas (BR) -Question Answering (QA)-. La creciente información existente en diferentes lenguas requiere, además, sistemas que permitan recuperar o extraer información solicitada en el idioma no sólo de origen (es decir, en el que *This research has been partially funded by the Spanish Government under project CICyT number TIN2006-15265C06-01 and by the Valencia Government under project number GV06-161, and by the University of Comahue under the project 04/E062. se formula la pregunta) sino también en el idioma destino (es decir, en el que está escrita la pregunta). La campaña internacional CLEF (Cross-Language Evaluation Forum) se centra en el desarrollo de infraestructura necesaria para la experimentación y evaluación de sistemas de recuperación de información que trabajen sobre las lenguas europeas en contextos monolingües y translingües, y en la creación de conjuntos de datos reutilizables por los sistemas desarrollados. 2. MARCO DE TRABAJO La actividad investigadora inicia con el desarrollo del sistema de búsqueda de respuestas (AliQAn) para el idioma español basado en patrones, el cual ha participado en las competencias del CLEF 2005 [6]. Se han realizadas mejoras a nivel semántico mediante estrategias de desambiguación en la selección en los sentidos de las palabras [2, 3]. En un contexto Cross-lingual, las consultas son formuladas en un lenguaje diferente al de los documentos, lo cual incrementa la dificultad. Sin embargo, los sistemas multilingües es un tema de gran importancia para el futuro de la RI por la naturaleza multilingüe de la información disponible. La tarea de BR cross-lingual fue introducida en las competencias CLEF en el año 2003 por primera vez. Así, en las compentencias CLEF 2006 [1], tanto AliQAn como el sistema BRILI han participado en dicha competencia. AliQAn para la tarea monolingual y el sistema BRILI es presentado para llevar a cabo la tarea Cross-lingual, es decir, preguntas en inglés y texto en castellano. Ambos sistemas son totalmente automáticos y basados en patrones sintácticos. Si se caracterizan los sistemas de BR presentados en competencias como el CLEF según el nivel de recursos de PLN utilizado, aquéllos que llegan hasta un nivel sintáctico o a lo sumo un nivel semántico superficial mediante la utilización de sinonimia, hiperonimia entre otras relaciones similares, no superan cierto rango de precisión (a lo sumo un 45 % de efectividad aproximadamente). Los sistemas que superan en gran medida dicho valor son debido a la utilización de técnicas más complejas mediante la utilización de fuentes de conocimientos. De lo expuesto anteriormente, es posible concluir que la efectividad de los sistemas es relativamente baja, por lo que aún queda mucho trabajo por hacer. Se ha empezado a desarrollar una herramienta robusta, capaz de inferir automáticamente en dominios abiertos [4, 5], la cual podrá ser integrada en distintas aplicaciones de PLN como BR, implicación textual entre otras. 2.1. BRILI Considerando la variedad de idiomas en los que los textos pueden estar escritos, el sistema BRILI (español, inglés) reduce el uso de MT evitando el efecto negativo que causa esta clase de estrategias en sistemas de BR Cross-lingual, por medio del uso del módulo ILI de EuroWordNet. A su vez, dos mejoras que tratan este efecto negativo son incluidos: BRILI considera más de una traducción por palabra por medio del uso de diferentes synsets de cada palabra en el módulo de ILI de WordNet. A diferencia de los sistemas de BR bilingües, el análisis de la preguntas es realizado en el lenguaje original sin el uso de la traducción. De esta manera se logra mejoras de un 19.12 % en relación a las MT. 2.2. Mecanismo de Inferencia aplicado sobre AliQAn Como se mencionó anteriormente los sistemas que utilizan recursos de PLN hasta un nivel sintáctico no superan un cierto rango de precisión de efectividad. Una nueva generación de sistemas ha comenzado que dan un paso más allá de estos tipos de sistemas. La nueva tendencia en los sistemas de BR tienden a incorporar más semántica en el proceso de comprensión de los textos mediante la utilización de técnicas más complejas que utilizan fuentes de conocimientos externas. No existen sistemas que utilicen Formas Lógicas (FL) para el idioma castellano y por lo tanto no existen recursos factibles de ser usados para ayudar al proceso de inferencia. Es conocido que el idioma inglés, es un idioma en el que la mayoría de recursos se encuentran disponibles. Teniendo en cuenta esto, se ha decidido optar por una representación que sea fácil adaptar a la representación resultante de la traducción al castellano de algunos de estos recursos, por lo que nuestra representación se ha basado en los trabajos de Moldovan et. al[7]. Las transformación codifica las relaciones sintácticas (sintagmas nominales, verbo, sintagmas preposicionales y adverbiales). En una teoría formal encontramos un conjunto de fórmulas bien formadas, un subconjunto de estas que son los axiomas y un conjunto de reglas de inferencias. Existen diversas clases de axiomas. Entre ellos podemos mencionar los axiomas generados automáticamente (relaciones de sinonimia, hiperonimia, etc.) y axiomas generados manualmente (axiomas representando relaciones linguiísticas). De esta manera, se ha logrado mejorar la precisión de preguntas cuya respuesta esperada es de tipo económico en un 75 %. 3. CONCLUSIONES Y TRABAJOS FUTUROS Como se ha comentado anteriormente, el desarrollo de estos sistemas aún están en sus albores y resta mucho camino por recorrer para que realmente sea práctico, es decir, hacer que los sistemas sean útiles para los usurarios requiere que se tenga confianza plena en sus resultados, algo que no ocurre dada a su exactitud actual. Por ello, es necesario un estudio minucioso de las técnicas utilizadas por los sistemas existentes y también de las técnicas factibles de ser usadas y que aún no lo han sido. Es un campo de acción en el cual existen una vasta cantidad de tareas por realizar, con lo cual es importante y sobre todo necesario sumergirse en esta labor haciendo posible un salto en la mejora de la precisión de los sistemas de BR. En el caso de los sistemas multilingues como BRILI, aún resta reducir el efecto de traducciones incorrectas de nombres propios realizando un reconocimiento de entidades nombradas (Name Entity Recognition, en su denominación inglesa) para detectar posibles nombres de personas, los cuales resultarán en la no traducción de los mismos. Además, el sistema será escalado para responder preguntas en inglés, español y catalán a partir de documentos en los mismos tres idiomas anteriormente mencionados. Además, un algoritmo de desambiguación de sentidos será aplicado a las preguntas para ordenar y pesar los diferentes sentidos de las palabras. Por otra parte en los sistemas monolingues se puede afirmar que la incorporación de mecanismos de inferencias y razonamiento en textos resultan en un salto en la precisión de AliQAn y de otros sistemas de BR en dominio abierto. Aunque todavía queda mucho trabajo por hacer para que sean realmente eficaces o que logren evolucionar a aquellos en dominios cerrados. Se ha desarrollado una herramienta de razonamiento para el idioma español, idioma en el cual aún no se han desarrollado este tipo de herramientas y del cual existen pocos recursos capaces de ser usados por un método de inferencia. Una tarea que no resulta fácil será el estudio y elaboración de herramientas para ser utilizadas como recurso de conocimientos. Estas herramientas podrán ser resultado de la traducción de herramientas disponibles en otro idioma, de la utilización ontologías a partir de corpus, entre otros medios. Como se ha demostrado en los sistemas que ya están utilizando inferencia en sus sistemas, la adquisición de tales recursos es fundamental en la mejora de la precisión de los sistemas de BR. El idioma español, a diferencia del inglés por ejemplo, es muy libre en la generación de sentencias válidas. Esto produce que una idea sea expresada en un número mayor de formas correctas. Esto juega negativamente en la automatización de un sistema de PLN. Sin embargo, es posible compensar esta característica, entre otras cosas mediante la incorporación de más axiomas que representan conocimiento externo válido para ser utilizado en el mecanismo de inferencia y así poder independizarse de la representación textual del texto. Es importante destacar que esta área de estudio es nueva y es necesaria la creación de recursos para el idioma español, afianzando nuestra lengua en este campo de investigación. Resumiendo, el desarrollo de estos sistemas están enfocados en la mejora en la precisión de los mismos, y en la incorporación de conocimiento en las fases requeridas para incrementar el rendimiento de nuestros sistemas.	﻿Sistema de Búsqueda , Extensiones , Respuesta AliQAn	es	20307
24	Investigación en sistemas operativos	﻿ Este artículo presenta los temas que se están comenzando a explorar en el Laboratorio de Investigación en Nuevas Tecnologías Informáticas como parte de una línea de investigación en sistemas operativos y expone los primeros resultados obtenidos. La rápida evolución en las tecnologías de hardware, las redes de datos y el diseño del software de usuario contrasta con el hecho de que el diseño del kernel y los mecanismos que implementan la protección y virtualización de los recursos, en los sistemas operativos actuales, no hayan tenido avances significativos en las últimas tres décadas. Esta disparidad se ve reflejada en un overhead significativo de procesamiento, conocido como intrusión del sistema operativo. Esta línea de investigación se enfoca en identificar, proponer y evaluar nuevas características relacionadas con la performance y escalabilidad del kernel Linux, teniendo en cuenta los nuevos desafíos que presentan las arquitecturas multicore y la computación centrada en las redes a los sistemas operativos de la próxima generación. 1. Introducción Asumiendo un futuro tecnológico cercano donde la computación centrada en las redes y las arquitecturas paralelas serán características fundamentales, se hace evidente la necesidad de replantear el diseño del software de base teniendo en cuenta los nuevos paradigmas de procesamiento y características del hardware. Por un lado, se espera un gran desarrollo tecnológico basado en las distintas formas de almacenamiento y procesamiento distribuido, soportado por el incesante avance en los estándares y tecnologías de redes de datos. Por otro lado, las arquitecturas con múltiples núcleos de procesamiento se están posicionando como el estándar actual para la mayoría de las configuraciones de hardware. En este escenario, se están planteando nuevas características (y adaptando antiguas ideas) que los sistemas operativos deberán incorporar en el corto y mediano plazo. Entre otras, se pueden mencionar el diseño de kernels asimétricos y activos para explotar al máximo el paralelismo en las nuevas arquitecturas, el desarrollo de una capa de llamadas al sistema completamente asincrónicas y basadas en memoria compartida para optimizar las operaciones de entrada/salida que cada vez son más frecuentes, las nuevas tecnologías de virtualización, y las funcionalidades que incorporan soporte para computación Grid en el kernel. A continuación se describen los temas que forman parte de la línea de investigación en sistemas operativos que se está desarrollando en el Laboratorio de Investigación en Nuevas Tecnologías Informáticas. 2. Línea de Investigación 2.1. Un kernel asimétrico y activo Generalmente, los sistemas operativos han utilizado a las máquinas SMP de manera simétrica, intentando balancear la carga entre todos los elementos de procesamiento disponibles. En los sistemas ASMP, los procesadores se especializan para tareas específicas. Por ejemplo, es posible que una CPU responda a todas las interrupciones del hardware, mientras el resto del trabajo en el sistema sea distribuido equitativamente entre el resto de las CPUs. O puede restringirse la ejecución del código en modo kernel a sólo un procesador (a uno específico o a un procesador a la vez), mientras el código en espacio de usuario se ejecuta en el resto de las CPUs. Yendo más allá, la idea de un kernel activo es una evolución del modelo de kernel vertical [4], y propone dedicar CPUs o cores a realizar exclusivamente actividades del kernel. Al ser un agente activo, es posible que el kernel se comunique con el hardware y el espacio de usuario utilizando mecanismos alternativos, menos costosos que los actuales (interrupciones por hardware y software). Como se detalla en la sección 3, nuestro trabajo se centra en el estudio de las posibilidades que ofrecen estos conceptos en el tratamiento de la entrada/salida de red. Sin embargo, se han identificado posibles aplicaciones en otros dominios (virtualización, tiempo real, etc). 2.2. Llamadas al sistema asincrónicas y basadas en memoria compartida La API que el kernel ofrece al espacio de usuario define la base de los servicios y operaciones que el sistema operativo provee a los procesos. En particular, define la sintaxis y semántica de las operaciones exportadas por el kernel, conocidas normalmente como llamadas al sistema. Los buffers para entrada/salida de datos aparecen en esta definición como argumentos a las operaciones. La semántica de pasaje de parámetros puede tener un impacto significativo en la eficiencia de la transferencia de datos entre el kernel y la aplicación. Se ha manifestado previamente [2] el costo que implica el modelo tradicional de llamadas al sistema en sistemas operativos que se ejecutan en máquinas donde se realizan principalmente tareas de entrada/salida. La línea de trabajo se centra en ideas alternativas orientadas a incrementar la eficiencia del kernel Linux en el manejo de entrada/salida, relacionadas principalmente con la posibilidad de realizar operaciones vectorizadas (utilizando más de un buffer de datos por invocación, lo que favorece las operaciones scatter-gather) y utilizando memoria compartida entre el kernel y el espacio de usuario, para evitar las copias físicas en memoria de un dominio de protección al otro. Además, para lograr mayor escalabilidad, evitando el overhead que implica el scheduling de grandes cantidades de threads [6], se están proponiendo APIs que poseen una semántica de entrada/salida asincrónica para operaciones de sockets y archivos. Las aplicaciones realizan operaciones sin ser bloqueadas y sin verificar previamente el estado del descriptor de socket o archivo, luego reciben eventos que señalizan la terminación de una operación previamente invocada. Este modelo permite que múltiples operaciones entrada/salida estén realizándose concurrentemente sin que el proceso o thread sea bloqueado por el sistema operativo. 2.3. Nuevas ténicas de virtualización Recientemente, dos nuevas técnicas de virtualización han logrado un nivel de performance sorprendente con respecto a la performance en la ejecución nativa. La paravirtualización y la virtualización asistida por hardware, por separado o combinadas (virtualización cooperativa), se posicionan como una nueva capa de abstracción estándar que revolucionará la estructura de los centros de cómputos gracias a la consolidación del hardware, un mayor nivel de seguridad y redundancia, la reducción en el uso de energía, la migración de hosts virtuales sin interrumpir los servicios, etc. Hasta ahora, la solución basada en un hypervisor era la manera estándar de implementar las técnicas mencionadas de virtualización. Sin embargo, en este caso es necesario duplicar ciertas funcionalidades que ya existen en un kernel. Por lo tanto, se están planteando ciertas ventajas al incorporar capacidades de virtualización al propio kernel. La línea de trabajo se enfoca en comparar las técnicas alternativas, evaluar la integración de esta funcionalidad en el kernel y detectar las opciones que ofrecen las arquitecturas multicore a éstas tecnologías. 3. Un prototipo y resultados preliminares Como se manifestó en un trabajo previo [2], los costos asociados a ciertos mecanismos del sistema operativo, agrupados bajo el nombre de intrusión del sistema operativo, pueden llegar a ser significativos. Esta sección presenta una modificación al kernel Linux 2.6 basada en la idea de un kernel activo, que reduce los costos relacionados con el manejo de interrupciones y la sincronización del subsistema de red en una máquina SMP. Además se exponen algunas mediciones realizadas, comparando una configuración del kernel estándar y un kernel modificado. Con esta modificación se logra desacoplar el procesamiento de las aplicaciones de usuario del procesamiento de red del kernel. El procesamiento TCP/IP es aislado de las CPUs que ejecutan el sistema operativo y los procesos de usuario (CPUs-HOST), y se delega a un procesador o subconjunto de procesadores dedicados a las comunicaciones (CPUs-NET). Las CPUs-HOST deben comunicarse con las CPUs-NET usando un medio de comunicación de bajo costo y no intrusivo, como la memoria compartida. Una CPU-NET no necesita ser multiplexada entre otras aplicaciones y/o funciones del kernel, con lo cual puede ser optimizada para lograr la mayor eficiencia posible en su función. Por ejemplo, es posible evitar el costo de las interrupciones de hardware si las CPUsNET consultan, con una frecuencia suficiente, el estado de las NICs (Network Interface Controller) para saber si necesitan ser atendidas. También se reduce la contención por datos compartidos y el costo de sincronización dado que el procesamiento de red ya no se realiza de manera asincrónica y descontrolada entre todas las CPUs de la máquina (el kernel Linux sigue el paradigma de paralelismo por mensaje [5]), sino que se ejecuta sincrónicamente en un subconjunto reducido de CPUs (posiblemente sólo una CPU, dependiendo del tráfico a procesar). En las placas de red se desactivan las interrupciones y se verifica el estado por polling. El resto de las interrupciones del sistema son enrutadas a la CPU-HOST usando los mecanismos de enrutamiento ofrecidos por el Controlador Programable Avanzado de Interrupciones de E/S (I/O APIC) [1]. De esta forma, la CPU-NET no debe manejar eventos asincrónicos no relacionados con la red. Sin embargo, la interrupción del timer del APIC local se mantiene habilitada en la CPU-NET, ya que se utiliza para implementar los timers por software del kernel Linux (muy utilizados por TCP), pero además deja abierta la posibilidad de disparar eventos periódicos, como pueden ser evaluar la situación y realizar una reconfiguración automática del subconjunto de CPUs-NET. Se modificó el kernel Linux 2.6.17.8 para implementar la arquitectura separada de CPUHOST y CPU-NET. Las pruebas se realizaron sobre máquinas con dos procesadores AMD Opteron de 64 bits, Serie 200 DE 2 GHz (denominadas CPU0 y CPU1). En la prueba A, se configuró el kernel para lograr afinidad total [3] del procesamiento de dos de las NICs a la CPU0 y el procesamiento de la tercer NIC se asoció a la CPU1. Esta es la configuración que logró mayor eficiencia sin modificar el código fuente. En la prueba B se utilizó el prototipo implementado. En ambas pruebas se logró un rendimiento de red cercano a 1,5 Gbps consumiendo aproximadamente el 70 % de la capacidad de cómputo de la máquina. El cuadro 1 muestra los cambios de contexto (cswch/s) e interrupciones (intr/s) por segundo durante ambas pruebas. Los cuadros 2 y 3 presentan los resultados del perfilamiento estadístico del kernel durante cada prueba, es decir, indican las funciones del kernel que utilizaron la CPU por más tiempo. Cuadro 1: Comparación de pruebas A y B medidas\pruebas A B cswch/s 157186,17 99275,49 intr/s 131363 0 Si se comparan las pruebas A y B, se observa que la segunda prueba reduce en más del 30 % la cantidad de cambios de contexto por segundo y lleva a cero la cantidad de interrupciones de hardware. Los costos de sincronización (la función spin lock()) bajan notablemente y el subsistema de interrupciones (las Cuadro 2: Perfilamiento del kernel durante la prueba A muestras % símbolo 154270 13.1928 copy to user ll 153381 13.1167 spin lock 94636 8.0930 handle IRQ event 58272 4.9833 do IRQ 45311 3.8749 schedule 39026 3.3374 tcp v4 rcv 30196 2.5823 qdisc restart 26420 2.2594 switch to 24781 2.1192 default idle funciones handle IRQ event() y do IRQ()) ya no genera costos importantes. Cuadro 3: Perfilamiento del kernel durante la prueba B muestras % símbolo 148053 10.3428 copy to user ll 120776 8.4372 net rx action 95700 6.6855 desencolar 68739 4.8020 qdisc restart 65176 4.5531 tcp v4 rcv 59659 4.1677 schedule 50669 3.5397 try to wake up 48140 3.3630 tcp rcv established 43827 3.0617 kfree 36767 2.5685 tcp select window 30361 2.1210 local bh enable 28993 2.0254 eth type trans Esta prueba de concepto hace suponer que es posible reducir la intrusión del sistema operativo en el procesamiento de red. En aquellas máquinas cuya principal función es el movimiento de datos, como es el caso con un Storage Element en una infraestructura Grid, puede ser beneficial dedicar una CPU o core de manera exclusiva al procesamiento de red. Actualmente se está adaptando esta idea a una arquitectura multicore y se están realizando varias optimizaciones a la implementación que resultarán en una mejora importante en la eficiencia de procesamiento.	﻿procesamiento de red , kernel asimétrico , resultados preliminares	es	20346
25	Prototipo para la Estabilización Digital de Imágenes	﻿   Se presenta un proyecto para la estabilización de videos digitales en tiempo real.  Diseñado para remover aquellos movimientos no deseados de las imágenes, y así lograr  una secuencia de video estabilizada, bajo restricciones generales de una aplicación de  tiempo real.      1.- Introducción    La captura de imágenes desde dispositivos en movimiento genera problemas de  estabilidad que dificultan la identificación y el seguimiento de patrones, en diversas  aplicaciones de tiempo real.    Los  movimientos indeseados presentes en las imágenes se deben a vibraciones,  cambios de dirección y aceleraciones de los dispositivos en cuestión.     Los equipos de estabilización de videos son una herramienta práctica al momento de  disminuir estas alteraciones  indeseadas. Las aplicaciones que utilizan cámaras  estabilizadas son muy diversas: transmisiones de televisión, controles de seguridad de  grandes áreas, sistemas de patrullajes, sistemas de antenas satelitales, astronomía, etc.    Entre las tecnologías actualmente presentes, encontramos las plataformas giroestabilizadas, las cuales  predicen y contrarrestan los movimientos que perturban la  imagen.    Las cámaras giro-estabilizadas para realizar filmaciones de alta calidad se componen de  cinco módulos:   • Acelerómetro: permite medir las aceleraciones que sufre el sistema considerando  el plano perpendicular al foco de la cámara.  • Plataforma mecánica: permite el montaje de una cámara sobre su superficie e  integra el sistema de servos electromecánicos.  • Actuadores: permiten mover la plataforma mecánica para evitar cambiar el plano  focal.  • Visualizador: permite ver la imagen de la cámara y que el operador realice su  control.  • Sistema de control-filtro-cálculo: es el cerebro del sistema, trabaja evitando que  los vibraciones del vehículo se transmitan a la cámara    En algunos casos, el diseño del sistema incluye un módulo adicional que permite  realizar los ajustes finos de la imagen capturada de forma digital. La tarea realizada por  este componente podría resumirse en dos pasos: la estimación del movimiento  indeseado y la corrección del mismo a través de la secuencia de imágenes.    La estabilización digital de videos es un proceso donde los movimientos de una imagen  son removidos para generar imágenes compensadas. El proceso genéricamente se podría  describir con cuatro pasos:  (a) Estimación del movimiento local: Los sistemas de estimación de movimiento  local generan vectores de movimiento local analizando sub-imágenes en  distintas posiciones del frame.  (b) Estimación del movimiento global: La estimación global determina el  movimiento global analizando los vectores resultantes de la etapa anterior. El  movimiento global es pasado a un filtro para permitir aquellos movimientos de  la cámara intencionales, mientras son removidos los movimientos de alta  frecuencia de la cámara no deseados.  (c) Filtrado de movimiento.  (d) Compensación del movimiento.    La estimación y corrección de movimiento puede ser realizada mediante  aproximaciones espacio-temporales o aproximaciones de matching entre regiones.  Dadas estas dos alternativas, la taxonomía estudiada que se plantea es la siguiente:      A) Aproximaciones de matching por regiones:    1. Bit-plane matching: Esta técnica calcula la estimación de movimiento usando  planos de un bit, los cuales son extraídos de la secuencia de videos. Este proceso  de estimación puede ser realizado usando sólo funciones boléanos, lo cual  reduce significativamente la complejidad computacional. Además se plantea un  esquema de corrección del movimiento basado en la mediana lo suficientemente  robusto en condiciones de objetos en movimiento.     2. Seguimiento de características: Esta técnica aplica una transformación,  denominada warping, a la imagen que compensa los movimientos de la cámara.  Se plantea un modelo de seguimiento de puntos característicos a lo largo de la  secuencia de imágenes con el objetivo de estimar la transformación que  permitirá la compensación necesitada.    3. Basado en SWT (Stationary Wavelet Transform): En este modelo se estima  el movimiento translacional, proyectando la descomposición de la onda  estacionaria, en el los ejes horizontal y vertical.    B) Aproximaciones espacio-temporales:    1. Optical Flow Estimation: Este esquema utiliza una técnica de flujo óptico,  basado en el brillo de los píxeles, para estimar los vectores de movimiento local,  obteniendo la velocidad de cada píxel en el frame actual. Estos vectores de  velocidad son utilizados para determinar el movimiento rotacional y  transnacional.    2. Linear Region Matching: Esta técnica realiza la estabilización de la imagen a  través de la correspondencia de puntos a lo largo de la secuencia del video. Se  selecciona un conjunto de puntos del primer frame, y se busca una  correspondencia de líneas en el siguiente frame, con el objetivo de estimar los  parámetros de movimiento.      2.- Análisis    Del estudio de la taxonomía presentada se analizan las distintas transformaciones  utilizadas en las diversas técnicas, como son traslación, rotación, warping, affine  (combinación de las 3 anteriores). Con el objetivo de determinar ventajas y desventajas  que permitan la selección de las técnicas que mejor se adaptan al objetivo planteado:  que funcione en tiempo real.     Se analizan los algoritmos matemáticos para su implementación software y  potencialmente hardware, se descartan las técnicas de alto costo computacional debido a  la imposibilidad de obtener modelos precisos para el seguimiento de características en  aplicaciones de tiempo real.     Se decide la utilización de operadores boléanos en la técnica de block matching, debido  a que su implementación permite algoritmos verdaderamente rápidos.      3.- Estado Actual del Proyecto    Se ha terminado el estudio del estado del arte del proyecto, y se ha comenzado el diseño  del sistema de estabilización. Se ha desarrollado un conjunto de algoritmos que  permiten realizar la predicción de los movimientos de un cuerpo, a partir del  movimiento analizado desde la imagen de video.    Se propone una implementación combinando varias técnicas que se espera mejore los  resultados obtenidos a partir de los algoritmos puros. La técnica a utilizar propone la  aproximación de block matching, planteando un modelo de corrección del movimiento  basado en mediana, de forma de encontrar la transformación más adecuada (warping)   con el objetivo de obtener alta calidad en los frames.    El diseño del sistema y los algoritmos de estabilización, permiten la estabilización  independiente de la plataforma en que se ejecute. Si bien en primer lugar se está  utilizando un prototipo software sobre simulador, se migrará a una plataforma software  que trabaja sobre un video desde disco, para luego migrar al uso de la cámara giroestabilizada real. Después de ese desarrollo, se materializará el diseño en hardware  basado en plataformas FPGA sobre el dispositivo electro-mecánico real.	﻿tiempo real , videos digitales , Estabilización Digital de Imágenes	es	20356
26	Comprensión de programas por inspección visual y animación	﻿ PCVIA (Program Comprehension by Visual Inspection and Animation) es un proyecto de investigación que estudia la construcción de métodos, técnicas y herramientas que ayuden al ingeniero del software en el análisis y comprensión de aplicaciones. Estos estudios tienen como objetivo contribuir en distintas actividades de la Ingeniería del Software como por ejemplo mantenimiento, reingeniería, ingeniería reversa, entre otras tantas aplicaciones. Para construir ambientes de comprensión de programas es necesario concebir herramientas que permitan extraer y visualizar información de los sistemas. Para lograr este objetivo es necesario analizar los métodos, técnicas, herramientas, etc. existentes con la finalidad de incrementar la funcionalidad de las mismas, o bien, proponer otras nuevas. En este artículo describimos un abordage para la construcción de herramientas de comprensión que se basa en la instrumentación del código fuente del sistema de estudio. Entre los objetivos de esta aproximación se encuentran la elaboración de estrategias de navegación y relación entre las distintas perspectivas de sistemas desarrolladas usando el paradigma imperativo. Por otra parte se planifica analizar la extensibilidad de las mismas a otros paradigmas como por ejemplo el orientado a objeto. Palabras Claves: Comprensión de Programas, Métodos, Técnicas, Herramientas. 1. Introducción El proyecto PCVIA tiene como principal objetivo estudiar, explorar e implementar técnicas y herramientas de comprensión de programas. La comprensión de programas se traduce en la habilidad de entender una pieza de código escrito en un lenguaje de alto nivel. Un programa no es mas que una secuencia de instrucciones que serán ejecutadas de forma de garantir una determinada funcionalidad. El lector de un programa consigue extraer el significado de un programa cuando comprende de que forma el código cumple con la tarea para la cual fue creado. El área de comprensión de programas es una de las más importantes de la Ingeniería del Software porque es necesaria para tareas de reutilización, inspección, manutención, migración y extensión de sistemas de software. Puede también ser utilizada en áreas como ingeniería reversa o enseñanza de lenguajes de programación. La tarea de comprensión de programas puede tener diferentes significados y puede ser vista desde diferentes perspectivas. El usuario puede estar interesado en como la computadora ejecuta las instrucciones con el objetivo de comprender el flujo de control y de datos, o puede querer verificar los efectos que la ejecución del programa tiene sobre el objeto que esta siendo controlado por el programa. Considerando estos dos niveles de abstracción, una herramienta versátil de inspección visual de código es crucial en la tarea de comprensión de programas. Existe un conjunto enorme de herramientas de comprensión de programas [4][5][7] construidas para diversos lenguajes que usan varios abordages. En el ámbito del proyecto PCVIA están siendo desarrolladas herramientas usando diferentes perspectivas. En este artículo se usa un abordaje dependiente del lenguaje, que permite la generación de visualizaciones [3] en varios niveles de abstracción y refuerza la importancia del mapeamiento entre las mismas. 2. Sistema de Comprension de Programas Normalmente, cuando el programador quiere entender un sistema necesita inspecionar diferentes aspectos del mismo. Para construir estas perspectivas es necesario extraer información desde los sistemas y representarla adecuadamente. La extracción de la información es importante porque es la base para construir diferentes vistas. Por otra parte, la visualización de la información es escencial para propósitos de comprensión. En las secciones siguientes describimos las vistas y las estratégias de extracción de información usadas en la construcción de herramientas de comprensión. 2.1. Vistas de un Sistema Una vista es una representación de la información de un sistema que facilita la comprensión de un aspecto del mismo. Cuando un programador esta comprendiendo un sistema la primera vista con la que se enfrenta es su código fuente. Esta vista es útil porque el programador esta familiarizado con los lenguajes de programación. Sin embargo, cuando el tamaño del sistema crece pierde claridad y otras prespectivas del sistema son necesarias. Un aspecto del sistema que se encuentra en un nivel de abstracción más alto consiste en visualizar las funciones del sistema y las relaciones existentes entre las mismas. Un ejemplo de esto es el Grafo de Funciones (GF). GF es un grafo donde el conjunto de nodos esta compuesto por las funciones del sistema de estudio y la relación entre ellas está dada por la comunicación de las funciones através de sus invocaciones. Normalmente, el grafo GF es una vista deseada por los programadores, sin embargo, de la misma forma que el código fuente, cuando el tamaño del sistema crece no presenta una ayuda a la comprensión. Como una alternativa al grafo GF el sistema puede ser visualizado a usando los módulos que lo componen. En este caso es posible definir un grafo que muestra la relación de comunicación entre ellos. Normalmente este grafo es conocido con el nombre de Grafo de Comunicaciónes de Módulos (GCM). Nuestros experimentos indican que GCM presenta una vista clara del sistema aun cuando el tamaño del mismo es grande. No obstante, al presentar un mayor grado de abstracción oculta detalles que pueden ser útiles en el proceso de comprensión. Las vistas descriptas hasta este momento pueden ser construidas usando la información estática del sistema. Sin embargo no siempre todas las funciones o módulos son usados cuando el sistema bajo estudio se ejecuta. Teniendo en cuenta este aspecto es importante recuperar información dinámica para visualizar, animar o describir sólo las componentes utilizadas por el sistema. Por ejemplo, podría ser útil presentar un subrafo de GF o GCM mostrando sólo las componentes utilizadas. Por otra parte también es posible visualizar el contenido los módulos objeto del sistema. Esta vista puede ser de importancia para el programador experto cuando desea modificar el código generado por el compilador. Finalmente es importante notar que las vistas permiten mostrar aspectos del sistema bajo estudio en distintos niveles de abstracción. 2.2. Relación entre las Diferentes Vistas de un Sistema Las vistas son importantes y ayudan en la comprensión de sistemas. Sin embargo, no es suficiente visualizarlas, es necesario posibilitar su navegación. Esto se debe a que normalmente el proceso de comprensión de programas implica visualizar aspectos de alto, medio y bajo nivel del sistema. Por ejemplo, puede ser importante estudiar cual es el módulo más importante del sistema, en ese caso GCM puede ayudar en esa tarea. Pero luego de identificar dicho módulo es interesante estudiar el GF correspondiente. Teniendo en cuenta esta observación construimos un prototipo de una arquitectura que permite la navegación entre las distintas vistas del sistema de estudio. Acontinuación describimos las componentes básicas que la arquitectura contiene actualmente. Sistema de Extracción de la Información: extrae información estática y dinámica de un sistema. Utiliza técnicas descriptas en [1][2][6][9] para lograr este objetivo. Repositorio de Información: almacena datos producidos por el Sistema de Extracción de la Información, como por ejemplo: módulos, funciones, tipos, datos, etc. Administrador de Visualizacion y Navegación: utiliza la información disponible en el repositorio de información para proporcionar navegación entre las distintas vistas. Visualizador Operacional : consta de un conjunto de módulos que implementan las distintas vistas. Es el sistema encargado de mostrar los objetos del dominio del programa Visualizador Comportamental : consite de la salida del sistema, en otras palabras el resultado. La relaciones entre las diferentes vistas es llevada a cabo por el Administrador de Visualización y Navegación. Esta componente recupera información desde el Repositorio de Información para lograr su objetivo. Podemos decir que todas las relaciones, con excepción de la vinculación entre las vistas operacional y comportamental, son logradas de esta manera. 2.2.1. Estrategia de Relación Operacional-Comportamental La estrategia de relación operacional-comportamental, denifinida por nuestro grupo de investigación, utiliza infromación dinámica y estática del sistema de estudio. Además se basa en la siguiente observación: La salida de un sistema esta compuesta de objetos del dominio del problema. Usualmente estos objetos son implementados por tipos de datos abstractos, en el caso de lenguajes imperativos, o por clases, en el caso de lenguajes orientados a objetos. Tanto los TDAs como las Clases tienen objetos de dato que almacenan su estado y un conjunto de operaciones que los manipulan. Entonces es posible describir cada objeto del dominio del problema utilizando los TDAs o clases que los implementan. Esta estrategia denominada BORS (Behavioral-Operational Relation Strategy) aplica los siguientes pasos para alcanzar su objetivo. 1. Detectar las funciones relacionadas con cada objeto del dominio del problema 2. Construir un árbol de ejecución de funciones usadas en tiempo de ejecución 3. Explicar las funciones encontradas en el paso 1 usando el árbol construido en el paso 2. El lector interesado puede encontrar en [8] una explicación detallada de la estrategia BORS. En la próxima sección presentaremos las técnicas de extracción de la información usadas para recolectar datos desde el sistema bajo estudio. 3. Métodos de Extracción de la Información Para la extracción de la información fue necesario construir un parser del lenguaje de programación utilizado por el sistema de estudio. Una vez realizado esta tarea se procedió a incorporar los atributos y acciones semánticas necesarias para extraer información estática del sistema. Para recuperar la información dinámica del sistema se instrumentó el código fuente con funciones de inspección y control. Las primeras fueron insertadas al inicio y en los puntos de retorno de cada función. Las segundas son utilizadas para controlar las iteraciones ya que dentro de ellas pueden haber invocaciones a funciones. Estas, en algunos casos, son redundantes porque son utilizadas para inicializar estructuras de datos. Con este esquema se pudo recuperar y controlar el flujo de ejecución de funciones. El lector interesado puede encontrar una explicación detallada de estas aproximaciones en [6][8]. Para la recuperación de datos se construyó una tabla de símbolos del lenguaje de programación (en este caso C) que posibilita recuperar información detallada de cada una de los objetos de datos del sistema. Esta característica permitirá en el futuro extender el esquema de instrumentación para inspeccionar datos. 4. Conclusión En este artículo presentamos el proyecto PCVIA y describimos las actividades actualmente realizadas. Presentamos características útiles de una herramienta de comprensión de programas. Describimos las componentes básicas de una arquitectura que responde a los requisistos de las herramientas basadas en vistas. Además, mencionamos que es importante que las herramientas de comprensión de programas permitan la navegación entre las distintas vistas que ellas proveen. Por otra parte, describimos un procedimiento denominado BORS para relacionar dos vistas muy importantes como lo son la operacional y la comportamental. Esta última técnica es uno de los resultados recientes y relevantes de nuestra investigación. Además de la estrategia BORS describimos sintéticamente distintas técnicas de instrumentación y extracción de la información que hemos desarrollado y aplicado con éxito. Es importante notar que estos procedimientos (los de instrumentación de código y extracción) no requieren intervención del usuario por ser totalmente automáticos. Los resultados obtenidos fueron satisfactorios en el sentido de que se lograron explorar algunos sistemas y relacionar distintas vistas con la información recuperada por nuestras técnicas de extracción de la información. Como trabajo futuro nos proponemos extender BORS con instrumentación de datos como así también proveer algún mecanismo de indentificación precisa de elementos del dominio del problema. Por otra parte, pretendemos construir un ambiente que permita decorar la salida del sistema con los objetos del dominio del programa.	﻿Comprensi´on de Programas , Métodos , Técnicas , Herramientas	es	20382
27	Ontologías en el proceso de descubrimiento de conocimiento en bases de datos	﻿    En este proyecto daremos las bases para la investigación y el desarrollo del proceso  de Descubrimiento de Conocimiento en Bases de Datos con Ontologías. La principal  motivación para la inclusión de ontologías en dicho proceso es la necesidad de  incluir el conocimiento previo en las sesiones de minería. Dicho conocimiento puede  ser provenir del proceso mismo o del dominio de aplicación involucrado.  Nuestro objetivo es el mejoramiento integral del proceso, a partir de un mejor  entendimiento del dominio de aplicación, de los resultados obtenidos en sesiones  previas y de la aplicación de la o las técnicas más convenientes de acuerdo a  problema a resolver.    1) Introducción  Descubrimiento de Conocimiento en Bases de Datos (KDD) se define como la extracción,  no trivial, de información previamente desconocida y potencialmente útil, en grandes colecciones  de datos (Fayyad et al, 1996). Puede ser considerado como una búsqueda de reglas interesantes,  patrones o excepciones en grandes colecciones de datos.  Es un área interdisciplinaria sustentada  por diversos campos, tales como: Estadística, Bases de Datos, Aprendizaje Automático, Inteligencia  Artificial, Teoría de la Información, Computación Paralela y Distribuida y Visualización entre  otros.  De acuerdo a la bibliografía (Fayyad et al., 1996; Han et al., 2001; Hernández Orallo et al,  2004) las técnicas más frecuentes pueden ser catalogadas en:  • Descriptivas: El objetivo de estos procedimientos es la búsqueda de la  caracterización o discriminación de un conjunto de datos. Las técnicas más  conocidas son: Agrupamiento o Clustering, Reglas de Asociación, Análisis de  Patrones Secuenciales, Análisis de Componentes Principales, Detección de  Desviación.   • Predictivas: El propósito de estos métodos es aprender una hipótesis la cual pueda  clasificar a nuevos individuos. Los  algoritmos principales son: Regresión y  Clasificación (Árboles de Decisión, Clasificación Bayesiana, Redes Neuronales,  Algoritmos Genéticos, Conjuntos y Lógica Difusa).   Cuando realizamos un proceso de Minería de datos, necesitamos tener en cuenta el  conocimiento previo; este puede derivar del proceso mismo(elección de variables, técnicas,  algoritmos, interpretación de resultados) o del dominio de aplicación.  Actualmente, en la mayor parte de proyectos de KDD, el conocimiento previo está sólo  presente implícitamente (en la mente del analista humano) o en la forma de documentación textual.  Inclusive en acercamientos intensivos al conocimiento como ILP (Induction Language  Programming), los conocimientos previos, a menudo, no son organizados alrededor de un modelo  conceptual gramaticalmente correcto. Esta práctica parece no hacer caso del último desarrollo en la  Ingeniería de Conocimiento, donde el conocimiento del dominio es típicamente definido por  ontologías formales.  En ambientes distribuidos, las ontologías son usadas para construir el servicio  semánticamente rico en descripciones. Técnicas para planificación, composición, edición,  razonando y el análisis sobre estas descripciones está siendo investigado y desplegado para resolver  la interoperabilidad semántica entre servicios(Canataro, et al, 2003).  Por lo expuesto, podemos observar que uno de los problemas más importantes y desafiantes  a ser investigado en Minería de Datos es, la definición del conocimiento previo. Nuestra  investigación se centrará en la utilización de Ontologías para la representación del conocimiento  previo, ya sea durante el proceso o para la representación del dominio de aplicación.    2) Desarrollo    El Descubrimiento de Conocimiento en Bases de Datos es un proceso exploratorio que  involucra la aplicación de varios procedimientos algorítmicos para la manipulación de datos,  construcción de modelos desde los datos y la manipulación de los mismos. El proceso de  Descubrimiento de Conocimiento (KD) (Fayyad, et al., 1996) es una de las nociones centrales del  campo de Descubrimiento de Conocimiento y Data Mining (KDD).   El proceso KD es digno de la mayor atención en la comunidad científica. Una razón es que  los procesos comprenden múltiples componentes algorítmicos, que interactúan en recorridos no  triviales. Todos los especialistas en Minería de Datos no se encuentran familiarizados con todo el  rango de componentes, y asimismo con el vasto espacio de diseño, de procesos posibles (Bernstein  et al., 2005).  No obstante, tanto novicios como especialistas en Minería de Datos, se encuentran aptos  para una utilización más abstracta o de un nivel mayor de generalización de las instancias de un  proceso KD. Se consideran herramientas que ayuden a los profesionales en Minería de Datos a  navegar por el espacio de procesos KD, de una manera más sistemática, y más eficiente.   En particular el proyecto de investigación se centraliza en un subconjunto de estados de los  procesos de KD (estos estados a su vez tienen múltiples componentes de algoritmos que pueden ser  aplicados). A este proceso le denominamos Minería de Datos, distinguido del proceso más extenso  de Descubrimiento de Conocimiento en Base de Datos. El proceso de KD se encuentra descrito por  Fayyad et al. (1996) y Chapman et al., (2000). Hay que poner énfasis en tres procesos de KD:  preproceso automático de datos, aplicación de algoritmos de inducción, y post-proceso automático  de modelos.  Se selecciona este conjunto de pasos, porque individualmente, se encuentran relativamente  bien comprendidos y pueden ser aplicados a una amplia variedad de conjunto de datos.   Figura 1 El Proceso de descubrimiento según Fayyad(KD)   DATOS SELECCION DATOS SELECCIONADOS PREPROCESO ALGORITMOSDE DM POST-PROCESO, INTERPRETACION Y VALIDACION DATOS PREPROCESADOS MODELO CONOCIMIENTO  Considerando la necesidad de incluir el conocimiento dentro del proceso de descubrimiento  por medio de las Ontologías  (definidas por  Gruber como: “Especificación formal explícita de una  conceptualización compartida”). Vemos que la base ontológica es una condición previa para el uso  automatizado eficiente de ese conocimiento.  En la figura se pueden observar las áreas donde se pueden aplicar las ontologías en el  proceso de descubrimiento de conocimiento.  De esta manera, podemos ver a la relación entre Ontologías y Minería de Datos de dos  modos:  • Desde las Ontologías a la Minería de Datos, incorporamos el conocimiento al  proceso por el uso de ontologías, es decir como los expertos entienden y realizan las  tareas de análisis. Las aplicaciones representativas son ayudantes inteligentes para el  proceso de  descubrimiento, la interpretación y la validación del conocimiento  extraído, Ontologías para recursos y descripción de servicios y Knowledge Grids(  Hotho et al., 2003; Bernstein et al., 2005; Cannataro et al. 2003, 2004, 2007;  Gridminer Project; Gottgtroy  et al., 2005; Rennolls, 2005).  • Desde la Minería de Datos a Ontologías, incluimos el conocimiento del dominio en  la información de entrada o usamos las ontologías para representar los resultados.  Por lo tanto el análisis es realizado sobre estas ontologías. Las aplicaciones más  representativas están en Medicina, Biología y Datos Espaciales, como: la  representación de Genes, Taxonomías, aplicaciones en Geociencias, aplicaciones  médicas (Breaux et al., 2005; Tadepalli  et al., 2004; Bogorny et al., 2005, 2006;  Sidhu et al., 2006).  En primer término, nuestra investigación se centrará en la definición y clasificación de  ontologías dentro del proceso de descubrimiento de conocimiento en Base de Datos con el objetivo  de mejorar la performance del proceso en sí mismo, como así también, la calidad de los  descubrimientos realizados.  La segunda etapa de este proyecto, consiste en el diseño y desarrollo de una herramienta que  abarque:  • Base de conocimiento ontológico para el dominio de aplicación.   Data Warehouse Metadata Data Marts Extración Transformación Carga BDs Operacionales Fuentes externas Conocimiento Organizacional  y Contextual Selección  de Algoritmos de minería Selección  de variables Resulta dos Visualización Conocimiento Nuevo Acciones Conocimiento actualizado Funciones de Integración Areas  de aplicación para las Ontologías Figura 2 Áreas de aplicación para las Ontologías en el Proceso KDD  • Base de conocimiento ontológico para las técnicas de Minería o estadística  empleadas. Esta base será empleada en la implementación del Asistente Inteligente  de Descubrimiento ideado por Bernstein(2005).  • Funciones de aprendizaje sobre la utilización de la herramienta, lo que nos permitirá  mejoras para distintos perfiles de usuario  • Funciones de meta aprendizaje para la evaluación de cada uno de los modelos  inducidos.  • Base Conocimiento conteniendo los patrones descubiertos.  El proyecto pretende prestar a un usuario de Minería de Datos, al menos, los siguientes  beneficios:  • Una enumeración sistemática de los procesos de Minería válidos, no sólo los  importantes, sino aquellos potencialmente utilizables.  • Un orden efectivo de dichos procesos válidos según criterios diferentes y una ayuda  para seleccionar entre las distintas opciones.  • Una infraestructura para segmentar el conocimiento de Minería, algo que los  economistas denominan redes externas.  • Un soporte arquitectónico genérico para permitir la inclusión del conocimiento del  dominio en las sesiones de minería.    3)Temas involucrados en el proyecto    Las áreas incluidas en el proyecto son: 1)Data Warehouse, 2)Bases de Datos, 3)Estadística,  4)Análisis de Datos, 5)Ingeniería del Conocimiento, 6)Data Mining, 7)Inteligencia Artificial,  8)Sistemas Inteligentes, 9)Aprendizaje Automático, 10) Ingeniería Ontológica, 11)Agentes  Inteligentes, 12) Visualización de datos.    4) Conclusiones    En la actualidad, el conocimiento tiene una importancia relevante en las organizaciones,  razón por la cual es necesario optimizar el proceso de su descubrimiento. El conocimiento empírico  o heurístico, ya sea proveniente del dominio de aplicación o del conocimiento de las técnicas,  mejora el resultado de los modelos inducidos en el proceso de Minería de Datos. Nuestra meta es  captar ese conocimiento y representarlo mediante ontologías.   La inclusión de la Ingeniería Ontológica en el proceso de descubrimiento, nos permitirá la  integración de diferentes técnicas de Minería de Datos, como así también su uso adecuado.   Creemos que la inclusión de las Ontologías no solo mejorará la performance del proceso  sino que también mejorará la calidad de los conocimientos descubiertos. Podemos considerar  entonces, que ya no estamos realizando Minería de Datos sino Minería de Conocimientos.	﻿Conocimiento en Bases de Datos , Ontologías , Proceso de Descubrimiento	es	20407
28	Organizaciones inteligentes	﻿ORGANIZACIONES INTELIGENTES  Gestión de Indicadores   Informática de Gestión    Autores  Gustavo Tripodi - gtripodi@exa.unicen.edu.ar  Gustavo Illescas - illescas@exa.unicen.edu.ar  Facultad de Ciencias Exactas- Universidad Nacional del Centro de la Provincia de Buenos Aires.  Grupo de Investigación en Informática de Gestión - Teléfono: +54 2293 439680. Dirección postal:  Campus Universitario, Paraje Arroyo Seco, (7000) Tandil, ARGENTINA    Introducción  Una nueva filosofía de Management tiene el objetivo de  capturar, diseminar y rehusar el  conocimiento disperso que poseen las organizaciones y que junto a sus recursos humanos  constituyen su Capital Intelectual, que forma parte de su activo más importante en este siglo. El  propósito es que las organizaciones posean Sistemas  que interpreten datos históricos, analicen  tendencias y midan performance, orientados a servir de soporte a los procesos para la toma de  decisiones. Este paradigma se encuentra en el marco del Knowledge Management.  La Knowledge Computing Management, se basa en Information Technologies and  Telecommunications (TICs) y Learning Organization. A fin de completar la estructura del  conocimiento adoptada, nos orientamos sobre una de las ramas hacia la Business Intelligence y en  la otra hacia las Management Technologies, apoyados en Information Systems, como indica la  Figura 1.                            En una Organizational Learning los roles del líder difieren enormemente de los habituales: ya  que son diseñadores, instructores y administradores. Estos nuevos roles del líder implican el  desarrollo de aptitudes también nuevas: capacidad de crear consenso, poner en evidencia y  cuestionar modelos mentales y promover maneras de pensar más afines con los Sistemas. En  resumen, los líderes de la construcción de organizaciones deben ayudar a la gente a expandir  continuamente su capacidad para forjar el futuro; es decir, son responsables del aprendizaje (Senge,  1990). Para consolidar los modelos y transmitir las ideas los líderes se basan en herramientas de  Management Technologies.     Objetivo  Objetivo General  Nos orientamos a conceptualizar y obtener resultados sobre temas que aporten a la comprensión  de la Sociedad del Conocimiento que estamos transitando, focalizados en el área de Knowledge  Computing Management para obtener Organizaciones Inteligentes sustentables.   Figura 1. Knowledge Management en una Intelligence Organization.  Information System  Knowledge Computing Management  Organizational Learning    Information Technologies  and Telecommunications  Management Technologies Business Intelligence  En este trabajo nos concentraremos en la Gestión de Indicadores a los cuales hemos clasificado,  agrupado y expuesto, profundizando el concepto tradicional. De esta manera aportan mayor  información y ayudan así a encontrar el camino del conocimiento de la organización para la toma de  decisiones.  Objetivos específicos  · Reconocimiento del valor estratégico de la información para lograr la efectividad en los  resultados esperados por la Organización.  · Posicionar a la Organización en los umbrales de construcción de Tableros de Control tendientes a  evolucionar hacia la implantación de un Cuadro de Mando Integral (Balanced Scorecard) que  permita gestionar la Estrategia de la Organización.    Planteo del problema  Para comenzar a tomar decisiones de corto y mediano plazo, donde la respuesta ante los  cambios del mercado, la competencia y la economía hacen imprescindible la información on-line,  es necesario tener resuelta la sistematización de las operaciones y los procesos de gestión. Además  en las decisiones de largo plazo nos debemos apoyar en información sólida donde las fuentes de  datos deben ser confiables.  En la arquitectura de los Information System conviven los Transactional System, el Data  Warehouse y las herramientas de las Management Technologies orientadas por la Business  Intelligence. La cantidad y calidad de los Sistemas Transaccionales  y de gestión facilitan el camino  hacia la Knowledge Management.   En líneas generales las Organizaciones pequeñas y medianas no están preparadas para  incorporar herramientas de las Management Technologies y Business Intelligence por su  infraestructura y/o cultura.  Algunas cuestiones que revelan problemas son las que siguen:  · Programas de computación aislados.  · Adaptación de las operaciones a los programas.  · Baja adaptación de los procesos en los programas.  · Inexistencia de procedimientos que incluyan funciones, recursos humanos, recursos materiales y  recursos de software.  · Inexistencia de reglas del negocio claras y explícitas.  · Falta de uso de herramientas para la toma de decisiones.  · Mal uso de las herramientas para la toma de decisiones.  · Adquisición de herramientas sin evaluar si su prestación contempla los requerimientos y las  necesidades de la organización.  · Falta de inclusión en el planeamiento estratégico del impacto de las TICs en la Organización.  · Falta de planificación para la implantación de sistemas integrales e integrados.  · Falta de división conceptual y física entre el area de sistemas y recursos de hardware.  Los problemas enunciados los podemos dividir en dos grandes rubros: i) falta de valorización  del área de Sistemas (área de conocimiento) para su involucramiento en la toma de decisiones (un  cambio de cultura), y como consecuencia, ii) la reconversión de los Sistemas para utilizar  adecuadamente las posibilidades que ofrecen las TICs.  Para realizar el diagnóstico de la Organización  con respecto a su estado y encontrar la solución  para incorporar herramientas y tecnologías utilizaremos la gestión de indicadores dentro de las  Management Technologies.    Indicadores : Herramienta de las Management Technologies  El conocimiento a obtener de la Organización esta basado en primer lugar en indicadores, que  pueden visualizarse en forma individual o agrupada según la evolución en Tableros de Control y  Cuadros de Mando Integral   Los indicadores son descripciones compactas de observaciones, resumidas en números o  palabras. Dichas observaciones pueden referirse a un tema concreto pero también pueden expresar  observaciones resumidas sobre un cierto número de cuestiones similares (Olve, Roy y Wetter,  2000).  Los indicadores son luces que comienzan a encenderse para iluminar la Organización. El  abanico de posibilidades de generar indicadores puede ser amplio, desde los más sencillos a los más  sofisticados. De todas formas es apropiado encontrar una mínima cantidad de ellos que reflejen los  aspectos más importantes (aprendizaje y crecimiento, procesos internos, finanzas y clientes) y de  mayor impacto organizacional. Ellos pueden ser específicos de una operación, síntesis de un  proceso o resumen de un conjunto de procesos, ya que logran crecer down y across. Ayudan para en  el control de Gestión, ya que su evolución permite la comparación de estados actuales con estados  anteriores y estados actuales con pronosticados. Por esta causa deben ser guardados los valores de  los indicadores y los valores que los generaron.  Dividimos los indicadores en dos clases: semiotics y health. Esta clasificación se encuentra en  concordancia con el estado de salud de un ser humano.  Si una Organización no esta sana, es difícil que pueda ofrecer productos y servicios de la mejor  manera. Primero debería lograr la calidad interna, para comenzar a brindar calidad a sus clientes o  cualquier individuo u Organización que interactúe con ella. Por ello comenzaremos a trabajar con  indicadores semiotics para luego ir incorporando los indicadores health. Estos últimos revelan la  evolución de los indicadores que dan mayor competitividad a la Organización.  Los indicadores evolucionan en el tiempo por causas endógenas y exógenas, es decir los rangos  deseados y no deseados para los resultados de los indicadores varían según decisiones subjetivas y  objetivas. Las causas exógenas son generalmente incontrolables y se trabaja sobre sus causas  (mayor competencia, nuevos productos y servicios que el mercado ofrece, aumento del precio de  insumos por escasez). Por el contrario, las causas endógenas pueden ser creadas y transformadas  por la Organización.  Las políticas, planes estratégicos, optimizaciones y capacitación, entre otros, pueden modificar  el resultado de los indicadores. Las decisiones internas  tratan de forzar los resultados de los  indicadores hacia los rangos deseados. Los indicadores semiotics deberían ir decreciendo en el  tiempo (Figura 4), al contrario de los indicadores health cuyos valores deberían crecer (Figura 5).                              El uso de indicadores se puede plantear en forma genérica para cualquier organización, donde,  con cierto grado de abstracción podemos plantear la Cadena de Valor como se muestra en la Figura  6. En la misma se observan los siguientes procesos:  · Logística de Entrada: Análisis y gestión de proveedores.  · Producción: de productos o servicios.  · Logística de Salida: distribución, seguimiento y control de Clientes.  0     1  ..       n-1        n  t  Períodos  Va lo r  de l  in di ca do r     Rangos específicos  sobre un período    Figura 4. Indicadores Semiotics  0         1       ..    n-1        n  t  Períodos  Va lo r  d el  i nd ic ad or   Rangos específicos  sobre un período    Figura 5. Indicadores Health                                   Semiotics Indicators recomendados. Los indicadores elegidos para comenzar a trabajar se muestran  en la Tabla 7:     Cadena de Valor Indicador Cálculo  sugerido Frecuencia  Sobreprecios de Insumos ( ) 1-n Referencia deValor  -  Valor  ESI 2 iå=  Bimestral  Entrada  Retrasos en la llegada de  Insumos Totales  Compras Retrasoscon    ComprasERI =  Trimestral  Artículos perecederos en Stock  Stocken     Articulos de  Total sPerecedero  ArtículosAPS =   Cuatrimestral  Vencimiento de artículos en  Stocks Adquiridos sPerecedero Articulos Total Vencidos Artículos de CantidadAVS =  Trimestral  Stock inmovilizado   Artículos de Cantidad ion)  UtilizacFecha  -  Entrada  Fecha(ASI å=  Bimestral  Valor de Inmovilización ]Valor * ion)  UtilizacFecha  -  Entrada  Fecha[(ASI å=  Bimestral  Almacenes  Diferencias de Inventario   Total RealStock  CalculadoStock  - realStock ADI =  Cuatrimestral y muestreos  Desvíos  Real Producción Estimada Producción  -  Real  ProducciónPDS =  Mensual  Desperdicios  Totales Unidades oDesperdici de Unidades PDP =  Mensual  Defectuosa  Producidas sDefectuosaPDF =  Por lotes  Producción  Remanufacturación  Producidas uradasRemanufactPRM =  Por lotes  Retrasos en registraciones Días de atraso por rubro quincenal  Circulante Efectivo Positivas sDiferenciaAD =+  Mensual  Diferencias de Caja  Circulante Efectivo Negativas sDiferenciaAD =-  Mensual  Demoras en atención a clientes    Por muestreo  Administración  Longitud de Colas    Por muestreo                        Base de Conocimiento  Administración General y Operaciones  Gestión de RRHH  Investigación y Desarrollo  GESTION DEL CONOCIMIENTO  D e m a n d a n t e s  O f e r e n t es     Producción   Logística de Entrada  Logística de Salida  Figura 6. Cadena de Valor personalizada para el caso de estudio   Demoras en las entregas a  Clientes Entregas  de  Total Retrasoscon    EntregasOEC =  Bimestral  Horas extras  Normales  Horas Extras HorasRHS =  Mensual  Ausencias  Total Personal Ausente PersonalRAS =  Mensual RRHH  Accidentes Numero de accidentes Mensual  Reclamos  nesTransaccio de Cantidad Reclamos de CantidadSRE =  Trimestral  Demora en solucionar  Reclamos Reclamos  de  Cantidad )Reclamo  Fecha  -Solución    Fecha(SDR å=  Bimestral  Reclamos no solucionados  Reclamos de Cantidad osSolucionad  no  Reclamos SRN =  Bimestral  Salida  Clientes perdidos  Clientes  de  Total Perdidos  Clientes SCP =  Cuatrimestral  Tabla 7. Semiotics Indicators    Conclusión  El cambio de cultura desde lo tradicional a los conceptos de la Knowledge Computing  Management debe ser elaborada en forma conjunta con y para todos los actores a través de un plan  estratégico enmarcado en la Misión y la Visión de la Organización. Este plan que incluye premisas,  propósitos, metas, objetivos y resultados esperados, deberá ser dividido en tres etapas:  implantación, consolidación y sustentación.  En este trabajo se abordo la implantación con propuestas para el conocimiento básico de la  Organización a través de conceptos, herramientas, recursos humanos y tecnológicos. Debemos  reconocer en que estrato se encuentra la Organización, y de esta manera el trabajo sería lograr pasar  a la etapa siguiente u ocuparse de la sustentabilidad en la s Organizaciones Inteligentes.	﻿Organizaciones inteligentes , Management , Capital Intelectual	es	20439
29	Características de Grids vs Sistemas Peer to Peer y su posible Conjunción	﻿ El proyecto Computación Distribuida de Alto Rendimiento y  Disponibilidad que se está desarrollando en el LISiDi ocupa varias líneas de  atención como: seguridad, problemas de exclusión mutua en sistemas  distribuidos, memoria compartida distribuida, movilidad y grids.  Dentro del tema de grids se abre una línea de estudio y desarrollo que es  tratar de compatibilizar las características de los sistemas peer-to-peer con  los grids y tratar de aprovechar lo mejor de ambos mundos.      Introducción  La computación grid es un paradigma que propone el agregado de computación heterogénea,  almacenaje y recursos de red geográficamente distribuidos  para proveer un acceso generalizado a  sus capacidades combinadas, en conjunto se lo llama simplemente grids.  Los grids de datos [2,6] tratan primariamente de ofrecer servicios e infraestructura para  aplicaciones intensivas sobre datos distribuidos que necesitan acceder, transferir y modificar  conjuntos de datos masivos almacenados en recursos de almacenaje distribuidos. Para que el  usuario logre el mayor beneficio de la infraestructura son necesarias las siguientes capacidades:  a) Habilidad para buscar entre los numerosos conjuntos de datos disponibles por el conjunto  requerido y descubrir los recursos adecuados para acceder a los datos.  b) Habilidad para transferir conjuntos de datos de gran tamaño entre recursos en el tiempo más  corto posible.  c) Habilidad para que los usuarios manejen múltiples copias de sus datos.  d) Habilidad para seleccionar los recursos de computación adecuados y procesar los datos en  ellos.  e) Habilidad para manejar los permisos de acceso a los datos.                                                               1 No hay jerarquía en la aparición de los nombres, todos son integrantes del LISiDi. Los e‐mail de acuerdo al orden de  aparición son: jra@cs.uns.edu.ar, je@cs.uns.edu.ar, kmc@cs.uns.edu.ar, mic@cs.uns.edu.ar, gfried@frbb.utn.edu.ar,  rbg@cs.uns.edu.ar, gl@cs.uns.edu.ar, ldm@cs.uns.edu.ar, cjp@cs.uns.edu.ar.  2 Tel +54 291 4595135, Fax +54 291 4595136  Los grids, en general, son estructurados y se han desarrollado y aún se desarrollan middlewares  con el propósito de establecer una estandarización como Condor [7] y Globus [3].  Las arquitecturas peer-to-peer son la base de la operación de sistemas de computación  distribuida como Gnutella [4], Seti@home [8], OceanStore [5], etc.  Estas arquitecturas comparten recursos computacionales (ciclos de CPU, almacenaje,  contenido, etc) y en general no requieren la intervención de un servidor centralizado.  Las motivaciones de su uso radican en su habilidad para funcionar, escalar y organizarse a si  mismo en presencia de una gran población de nodos, redes y fallas de computadoras sin necesidad  de un servidor central y la sobrecarga de su administración.  Tiene características inherentes como escalabilidad, resistencia a la censura y control  centralizado y creciente acceso a recursos. La administración está repartida. Aceleran procesos de  comunicación y reduce los costos de colaboración por medio de una administración de grupos de  trabajo.  Resulta difícil lograr un acuerdo sobre una definición correcta de lo que es un sistema peer-topeer. Lo que persiste es la percepción externa del sistema.  Hay dos características de las arquitecturas peer-to-peer que la definen:  • Compartir recursos de computación por intercambio directo.  • Su habilidad para manejar lo instantáneo y la conectividad como norma.  Se adopta la siguiente definición [1] sin perjuicio de cualquier otra:  Sistemas peer-to-peer son sistemas distribuidos consistentes de nodos interconectados capaces  de organizarse a si mismo dentro de topologías de redes con el propósito de compartir recursos  tales como contenidos, ciclos de CPU, almacenaje y ancho de banda, capaces de adaptarse a las  fallas y acomodarse a poblaciones transitorias de nodos mientras mantienen una aceptable  conectividad y rendimiento, sin requerir intermediación o soporte de un servidor o autoridad  global centralizada.   Puede hacerse una clasificación de las aplicaciones peer-to peer:  • Comunicación y colaboración (chat, aplicaciones de mensajería instantánea).  • Computación distribuida (potenciar la computación como seti@home, genome@home).   • Soporte de servicio Internet (aplicaciones multicast, indirección, aplicaciones de  seguridad).   • Sistemas de Bases de Datos (PIER, Piazza).  • Distribución de contenido (compartir medios digitales y datos entre usuarios como  Napster, Publius, Gnutella, Kazaa, Freenet, Oceanstore, MojoNation, PAST, Chord,  Scan, FreeHaven, Groove, Mnemosyne)  La distribución de contenidos se pueden agrupar más específicamente en:   Aplicaciones peer-to-peer.       Según objetivos:  • Intercambio de archives.   • Publicación de contenidos y sistemas de almacenaje.    Infraestructura peer-to-peer.  • Proveen frameworks de aplicación y servicios básicos   • Ruteo y localización   • Anonimato   • Administración de la reputacíón  Para la localización y ruteo de objetos la red funciona sobre alguna red física, ésta es una red  solapada (overlay network).  La estructura de la topología y el grado de centralización de la red solapada, el ruteo y los  mecanismos de localización que emplea para mensajes y contenidos es crucial para la operación del  sistema porque afecta a:   • la tolerancia a las fallas   • automantenimiento   • adaptabilidad a las fallas   • rendimiento   • escalabilidad    • seguridad     Este último punto sobre seguridad los nodos de la red no deben considerarse confiables y no se  pueden hacer suposiciones sin considerar su comportamiento. Se analiza el almacenaje y ruteo  seguro, dentro de este, el asignamiento seguro de identidades de los nodos, la evasión de nodos  maliciosos, el mantenimiento de tablas de ruteo y el reenvío seguro de mensajes   El control de acceso, autenticación y manejo de identidades generalmente son ignorados en los  sistemas peer-to-peer.    Propuesta de trabajo.  Los sistemas peer-to-peer y grid son dos propuestas de computación distribuida, ambas  concernientes con la organización de recursos compartidos en sociedades computacionales en gran  escala.  Los grids son sistemas distribuidos que habilitan el uso coordinado en gran escala de recursos  distribuidos geográficamente, basados en la persistencia, infraestructuras de servicios  estandarizadas, frecuentemente con una orientación al alto rendimiento.  En tanto que un sistema grid se incrementa en escala comienza a requerir soluciones para la  autoconfiguración, tolerancia a las fallas y escalabilidad para lo cual la investigación en peer-topeer tiene mucho para ofrecer.  Los sistemas peer-to-peer, por otro lado, se enfocan en tratar con la instantaneidad, las  poblaciones transitorias, la tolerancia a las fallas y la autoadaptación.   Hoy en día, sin embargo, los desarrolladores de peer-to-peer han trabajado principalmente en  aplicaciones integradas más que en buscar definir protocolos comunes e infraestructuras  estandarizadas para interoperabilidad.  En resumen, se puede decir que la computación grid apunta a infraestructura pero no a fallas,  mientras que peer-to peer apunta a fallas pero no a infraestructura   En adición a esto, la forma de compartir inicialmente apuntada por peer-to-peer tiene una  limitada funcionalidad, proveyendo una distribución de contenidos global y un espacio de archivos  compartidos sin ninguna forma de control de accesos.  En tanto que las tecnologías peer-to-peer avancen hacia aplicaciones más sofisticadas y  complejas, tales como distribución de contenido estructurado, colaboración por desktop y  computación en la red, se espera que habrá una fuerte convergencia entre peer-to-peer y  computación grid.  El resultado será una nueva clase de tecnologías combinando elementos de ambos, lo cual  apuntará a escalabilidad, autoadaptación y recuperación de fallas al mismo tiempo que provee  una  infraestructura persistente y estandarizada para interoperabilidad.  Los problemas abiertos que se presentan son:  1) Diseño de nuevas formas de ubicación, ruteo y estructura de datos, hash distribuidos y  algoritmos para maximizar el rendimiento, seguridad y escalabilidad, sea cual fuera  arquitectura.   2) Anonimato más seguro, más seguridad (más eficiente) y esquemas resistentes a la censura.  3) Agrupacion semántica de la información (Web Semántica).  4) Convergencia de peer-to-peer y grids.	﻿Grids , Sistemas Peer to Peer , Conjunción	es	20464
30	Implementación de agentes BDI en JADEX	"﻿ Este artículo describe, en forma resumida, parte de los trabajos de investigación y desarrollo que se están llevando a cabo en la línea “Agentes y Sistemas Multi-agente” del LIDIC. El objetivo de este trabajo es presentar las principales temáticas que están siendo abordadas actualmente en el área de modelos y arquitecturas de agentes cognitivos, para posibilitar un intercambio de experiencias con otros investigadores participantes del Workshop, que trabajen en líneas de investigación afines. Uno de los objetivos principales de esta línea, es el estudio y desarrollo de sistemas con agentes basados en el modelo BDI. Las arquitecturas (y modelos) BDI proponen a la trinidad BDI (Beliefs, Desires e Intentions) como los elementos claves del estado mental de un agente para tomar las decisiones acerca de cuándo y cómo actuar. Este tipo de enfoque ha demostrado una gran flexibilidad y efectividad en diversos problemas de gran complejidad del mundo real, lo que ha llevado a un creciente interés en la investigación de sus aspectos téóricos pero tambien de las plataformas que soportan el desarrollo de este tipo de agentes. En este, sentido, el objetivo general de este trabajo es realizar una breve descripción de las motivaciones y objetivos que perseguimos al implementar agentes BDI utilizando frameworks de agentes dedicados a tal fin. En particular, se propone el framework de distribución gratuita Jadex que ya ha sido utilizado en distintos problemas vinculados a la logística de hospitales en Alemania. 1. Introducción Cada día es más frecuente la utilización de enfoques basados en agentes inteligentes [1, 2, 3] para abordar problemas de gran complejidad del mundo real. La principal fortaleza de este enfoque en estos casos reside en la capacidad de sus componentes (agentes) para exhibir un comportamiento flexible. La idea de flexibilidad en este contexto refiere a la capacidad de los agentes para: Percibir directamente un ambiente dinámico y reaccionar oportunamente a eventos y condiciones cambiantes (reactividad). Tomar la iniciativa cuando sea necesario e iniciar comportamientos dirigidos por un objetivo (pro-actividad). Interactuar y comunicarse, cuando es apropiado, con otros agentes artificiales o humanos (sociabilidad). 1Las investigaciones realizadas en el LIDIC son financiadas por la Universidad Nacional de San Luis y por la Agencia Nacional de Promoción Científica y Tecnológica. Proveer con estas capacidades a un agente no es una tarea sencilla. De hecho, una de las áreas de investigación más activa en el ámbito de agentes ha sido la definición de arquitecturas de agentes que intentan dar una respuesta a este problema. Existen arquitecturas que se han concentrado en el aspecto de la reactividad [4, 5, 6] y otras en cambio que han privilegiado los mecanismos de deliberación y planning necesarios para proveer un comportamiento pro-activo[7]. Sin embargo, existe actualmente un consenso generalizado en que cualquier arquitectura realista de agente, debería proveer un soporte adecuado para todas estas capacidades. Las arquitecturas híbridas [8, 9] y las basadas en comportamientos [10, 11] han intentado lograr un adecuado balance entre reactividad y pro-actividad. Sin embargo, las arquitecturas que mayor atención han recibido para este propósito son las denominadas arquitecturas BDI [12]. El modelo BDI, al igual que la teoría de decisión clásica y la teoría de decisión cualitativa son modelos de razonamiento práctico. Razonamiento práctico es el razonamiento dirigido a la acción. Se diferencia del razonamiento teórico en que este último está dirigido a las creencias. Concluir que “Sócrates es mortal” es razonamiento teórico, dado que solamente afecta mis creencias sobre el mundo. Decidir si tomar un tren o un colectivo es razonamiento práctico, ya que es razonamiento dirigido a la acción. Podemos encuadrar filosóficamente al modelo BDI dentro de lo que se suele referenciar como postura intencional. Este enfoque plantea esencialmente que un agente debe ser conceptualizado y/o implementado usando conceptos y nociones o estados mentales usualmente asociados a los humanos, como por ejemplo creencias, deseos, intenciones, obligaciones, compromisos, etc. El filósofo Daniel Dennet, ya en 1987 utiliza el término sistema intencional para describir entidades “cuyo comportamiento puede predecirse atribuyéndole creencias, deseos y perspicacia racional”. Básicamente, las nociones intencionales son herramientas de abstracción, que proveen una manera conveniente de describir, explicar y predecir el comportamiento de sistemas complejos. En el caso particular de las arquitecturas (y modelos) BDI se proponen a la trinidad BDI (Beliefs, Desires e Intentions) como los elementos claves del estado mental de un agente para tomar las decisiones acerca de cuándo y cómo actuar. A continuación describimos brevemente cada una de estas componentes: Beliefs (Creencias): Son sentencias que un agente toma como verdaderas (que a diferencia del conocimiento pueden ser falsas) acerca de propiedades de su mundo (y de sí mismo). Las creencias intentan capturar el estado de información (“informational state”) del agente. Desires (Deseos): Son acciones que un agente desea realizar o situaciones que prefiere y quiere lograr. Los deseos intentan capturar el estado de motivación (“motivational state”) del agente. Los Deseos se asemejan a los objetivos (goals), pero los objetivos involucran cierto grado de compromiso del agente en su realización y que el conjunto de objetivos perseguidos sea consistente. Intentions (Intenciones): Acciones factibles, planes o situaciones deseadas que el agente ha seleccionado y se ha comprometido a realizar o lograr. Las intenciones intentan capturar el estado deliberativo (“deliberative state”) del agente. El modelo BDI, puede dar una respuesta adecuada a los requerimientos que deberán enfrentar los sistemas de software en el futuro. En particular en [13] se reconoce que los ambientes complejos (dinámicos, inciertos, limitados en recursos y parcialmente observables) tienden a ser la norma y se requieren enfoques alternativos al desarrollo de software tradicional. En este tipo de problemas, el aporte de cada una de las componentes del modelo BDI se torna evidente: Beliefs: el mundo cambia y por consiguiente debo recordar eventos pasados. La percepción es incompleta por lo que debo recordar lo que no percibo actualmente. Además el sistema es acotado en recursos computacionales por lo que no conviene recomputar toda la información relevante a partir de la entrada perceptual. Desires (y Goals): el software tradicional (orientado a tareas) no tiene ningún registro de cual es el motivo por el cual está siendo ejecutado. Cuando el estado motivacional es explícitamente representado, el sistema puede recuperarse automáticamente ante las fallas. Puede además, aprovechar oportunidades que surgen dinámicamente. Intentions: el agente necesita reconsiderar los planes que ha adoptado y está ejecutando en el contexto de un mundo cambiante. Esta posibilidad de razonar sobre los planes adoptados para atender a necesidades más urgentes o determinar que una intención ha perdido razón de ser, no está disponible en el software tradicional donde no se reconsidera nunca. Más allá de los fundamentos filosóficos [14] y formales [15] del modelo BDI, su efectividad en problemas concretos de gran complejidad ha quedado demostrada en los últimos años. Algunas de las aplicaciones más conocidas son: Sistema de control de tráfico aéreo OASIS (aeropuerto de Sydney) Simulador para la fuerza aérea australiana (SWARMM) Sistema de propulsión de una nave espacial de la NASA (RCS) Sistema administrador de procesos de negocios (SPOC) Sistema para el diagnóstico, control y monitoreo de una red de telecomunicaciones de Telecom Australia (IRTNMS) Sin embargo, a pesar de la relevancia de las arquitecturas BDI para el desarrollo de software para dominios complejos, se podría decir que en nuestro país es un área prácticamente inexplorada tanto en el sector industrial como universitario. Si bien se han realizado algunos trabajos teóricos de agentes BDI abordados con enfoques argumentativos [16], hasta donde sabemos, no existen trabajos concretos con plataformas y herramientas específicos para el desarrollo de este tipo de agentes. En este contexto, dentro de nuestro grupo de trabajo nos proponemos hacer una experiencia de desarrollo de agentes BDI utilizando la plataforma JADEX. 2. Antecedentes Las arquitecturas BDI comienzan a recibir una atención creciente a partir de la aplicación de las ideas del filósofo Bratman en la arquitectura IRMA (Intelligent Resource-bounded Machine)[12]. En esencia, Bratman argumenta que un agente racional tenderá a enfocar su razonamiento práctico sobre las intenciones que ya ha adoptado y tenderá a “bypasear” aquellas opciones que entran en conflicto con estas intenciones. La característica distintiva de la propuesta de Bratman es el énfasis en el rol de las intenciones para ayudar a enfocar los procesos de deliberación y el razonamiento medios-fines (planning). Más allá del impacto que tuvieron las ideas planteadas en IRMA, se la puede considerar todavía una arquitectura abstracta donde existen distintas componentes cuya implementación no está totalmente especificada. En este sentido, el primer sistema de uso industrial basado en el modelo BDI es el Sistema de Razonamiento Procedural (PRS) [17], el cual contaba con estructuras de datos explícitas que correspondían a los estados mentales BDI. A partir del suceso del sistema PRS en varios problemas de envergadura, surgieron distintas variantes de PRS que en muchos casos fueron simples reimplementaciones en otros lenguajes, o bien extensiones para cubrir aspectos no considerados en el sistema original. Así, comienza un período donde se implementan y difunden distintas plataformas para el desarrollo de agentes BDI, algunos de índole académico y otras pensadas para el uso industrial. Entre las más conocidas podemos citar a JAM [18], Jack [19], AgentSpeak(L) [20], dMars [21, 22] y Jadex [23]. De todas estas plataformas, la más relevante para nuestro trabajo es la plataforma Jadex. Jadex es una extensión al poderoso “middleware” de agentes Jade [24]. Al igual que otros “middleware” de agentes, Jade provee funcionalidades genéricas para facilitar el desarrollo de agentes, que incluyen la administración de agentes, servicios de directorio y distribución de mensajes confiable entre los agentes. Todas estas facilidades están implementadas siguiendo el modelo de referencia de FIPA 2. Como antecedentes más cercanos en el uso de Jade podemos mencionar distintos trabajos finales y trabajos de investigación desarrollados en el LIDIC de la Universidad Nacional de San Luis [25, 26, 27]. Jadex surge en el contexto del proyecto MedPage (“Medical Path Agents""), donde se plantea la necesidad de contar con una plataforma de agentes que soporte comunicación conforme a los requerimientos de FIPA y además provea una arquitectura de agente de alto nivel del tipo BDI. El proyecto MedPage es parte del siguiente programa de investigación prioritario de Alemania: “1013 Intelligent Agents in Real World Business Applications. Surge a partir de la cooperación del departamento de administración de negocios de la Universidad de Mannheim y el departamento de ciencias de la computación de la Universidad de Hamburgo, en un trabajo conjunto para investigar las ventajas de usar tecnología de agentes en el contexto de la logística de los hopitales [28]. El proyecto Jadex comenzó en Diciembre de 2002 para proveer el sustento técnico a los prototipos de software de MedPage desarrollados en Hamburgo. 3. Objetivos El objetivo general de nuestro trabajo es realizar una primera aproximación al problema de implementar agentes BDI utilizando frameworks de agentes dedicados a tal fin. En particular, se utilizará el framework de distribución gratuita Jadex que ya ha sido utilizado en distintos problemas vinculados a la logística de hospitales en Alemania. En este contexto, un objetivo parcial a cumplir será el estudio y análisis de otros frameworks para el desarrollo de agentes BDI, a los fines de individualizar las similitudes y diferencias de los mismos con Jadex. También se experimentará con los ejemplos introductorios provistos con Jadex para adquirir experiencia en las herramientas para el desarrollo, visualización y depuración de aplicaciones BDI que provee este framework. En base al trabajo de investigación y experimentación previo, se propondrá un problema donde claramente se visualice la potencialidad y flexibilidad del enfoque BDI. Este problema involucrará la ejecución de comportamientos dirigidos por el objetivo que con frecuencia deberán ser reconsiderados debido a la ocurrencia inesperada de eventos (por ejemplo generada desde dispositivos móviles) que pueden requerir de un tratamiento urgente. Todas estas componentes, al igual que las capacidades de meta-razonamiento para la reconsideración de intenciones serán implementadas en Jadex."	﻿JADEX , agentes BDI	es	20532
31	Análisis de plataforma sewart utilizando simmechanics aplicada al desarrollo de simuladores de vuelo	﻿   Este proyecto tiene como objetivo el modelado y control de una plataforma de 6 GDL (Grados de  Libertad). El mismo forma parte de las actividades que se están realizando en el marco del proyecto  “Simulador de Vuelo” (PAE2004-22614 ANPCyT). Se ha comenzado el análisis a partir del modelo  desarrollado por Smith –Wendlant[1] de una plataforma tipo Stewart utilizando Matlab con su paquete  de simulación mecánica el cual fue modificado de modo de poder analizar y simular mejor la situación  de un simulador de vuelo. El propósito de estas simulaciones es aplicarlas en una etapa posterior como  herramientas para el diseño mecánico de las plataformas. Otro aspecto que se va a estudiar más  adelante es el modelado y simulación del sistema de control.    Palabras claves: simulación- control automático-robótica- plataformas Stewart  1.  Introducción  El desarrollo de simuladores de vuelo aborda distintos temas específicos de diversas áreas como son:  simulación dinámica del vuelo, modelo de aeronaves, modelo de datos para representar entornos  topográficos, modelos de control de la plataforma mecánica, estudio de la percepción humana del  movimiento y de la interacción entre los sistemas visual y vestibular en la percepción y diseño de  algoritmos que resuelvan numéricamente las ecuaciones de los modelos matemáticos  desarrollados. La  importancia de los simuladores no sólo radica en su utilización para el  entrenamiento de pilotos sino  también como herramienta de diseño de aeronaves (en la actualidad éstas son exhaustivamente  probadas y voladas en simulaciones, aún antes de la construcción del primer prototipo) u otras  aplicaciones como peritajes, entrenamiento en condiciones de desorientación espacial, etc. El  simulador que se percibe como objetivo es altamente realista sobre lo que experimenta el piloto siendo   una réplica a tamaño real de una cabina con un sistema visual y una plataforma móvil.  2. Desarrollo del Proyecto  En esta primera etapa se estudiaron distintas configuraciones de plataformas conocidos como  manipuladores o robots paralelos, como ser: de 3 GDL  de modo de posicionar a la plataforma (Fig. 1)  y de 6 GDL de modo de poder además orientarla (Fig.2).  Ambas plataformas están formadas por una base fija, tres piernas y la plataforma móvil.  La función de control del simulador  es la de llevar la plataforma móvil a una ubicación deseada para  ello se necesita conocer la posición y orientación que debe tener la misma en todo momento.  Para poder realizar dicho estudio se utilizará un sistema de coordenadas solidario al sistema móvil y  otro al fijo. De modo de obtener luego la relación entre los dos sistemas mediante matrices de rotación.  Las matrices de rotación necesitan de nueve parámetros por lo que  se suele utilizar  los ángulos de  Euler ya que sólo es necesario conocer tres componentes ( )ψθφ ,, .                                           Fig. 1                                                           Fig. 2  Existen tres posibilidades que dan origen a las configuraciones conocidas como ZXZ, ZYZ, o Roll,  Pitch, Yaw  (alabeo, cabeceo y guiñada). El modo de estudiar la posición puede realizarse con lo que se  conoce como cinemática directa (se conocen las longitudes de las piernas y se debe hallar la ubicación  del vector que une los dos sistemas de coordenadas y la matriz de rotación) o cinemática inversa ( se  conoce la ubicación del vector que une los dos sistemas de coordenadas y la matriz de rotación y se  deben  hallar las longitudes de las piernas) .  En el año 1965 Stewart  (Fig. 3) presentó una construcción mecánica conocida como Plataforma  Stewart que permite realizar este tipo de movimientos.                                                                        Fig. 3  En esta etapa se ha comenzado el análisis a partir del modelo desarrollado por Smith –Wendlant[1]. En  un principio se ha utilizado un modelo de plataforma tipo Stewart utilizando Matlab con su paquete de  simulación mecánica.  El control de la plataforma consta de dos partes: a) generación de la posición: consiste en calcular la  longitud necesaria de cada pierna para ubicar la plataforma en una posición determinada y b) calcular  la fuerza necesaria para darle a cada pierna la longitud previamente calculada. El diagrama de  simulación es el siguiente se muestra en la Fig. 4.  La planta modela la plataforma y un controlador que es de tipo PID. A partir de las trayectorias  deseadas necesitamos conocer cual es la acción a realizar en los actuadores, este problema de calcular  la longitud de las patas en función la ubicación final deseada  se conoce como cinemática inversa.   Controlando la longitud de las 6 patas se puede ubicar la plataforma según una orientación y posición  dada. Se debe luego tener en cuenta que hay movimientos que no son posibles de realizar y por lo tanto  puede que no sea posible ubicar la plataforma es dichas posiciones.  El modelo propone controlar mediante un PID las fuerzas aplicadas a la piernas de modo de lograr la  trayectoria deseada.  El programa de simulación trae un modelo de plataforma que se tomó como base y a partir de allí se  hicieron modificaciones. La primer modificación realizada fue en el modelo ya que el mismo no tenía  en cuenta la presencia de un cuerpo sobre la plataforma (Fig. 5). En este caso es un elemento  importante a tener en cuenta ya que la masa ubicada sobre ella es la cabina del avión.   2 Vel1 Pos 1 B F Weld C G C S 1 C S 2 Top Plate1 C G C S 7 C S 1 C S 2 C S 3 C S 4 C S 5 C S 6 Top Plate Env F or ce P os V el T op B as e Leg 6 F or ce P os V el T op B as e Leg 5 F or ce P os V el T op B as e Leg 4 F or ce P os V el T op B as e Leg 3 F or ce P os V el T op B as e Leg 2 F or ce P os V el T op B as e Leg 1 [V2] [P2][P1] [V1] [F6][F5][F4][F3] [P6][P5][P4][P3] [V6][V5][V4][V3] [F2][F1] [P4][P3][P2][P1] [F6][F5][F4][F3] [V4][V3][V2][V1] [V6][V5][P6][P5] [F2][F1] Demux 1 Force      Fig. 4                                                                Fig. 5  3  Simulación  Las simulaciones realizadas sobre el modelo propio además de tener en cuenta una masa ubicada sobre  la plataforma tiene en cuenta además la variación de su  centro de gravedad  con respecto al de la  plataforma.  Este análisis permite evaluar las fuerzas involucradas en cada una de las piernas para posicionar a la  plataforma y poder diseñar los actuadores. Desde el punto de vista del control de trayectoria se  realizaron pruebas de sintonía del controlador PID (Proporcional+Integral+Derivativo) para analizar la  velocidad de respuesta.  La posición de la plataforma en reposo estaba ubicada 2 metros  en el eje z y su nueva posición a  alcanzar  era 3 metros, es decir se deseaba elevarla sólo en forma vertical.  El análisis original (sin cuerpo sobre la plataforma) presenta los siguientes valores: en la Fig. 6 se ven  la posición a la que se lleva la plataforma, la evolución del error, y las fuerzas en las piernas. El sistema  de unidades es el MKS y es de notar el valor inicial de las fuerzas debido al orden de magnitud que  alcanzan. Cuando se llega a la posición deseada el valor de las fuerzas en cada pierna es el que se  presenta en la Fig. 7.  Al ubicar una masa de 1 Tn. cuyo centro de gravedad coincide con el de la plataforma se obtiene una  variación de las fuerzas (Fig. 9) en cada una de las piernas para ser llevadas a la misma posición no  notándose una gran variación en las fuerzas iniciales (Fig. 8).                                            Fig. 6                                                                         Fig. 7                                             Fig. 8                                                                            Fig. 9  El siguiente análisis se realizó sobre una masa de 1 Tn. variando su ubicación en la plataforma de  modo de tal que queda corrida hacia el eje x (Fig. 10) o hacia eje y (Fig. 11).  Al observar las figuras correspondientes a dichas simulaciones se observa que la distribución de la   fuerza inicial aplicada a las piernas no es la misma según donde  esté desplazada la masa .  La información brindada por la simulación sobre las fuerzas iniciales en las piernas resulta importante  cuando se debe decidir la posibilidad de los actuadores comerciales, mientras que en estado de  equilibrio se observa que la distribución de fuerzas en un caso  se halla repartida de a pares.    4 Conclusiones y Trabajos Futuros:  En el marco del proyecto de Simuladores de vuelo se ha cumplido con el objetivo propuesto para esta  etapa y se cuenta con un entorno de simulación que permite analizar diferentes configuraciones de  plataformas en cuanto a su constitución mecánica y a la ubicación de las cargas. Así mismo, este  entorno, es adecuado para el análisis de diferentes estrategias de control.                   .                            Fig. 10       Fig. 11                                                                           Fig. 12             Fig. 13  En la actualidad se está proyectando la compra de una plataforma Stewart con capacidad para 1000kg  de carga marca MOOG. De poder contar con este equipamiento, los trabajos se orientarán hacia la  generación de trayectorias que den la percepción de vuelo.   En caso de no tener acceso a la plataforma real, se deberá adicionar una tarea extra que consiste en el  desarrollo de una maqueta de plataforma que permita ensayar estrategias de control.	﻿control automático robótica , simulación , plataformas Stewart	es	20558
32	Aplicación móvil que implementa un catálogo de circuitos integrados reduciendo requerimientos de memoria usando SVG	﻿Aplicación Móvil que Implementa un Catálogo de Circuitos Integrados  Reduciendo Requerimientos de Memoria usando SVG   Basurto, Juan C.   Grupo TAWS   Escuela Superior Politécnica del Litoral (ESPOL)   Campus Gustavo Galindo, Km 30.5, vía Perimetral   Facultad de Ingeniería Eléctrica y Computación   jbasurto@espol.edu.ec           1. Introducción     La demanda actual de teléfonos móviles  a nivel mundial ha traído consigo no  solamente la necesidad de diversificar  los servicios, sino que también de crear  soluciones a los problemas cotidianos.  Con 2.9 mil millones de móviles  alrededor del mundo, el desarrollo de  aplicaciones móviles para resolver  algunos de ellos resulta una alternativa  viable para entregar información que el  usuario requiere tener a la mano con  frecuencia.   Este requerimiento de disponibilidad de  información se observa cuando se  trabaja en el diseño de Circuitos  Electrónicos, pues es necesario  consultar constantemente la asignación  de pines de integrados Digitales y  Analógicos. Un integrado es la reunión  de Puertas Lógicas y otros integrados,  tiene como objetivo ahorrar espacio,  tiempo y dinero en el diseño de un  Circuito. Los pines son los medios por  los cuales el integrado se conecta con el  exterior y cada pin tiene una función  diferente. Por el gran número de  integrados existentes, memorizar la  asignación individual para cada uno es  imposible. Una solución preliminar  podría ser la consulta en el Web o en  catálogos impresos.   Este es un problema que experimentan  estudiantes de las materias de los  laboratorios de Sistemas Digitales,  Electrónica A y Electrónica B, dada la  necesidad de tener a mano los medios  de información mencionados, en  función del desarrollo de los proyectos  que se envían en dichas materias.   Sin embargo, para los estudiantes no es  común tener acceso a Internet o a los  catálogos impresos fuera del  laboratorio, impidiendo así que se pueda  trabajar eficientemente en los circuitos  que diseñan cuando trabajan en casa.  Por lo tanto, el problema consiste en la  dificultad de acceder a los medios de  información.   Si el problema principal es la  accesibilidad, entonces la solución  debería estar relacionada a la movilidad.  Una solución óptima es el desarrollo de  una Aplicación Móvil que dado el  número de un integrado recupere su  información de un catálogo almacenado  en el dispositivo. Es importante que  dicho catálogo permita almacenar datos  de varios cientos de integrados.   El objetivo principal de este trabajo es  presentar una Aplicación Móvil que  resuelva este problema, considerando  las ventajas de un teléfono celular: la  movilidad y el acceso y explorando  alternativas para superar una limitación  de estos dispositivos: capacidad  limitada de memoria.     2. Materiales y Métodos     En el desarrollo de Aplicaciones  Móviles, es necesario tomar en cuenta  ciertos puntos muy importantes:     • Herramientas de Desarrollo   • Usabilidad   • Espacio que ocupa en Memoria     El IDE escogido para desarrollar la  aplicación es Netbeans 5.5.1, pues no  solamente dispone del Mobility Pack  5.5+ para trabajar con dispositivos  móviles, sino que también ofrece un  entorno de programación amigable al  disponer de una interfaz gráfica que  simplifica el desarrollo de la aplicación.   En términos de usabilidad, el proyecto  debe cumplir con lo siguiente:     • Debe proveer una interfaz sencilla  pero llamativa   • Debe contener la mayor  cantidad de  integrados posible, tomando en  cuenta que la cantidad de memoria  que ocupa debe ser mínima     Al implementar la aplicación es  necesario en primer lugar determinar la  presentación gráfica que se usará.   Para la presentación de cada integrado,  es necesario visualizar la asignación de  pines, por lo que debe existir una  imagen por cada integrado.   Sin embargo, al tener un gran número  de elementos es necesario tomar en  consideración el tamaño que tiene la  imagen en diferentes formatos.   La Figura 1 muestra el integrado 7400  que tiene 14 pines y cuatro puertas  NAND.    Figura 1. Integrado 7400      La Tabla 1 muestra el tamaño en KB de  la misma imagen en varios formatos  posibles.     Formato  Tamaño en KB   BMP (256 colores)  59   JPEG  41   GIF  6   PNG  4   Tabla 1. Formato versus Tamaño en KB para la  Figura 1     Como se puede observar, el formato que  requiere menos espacio es PNG  (Portable Network Graphics).   Con 4 KB por imagen, para 200  integrados se requieren 800 KB.  Esto es un problema si en un futuro se  necesita ampliar la cantidad de  integrados. Y si no es posible aumentar  esta cantidad, entonces habrán menos  integrados disponibles y el Usuario  tiene menos probabilidades de encontrar  el que busca.   Además, las imágenes resultan ser muy  básicas. Si se aumenta la cantidad de  colores que tiene la Figura 1, su tamaño  en KB aumentará y por ende el  conjunto. Es necesario contar con  imágenes que tengan una mejor  presentación, pero habría que sacrificar  espacio en memoria. Existe un formato  denominado SVG cuyas ventajas se  analizan a continuación     2.1 Introducción de SVG     SVG es un estándar de la W3C para  representación de imágenes vectoriales.  El uso de SVG en dispositivos móviles  es posible gracias a que se ha definido  un subconjunto de elementos de este  estándar denominado SVG Tiny  exclusivo para uso en celulares. El  Wireless Toolkit 2.5.1 incluido en el  Mobility Pack 5.5.1 agrega la  funcionalidad necesaria para manipular  imágenes SVG en aplicaciones móviles  desarrolladas usando Netbeans.   La figura 2 muestra tres partes de la  Aplicación: Presentación (Figura 2a),  Menú (Figura 2b) y el presentación del  integrado (Figura 2c) que en  comparación a la Figura 1 resulta ser  superior. Sin embargo, la imagen SVG  asociada a esta Figura tiene un tamaño  de 5 KB, 1 KB más que en la Figura 1,  y por ende para 200 imágenes se tendría  un total de 1000 KB.   El problema del incremento de memoria  requerida se soluciona tomando en  cuenta que una imagen SVG guarda la  información en texto.   Se puede tener cuatro plantillas de  imágenes de integrados de 4, 8, 14 y 16  pines de 5 KB cada una y en un archivo  de texto guardar 200 líneas, donde cada  línea representa un integrado. El archivo  de texto resultante tiene un tamaño de  aproximadamente 11 KB.   Cuando se solicita la información de un  integrado, se consulta el archivo de  texto y se modifica la imagen SVG  usando código escrito en Java.   Las imágenes adicionales para  presentación, menú y error (si no se  encontrara el integrado). Representan en  conjunto solamente 22.7 KB.   Debido a que no es posible modificar el  contenido dentro un archivo “.jar” y  volverlo a compilar (como modificar  una imagen SVG dentro del archivo  “.jar”), se utilizaron métodos para  trabajar con archivos que se encuentren  fuera del entorno de la aplicación, de  modo que las imágenes SVG se generan  en una ubicación determinada de la  memoria del dispositivo móvil y se  modifican conforme se generen las  solicitudes.    3. Resultados     Se obtiene una Aplicación Móvil que  resuelve el problema de no disponer de  un medio de consulta inmediato para la  creación de proyectos electrónicos y  que además proporciona una interfaz  amigable.  Adicionalmente, se obtuvo un ahorro  notable de la memoria requerida para la  aplicación. El paquete generado tiene un  tamaño de 31 KB para 200 Integrados.  Considerando únicamente el catálogo de  los Integrados usando imágenes PNG, el  espacio requerido para 200 integrados  es 800 KB, mientras que con SVG se  requieren solamente 25.4 KB, un ahorro  total de 774.6 KB que representa un  ahorro porcentual de 3049%. Estas  mediciones de espacio no consideran  tamaño del código escrito utilizando  J2ME que en ambos casos representa 17  KB.   Esta comparación se puede hacer desde  otro punto de vista: si se tiene  disponible 800 KB de espacio para toda  la aplicación, utilizando imágenes PNG  se logran almacenar 200 Integrados a  razón de 4 KB por imagen. Por otro  lado, utilizando SVG se requieren 20  KB para 4 plantillas y un documento de  texto de 780 KB con la distribución de  los pines, esto en conjunto representa  800 KB en memoria, ya que la  información se guarda en el archivo de  texto. En estos 780 KB de texto  alcanzan 70.9 archivos de texto de 11  KB cada uno. Considerando que cada  archivo de texto representa la  información de 200 integrados,  utilizando el número de archivos  mencionado, el catálogo puede contener  un total de 14181 integrados  aproximadamente, 13981 más de lo que  se obtendría usando PNG.   Por otra parte, con respecto al archivo  “.jar” ejecutable del programa, para el  caso de PNG, el archivo tiene un  tamaño de 612 KB, mientras que en  SVG son 37 KB. Es decir hay un  Ahorro Real de memoria de 575 KB, es  decir un 1654 por ciento.     4. Discusión y Conclusiones     El desarrollo de aplicaciones móviles  debe enfocarse a la solución de  problemas cotidianos.     Gracias a la implementación de  imágenes SVG Tiny en aplicaciones  para celulares, es posible no solamente  mejorar la presentación, sino que  también es posible ahorrar un gran  espacio en memoria del celular.     El uso de Netbeans como IDE facilita el  trabajo con aplicaciones móviles gracias  a su ambiente de trabajo amigable.     El ahorro en memoria es importante por  cuanto permite el crecimiento del  número de elementos en un proyecto.  Esto a su vez permite aumentar las  probabilidades para el Usuario de  encontrar lo que desea.     5. Agradecimientos     Este trabajo se pudo llevar a cabo  gracias a la estandarización W3C de  imágenes SVG Tiny investigado por  TAWS y a la Ing. Carmen Vaca por su  ayuda en la redacción de este trabajo.        Figura 2. Aplicación Móvil.	﻿Aplicación Móvil , Catálogo de Circuitos Integrados , Requerimientos de Memoria , SVG	es	20562
33	Caracterización de conjuntos de datos en visualización	﻿ Contar con una taxonomía que clasifique los conjuntos de datos es una guía que asiste a la hora de elegir la técnica de visualización apropiada para determinado conjunto de datos. Las taxonomías de datos existentes en la literatura son presentadas desde un punto de vista estadístico. La importancia de definir una clasificación de los datos orientada a la visualización radica en la necesidad de asistir a los usuarios en la elección de técnicas o estrategias de visualización que se adecúen a sus propósitos. Palabras Claves: visualización, conjuntos de datos, taxonomía, clasificación. 1. Introducción y trabajo previo Dada la diversidad y el voluminoso tamaño de los conjuntos actuales de datos, la tarea de elegir una técnica de visualización adecuada no es sencilla. Un método de clasificación para los diversos conjuntos de datos brinda una primer aproximación en el camino de la elección de la técnica a utilizar. Tamaño Descripción Bytes Diminuto Entra en un pizarrón 102 Pequeño Entra en unas cuantas páginas 104 Mediano Entra en un disquete 106 Grande Entra en un disco rígido 108 Enorme Necesita varios discos rígidos 1010 Tabla 1: Clasificación de Huber Desde un punto de vista estadístico, Huber ([5]) ha planteado una clasificación (Tabla 1) que divide los datos según su tamaño. Posteriormente Wegman ([6]) extendió la clasificación para contemplar *El presente trabajo fue parcialmente financiado por PGI 24/ZN12 y PGI 24/N020, Secretaría General de Ciencia y Tecnología, Universidad Nacional del Sur, Bahía Blanca, Argentina. conjuntos de datos aún más grandes (Tabla 2). Sin embargo, estas dos clasificaciones tienen en cuenta solamente el tamaño en bytes del conjunto de datos y, en general, las técnicas de visualización son computacionalmente aplicables sólo dentro de los conjuntos de datos más pequeños. Una clasificación de los datos basada únicamente en su tamaño, no brinda suficiente información para elegir una técnica o estrategia de visualización acorde a los datos. Tamaño Descripción Bytes ... ... ... Monstruoso Cintas magnéticas 1012 Tabla 2: Extensión de Wegman Por lo dicho anteriormente, es que resulta de suma importancia definir una clasificación de los datos orientada a la visualización. Una clasificación tal debe considerar aspectos adicionales más allá del tamaño en bytes, como por ejemplo cantidad de ítems de datos del conjunto, relaciones existentes entre diferentes ítems o cantidad de atributos, para que de esta forma, cada categoría brinde la información necesaria para estar en condiciones de preferir una técnica de visualización sobre otra. 2. Bases de la clasificación propuesta Una clasificación de datos orientada a la visualización debe tener en cuenta al menos cinco aspectos: cantidad de objetos distintos, cantidad de ítems de datos, cantidad de atributos, cantidad de relaciones, y complejidad de los datos. Cada uno de estos aspectos determina alguna de las características deseables con la que debería contar la técnica más apropiada a utilizar en la visualización de dichos datos. El desafío es encontrar métricas que, no solo permitan evaluar en forma sencilla cada uno de dichos aspectos, sino que también permitan una conveniente clasificación de los datos. Una clasificación debe ser conveniente en el sentido de que cada categoría brinde información relevante y suficiente para la visualización de los conjuntos ahí contenidos. Una medida que refleje la cantidad de ítems de datos permitirá determinar cuán importante es la escalabilidad de la técnica a utilizar. Una medida que refleje la cantidad de atributos dará la pauta de cuán necesaria es una técnica de visualización de datos multidimensionales. Una medida que refleje la cantidad de relaciones dirá cuán necesaria es, por ejemplo, una representación mediante grafos o si con alguna técnica que más simple es suficiente. La cantidad de objetos distintos a representar y la complejidad de los datos determinarán las interacciones que serán necesarias para explorar y analizar los datos. Una taxonomía que permita diferenciar al menos estas características mínimas en los conjuntos de datos podrá dar pautas iniciales para será una gran contribución al momento de Figura 1: Modelo Unificado de Visualización 3. Clasificación en el contexto del MUV El Modelo Unificado de Visualización (MUV [3]) refleja tanto los estados como las transformaciones intermedias que deben atravesar los datos desde que ingresan al sistema de visualización hasta que son finalmente visualizados (Figura 1). El conjunto inicial de datos es el estado de Datos Crudos que, una vez seleccionado qué se quiere visualizar los datos pasan al estado de Datos Abstractos. En este estado se encuentran los datos potencialmente visualizables. Un subconjunto de este último conjuntos representa el estado de Datos a Visualizar, que son los datos que efectivamente estarán en la visualización. Una vez determinado el conjunto de Datos a Visualizar, la transformación de Mapeo Visual determina cómo se van a visualizar los datos: sustrato espacial, elementos visuales y atributos gráficos se emplearán en la visualización, dando lugar al estado de Datos Mapeados visualmente. Como última transformación, se aplica la Técnica (o transformación de Visualización) donde se definen demás elementos extras a la visualización de los datos (luces, colores, etc.) concluyendo en el estado final de los datos, Datos Visualizados o Vista. Esta clasificación de los datos ayudará en el proceso de aplicación de las transformaciones de Mapeo Visual y eventualmente la transformación de Visualización. Dado un conjunto de datos, una vez determinado en qué categoría de la clasificación encuadra más acertadamente, es posible determinar con mayor facilidad qué elementos y atributos visuales son apropiados aplicar en cada caso. 4. Objetivos de la investigación El objetivo de esta investigación es definir una clasificación de los conjuntos de datos orientada a la visualización. Una clasificación de estas características permitirá determinar técnicas de visualización aptas para cada categoría de datos, agilizando el proceso de elección de la técnica adecuada. Para cada categoría se buscarán conjuntos de datos representativos de las mismas, de forma que sea posible evaluar la validez y la efectividad de la taxonomía como soporte en el proceso de visualización.	﻿visualización , conjuntos de datos , taxonomía , clasificación	es	20575
34	Integración de modelos de entornos topográficos aplicada al desarrollo de simuladores de vuelo	﻿Integración de Modelos de Entornos Topográficos Aplicada al Desarrollo  de Simuladores de Vuelo  Alexandra Diehl1, Horacio Abbate1, y Claudio Delrieux2    1CITEFA (Centro de Investigaciones Científicas y Técnicas para la Defensa) -  San Juan Bautista de  La Salle 4397.  Villa Martelli – Tel/Fax: (011)4709-8100/(011)4709-8228 – aldiehl@fi.uba.ar,  habbate@citefa.gov.ar   Financiado por el subsidio PAE2004–22614 ANPCyT (Agencia Nacional de  Promoción Científica y Tecnológica).  2Departamento de Ing. Eléctrica y Computadoras – Universidad Nacional del Sur – Alem 1253, Bahía  Blanca, Argentina –Tel/Fax: (0291)-4595153/(0291)-4595154 cad@uns.edu.ar – Parcialmente  financiado por SECyT-UNS.  1. Introducción  Este proyecto tiene como objetivo la integración de bases de datos y modelos de entornos topográficos  dentro de la tecnología actual de simuladores de vuelo. De esa manera, es posible construir simuladores  de entrenamiento con la capacidad de emular misiones en espacios geográficos reales, fusionar la  información topográfica con otras bases de datos geográficas, etc. Pese a los beneficios evidentes de  esta integración, son muy pocos los desarrollos actualmente publicados en esta dirección. La  representación de un entorno topográfico, realizada de manera de poder ser incorporada dentro del  desarrollo de simuladores de vuelo, implica el manejo de información topográfica en grandes  volúmenes, porque se requiere poder modelar y representar extensas áreas geográficas que permitan la  simulación de actividades aéreas durante horas. Este trabajo reseña las actividades que se está  realizando en el marco del proyecto “Simulador de Vuelo” (PAE2004-22614 ANPCyT) con el objetivo  de realizar tal integración.  Como primer objetivo, se planteó la necesidad de construir un modelo de dominio para representar  entornos topográficos con independencia de su extensión, que sea apropiado para el modelado a escala  global de toda la superficie terrestre y con la capacidad de establecer un sistema de referencia propio  del entorno topográfico, y que permita el modelado de regiones de existencia real o regiones artificiales  y sea optimizado para ser accedido en tiempo real.   Asimismo, se consideró como requerimiento esencial que las soluciones desarrolladas sean coherentes  con los marcos regulatorios y estándares internacionales que rigen la industria y el uso de simuladores  de vuelo, así como las metodologías de diseño, desarrollo y validación de software. Dentro del contexto  del proyecto, se consideró importante lograr un diseño que permita su aplicación, no solo a simuladores  de vuelo, sino a una amplia gama de simulaciones: terrestres, marítimas, fluviales, urbanas, etc.     2. Desarrollo del Proyecto  El proyecto está dividido en tres etapas. La primera etapa abarca el estudio de los modelos de entorno  topográfico existentes, análisis de sus ventajas y limitaciones. La segunda etapa incluye la construcción  de un modelo topográfico propio. Y por último la tercera etapa que considera ensayos sobre el modelo  propuesto, la integración con los demás componentes del simulador, y los ensayos globales sobre todo  el sistema. En este trabajo cubrimos solamente la descripción del trabajo actualmente realizado en  torno a la primera etapa del proyecto, discutiendo en los trabajos futuros los pasos a dar para la segunda  y tercera etapa del mismo.  Durante la primera etapa se realizó un análisis de los modelos de entorno topográfico de los  simuladores de vuelo: “FlightGear 1.0”[3][4] , “X-Plane 7.3”[6], y “Microsoft Flight Simulator  2004”[5]; y el estándar “OpenFlight”[1][2] para armado de escenas para simuladores de vuelo. Los  modelos analizados tienen en común la división de la tierra en una grilla uniforme, y la representación  de entidades orientadas a modelar:  • Información del terreno,  • objetos que pueden adoptar una ubicación dentro del entorno topográfico, por ejemplo edificios,  balizas, radio-faros, etc.,  • entidades orientadas a modelar objetos que por su naturaleza no pueden asociarse con una única  posición, pero sí son objetos que se ubican en el entorno topográfico, como ser carreteras,  límites, costas, etc.  En las Figs. 1, y 2 se pueden observar los distintos modelos de dominio simplificados para cada uno de  ellos.        Fig. 1 Modelo topográfico simplificado de MSFS 2004, y FlightGear 1.0.    Fig. 2 Modelo topográfico simplificado de X Plane 7.3, y OpenFlight Specification 16.3.    Las soluciones estudiadas se diferencian en el enfoque utilizado para modelar las entidades que  conforman el modelo de entorno topográfico. En el caso de Microsoft Flight Simulator, FlighGear y XPlane definen y clasifican a las entidades con una semántica propia que permite diferenciarlos dentro  del modelo. En el caso de OpenFlight el enfoque para modelar la información es genérico y permite  representar a las entidades como una jerarquía de nodos y relaciones entre nodos.  En la Fig. 3 se puede observar un detalle del modelo de entorno topográfico de OpenFlight. Este último  es el enfoque que adoptamos para nuestro modelo.     3. Implementación  Para implementar el modelo de dominio se realizó un análisis de las tecnologías existentes para la  representación de datos geográficos, y también de los estándares existentes OpenGIS para  almacenamiento y transporte de la información. Para la representación de datos geográficos se  analizaron dos alternativas: la construcción de una librería propia para la manipulación y consulta de  los datos geográficos, con un sistema de archivos espaciales indexados e interfaces propietarias; o bien,  la utilización de un DBMS(Database Management System) con soporte para información geográfica.  Los DBMS relacionales con soporte para datos geoespaciales, GIS(Geospatial Information System),  intentan proveer un modelo que soporte datos no geoespaciales como datos geoespaciales, ampliando  las posibilidades de explotación de la información. En el caso de información no geoespacial, con SQL  estándar; y en el caso de información geoespacial con nuevas funcionales para los datos espaciales.    Figura 3 – Detalle del modelo de entorno topográfico de OpenFlight    A continuación figura un resumen de las extensiones que tiene que tener un DBMS relacional para que  permita objetos geoespaciales y sus operaciones. Estás son las áreas importantes que deben cubrir:    • Tipo de Datos: se necesitan de tipos de datos específicos para información geoespacial.  • Operaciones: se necesitan operadores adicionales que manipulen objetos multidimensionales, y  otro tipo de funciones específicas para tipos de datos geoespaciales.  • Entrada/Salida de Datos GIS: para la interoperabilidad de los sistemas, la OGC(Open Geospatial  Consortium) ha especificado como debe ser representado el contenido de los objetos geoespaciales  en forma binaria y de texto. Los DBMS deben cumplir con estas especificaciones.  • Indexado espacial: para poder realizar las consultas sobre datos geoespaciales, en forma eficiente.  • Metadata: metadata específica para contextualizar la información GIS.  • Sistema de coordenadas de Referencia (CRS): los datos geográficos deben tener una referencia  al sistema de coordenadas que se está utilizando para representar la información. El DBMS debe  permitir establecer el sistema de coordenadas utilizado para representar la información.  Se optó por la utilización de un DBMS con soporte GIS para la construcción del prototipo. Dentro de  los DBMS existentes en el área que soportan información geográfica, se analizaron las siguientes  alternativas:  • PostGIS: Extensión GIS para PostGresSQL.  • Extensión GIS para MySQL(opensource): MySQL 4.1  • Extensión GIS para ORACLE(comercial).    Se resolvió la utilización de PostGIS][9] por ser un software libre y abierto, y estar alineado con los  estándares “OpenGIS Simple Feature Specification”. PostGIS permite almacenar objetos geográficos  de acuerdo a los estándares de OpenGIS][7]  y validar que estén correctamente definidos.  Por último se estudió el lenguaje de marcado GML[8] para el transporte de la información geográfica  almacenada en el modelo. Se analizaron los diferentes esquemas de aplicación de GML para mantener  alineado el modelo a los estándares de OpenGIS y se resolvió la utilización de GML profiles para  generar las interfaces del modelo. Tanto la generación de herramientas de diseño, como las  herramientas para la construcción del modelo serán opensource y portables, se implementarán tanto  para la plataforma VisualC++/Windows como para ggc/Linux.  4. Resultados Obtenidos y Trabajo en Curso  Se construyó un modelo propio inspirado en el modelo que propone la especificación OpenFlight, con  contribuciones de los otros modelos de entorno topográfico analizados. Se analizaron los estándares y  tecnologías existentes para manipulación de los datos geográficos, y se resolvió la utilización de un  DBMS objeto relacional alineado con los estándares OpenGIS. Para la segunda etapa se prevé la  realización de tareas de construcción y validación de prototipos, y determinación del conjunto de datos  involucrados en el modelo de entorno topográfico, y la construcción del modelo de dominio definitivo.  Será necesario desarrollar herramientas de diseño y utilitarios para Edición y Testeo de los modelos de  dominio arriba detallados, y su correspondiente representación visual. Estas herramientas deberían  poder ser usadas por personas sin formación informática como: dibujantes, pilotos, cartógrafos, etc.  Para la tercera etapa (etapa final) las tareas estarán orientadas a ensayos, evaluación de rendimiento, e  integración del modelo de entorno topográfico con los demás componentes desarrollados.	﻿Desarrollo de Simuladores de Vuelo , Integración , Modelos de Entornos Topográficos	es	20599
35	Servicios de información y descubrimiento de recursos en una infraestructura grid	﻿ Este artículo trata los conceptos de monitoreo y descubrimiento de recursos, los cuales son de gran importancia en una infraestrutura Grid, ya que juegan un importante rol en varios aspectos relacionados con la misma. Son variados los escenarios en los que es necesario conocer la disponibilidad o el nivel de utilización de un recurso. Como ejemplo, se puede mencionar la utilización de herramientas de diagnóstico que tienen como objetivo detectar comportamientos anómalos en el desempeño de un proceso. Así mismo, también es importante saber qué tipos y cantidad de recursos se encuentran disponibles en un Grid, por ejemplo, para un planificador que necesita obtener ciertos recursos para que una tarea se lleve a cabo. Estos escenarios tienen en común que recurren a uno o varios servidores de información [2], los cuales proveen herramientas para el monitoreo y descubrimiento de recursos que forman parte del Grid y así obtener la información necesaria sobre los mismos. En este artículo se tratará también sobre los trabajos realizados en este área de computación Grid, así como también las posibles líneas de investigación a seguir. Palabras Clave: Computación Grid, Descubrimiento (Discovery), Servicios de Información, Registración. 1. Introducción El crecimiento de Internet y la disponibilidad de poderosas computadoras, redes de alta velocidad y componentes útiles de bajo costo conformando parte de las mismas, está cambiando la forma en que se procesa y el uso de las computadoras en la actualidad [1]. De esta manera, problemas de gran escala pueden ser abordados sin la necesidad de contar de forma local con recursos de cómputo costosos, sino por medio de la suma de los recursos de cómputo disponibles en las redes interconectadas subyacentes al Grid. *Becario de la Comisión de Investigaciones Científicas (CIC) de la Provincia de Buenos Aires, Argentina Dada una aplicación con una especificación de requerimiento de los tipos de recursos que necesita, se denomina descubrimiento de recursos a la búsqueda de los nodos entre los disponibles que cumplen con el criterio especificado [4]. Para ésto es importante contar con algún mecanismo que aporte datos sobre estos recursos de cómputo, así como también otro tipo de recursos necesarios para el desempeño de una tarea, sin la necesidad de tratar directamente con la locación de los mismos. Además, como se menciona anteriormente, esta información no está únicamente destinada a la alocación de recursos para la ejecución de tareas, sino que también puede ser utilizada en beneficio de la performance del Grid, detectando los nodos que se encuentran sobrecargados, y migrando algunas tareas de estos nodos a otros menos saturados. Esta tarea de determinar el nivel de ocupación de uno o más recursos, así como también detectar cambios importantes en sus estados, se denomina comúnmente monitoreo de recursos. Estas necesidades pueden verse cubiertas por la presencia de servicios de información, los cuales a través de colecciones de índices organizados de forma descentralizada, mantienen información relativa a los atributos de los distintos tipos de recursos que forman parte de un Grid. Los servicios de información están constituidos por dos tipos de componentes [2]: • Una gran colección distribuida de proveedores de información que permiten el acceso a información sobre recursos. • Servicios de más alto nivel que se encargan de obtener la información proveniente de los proveedores de información. Entre este tipo de servicios, se distinguen los servicios de directorios de agregación que facilita el descubrimiento y monitoreo a través de la implementación de vistas genéricas y/o especializadas. Las interacciones entre los servicios de alto nivel (o los usuarios) y los proveedores son definidas en términos de dos protocolos básicos: un protocolo de registración para identificar a las entidades participantes de los servicios de información, y un protocolo de consulta para obtener información sobre esas entidades. En esencia, los proveedores usan la registración para notificar a un servicio de más alto nivel sobre su existencia y estos últimos usan las consultas para obtener información sobre las entidades conocidas por un proveedor y unirlas en una vista que agrupe todas las entidades conocidas por todos los proveedores registrados en el servicio. Dado que los recursos y entidades integran y dejan el grid de forma dinámica, en algunos sistemas de información [8, 10] se establece un mecanismo de registración con tiempo de vida, en el cual los proveedores de información deberán renovar su registración de forma periódica para informarle a los servicios de más alto nivel que todavía se encuentran disponibles. A su vez, estos últimos pondrán estampillas de tiempo a las entradas de registración tanto en la primera registración de los proveedores como en sus sucesivas renovaciones de forma que, cada cierto período establecido con los mismos, se recorra esta lista de entradas y aquellas que no han renovado su registración sean removidas de la lista. Con este mecanismo se asegura que aquellas entidades que quedan fuera del grid, ya sea por propia voluntad o por una caída de nodo o de red, no figuren como disponibles, pudiendo volver a registrarse cuando lo deseen o, en el caso de una caída, cuando vuelvan a estar en línea. Una desventaja de este mecanismo es que los servicios que mantienen las registraciones no pueden distinguir cuando no llegan los pedidos de renovación dentro del período establecido debido a un problema de congestión de red e interpretan la falta de renovación como una ausencia de la entidad o el recurso correspondiente a la entrada de registración, eliminándola de la lista. Las sucesivas secciones de este trabajo se organizan de la siguiente manera: en la sección 2 se presentan distintos sistemas de información utilizados en Grid que hacen uso de estos conceptos presentados. En la sección 3 se exponen las conclusiones pertinentes y las posibles líneas de investigacíon a explorar en este área. 2. Trabajos realizados en el área Dada la importancia de contar con sistemas de información, existen varios trabajos realizados en el área. Entre los sistemas que podemos destacar está MDS (Monitoring and Discovery System), el sistema de información provisto por Globus Toolkit. En su version MDS2 [2], este sistema utiliza el protocol LDAP (Lightweight Directory Access Protocol) para la registración de recursos así como también para las consultas. Este sistema está conformado por 2 tipos de componentes: GRIS (Grid Resource Information Services) que actúan como proveedores de información haciendo de interfaz con los recursos, y GIIS (Grid Index Information Services) que son los servicios de más alto nivel que se encargan de recolectar la información de los GRIS y de otros GIIS, pudiendo conformar, de esta manera, una estructura jerárquica. La evolución del mismo utiliza Servicios Web para la comunicación entre proveedores de información y servicios de más alto nivel (IndexService). A su vez agrega un servicio TriggerService que ofrece una funcionalidad similar a la de IndexService con la diferencia de que puede ser configurado para tomar determinadas acciones en base a la información recolectada. Globus puede ser configurado para que la información que manejan estos servicios pueda ser provista por otros sistemas de información externos al tookit. La última version de MDS, es la que corresponde a la version 4 de Globus Toolkit [3] (MDS4), que utiliza WSRF (Web Service Resource Framework) como plataforma para los Servicios Web utilizados, en oposición con MDS3 que utiliza OGSI (Open Grid Services Interface). Otro sistema de información muy utilizado para el monitoreo de clusters es Ganglia [6], que está basado en un diseño jerárquico. Ganglia utiliza un protocolo en el que, a través de multicasting, un nodo envía su información a los otros nodos del cluster al que pertenece, a su vez también procesa la información proveniente de los otros nodos. De esta manera, cualquier nodo del cluster podría responder ante una solicitud por parte de un cliente y devolver una descripción sobre el estado de todo el cluster. La implementación consiste de 2 demonios (daemons), gmond y gmetad, un programa de línea de comando gmetric y una librería de programación para clientes. Ganglia Monitoring Daemon (gmond) provee monitoreo sobre un cluster implementando el protocolo mencionado anteriormente y responderá a los pedidos de clientes devolviendo un XML con la representación del cluster. gmond corre en cada nodo del cluster. Ganglia Meta Daemon (gmetad), por otra parte, sólo corre en algunos nodos y provee federación de clusters : Se establece un árbol de conexiones TCP entre múltiples gmetads, algunos de los cuales obtendrán información provenientes de gmonds (representando clusters específicos) y otros la obtendrán de otros gmetads (representando conjunto de clusters). Por último, gmetric es un programa que las aplicaciones pueden usar para publicar métricas específicas de una aplicación que no estén contempladas por gmond. Además existe una herramienta desarrollada por el equipo de Globus Toolkit para la integración de Ganglia al sistema de MDS, usando el primero como un proveedor de información y permitiendo la publicación a través del IndexService de datos como nombre y ID de los hosts, tamaño de la memoria, nombre y versión del sistema operativo, datos sobre sistemas de archivos y otros datos básicos referidos al cluster. Otro sistema importante para mencionar es MonALISA (MONitoring Agents using a Large Integrated Service Architecture, [8]). Este sistema está conformado por un conjunto de subsistemas autónomos y multihilados basados en agentes que son registrados como servicios dinámicos y son capaces de colaborar en aplicaciones de gran escala realizando tareas de monitoreo, y pueden ser descubiertos y utilizados por otros servicios o clientes que requieran su información. Su implementación está basada en Java JINI y servicios web, y está diseñado para integrar fácilmente herramientas de monitoreo existentes y proveer esta información en forma dinámica a otros servicios y clientes. El núcleo del servicio de monitoreo está basado en un sistema de múltiples hilos (threads). Con el fín de reducir la carga del sistema en que corre MonALISA, se crea un repositorio dinámico de hilos una única vez, y los hilos son reutilizados una vez que la tarea asignada al hilo es terminada. Esto permite correr concurrentemente un gran número de tareas de monitoreo y que el sistema no se vea retrasado o afectado de alguna manera, en caso de que una de ellas falle debido a errores de I/O. Por último, tenemos otros casos que hacen integración de algunos de estos sistemas con herramientas del área de sistemas Peer-to-Peer (P2P) [4, 5, 7, 9, 11]. Una de las herramientas más utilizadas en estos sistemas son las Tablas de Dispersión Distribuidas (DHT, por su sigla en inglés). Por ejemplo, gracias a estas tablas es posible dispersar las entradas correspondientes a los índices de registración entre los peers (pares) participantes del sistema haciendo una división del espacio lógico determinado por los atributos de los recursos a monitorear. De esta forma, se evita posibles problemas de saturación en los servidores de información a la vez que se provee de robustez y escalabilidad, propiedades provistas por los sistemas P2P ya que cubren un espectro distribuido muy amplio. 3. Conclusiones y trabajos futuros En este artículo se ha expuesto la necesidad de servicios de información en una infraestructura Grid, con el fin de contar con herramientas para el monitoreo y descubrimiento de recursos, objetivo necesario en la mayoría de las aplicaciones que puedan llevarse a cabo en la misma. Las opciones son variadas como se pudo observar en la sección 2, destacando que no necesariamente estas tecnologías son excluyentes, sino que es posible combinarlas para optimizar el sistema de información a implementar en un Grid. Además muchos de estos sistemas proveen librerías de programación con lo cual se simplifica la tarea de acceder a los mismos desde una aplicación en desarrollo. Es por esto, que en trabajos futuros se explorarán las posibilidades de integración de estos sistemas con el fin de experimentar y llevar la idea a la práctica sobre uno de los clusters con los que cuenta el LISiDi. Como objetivo complementario, se investigará sobre la aplicación de técnicas provenientes del área de Sistemas Peer-to-Peer con el fin de proveer escalabilidad y descentralización en el manejo de la información.	﻿Servicios de Información , Computación Grid , Descubrimiento , Registracón	es	20651
36	Hacia la modelización del aprendizaje con tecnología móvil	﻿    La educación actual y los ambientes de aprendizaje interactivos han tenido su mayor punto de  convergencia gracias a los avances de la tecnología personal, “la que está al alcance de la mano”; por  ejemplo, el computador portátil y computadores de bolsillo también conocidos por su sigla en inglés  PDA o Personal Digital Assistant, más la expansión de conexiones inalámbricas. Debido a la  flexibilidad de las plataformas de m-learning es posible integrar nuevas teorías instruccionales y  didácticas especiales, con las clásicas. Por ejemplo, una de las más conocidas, denominada estrategia  instruccional, se aplica en la creación de plataformas móviles, combina teorías cognitivas con las  técnicas de mapeo mental (en inglés mind maping). En función a lo expuesto, en este trabajo se  abordará el estudio de estrategias educativas para el desarrollo de educación con tecnología móvil, con  la finalidad de definir lineamientos pedagógicos - didácticos para el diseño de ambientes de educación  móvil. Finalmente, se quiere analizar distintas teorías de aprendizaje que puedan aplicarse en la  educación móvil y definir desde ellas, lineamientos a tener en cuenta en el diseño de sistemas de  aprendizaje con PDA.  Palabras clave: tecnología móvil, educación móvil, teoría instruccional, diseño educativo.       ABSTRACT    The major convergence of actual education and interactive learning environme nts have been pointed  where personal technology advances today, so it is called the “technology by hand”; for example,  handheld learning devices named PDA or Personal Digital Assistants more the enlargement of wireless  communication are examples of this technology.   Due to the flexibility of mobile learning platforms, now it is able to integrate new instructional theories  with traditional ones. For example, the instructional theory used for the design of mobile platforms is a  best recognized one; it studies cognitive theories with mind maping techniques.   In this work, educational strategies will be studied to develop mobile education environments, in order  to define pedagogical and didactic baselines for the design of mobile learning environments. Finally,  different learning theories will be evaluated to identify which one could be applied in mobile education,  and also promote pedagogic considerations for the design of mobile learning environments.        Keywords : mobile technology, mobile learning, instructional theory, educational design.     1. INTRODUCCIÓN  La tecnología PDA en la educación crea nuevos ambientes de aprendizaje que posibilita la integración  de mapas de conocimiento visual, bancos de imágenes y establecer contacto con profesores o expertos  en la materia de estudio en cualquier momento y lugar. Uno de esos ambientes, es el aprendizaje móvil  o m-learning que disminuye la brecha entre la clase tradicional y la clase a distancia, sin barreras de  espacio ni de tiempo. El objetivo de m-learning en educación, radica precisamente en su capacidad de  estar disponible dónde y cuando se necesite. Desde su creación por Jeff  Hawking, se asoció a los  sistemas móviles de comunicación con el funcionamiento de la inteligencia y el aprendizaje basado en  el cerebro [11].   La fusión entre la tecnología móvil y la educación formal tiene origen en la teoría descripta por [8]  llamada Teoría de Presentación de Componentes (Component Display Theory CDT). Esta teoría ha sido  ampliamente aplicada a los programas de enseñanza basados en computadores o por medios  electrónicos, como el proyecto educativo denominado Time-shared Interactive Computer Controlled  Information Television, bajo las siglas TICCIT [13]. Además debido a la flexibilidad de las plataformas  de m-learning es posible integrar nuevas teorías instruccionales y didácticas especiales, con las  clásicas. Por ejemplo, una de las mas conocidas, denominada estrategia instruccional, se aplica en la  creación de plataformas móviles, combina teorías cognitivas con las técnicas de mapeo mental (en  inglés mind maping) [9].     En función de lo expuesto en este trabajo , se abordará el estudio de estrategias o perspectivas   educativas para el desarrollo de educación con tecnología móvil, con la finalidad de definir  lineamientos pedagógicos - didácticos para el diseño de ambientes de educación móvil. Finalmente, se  quiere analizar distintas teorías de aprendizaje que puedan aplicarse en la educación móvil y definir,  desde ellas, lineamientos a tener en cuenta en el diseño de sistemas de aprendizaje con PDA .  Este documento se estructura del siguiente modo: en la sección dos se explican definiciones y se  referencian antecedentes de la educación móvil; en la sección tres se presentan modelos educativos que  introdujeron características en la educación móvil. En la sección cuatro se presentan las estrategias  educativas propuestas para el diseño de mobile learning. Finalmente en la sección cinco se redactan las  conclusiones.    2. ANTECEDENTES EN LA EDUCACIÓN MÓVIL    El aprendizaje móvil puede ser definido de diversas formas. Para algunos, el “aprendizaje móvil” se  distingue por PDAs y teléfonos celulares; para otros iPODs y reproductores de medios; y aún también  como cámaras digitales y puertos USB. No obstante, la mayoría de las personas asocian aprendizaje  móvil con los últimos gadgets  (dispositivos) portables. Esto hace que la visión de aplicar aprendizaje  móvil parezca “temeroso”  para algunos educadores, y quizás bastante “atractivo” para otros.  De todos modos, en contraste  con tales visiones, en [5] se identifica el aprendizaje móvil como un  subconjunto del e-learning o “aprendizaje electrónico” ; mientras que [9] sostiene los vínculos del  aprendizaje móvil debido al modelo “suficiente, justo a tiempo, y solo para mí” del aprendizaje flexible  mencionado en [10]; y se respalda a una de las salidas reflexivas del proyecto 2004 European  MobiLearn que dice:” es el usuario estudiante quien es móvil, no la tecnología” [18].  De los numerosos proyectos de educación móvil han surgido modelos de procesos de diseño, basados  en cómo las personas piensan, aprenden, perciben, trabajan e interactúan con respecto a un tema     específico. Uno de los más utilizados es el modelo de Engeström [4], que evalúa el desarrollo  tecnológico en un ciclo de diseño interactivo.  Por otra parte [14], describe los requerimientos generales que debe tener una aplicación m-learning  como:  • Desarrollar un programa con alta capacidad de movilidad.  • Realizar un diseño para asistir el aprendizaje personalizado.   • Lograr disponibilidad, adaptabilidad, persistencia y utilidad de la aplicación.  • Buscar el resguardo de fuentes de conocimiento, a pesar de cambios de la tecnología.  • Desarrollar una interfase amigable, de fácil usabilidad personal y que no requiera experiencia en   tecnología.    En el campo de las teorías que sustentan los proyectos de desarrollo educativos , se pueden diferenciar  dos claras tendencias. La primera corresponde  a las teorías educativas generalizadas en amplios campos  filosóficos, psicológicos, ontológicos y epistemológicos, que han sido desarrolladas desde los años  cincuentas mucho antes del advenimiento de la teleducación. La segunda, son denominadas teorías  instruccionales, que como mínimo involucran tres partes: conceptos, definiciones y proposiciones [12],  o mejor como se expone en [12], deben tener tres partes básicas como son (en su orden): pronóstico,  métodos y condiciones. Estas teorías son más susceptibles al cambio, más puntuales y claras con  respecto al desarrollo de programas de educación móvil. Las plataformas de mobile learning (mlearning) son tan flexibles que permiten integrar nuevas teorías instruccionales con las clásicas. Una  estrategia instruccional en la creación de plataformas móviles, consiste en aplicar teorías cognitivas con  las técnicas de mapeo mental (en inglés mind maping) [9].  Hace cuatro años [17] describió la llamada teoría COLL (de las siglas en inglés Contextual life-long  learning), que es una reconceptualización del aprendizaje, donde el conocimiento es entendido como  una enseñanza que no está sujeta a un lugar o sitio dentro del desarrollo profesional o vocacional, y que  pasa a ser un proceso dinámico, individual e interactivo de los estudiantes.  Desde el punto de vista pedagógica, la teoría CDT  se fundamenta en una estructura cognitiva compleja  y sofisticada; sin embargo, se puede concluir que se sustenta en dos principios básicos: el contenido y  el desempeño. Dentro del primero se incluyen los hechos, conceptos, procedimientos y principios,  mientras que en el segundo se ubican las generalidades y las aplicaciones. Según su propio autor, la  CDT tiene como principio fundamental que “una instrucción que no enseña, no tiene valor”.     En 1994, [8] presentó una nueva versión de la teoría llamada Component Design Theory, donde pone  mayor énfasis en las estructuras y algoritmos del aprendizaje que en lecciones escritas o magistrales.  Esta teoría  representa una contribución importantísima al campo de la educación tecnológica; se  muestra como el primer intento de separar la estrategia educativa de su contenido, con lo que  el  estudiante puede seguir su propia manera de elaborar conocimientos a partir de reglas básicas.  La Teoría del Uso [6] es un análisis de la educación que puede informar sobre el diseño de un sistema  operat ivo desde el punto de vista pedagógico, cognitivo y social, y que se puede resumir como las 3 C’s  de construcción, conversación y control. Estos conceptos se definen como:  · Construcción: por ser un proceso de elaboración que trae soluciones a problemas relacionados  con nuevas experiencias del conocimiento [3].  · Conversación: por que es el método de cuestionamiento de conceptos de aprendizaje que se  expresan con profesores o con otros estudiantes [9].  · Control: donde se realiza un ciclo de experimentación y reflexión activa de los procesos de  aprendizaje [6].     3. MODELOS EDUCATIVOS  CON TECNOLOGÍA MÓVIL    El proyecto Classroom 2000 del Instituto de Tecnología de la Universidad Georgia , según [1],  desarrolló una tecnología educativa móvil que permite a los estudiantes leer las diapositivas del  profesor directamente sobre sus PDA’s en tiempo real. Otro proyecto desarrollado en la Universidad de   Birmingham, es el llamado Handheld Learning Resource (HandLeR) [15], en el que personas de todas  las edades pueden desarrollar una educación personal a través de la experiencia a lo largo de sus vidas.  Dicho proyecto fue aplicado en el área de  radiología, en donde una residente de radiología en su primer  año de  entrenamiento en neuroradiología revisaba los casos del día de resonancia magnética a través de  su PDA o Tablet PC; por medio de una red inalámbrica con posibilidad de selecciona r casos y hacer  reportes o descripciones previas, que podían compararse con las descripciones de los fellows o  profesores, y capacidad de revisar casos después del trabajo con otros colegas, o solicitar aclaraciones  al profesor para el siguiente día con el tutor de Resonancia Magnética [16].    Según [21], los componentes de un software educativo multimedia móvil son:  § El contenido que debe ser enseñado;  § Los elementos mediales usados para proporcionar la información;  § La interfase de usuario, como la forma en que el software educativo se presenta al usuario;  § Los dispositivos de interacción, a través de los que el usuario interactúa con la computadora,  haciendo elecciones, respondiendo a preguntas o desarrollando actividades, y proporcionando  feedback sobre todas ellas;  § La estrategia instruccional adoptada;  § El acceso que hace referencia a los caminos de navegación, por los cuales el usuario puede  solicitar el contenido de su interés ;  § La plantilla de presentación que proporciona una guía para definir las estrategias de  representación del contenido, y las operaciones visibles para el usuario;  § La operación del sistema que no es visible para el usuario, pero es esencial para la construcción  del requerimiento que demanda el mismo.    Existen proyectos como EngageMe (http://www.engageme.net/), y Australian Flexible Learning  Framework New Practices 2004 (AFLF 2004) que usan dispositivos portátiles en horticultura, más un  rango de proyectos  de aprendizaje móvil que se desarrollan en Europa (Ultralab y CTAD 2005) [18].  Estos tienden a la enseñanza diaria más allá de la institución, y reconoce al alumno como el “ciudadano  del aprendizaje” [18].    4. ESTRATEGIAS EDUCATIVAS PARA EL DISEÑO DE APRENDIZAJE MÓVIL    En función de lo expuesto, se presentan a continuación estrategias educativas para el desarrollo de  educación con tecnología móvil, con la finalidad de definir lineamientos pedagógicos - didácticos para  el diseño de ambientes de educación móvil:  · Una de las estrategias pedagógicas posibles de utilizar en un ambiente virtual es la de  scaffolding, que surge desde la teoría social de Vigotsky [7]. Aunque ya ha sido aplicada en los  últimos años, en las áreas de aprendizaje basado en la web, se puede trabajar sobre el rol y los  mecanismos de interacción estudiante-tutor orientador, más la búsqueda de conocimiento  autorregulado por el alumno. De este modo, se propone  un desafío en la confección del modelo  del alumno (como usuario) y su interacción con el tutor (también usuario), en etapas tempranas  del ciclo de vida del software, como lo es el diseño de usuarios.     · Otra estrategia pedagógica que se ha estudiando en los últimos años, en su interacción con la  tecnología, son las múltiples representaciones externas (MER) y la forma en que ellas mejoran  el aprendizaje ; no se han observado estudios que relacionen su utilización en ambientes  virtuales , en lo que concierne a que el alumno arme su propio material de estudio [2].   Se propone contemplar esta estrategia en la definición de los requisitos no funcionales del  producto educativo, a modo de lograr calidad del proyecto; se debe disponer para ello, una base  de conocimiento donde los requerimientos no funcionales del proyecto educativo sean  priorizados a través de las perspectivas de los stakeholders, en este caso, el usuario y el tutor.  · La teoría de la Flexibilidad Cognitiva [19] quiere demostrar que el uso del hipertexto puede  fomentar no sólo el aprendizaje de contenidos, sino también el pensamiento crítico y la  autorreflexión sobre la naturaleza del aprendizaje. Esta teoría sostiene que el tratamiento de un  tema complejo no puede limitarse a una sola dirección, porque entonces se generará un sistema  relativamente cerrado, con muy poca flexibilidad y con muchas posibilidades de generar  concepciones erróneas. Es necesario recurrir al hipertexto que diseñado en forma apropiada,  puede ser suficiente y mucho más eficaz para transmitir áreas complejas de conocimiento que  un texto lineal.  Entonces, el papel de estos sistemas debe orientarse a facilitar o promover procesos de   pensamiento y el uso de estrategias de aprendizaje cognitivas y metacognitivas, pero también se  debe cuestionar la determinación de enseñar únicamente destrezas cognitivas.    Se propone investigar sobre qué principios y métodos subyacen a estos sistemas, como  desarrollar la autorregulación del proceso de aprendizaje en los alumnos, cómo se puede evitar  el sentimiento de desorientación del usuario, qué procesos cognitivos y metacognitivos  subyacen a estos procesos, cómo diseñar tareas efectivas, etc. todo ésto destinado a mejorar el  rendimiento de sistemas hipermediales.  Finalmente, la Teoría de la Flexibilidad Cognitiva [19] permite estudiar qué grado de control  puede llegar a tener el usuario sobre el entorno de aprendizaje, y cómo influye la interfaz del  usuario y el modo de presentación de la información en el usuario.  · La ingeniería socio-cognitiva [15] es un enfoque coherente para describir y analizar las  interacciones complejas entre los usua rios y la tecnología basada en computador, así como para  informar el diseño de sistemas socio-técnicos  (la tecnología en su contexto social y técnico). Se  extiende a trabajos previos que incorporan ingeniería de software, ingeniería de tareas,  ingeniería de conocimiento e ingeniería organizacional, todos ellos con una base pedagógica e  interdisciplinaria.  · Un agente pedagógico virtual inteligente (APVI) es una personificación para la figura del tutor,  que instruye, guía y define las estrategias pedagógicas a aplicar, en un ambiente virtual de  aprendizaje. Las bases en que se apoya su desarrollo están en los sistemas tutoriales inteligentes  (STI), los entornos virtuales (EV), la pedagogía y la psicología [4]. Se propone realizar una  simulación para evaluar el efecto de la personificación del tutor y de su comportamiento, en el  momento de supervisar al estudiante; se busca averiguar el efecto que esto tiene en el proceso  de aprendizaje, y determinar con que forma de visualización del tutor el estudiante se siente más  cómodo en le momento de realizar las tareas encomendadas dentro del entorno virtual. La  visualización del tutor se refiere a tres variaciones del agente: la ausencia visual del tutor  dejando sólo mensajes textuales ó de audio; la presencia del tutor siempre a una cierta distancia  del estudiante, es decir la presencia del tutor más próxima al lugar en el que está trabajando el  estudiante. Finalmente, visualizar al tutor como un icono minimizado, siempre disponible  cuando se lo requiera.     5. CONCLUSIONES    Es necesario, perspectivas teóricas de aprendizaje  que aporten recursos para el diseño instructivo de  dominios del conocimiento, más hoy en día en que los sistemas hipermediales son desarrollados como  proyectos educativos para la transferencia de conocimiento. La investigación sobre la adquisición de  conocimiento en hipertexto e hipermedia es compleja, pero los intentos por  demostrar su validez en los  procesos de aprendizaje comienzan a cobrar importancia. Por lo mencionado, se presentó en este  trabajo perspectivas con base pedagógica, a tener en cuenta en la etapa de diseño de un proyecto  educativo de mobile learning, atendiendo a la figura de los usuarios, como estudiante y tutor. Se espera  contribuir al mejoramiento del aprendizaje de un dominio de conocimiento, a través del uso de  tecnología móvil, para generar un entorno lo más natural posible pa ra el estudiante, y basado en  estrategias pedagógicas e instruccionales.	﻿teoría instruccional , tecnología móvil , educación móvil , diseño educativo	es	20674
37	La Educación Basada en la Web y la tecnología móvil	﻿   Se presenta en este trabajo una línea de investigación inscripta en el proyecto trianual: Educación Basada en la Web.  Confluyen en ella los  estudios y desarrollos sobre e-learning  llevados a cabo en los últimos veinte años en el LIDInE  (Universidad Nacional del Sur, Argentina) e investigaciones sobre tecnología móvil que se vienen realizando en el  Laboratorio de VERIMAG, de la Universidad Joseph Fourier en Grenoble, Francia.     1. Significado de la investigación    Esta línea de investigación está basada en dos ejes: la Educación Basada en la Web (EBW) y la  movilidad de los actores, entendiendo por actores a los que intervienen en todo el proceso de  aprendizaje: los autores o profesores, los alumnos y el medio electrónico que brinda la posibilidad  de comunicación y encuentro. El objetivo será brindar pautas tanto educacionales como  tecnológicas para alcanzar la construcción de conocimientos y el desarrollo de competencias.   El aprendizaje y la enseñanza basados en la Web introducen nuevas variaciones en los modelos o  supuestos de la educación formal. El aprender a aprender, las comunidades de aprendizaje, la  formación continua, el aprendizaje autónomo, la promoción de un auténtico interés en el alumno y  el aprendizaje solidario han adquirido relevancia notoria. A ello se agrega la cognición e  información situada, así como la inteligencia distribuida, procesos que permiten que solidariamente  se aborde la identificación de problemas y el planeamiento y ejecución colectiva de las opciones  más productivas de solución a los mismos.  Todo ello presiona para la definición de un nuevo  paradigma educativo en el que las Ciencias de la Computación y las Ciencias de la Educación  tienen mucho que aportar.   Permanentemente se buscan superaciones tecnológicas para poder diseñar y fundamentalmente  mantener ambientes de aprendizaje personalizados, con contenidos y materiales apropiados para las  aspiraciones y necesidades de cada uno. Para que ello sea posible quedan aún muchos problemas  por resolver, entre otros los relacionados con la evaluación, con el seguimiento de los aprendices,  con la relación número de estudiantes vs. número de tutores, con la organización conceptual de los  contenidos y con su contextualización en la red.   Desde otro punto de vista, la EBW permite que los 'alumnos' adquieran mucha más movilidad y  libertad; el hecho de no tener que 'asistir' a un sesión educativa a determinada hora y lugar, permite  la elección del mejor momento para aprender. Este simple hecho, también extiende la educación  general a un nuevo publico: aquel que quiera adquirir una formación o perfeccionamiento sin las  restricciones de la enseñanza tradicional, probablemente un público adulto que ya haya pasado 'la  edad tradicional de ser estudiante' pero que está obligado, por las leyes del mercado laboral, a  aprender.  La movilidad, otrora física y actualmente física y virtual, es un principio básico de la universalidad  de la educación. Desde tiempos inmemoriales, los grandes estudiosos se trasladaban de una ciudad a  otra para imbuirse de su cultura. En tiempos actuales, se han creado innumerables programas para el  intercambio de estudiantes que promueven el traslado físico de los aprendices a otra entidad  educativa. Actualmente, también se proponen programas de intercambio 'virtual' entre instituciones.  De este modo, un estudiante de una universidad  puede 'cursar' virtualmente en otra entidad, bajo un  acuerdo de cooperación. Sin duda, la visibilidad y el atractivo de la educación superior pasa por la  internacionalización de las universidades. Sin embargo el futuro no muy lejano nos depara nuevos  desafíos: la educación móvil, apoyada en toda la nueva tecnología de comunicación a un precio  accesible.   En los próximos dos años los costos de los teléfonos móviles con conectividad y verdaderos  sistemas operativos serán sin dudas el estándar de los celulares. El desafió inmenso de la  incorporación didáctica de estos dispositivos a la educación va mas allá incluso de los planes OLPC  (one laptop per child) Los teléfonos móviles ya están en las aulas en sus versiones actuales. Pocas  son las oportunidades donde escuchamos hablar de la aplicación didáctica de los mismos y muchas  acerca de represiones sobre su uso. Desde grabación de las clases en mp3, hasta trabajos con  imágenes y filmaciones, y narrativas con restricción retórica en 140 caracteres son alguna de las  oportunidades que se nos presenta con lo ya existente como promedio, de 2 GB de memoria.   Otra tecnología que se incorpora es el uso de i-pod y el nuevo modelo de podcasting, un fenómeno  que está revolucionando el mundo de la comunicación radiofónica. Un podcast es un archivo de  audio, normalmente gratuito, que aparece colgado en Internet con cierta periodicidad y que puede  ser reproducido en una computadora o en cualquier momento en algún aparato reproductor. Su  contenido abarca todas las áreas, desde noticias, música o poesía, hasta lecciones de inglés o  conferencias. Inicialmente, se refería exclusivamente a emisiones de audio, aunque este concepto se  ha ampliado ya a contenidos multimediales, de video y audio. Este tipo de archivos pueden ser  escuchados en un ipod y son de suma utilidad ya que liberan aún más al usuario, ya que no es  necesario estar conectado físicamente a una computadora para 'aprender'.  Desde un punto de vista más tradicional, existen plataformas para el e-learning de gestión de  comunidades, que ofrece herramientas de comunicación sincrónica y asincrónica: foros,  calendarios, noticias, etc. Estas plataformas normalmente residen en un lugar físico, mientras que  los actores se conectan remotamente; es decir que la comunicación es central para el éxito del  proceso de aprendizaje. Este es un nuevo elemento de análisis que obliga a prever acciones en caso  que la comunicación no sea factible.    2. Nuevo Espacio Social     Las nuevas TICs han posibilitado la creación de un nuevo espacio social para las interrelaciones  humanas. Este nuevo espacio se suma a los dos que ya existían: el rural y el urbano, y se aplica a  todos los órdenes de la vida cotidiana (desde pagar facturas por internet, compras de productos,  lectura de la prensa). En nuestro caso particular, nos interesa marcar la existencia de un nuevo  espacio educativo, diferente de los espacios preexistentes. La emergencia de este espacio tiene  particular importancia ya que:   posibilita nuevos procesos de aprendizaje y transmisión del conocimiento a través de las  redes telemáticas.    para ser activo en el nuevo espacio social se requieren nuevos conocimientos y destrezas que  habrán de ser aprendidos en los procesos educativos.    las instituciones educativas y la formación en general necesitan ser redefinidos en el nuevo  espacio social. Se requiere crear un nuevo sistema de centros educativos, a distancia y en  red, así como nuevos escenarios, instrumentos y métodos para los procesos educativos.    Por estas razones básicas, a las que podrían añadirse otras, hay que replantearse profundamente la  organización de las actividades educativas, implantando un nuevo sistema educativo en este   entorno. Este nuevo espacio social tiene una estructura propia, a la que es preciso adaptarse. “Se  trata de un espacio telemático, cuyo eje central actual es la red Internet, no es presencial, sino  representacional, no es proximal, sino distal, no es sincrónico, sino multicrónico, y no se basa en  recintos espaciales con interior, frontera y exterior, sino que depende de redes electrónicas cuyos  nodos de interacción pueden estar diseminados por diversos países. De estas y otras propiedades se  derivan cambios importantes para las interrelaciones entre los seres humanos, y en particular para  los procesos educativos”, (Echeverría, 2000).   Además este nuevo entorno no sólo es un nuevo medio de información y comunicación, sino  también un espacio para la interacción, la memorización, el entretenimiento y la expresión de  emociones y sentimientos. Precisamente por ello es un nuevo espacio social, y no simplemente un  medio de información o comunicación. Cada vez se requerirá un mayor grado de competencia para  actuar eficientemente en él. Por ello es preciso diseñar nuevos escenarios y acciones educativas, es  decir, proponer una política educativa específica para este entorno. Aunque el derecho a la  educación universal sólo se ha logrado plenamente en algunos países, motivo por el cual hay que  seguir desarrollando acciones de alfabetización y educación en el entorno clásico, lo cierto es que la  emergencia de este nuevo espacio exige diseñar nuevas acciones educativas para todo el mundo  No sólo se trata de transmitir información y conocimientos a través de las TICs, sino que, además,  hay que capacitar a las personas para que puedan actuar competentemente en los diversos  escenarios de este entorno. La interacción con las TICs como una habilidad básica del mismo nivel  que las competencias instrumentales (lectura, escritura y cálculo), ayuda a concretar los ámbitos o  dimensiones de la competencia, las acciones educativas, y sus medios organizativos y  metodológicos para alcanzarla. Por ello, además de aplicar las nuevas TICs a la educación, hay que  diseñar ante todo nuevos escenarios educativos donde los estudiantes puedan aprender a moverse e  intervenir en el nuevo espacio telemático. El acceso universal a esos escenarios y la capacitación  para utilizar en forma competente las nuevas tecnologías se convierten en dos nuevas exigencias  emanadas del derecho a que cualquier ser humano reciba una educación adecuada al mundo en el  que vive.   Las TICs están transformando profundamente las sociedades contemporáneas, y en particular los  procesos educativos. Las redes telemáticas son protagonistas de este cambio, aunque hay otras  muchas tecnologías involucradas. Las siete tecnologías a tener en cuenta son el teléfono, la radio y  la televisión, el dinero electrónico, las redes telemáticas, las tecnologías multimedia, los  videojuegos y la realidad virtual. A los efectos educativos, las cuatro últimas son las más  relevantes, junto a la televisión y el celular. A excepción de este ultimo, los otros medios han sido  estudiados para ver sus  efectos en los procesos educativos en el marco de las enseñanzas no  regladas.   Los pedagogos suelen hablar de educación para los medios, de alfabetización audiovisual y de  alfabetización informativa. Si bien es cierto que la televisión, los videos, las computadoras y los  soportes multimedia son nuevos medios educativos, la incorporación de las TICs suscita un cambio,  ya que no sólo es un nuevo instrumento docente, sino que afecta a la estructura del espacio social y  educativo. Estas tecnologías posibilitan la construcción de este nuevo espacio social, cuya  estructura es muy distinta a la de los otros dos (el natural y el urbano) en donde tradicionalmente se  ha desarrollado la vida social, y en particular la educación.  Las TICs, basadas en la microelectrónica, la informática y las redes de comunicación, constituyen  en la actualidad un factor de transformación del mismo modo que en su momento lo fueron  la  imprenta o la máquina de vapor. Las TICs se materializan en numerosos dispositivos y programas  que van desde las computadoras personales y grandes equipos hasta los teléfonos fijos y móviles  pasando por Internet y otras redes. Máquinas, redes, programas y servicios facilitan la  comunicación entre personas, el acceso a ingentes cantidades de información en formato digital.  Si durante la Revolución Industrial, las fábricas eran el principal motor de riqueza, al emerger la  sociedad de la información este papel lo tienen las redes de comunicaciones y la capacidad  intelectual de los ciudadanos para transformar la información en conocimientos. Además, inciden  sobremanera en todo lo que se refiere al conocimiento humano. Por ello suele hablarse de una  sociedad de la información y del conocimiento, denominación ésta que empieza a ser insuficiente,  porque la cuestión es qué tipo de sociedad estará en relación con  parámetros tales como la cultura,  la organización democrática o la justicia. La sociedad de la información constituye el cambio más  importante que están experimentando las sociedades occidentales desde la revolución industrial.  Como ocurrió en ese momento, los cambios afectan a la manera de trabajar de la mayoría de la  población, a los objetos de producción, a la forma de relacionarse entre las personas, al acceso y  participación de la cultura.   Según Echeverría se requieren acciones enérgicas para garantizar el derecho universal a la  educación en el nuevo espacio social. Por ello, más que en una educación para las TICs hay que  pensar en un sistema educativo para el entorno social que se está formando. Las redes educativas  telemáticas son las nuevas unidades básicas de dicho sistema educativo, que incluye el diseño y la  construcción de nuevos escenarios educativos, la elaboración de instrumentos educativos  electrónicos y la formación de educadores especializados para realizar la práctica docente en el  nuevo espacio social. Es importante reflexionar profundamente sobre la educación en este ambiente   mediado por las TICs, con el fin de promover acciones coherentes para organizar los procesos  educativos en este nuevo entorno social. El modo en que se desarrollen allí los procesos educativos    incidirá profundamente en el tipo de sociedad de la información que vayamos a tener. No está claro  que dicha sociedad de la información vaya a ser democrática, ni tampoco que vaya a estar basada en  el principio de igualdad de oportunidades entre sus ciudadanos. Son pilares fundamentales a tener  en cuenta para esta planificación de tanta trascendencia.    3. Aplicaciones    Analizado entonces el estado del arte del tema, es importante proponer cambios educativos que  acompañaran lógicamente a los cambios tecnológicos. De nada servirán las nuevas tecnologías, si  no podemos sacar de ellas el máximo provecho para todos y tampoco servirá proponer grandes  cambios en la educación, si esta tecnología es inalcanzable. Este segundo aspecto es relativamente  menos problemático en occidente, ya que los medios tecnológicos son cada vez más económicos y  por lo tanto accesibles a todos. Sin embargo, deberíamos también pensar en continentes como  África o ciertas regiones de Asia y América, donde esto no es tan real.      Basándonos entonces en la incorporación de telefonía celular, i-pod y computadoras portables de  largo alcance y bajo consumo, podemos pensar en nuevas herramientas educativas para:      Mejorar los cursos presenciales de educación formal tradicional.    Incorporar estas tecnologías en cursos a distancia mediados por la Web.   Diseñar cursos de formación continua, destinados a adultos que trabajan en grandes centros  urbanos y que deciden aprovechar el tiempo de traslado para tomar esos cursos.   Crear instructivos interactivos para la puesta en marcha de equipos instalados en lugares  alejados.   Fundar una interacción cooperativa de grupos de trabajo espacialmente dispersos.    4. Futuros desarrollos    Tecnológicos: aplicaciones basadas en el uso de la telefonía celular y en el i-pod.     Didácticos: concepción, diseño y planteo de nuevas metodologías y estrategias para la construcción  de conocimiento y desarrollo de competencias a partir de interacciones en el nuevo entorno  educativo.	﻿Educación Basada en la Web , tecnología móvil	es	20703
38	Programación de robots físicos mediante interfases visuales	﻿ Este trabajo tiene como objetivo realizar un análisis preliminar de la programación de robots físicos por medio de interfases gráficas. Se describe el diseño de un compilador que toma como entrada un programa generado desde una interfase visual y genera como salida un programa en código intermedio. Luego, este programa intermedio puede ser interpretado por el robot para ejecutar las acciones indicadas en el programa fuente. 1. Introducción El vertiginoso avance de la robótica en los últimos años, ha provocado un desarrollo notorio en las herramientas de software relacionadas. Una de ellas, los lenguajes de programación, han sido adaptados para soportar la programación de robots. El caso más común se da en los lenguajes de programación tradicionales. Estos fueron, en su mayoría, extendidos con el fin de proveer al programador las funciones necesarias que le permitan programar robots físicos. *Este trabajo está parcialmente financiado por la Universidad Nacional del Comahue, en el contexto del Proyecto de Investigación “Técnicas de Inteligencia Computacional para el diseño e Implementación de Sistemas Multiagentes” (COD 04/E062), por el Grupo de Investigación en Robótica Inteligente y por la Universidad Politécnica de Madrid a través del Proyecto AL05_PID_0040, “Implementaciones y Modelos de Razonamiento basado en Programación Lógica”. La utilidad y flexibilidad de un robot está determinada, en gran medida, por la potencia del software que posee. Esto ha dado lugar a una clasificación basada en el nivel del lenguaje de programación en el que fueron programados. Esta clasificación[5] se presenta de la siguiente manera: Sistemas guiados: el usuario conduce el robot a través de los movimientos a ser realizados. Sistemas de programación de nivel-robot: el usuario escribe un programa en un determinado lenguaje, especificando los movimientos y sensores. Sistemas de programación de nivel-tarea: el usuario especifica la operación por medio de las acciones sobre los objetos que el robot manipula. Esto es, solo se programan las acciones que el robot realizará sobre determinados objetos. Por ejemplo, un robot grúa tendrá funciones para levantar un determinado objeto y funciones para girar, siempre en un lugar fijo. La segunda categoría de esta clasificación es de especial interés, puesto que involucra a lenguajes de programación tradicionales, extendidos para soportar la programación de robots. Estos lenguajes contienen funciones que permiten programar las distintas acciones que el robot debe realizar. La programación con lenguajes tradicionales es ampliamente conocida[6]. En este caso, el programador introduce sentencias (código propio de cada lenguaje), que le permiten especificar las acciones que el robot debe realizar. Estas sentencias permiten controlar los motores y sensores y determinan el comportamiento mediante el cual, el robot se desempeñará en un entorno determinado y predefinido. De esta manera, el software que controla las acciones del robot puede ser construído como cualquier otro programa, solo debe conocerse el lenguaje en el que se va a programar y las funciones que permiten controlar los distintos elementos que conforman un robot, sus sensores y motores. Pero la manera tradicional de programar robots tiene una limitante importante. El interesado en realizar esta tarea debe conocer en detalle el software que controla el robot. Una alternativa a la programación de robots físicos con lenguajes tradicionales extendidos, es la de programar mediante una interfase gráfica visual. En este caso, no es necesario introducir código de programación, sino elementos que representen bloques de código que realizan alguna acción concreta. El objetivo de este trabajo es mostrar cómo, la utilización de la interfase gráfica de Lego MindStorms[7], nos permite facilitar la programación de robots físicos. Con este objetivo como guía, realizamos el análisis de la interfase gráfica de Lego y presentamos el diseño de un compilador. La tarea fundamental de nuestro compilador es tomar como entrada un programa generado desde la interfase visual y generar otro programa equivalente en código intermedio que luego puede ser interpretado por el robot. El artículo está organizado de la siguiente manera. En la sección siguiente describimos la interfase gráfica de Lego. En la sección 3 introducimos los conceptos que hacen referencia al código intermedio LASM. La siguiente sección contiene el principal aporte de este trabajo. Se presenta el diseño de un compilador que nos permite generar código intermedio LASM, a partir del programa fuente realizado desde la interfase gráfica de Lego. La sección 5 contiene las conclusiones del trabajo y se describen las líneas de trabajo futuro. 2. La Interfase Gráfica de Lego El Sistema de Invención Robótica de Lego Mindstorms[7], fue pensado, en principio, para niños1. El lenguaje oficial del RCX2 se caracteriza por su simplicidad y legibilidad. Este sistema propone programar robots montando bloques de comandos gráficos. Estos bloques de comandos se conocen como Lego bricks y representan instrucciones para el robot. Mediante estos bloques puede lograrse que el robot se mueva, emita un sonido o que reaccione a distintos eventos. El entorno de programación es una interfase de programación visual orientada a objetos. Existen íconos que representan bloques de código que realizan acciones concretas. El usuario puede elegir de una lista de comandos y colocarlos en un orden lógico que el RCX puede ejecutar. 2.1. Tipos de Comandos Los tipos de comandos que se pueden utilizar desde la interfase son los siguientes: Movimientos y sonido: Con estos comandos se logra animar al robot. Permiten que se mueva en diferentes sentidos, que gire y realice determinados sonidos. Variables: Las variables son utilizadas para personalizar al robot. Por ejemplo, las variables pueden ser utilizadas para especificar la duración de algunas de las acciones. Esperar y repetir: Estos comandos son usados para hacer que el robot responda y reaccione al mundo que lo rodea. Condicionales: Los comandos condicionales permiten seguir diferentes cursos de acción. Comandos personalizados: Estos comandos sirven para crear bloques de códigos personalizados. Sensores: Los sensores son usados para que el robot realice testeos y reaccione a su entorno. 1Según Lego, “... es un entorno de programación visual que permite a los niños recoger, soltar y apilar comandos y trozos de código.” 2 RCX es el microprocesador de cada robot Mindstorms. 2.2. Descripción del Recipiente de Bloques El recipiente de bloques contiene los distintos comandos que pueden ser utilizados. Determinados bloques están habilitados o no, dependiendo del tipo de robot que se requiere programar. Los distintos bloques que se encuentran en el recipiente de bloques son: Bloques Grandes: Con estos se logra controlar las partes móviles del robot. Aquí tenemos comandos que, por ejemplo, determinan movimientos hacia delante o hacia atrás. Bloques Chicos: Hacen posible un control total sobre el robot. Permiten manipular variables y personalizar al robot. Mi Bloque: Este tipo de bloques permiten mantener un programa de manera eficiente y ordenada. Esperar (Wait): Este bloque permite que el programa espere una determinada cantidad de tiempo o que espere a que un sensor sea disparado. Repetir: Este bloque permite repetir un conjunto de comandos. Condicionales: Estos bloques permiten adherir un salto en el programa basado en la lectura de un sensor. Sensores: Estos bloques sirven para monitorear los diferentes sensores y disparar una tarea de comandos cuando se cumple determinada condición. La programación de los robots con la interfase gráfica se basa en apilar estos bloques en el orden en que serán ejecutados. Cada uno de estos, como ya fue mencionado, representan un conjunto de comandos que conforman una acción concreta que el robot puede realizar. De esta forma, en ningún momento el programador introduce código, solo interactúa con los elementos (bloques) que ofrece la interfase. 3. El Código Intermedio LASM Como se describió en la introducción, el objetivo de este trabajo es mostrar cómo la interfase gráfica de Lego nos permite programar robots físicos. Nuestra propuesta de compilador toma como entrada un programa realizado desde la interfase visual y genera un programa equivalente en código intermedio. Luego, el programa intermedio es enviado, a través de la herramienta MindScript[8], al procesador del robot, para ejecutar las acciones indicadas en el programa fuente. Para este trabajo, el código intermedio elegido es el LASM (Lego Assembler)[1]. LASM contiene instrucciones que permiten manipular los distintos componentes de un robot. Componentes físicos, como motores y sensores y componentes no físicos, como los eventos. Los comandos de LASM, como por ejemplo, WAIT, EVENT y SETV, tienen 2 (dos) parámetros. Uno, el denominado fuente, indica el tipo de parámetro. El otro parámetro es el valor e indica el índice del tipo del parámetro. A continuación, se exhibe un comando típico de LASM: wait 0,5 donde 0 (cero) indica que el tipo de parámetro es una variable. El segundo parámetro es el índice donde esa variable está alojada. El código LASM no contiene instrucciones que permitan almacenar valores en una pila. Todos los valores están alojados en variables que luego son accedidas mediante un índice como se explica arriba. Algunas de las instrucciones más importantes en LASM, son las que permiten controlar los sensores, los eventos y los motores del robot. La instrucción EVENT, para el caso de los eventos, permite inicializar un evento. La instrucción DIR permite controlar los motores del robot. Para mayores detalles referidos al código LASM se recomienda [1]. 4. El Compilador Los programas generados desde la interfase visual de Lego contienen una estructura similar a un programa realizado en lenguaje C. Esto es, luego que un programa es diseñado desde la interfase visual, se genera un archivo con un programa fuente que contiene la estructura que se muestra a continuación: program ejemplo { declaraciones main {comandos} watcher {} } Un bloque global (program ejemplo {}), que encierra todo el código que el robot ejecutará y en donde se declaran las variables, sensores, motores y eventos. Un bloque principal (main {}), que contiene las acciones específicas que realizará el robot, por ejemplo, las acciones que hacen que un evento se dispare o aquellas que se ejecutan ante la percepción de algún sensor. Por último, un bloque opcional de tareas (watcher {}), permite ejecutar tareas en paralelo. El lenguaje del programa generado por la interfase es el lenguaje Script[1]. Nuestro compilador toma como entrada un programa, con la estructura indicada arriba, realizado desde la interfase gráfica. El primer paso en la construcción de la herramienta, fue especificar la gramática que genera los programas que se pueden obtener desde la interfase. El diseño general del compilador sigue los lineamientos básicos de [9], referencia obligada en el tema. El compilador cuenta con cuatro etapas bien definidas. La primera de ellas es la que corresponde al análisis léxico. En esta primera etapa, el compilador lee el programa de entrada caracter a caracter identificando componentes léxicos. La segunda etapa es la que corresponde al análisis sintáctico. En esta fase el compilador, a partir de la secuencia de componentes léxicos, impone una jerarquía de acuerdo a la gramática especificada. La tercera instancia está marcada por el análisis semántico. Aquí se analiza si existen errores semánticos y se genera la información necesaria para la última etapa de este compilador. Esta última fase genera el código intermedio LASM, teniendo en cuenta la información generada en la etapa anterior. El programa de entrada del compilador es recorrido en su totalidad, realizando las verificaciones sintácticas y semánticas. Conjuntamente, se genera el programa de salida en código LASM. En el recorrido del programa fuente se genera una tabla de símbolos que almacena, tanto los nombres de las variables, como el de los eventos, sensores y motores. De esta tabla se extrae la información necesaria que es utilizada para generar el código LASM. A su vez, la información contenida por la tabla también es usada para realizar el análisis semántico. 5. Conclusiones Hemos podido comprobar, después de varias pruebas, que los programas realizados desde la interfase Lego son tomados correctamente por el compilador y que se genera el código LASM correspondiente. De esta manera, usando una interfase visual como la del Lego, se puede programar robots físicos sin introducir código de programación. La interfase visual permite al usuario independizarse de ciertos detalles de los lenguajes de programación. Las distintas salidas generadas por el compilador muestran a los programas generados desde la interfase gráfica en código LASM. Cada una de estas salidas fue enviada al robot a través del software MindScript[8] y fueron ejecutadas con éxito. Esta característica permite programar los robots desde el ambiente visual de Lego sin necesidad de introducir código. Un trabajo futuro es realizar un ambiente visual de prueba que tome el código LASM generado desde el compilador y simule el funcionamiento que el robot realizaría. De esta manera, no será necesario un robot físico para realizar las pruebas de lo programado desde la interfase. Un trabajo adicional a considerar sería modificar la etapa inicial del front-end del compilador para permitir tomar como entrada programas desde alguna otra interfase visual.	﻿Interfases Visuales , Robots Físicos , Programación	es	20747
39	Implementación de un digesto digital paralelo	﻿ El crecimiento de la cantidad de información que se pone a disposición en Internet a través de la Web presenta el desafío de satisfacer, en el menor tiempo posible, a los clientes que realizan búsquedas sobre esa información y a la vez mejorar el uso eficiente de los recursos. Los modelos de computación paralela permiten acercarse a este objetivo. Este trabajo presenta una solución eficiente y de bajo costo basada en el modelo de computación Bulk Synchronous Parallel, para la implementación de un Digesto Digital basado en un motor de búsquedas paralelo que utiliza bases de datos relacionales, en un entorno de acceso Web. Palabras claves: Bases de Datos, Procesamiento Paralelo de Consultas SQL, Computación Paralela y Distribuida, BSP 1. Introducción La web se ha convertido en un recurso ubicuo para la computación distribuida, haciendo relevante la investigación de nuevos caminos para proveer acceso eficiente a los servicios disponibles en los sitios dedicados. El crecimiento exponencial que ha experimentado desde sus comienzos en cuanto al volumen de información y al número de usuarios que la utilizan hace que la búsqueda, organización, acceso y mantenimiento de sus contenidos sea cada vez más difícil. En respuesta a esta expansión de las fuentes potenciales de información, los motores de búsqueda han hecho énfasis en ampliar su velocidad y *Este trabajo fue financiado por la Universidad Nacional de la Patagonia Austral, Santa Cruz, Argentina, Proyecto 29/A164 cobertura, brindando poca importancia a la eficiencia. Debido a esto, diversos estudios se han abocado al desarrollo de nuevas estrategias que permitan satisfacer estas demandas a través del procesamiento paralelo, el cual ha demostrado ser un paradigma que permite mejorar los tiempos de ejecución de los algoritmos. En este trabajo se propone una solución basada en el modelo BSP [13, 4] de computación paralela, el cual utiliza una configuración de base de datos distribuida para acelerar las consultas, realizando el procesamiento de la cola de consultas en forma secuencial. La implementación de este sistema requirió en primer lugar la modificación de la librería de comunicaciones de la Paderborn University, BSPPUB [5, 3], para hacer posible la ejecución de programas BSP a través de sockets. Esta modificación permite la ejecución como daemon (es decir, con la capacidad de mantenerse en ejecución en forma continua) del módulo encargado de recibir las consultas, lo cual no era posible realizar con la versión estándar de la librería [2]. La máquina en la que reside este daemon es denominada broker, y forma parte del cluster o red de computadoras compuesta por P procesadores. Para evitar la sobrecarga de este procesador, no realiza consultas locales sino que únicamente administra la cola de consultas recibidas desde el servidor web, las distribuye entre cada uno de los procesadores restantes, y recopila los resultados obtenidos, devolviéndolos agrupados al servidor. Cada una de las restantes máquinas del cluster posee un servidor de base de datos secuencial MySQL, y trabaja localmente sobre una porción de los datos. Un logro a destacar es que la modificación de la librería estándar de BSP-PUB nos ha permitido implementar una aplicación real y funcional, utilizando tecnología gratuita existente, con costos de implementación y operación muy bajos, ya que se utiliza equipamiento de bajo costo y software libre, a la vez que obtener un rendimiento cercano al óptimo en cuanto a la eficiencia en la utilización de los recursos involucrados. Adicionalmente hemos propuesto una estrategia simple para distribuir uniformemente los registros almacenados en las bases de datos locales de cada uno de los procesadores que componen el cluster. En las secciones siguientes brindaremos un panorama del funcionamiento del modelo de computación paralela BSP, explicaremos la configuración de cada uno de los componentes mencionados, así como describiremos las responsabilidades de los mismos y finalmente presentaremos los resultados preliminares de laboratorio logrados a través de la aplicación de nuestras investigaciones en un desarrollo concreto que tiene por objetivo la publicación de los documentos producidos por los organismos de gobierno de la Universidad y que hemos denominado Digesto Digital Institucional. 2. Modelo de computación paralela BSP Varios productos comerciales fueron desarrollados para máquinas multiprocesadores de memoria compartida y distribuida, y más recientemente para clusters de computadores. Todos estos desarrollos están basados en modelos tradicionales de computación paralela como paso de mensajes y memoria compartida. En este trabajo se presenta una solución funcional basada en un modelo relativamente nuevo de computación paralela llamado Bulk Synchronous Parallel, BSP. En BSP un computador paralelo es visto como un conjunto de procesadores con memoria local e interconectados a través de una red de comunicaciones de topología transparente al usuario. En este modelo, la computación es organizada como una secuencia de supersteps. Tal como lo indica la Fig. 1, un superstep está formado por una fase en la que cada procesador puede realizar operaciones sobre datos locales únicamente y depositar mensajes a ser enviados a otros procesadores. Al final del superstep, todos los mensajes son enviados a sus destinos y los procesadores son sincronizados en forma de barrera para iniciar el siguiente superstep. Es decir, los mensajes están disponibles en sus destinos al instante en que se inicia el siguiente superstep. El modelo práctico de programación paralela en BSP es el conocido SPMD (Simple Program Multiple Data), el cual es realizado mediante P copias del mismo programa corriendo en un cluster de P procesadores, cada una actuando sobre un subconjunto de los datos, donde la comunicación y sincronización de las copias es realizada mediante librerías tales como BSPlib o BSPPUB. Enfatizamos que el modelo BSP es en realidad un paradigma de programación en paralelo y no una librería de comunicaciones en particular. En la práctica, es ciertamente posible implementar programas BSP utilizando librerías tales como PVM (Parallel Virtual Machine) y MPI (Message Passing Interface). Nótese que varios estudios han mostrado que algoritmos BSP presentan un desempeño más eficiente que sus respectivas contrapartes en los enfoques de paso de mensaje y memoria compartida en varias aplicaciones. La estructura del modelo BSP facilita la predicción del desempeño de programas y algoritmos. El costo de un programa está dado por la suma del costo de todos sus supersteps, donde el costo temporal de cada uno de ellos está dado por la suma del tiempo de computación sobre datos locales, el tiempo de comunicación entre procesadores y el tiempo de sincronización. Figura 1: Modelo BSP y supersteps. 3. Configuración del Motor de Búsquedas Paralelo Esta herramienta de software se compone de los siguientes elementos: una aplicación web desarrollada fundamentalmente con PHP, instalada en un servidor Web con infraestructura LAMP (Linux, Apache, MySQL, PHP), un servidor de base de datos distribuida implementado con MySQL, un programa ejecutable desarrollado bajo el modelo de computación paralela BSP y un broker realizado en base a la modificación del daemon del BSP-PUB, pubd. Estos componentes se interrelacionan como se explica a continuación: en primer lugar los clientes acceden a la aplicación a través del servidor web, donde elaboran la consulta a realizar. El servidor web envía la consulta al broker a través de sockets, ejecutando el programa BSP en cada procesador que forma parte del cluster. En cada procesador se ejecuta la consulta en el servidor local de base de datos MySQL y, cuando se produce la etapa de sincronización del superstep BSP, los resultados son enviados a la máquina broker, quien los agrupa y envía al servidor Web a través de los sockets definidos para que de esta manera el cliente pueda obtener los resultados requeridos. A continuación se describen los componentes mencionados: 3.1. Configuración del Servidor Web Para este trabajo se ha desarrollado una aplicación Web donde los clientes pueden realizar consultas relativas a los documentos emanados por las autoridades de Gobierno de la Universidad. Adicionalmente, el servidor Web almacena un archivo en formato PDF por cada uno de los documentos almacenados en la Base de Datos. Es este archivo PDF el que será accedido por los clientes una vez que reciban las tuplas que coincidan con el criterio de búsqueda ingresado. Cada tupla cuenta con un link al archivo PDF correspondiente. Se ha tomado esta decisión para minimizar el tráfico interno dentro del cluster, ya que cada cliente accederá a un archivo PDF por vez, por lo que el procesamiento paralelo para esta etapa no aportaría beneficios adicionales. 3.2. Configuración del Servidor de Base de Datos Distribuida La implementación del servidor es como sigue. Existe un conjunto P de procesadores ejecutando los superstep de BSP. En cada máquina existe un administrador de bases de datos relacionales (MySQL en nuestro caso). Este administrador es operado mediante instrucciones en lenguaje SQL enviadas a través de una conexión socket implementada por una API para C++. De esta manera, cada máquina del cluster puede ejecutar comandos SQL sobre su base de datos local. Cada una de las P máquinas mantiene el mismo esquema de la base de datos, es decir, las mismas tablas pero con distintas tuplas. Las tuplas están distribuidas uniformemente en la base de datos y almacenan la información como texto plano para la realización de las consultas. Si bien se consideraron varias alternativas para realizar esta distribución, la solución adoptada consiste en realizar una consulta a todos los procesadores del cluster sobre la cantidad de registros existentes en cada uno de ellos antes de realizar una inserción. Para realizar esta operación se seleccionará siempre el procesador con el número mínimo de registros existentes. De esta manera es posible administrar en forma automática la distribución uniforme de la base de datos ante eliminación o inserción de registros e incluso ante cambios en la cantidad de procesadores que componen el cluster. 3.3. Broker o Agente de Gestión La gestión de las consultas es crítica para un aprovechamiento global de los recursos, y también para garantizar que la arquitectura es adecuadamente modular y escalable. En el modelo BSP, esta optimización depende del tipo de consultas, generalmente determinada por la programación de un broker o agente de gestión. Este broker tiene por responsabilidad administrar una cola de consultas recibidas desde la máquina front-end, ya que cada consulta se realiza en forma secuencial con respecto a las otras consultas. Adicionalmente realiza la tarea de intermediación entre la máquina front-end y el cluster. Al investigar la posibilidad de implementar un sistema operacional que recibiera consultas de los clientes a través del servidor Web, nos encontramos con la imposibilidad de utilizar la librería BSP-PUB tal como fue desarrollada, ya que la misma prevee la ejecución de los programas a través de una ventana de comandos que se carga al ejecutar el daemon que genera la intercomunicación de los procesadores participantes y genera los buffers que se utilizarán en esa comunicación. Este daemon está definido como pubd. Por esta razón fue necesario modificar el daemon pubd para que permitiera la ejecución de programas BSP a través de sockets, permitiendo el acceso al mismo directamente desde la aplicación Web desarrollada. Es importante destacar que si bien el broker participa del cluster BSP, este tiene un tratamiento especial, no contando con base de datos, y por lo tanto no se realizan búsquedas en ese procesador. Esto se ha definido así ya que en la misma máquina del broker se encuentra el servidor Web y, en cualquier caso, la existencia dentro del cluster de un broker participando además como servidor de base de datos generaría una carga adicional que haría perder el balance del sistema, afectando la performance global y convirtiéndose en un posible cuello de botella. Alternativamente el broker podría ubicarse en una máquina distinta al servidor web. Al culminar el procesamiento de las consultas, los procesadores involucrados devuelven a P0 (el procesador que llamó a la ejecución del programa BSP, en este caso el que contiene pubd en ejecución, es decir el broker) los resultados obtenidos, y éste los almacena en un archivo que será consultado por el servidor web, de manera de poder entregar al cliente que realizó la solicitud los resultados en forma parcial, de acuerdo a la demanda del usuario. Para evitar que el broker se transforme en un cuello de botella del sistema se ha optado por realizar el almacenamiento de las consultas en un archivo temporal en el broker, el cual podrá ser consultado por la sesión iniciada por el cliente, y que se eliminará cuando se cierre dicha sesión. 4. Digesto Digital Institucional La configuración del motor de búsqueda expuesta precedentemente se ha implementado exitosamente en una aplicación en etapa de prueba que hemos denominado Digesto Digital Institucional. Originalmente el término Digesto se aplicó a la codificación del Derecho Romano, pero actualmente y por extensión se conoce como digesto a la compilación ordenada de toda norma jurídica. El Digesto Institucional permite acceder a todo lo actuado, sancionado y legislado en el tiempo, por una Institución dada. Constituye el cuerpo de leyes o reglamentaciones por el cual se rige la actuación y las decisiones de una administración, compendiando además, todo lo resuelto o actuado en función y con atención a ese conjunto de reglamentaciones básicas. Hemos optado por implementar el Digesto Digital de la Universidad Nacional de la Patagonia Austral, ya que el volumen de datos involucrados resulta atractivo para realizar las pruebas de laboratorio de esta investigación. En principio hemos estimado un volumen de datos de al menos 500.000 páginas de documentos correspondientes a los órganos de gobierno de la Universidad de los últimos 10 años, las que se almacenarán en formato texto en la base de datos distribuida (lo que permitirá realizar las búsquedas) y en formato de archivos PDF almacenados en el servidor web (para poder obtener un copia con autenticación por parte de la Universidad). 4.1. Resultados Preliminares Luego de realizar en investigaciones anteriores [8, 6, 7, 9, 10, 12, 11, 1] diversas consideraciones con respecto a servidores paralelos sobre bases de datos distribuidas a través del modelo de computación paralela BSP, hemos estimado conveniente aplicar los resultados de laboratorio obtenidos, fundamentalmente a través de simulaciones, en una sistema real que nos permita por una parte verificar los resultados obtenidos en laboratorio y por otro permitir el desarrollo de una aplicación que pueda ser ampliada y mejorada a través de los aportes de los usuarios e investigadores que accedan a la misma. Resultados preliminares de las pruebas realizadas con el compendio de normas almacenadas hasta el momento demuestran valores similares a los obtenidos en anteriores pruebas de laboratorio. En este caso sólo fue necesario realizar la simulación de las consultas generadas por los clientes. El desempeño paralelo, en comparación al secuencial, obtiene mejor rendimiento cuanto más tuplas existen en las tablas, ya que se realiza un procesamiento local más intensivo. En la figura 2 se muestra la relación tiempo secuencial/tiempo paralelo para el caso de un cluster compuesto por 4 procesadores y un promedio de resultados de 1000 tuplas para cada consulta. Como puede observarse, a medida que aumenta la cantidad de tuplas en la base de datos la relación indicada se acerca al óptimo, que para este caso es 4. Debe considerarse que para el caso de pocos documentos almacenados, la mejora en el procesamiento local no es suficiente para contrarrestar los costos de comunicación para la transmisión de las 1000 tuplas resultantes. Figura 2: Relación entre el tiempo secuencial/paralelo de acuerdo al número de documentos almacenados en la base de datos para un promedio de 1000 tuplas de resultados para una consulta. Un aspecto interesante es que el efecto del aumento en comunicación en el caso paralelo no es muy significativo frente a la mayor actividad de disco generada producto de tablas de mayor tamaño en el caso secuencial. 4.2. Trabajos Futuros Trabajos futuros permitirán mejorar la administración de la cola de consultas manejada por la máquina broker, por ejemplo a través de la evaluación de prioridades. Otro tema de estudio es la recuperación de la base de datos distribuida a raíz de la caída de una máquina del cluster, mediante la replicación de registros u otra estrategia. También es posible su aplicación a redes de Datos del tipo Grid, si se piensa que en una aplicación como el Digesto pueda plantearse la necesidad de contar con una base de datos ditribuida geográficamente para respetar la autonomía de las organismos intervinientes. Por último es posible investigar la aplicación de este modelo en la conexión de las Bases de Datos relacionales con una ontología legal y el acceso a la misma a través del desarrollo de una web semántica. 5. Conclusiones En este trabajo hemos presentado una solución concreta al problema de acceso a grandes volúmenes de datos a través de la Web, mediante el desarrollo de varios componentes que conforman un motor de búsquedas paralelo, con acceso a una base de datos distribuida, implementando el modelo de computación paralela BSP, en particular a través de la librería BSP-PUB. Este modelo soporta una metodología estructurada de diseño de software que es simple de utilizar y permite el uso de tecnología existente y gratuita para obtener sistemas de bajo costo y alta eficiencia. En particular, en este trabajo nos hemos centrado en el desarrollo del servidor de consultas y el broker asociado, que permite distribuir las consultas y enviar los resultados a los clientes que las han generado. Con las modificaciones realizadas a la librería BSP-PUB ahora es posible su utilización en sistemas de búsqueda y recuperación de información en un entorno paralelo con bases de datos relacionales, permitiendo lograr eficiencia a un bajo costo, con tiempos de respuesta superiores a los sistemas secuenciales. En los resultados preliminares de laboratorio obtenidos a través de un sistema denominado Digesto Digital Institucional, se han obtenido valores cercanos al óptimo en cuanto a eficiencia en relación con el caso secuencial, para el caso de grandes cantidad de registros, donde se justifica la utilización de modelos paralelos.	﻿Procesamiento Paralelo de Consultas SQL , Computación Paralela y Distribuída , Bases de Datos , BSP	es	20763
40	Minería de imágenes	﻿ Durante los últimos años, la proliferación de los medios digitales ha creado la necesidad del desarrollo de herramientas para la eficiente representación, acceso y recuperación de información visual. La minería de imágenes se ha convertido en una importante rama de investigación a causa del potencial que posee en descubrir patrones característicos a partir de un importante conjunto de imágenes. No obstante, la minería de imágenes es más que una simple extensión de la minería de datos al dominio de las imágenes. Es un esfuerzo interdisciplinario que requiere experiencia en diversas áreas. Si bien algunas de ellas se encuentran desarrolladas, el área de minería de imágenes está aún en un estado preliminar. Es de importancia entonces el poder establecer cual es su ámbito de trabajo y alcance en las investigaciones futuras. Keywords: Minería de Imágenes, Minería de Datos, Procesamiento Digital, Visión, Reconocimiento de Patrones. 1. Introducción En los últimos años, la industria de la computación se ha desarrollado rápidamente en todos sus campos: tecnológico y, de procesamiento, almacenamiento y acceso a la información. El avance tecnológico ha habilitado a la digitalización de datos; en la actualidad es extremadamente fácil la obtención y almacenamiento de grandes cantidades de datos, particularmente imágenes digitales. La manipulación y administración de repositorios de imágenes es, en sí misma, un tema de importancia a ser tratado. Los sistemas de bases de datos de imágenes pretenden establecer un soporte para el almacenamiento y recuperación de las mismas. Las imágenes son recuperadas a partir de una imagen ejemplo entregada al sistema, donde las características típicas de las imágenes almacenadas son contrastadas con las de la imagen entregada en la consulta. Luego de la evaluación, las imágenes son ordenadas en función a la similitud con la imagen de referencia. La recolección de imágenes y su consecuente generación de repositorios se realiza en un amplio rango de ámbitos: comerciales, institucionales, militares. Su uso generalizado se debe a que las mismas son una fuente potencial de información a ser posteriormente analizada y procesada. Ello implica la identificación de las características presentes en una imagen y su consecuente evaluación. Una dificultad en esta tarea es disponer del conocimiento de todo el dominio de imágenes existentes para poder luego inferir información. Actualmente existe una creciente demanda por sistemas que puedan, en forma automática, analizar imágenes y extraer información semánticamente relevante, al mismo tiempo que modelen relaciones y categorías en una colección de imágenes. La Minería de Imágenes es el área destinada al estudio y desarrollo de nuevas técnicas que permitan realizar estas tareas. El desafío fundamental en la minería de imágenes es el poder determinar cómo la representación de los pixels contenidos en una imagen pura o conjunto de imágenes a bajo nivel pueden ser eficiente y efectivamente procesados para identificar objetos espaciales de alto nivel y sus interrelaciones. En otras palabras, la minería de imágenes intenta extraer conocimiento implícito (características globales de una imagen) a partir de los pixels y sus interrelaciones (aspectos locales de la misma) que no se encuentran explícitamente almacenadas en una base de datos de imágenes. Este es un esfuerzo interdisciplinario que esencialmente requiere experiencia en: visión por computadoras, procesamiento de imágenes, recuperación de imágenes, minería de datos, aprendizaje automático, bases de datos e inteligencia artificial. *Grupo soportado por la UNSL y ANPCYT (Agencia Nac. para la Prom. de la Ciencia y Tec. Mientras que algunos de estos campos se encuentran bastante desarrollados por sí mismos, la minería de imágenes es hoy en día un área de investigación creciente y se encuentra aún en estado de experimentación. El principal obstáculo para el rápido progreso de las investigaciones en el área es la falta de acuerdo sobre cuales son las líneas o temas de investigación involucrados en la minería de imágenes. Muchos investigadores tienen la impresión errónea de que la minería de imágenes es simplemente una extensión de las aplicaciones de minería de datos, mientras que otros la consideran como como una manera diferente de identificar al área de reconocimiento de patrones. A continuación se intentará bosquejar los aspectos principales a ser considerados cuando se hace referencia al área. 2. Marco Contextual Tal vez, uno de los errores más comunes que se comete cuando se habla del área es el no poder identificar claramente los alcances y limitaciones de la misma. Es claro esta es diferente a las áreas de visión por computadora y procesamiento de imágenes. La minería de imágenes se focaliza en la extracción de patrones de una gran colección de imágenes, mientras que las otras dos se concentran en el entendimiento y/o extracción de características específicas a partir de una única imagen. Por otro lado se podría pensar que está muy relacionada con el área de recuperación basada en el contenido, pues ambas trabajan con colecciones de imágenes. Sin embargo, la minería de imágenes va mas allá del simple hecho de recuperar imágenes relevantes, ella se concentra en descubrir patrones sobresalientes en una colección de imágenes. La interpretación errónea más general es pensar que la misma involucra simplemente la aplicación de algoritmos de minería de datos ya existentes, sobre imágenes. Debido a ello, usualmente las investigaciones en minería de imágenes se dividen, a grandes rasgos, en dos direcciones principales. La primera de ellas involucra aplicaciones de dominio específico, donde el objetivo es extraer las características más relevantes de una imagen de manera que puedan ser utilizadas en minería de datos [6, 14, 10, 13, 7, 15]. La segunda dirección involucra aplicaciones generales, donde el objetivo es descubrir patrones de imágenes que podrían ser útiles en el entendimiento de las interacciones existentes entre la percepción humana de las imágenes a alto nivel y las características de una imagen a bajo nivel [16, 12, 18, 17, 2, 23]. Las investigaciones en esta dirección intentan desarrollos con mayor certeza de éxito en las imágenes recuperadas desde una base de datos. No obstante, una base de datos de imágenes conteniendo imágenes puras como dato, no puede ser directamente utilizada para propósitos de minería. Las imágenes deben ser tratadas a modo de poder generar información que pueda ser utilizada en niveles más altos de proceso. El desarrollo de un sistema de minería de imágenes es un proceso complejo pues implica la articulación de diferentes técnicas que van desde la recuperación de las imágenes, pasando por esquemas de indexación hasta la minería de datos y el reconocimiento de patrones. Además, se espera que un buen sistema de minería de imágenes provea un acceso al repositorio de imágenes en forma efectiva para los usuarios, al mismo tiempo que reconoce patrones y genera conocimiento por debajo de la representación de las imágenes. Resumiendo, un sistema de tales características debería reunir básicamente las siguientes funciones: almacenamiento de las imágenes, procesamiento de las imágenes, extracción de características, indexado y recuperación de imágenes, y generación de conocimiento y reconocimiento de patrones. En función a lo expresado, se podría establecer una nueva categorización para los sistemas de minería de imágenes y por consiguiente habilitar ámbitos de trabajo para nuevas investigaciones. Esta nueva categorización divide a los sistemas en función del aspecto bajo el cual fueron concebidos: sistemas dirigidos por la función o sistemas dirigidos por la información. La primer categoría se centra en las funcionalidades de los diferentes módulos componentes para organizar un sistema de minería de imágenes (una estructura simple de tal sistema debería contener al menos dos módulos: un módulo de adquisición y preprocesamiento, y otro de almacenamiento de los datos) [3, 24]. La segunda categoría se centra en el análisis de los diferentes niveles de representación de la información, requeridos para la funcionalidad total del sistema (en forma simple se pueden establecer dos niveles: un nivel bajo a cargo del procesamiento de información pura, y un nivel alto a cargo de la generación de conceptos semánticos y obtención de conocimiento) [25, 8, 5]. Para poder llevar adelante cualquiera de los enfoques mencionados, los investigadores se han basado en el uso de técnicas tradicionales existentes, pertenecientes a otras áreas, las cuales les permitan realizar la minería de la información. Las técnicas más comunmente involucradas se relacionan con el reconocimiento de objetos [4, 1, 7], recuperación e indexado de imágenes [4, 20, 21], clasificación y agrupamiento de imágenes [24, 11, 22], formulación de reglas de asociación [24, 16, 14] y redes neuronales [6, 9, 19]. 3. Lo que se pretende La minería de imágenes es un área diferente a aquellas con las cuales se encuentra relacionada, tales como el procesamiento de imágenes, reconocimiento de patrones y la minería de datos tradicional. Es un proceso no trivial para descubrir conocimiento válido, original, potencialmente útil y entendible desde un conjunto relativamente grande de imágenes en bases de datos de imágenes. Existe entonces, una amplia variedad de situaciones que habilitan al enriquecimiento y desarrollo de conocimiento en diferentes líneas de trabajo, algunas de las cuales se detallan a continuación: La propuesta de nuevos esquemas de representación para patrones visuales que sean capaces de codificar suficiente información contextual, habilitando la extracción significativa de características visuales útiles. El desarrollo de técnicas de visualización para dichos patrones, los cuales deben ser validados por los usuarios mediante la observación. El desarrollo de técnicas de recuperación e indexado de imágenes basadas en el contenido que permitan accesos rápidos y efectivos en grandes repositorios de imágenes. El diseño de lenguajes de consulta semánticamente potentes para las bases de datos de imágenes.	﻿Minería de Imágenes , Minería de Datos , Procesamiento Digital , Visión , Reconocimiento de Patrones	es	20820
41	Aplicación de visualización de datos en sistemas de educación a distancia	﻿ Se presenta una herramienta interactiva gráfica que  procesa los datos correspondientes a accesos y usos  de los recursos disponibles en plataformas de  educación a distancia desarrolladas en Moodle. El  empleo de Visualización de la Información sobre  los datos registrados en estos sistemas facilitará a  los instructores de estas plataformas en el  monitoreo y seguimiento de los estudiantes  registrados. En la Universidad Nacional del  Comahue se ha implementado una Plataforma de  Educación a Distancia, un entorno virtual que se  utiliza  como medio de comunicación y soporte de  contenido. La herramienta gráfica instalada en la  plataforma permite la implementación de técnicas  de visualización de datos de forma intuitiva para el  docente para obtener la representación gráfica mas  adecuada a cada necesidad.  Los profesores de las diferentes materias requieren  analizar la participación de los estudiantes para  evaluar el aprovechamiento de los recursos  ofrecidos por la plataforma con el objetivo de  brindar capacitación efectiva.   Conocer las utilidades de la herramienta gráfica  permitirá poder evaluar sus resultados y adecuarlos  a las necesidades de los instructores. Se apunta  además, a aportar nuevas estructuras visuales a  través de técnicas de visualización que permitan  manejar cantidades de datos crecientes e incorporar  mayor información gráfica en las aplicaciones.     Palabras claves : educación a distancia,   herramienta gráfica interactiva, moodle,  visualización de datos, representación visual  1. Introducción  En la Universidad Nacional del Comahue se ha  implementado la plataforma PEDCO[1] para  brindar soporte y capacitación a distancia, donde  los instructores de los cursos que la utilizan,  necesitan saber qué es lo que sucede con sus  estudiantes respecto al avance, participación y  motivación en los mismos. Para conocer esto los  profesores necesitan saber si los estudiantes leen el  material, acceden regularmente a los cursos,  participan en las discusiones, realizan sus  actividades a tiempo, tienen dificultades con ciertos  temas, etc.  GISMO[2] es una herramienta que extrae los datos  registrados sobre el uso de los recursos por cada  estudiante y genera representaciones gráficas que  pueden ser analizadas por los instructores del curso  para examinar varios aspectos de los estudiantes a  distancia.   Esta herramienta [3] utiliza técnicas de la  visualización de la información para construir en una  manera gráfica apropiada los datos  multidimensionales registrados por el sistema de los  estudiantes. Este tipo de herramientas busca asistir a  los instructores para que conozcan lo que ocurre en  sus clases y así proveer una mejor respuesta a los  estudiantes.  La visualización Multidimensional de Datos  Multivaluados[4], MDMV, trata del análisis de datos  con múltiples parámetros cada uno de los cuales  pueden tomar distintos valores, y de las relaciones  claves que existen entre ellos y las maneras de  visualizarlos. Ha sido estudiada por estadísticos y  psicólogos antes de que la ciencia de computación la  considerara una disciplina.   En general, estas aplicaciones han sido diseñadas  para una tarea específica por lo que han sido  optimizadas para un conjunto de datos también  específicos. Transformarlas en sistemas que trabajen  con datos generales requiere más tiempo de  desarrollo. Otras aplicaciones utilizan bases de datos  extremadamente complejas y detalladas, como  aquellas que se utilizan para restituir volúmenes. Y  aunque los estudios de usabilidad de algunas de estas  aplicaciones han mostrado que los usuarios obtienen  mayor efectividad cuando trabajan con interfaces de  visualización, las pruebas de usabilidad requieren  algunas veces, un período de ajuste al modelo visual  resultando un problema para el usuario general.  La necesidad de manejar cantidades de información  crecientes ha instado a los diseñadores a incorporar  mayor información gráfica en las aplicaciones. El  éxito de estas aplicaciones, a través de la  disponibilidad de herramientas de visualización,  seguramente inducirá al uso de la visualización de la  información.  El uso de la visualización MDMV permite una  evaluación coherente del comportamiento del  sistema social estudiado, en nuestro caso los  estudiantes de cursos a través de la plataforma de  educación a distancia.  Es este trabajo se presenta, en la sección 2, la  herramienta GISMO. Finalmente se presentan  nuestras conclusiones.  2. GISMO  Una vez que GISMO ha sido instalado, aparece  como un bloque más dentro del Moodle[5]  accesible por los profesores pero oculto para los  estudiantes. El requisito para usar la herramienta es  contar con un navegador capaz de correr applets  Java (1.4.x).  Las representaciones gráficas que pueden generarse  se clasifican inicialmente en 4 categorías de  visualización: Estudiantes, Recursos, Actividades y  Conceptos. Analizaremos las cuatro categorías en  detalle.  2.1. Estudiantes   Accesos a los cursos   Con esta opción se representa  gráficamente la participación que han  tenido los estudiantes en un curso. La  representación consiste en una matriz  donde  los nombres de los estudiantes  están en representados en el eje Y, y las  fechas de acceso a los cursos en el eje X.  Cada marca significa al menos un acceso  efectuado por el estudiante en una fecha a  un cierto curso.   Debajo de esta representación hay otra en  la que se grafica el número global de  accesos a los cursos hechas por cada  estudiante en cada fecha. En la figura 1  vemos la representación para la materia  RPA[6].       Fig.1 Representación gráfica de los  accesos de los estudiantes a los cursos.    Uso de los recursos   Con esta representación se analizan los  accesos que los estudiantes, representados  en el eje X, hacen a los recursos,  representados en el eje Y. En la figura 2 se  puede observar una representación de esto.      Fig. 2 Acceso a los recursos por parte de  los estudiantes     Detalle del acceso a los recursos   Las fechas son representadas en el eje X y  los recursos en el eje Y, la forma en que  están ordenados representa la incidencia de  cada recurso dentro del curso. Debajo de  este gráfico se representa un histograma  con el número total de accesos hechos por  los estudiantes a todos los recursos del  curso.    Estudiantes-Recursos  Con este gráfico los profesores conocen  qué estudiantes acceden a cuáles recursos y  cuándo. Los estudiantes están representados  en el eje Y, y los recursos en X. Un punto  representa los accesos y el número de veces  que se accedió se representa con el tono.  2.2. Recursos   Accesos  Se representa el número total de accesos  que cada estudiante ha hecho a cada  recurso. En la figura 3 se muestra la  representación obtenida. Desde esta  representación puede accederse al detalle  de los accesos del recurso. En la Figura 4  podemos ver un ejemplo de la  representación, las fechas son representadas  en X y los estudiantes en Y. El histograma  de abajo representa los accesos hechos por  los estudiantes al recurso en cada fecha.       Fig 3 Accesos a los recursos       Fig 4 Detalle del acceso a un recurso  particular   2.3. Actividades   Discusiones  Con este gráfico los profesores pueden  conocer en cuales discusiones ha  participado cada uno de los estudiantes,  pueden observarse el número de mensajes  que han colocado, el número de mensajes  vistos y el número de temas iniciados por  cada uno.  Cada tipo de acción se  representa como un punto con diferente  icono (círculo, cuadrado, triángulo  respectivamente).     Actividades y tareas   Los profesores pueden observar las fechas  en que los estudiantes han resuelto las  diferentes actividades. Hay dos  representaciones que pueden ser  obtenidas, la primera en la cual se  representa en cada línea vertical a cada  una de las actividades y en el eje Y están  representados los estudiantes, una marca  en cada intersección denota el acceso a la  actividad por parte del estudiante.  La segunda representación consiste en  una matriz en la que se representan las  actividades en el eje X y los estudiantes  en Y, un cuadradito en la intersección  indica el acceso por parte de los  estudiantes, el tono del mismo representa el  número de accesos.  2.4. Conceptos   Actividades y conceptos   Moodle permite a los instructores de cada  curso definir conceptos y relacionarlos con  las actividades y cuestionarios para  evaluaciones. Luego es posible generar  gráficos en los que las actividades son  representadas en el eje Y, los conceptos  asociados en el X.     Estudiantes y conceptos   Con esta representación es posible  reconocer la perfo mance de cada  estudiante, representados en el eje Y,  respecto a cada concepto, representado en  el eje X. El tono de la marca en cada  intersección representa la performance  obtenida.  3. Conclusiones  En este documento se ha presentado una de las líneas  de trabajo sobre visualización de datos aplicadas a un  sistema que está siendo usado en materias de las  carreras de computación de nuestra Universidad.  Hemos presentado la herramienta de seguimiento y  monitoreo de estudiantes, GISMO, la cual extrae  datos de sistemas Moodle y genera representaciones  gráficas que son usadas por los instructores de los  cursos para examinar varios aspectos relacionados  con los estudiantes.  La herramienta descripta corresponde a una  aproximación nueva del uso de datos web log  generados por los accesos y utilización de la  plataforma de educación a distancia para ayudar a los  instructores a estar en conocimiento de lo que ocurre  en el dictado de las clases.   Técnicas de visualización de la información se han  aplicado para representar gráficamente la   complejidad multidimensional de los datos  registrados de los estudiantes.  La herramienta se puede considerar útil para ayudar a  los instructores a formar un modelo mental de sus  clases y consecuentemente ofrecer a los estudiantes  una asistencia adecuada.  La herramienta GISMO es parte del proyecto  “EDUKALIBRE, Libre software methods for EEducation”[7] fundado por la Unión Europea cuyo  objetivo era la creación de contenido adecuado para  ser utilizado como material para educación.  Actualmente un conjunto de expertos está evaluando  GISMO  Nuestra investigación inmediata apunta a la  aplicación de la herramienta en otros cursos  disponibles en PEDCO para poder evaluar la  utilidad de las visualizaciones. En el mismo sentido  intentaremos proponer otras metáforas de  visualización.	﻿visualización de datos , educación a distancia , herramienta gráfica interactiva , moodle , representación visual	es	20841
42	Proyectos interinstitucionales en ambiente colaborativo	﻿   La implementación del modelo para la generación de Proyectos Educativos  Colaborativos Soportados por Computadora (PECSCs) [1], tiene como marco didáctico  al aprendizaje colaborativo, al abordaje por proyectos y a la construcción colectiva.  A partir del modelo y con las herramientas de soporte para aprendizaje y trabajo  colaborativos, se presentan dos prototipos aplicativos que se están implementando en  Escuelas de la Provincia del Neuquén en el período 2005 y 2006.    Introducción    En seis escuelas medias de la Provincia del Neuquén y con el aporte del Proyecto de  Investigación E065 – Software para Aprendizaje y Trabajo Colaborativos, se esta  desarrollando un espacio de reflexión interinstitucional desde el que se diseñan  experiencias educativas apoyadas por recurso informático y se avanza en la  construcción colectiva de un marco teórico de referencia para la inserción de la  informática en la escuela media [2].    En este contexto, se plantea el desarrollo de Proyectos Educativos Colaborativos  Soportados por Computadora – PECSCs - como enfoque didáctico general, que se  estructura metodológicamente a partir de los conceptos de aprendizaje colaborativo,  abordaje por proyecto y construcción colectiva.    El aprendizaje colaborativo, entendido como el conjunto de estrategias didácticas que  ponen especial atención en la interacción inter-intra grupo y en la organización de la  influencia recíproca en función de lograr aprendizajes colectivos e individuales, aporta  el marco conceptual para plantear la organización social del proceso educativo en la que  los intercambios de experiencias resultan enriquecimientos colectivos.    El abordaje por proyecto actúa como estructurante del proceso de aprendizaje y a partir  del cual se abordan los contenidos específicos favoreciendo la construcción de un  esquema conceptual articulado y fuertemente vinculado a situaciones problemáticas  concretas. Trabajar con una propuesta continua, promueve la participación activa de los  alumnos y el desarrollo de habilidades y destrezas vinculadas al planeamiento y  ejecución de proyectos.    La construcción colectiva, concebida como la concreción de alternativas de producción  grupal, potencia las instancias de socialización y discusión en las que se reconstruyen  estructuras cognitivas previas. Trabajar con una producción tangible logra enfocar al  grupo sobre un objetivo común sumando puntos de vista y enfoques múltiples a una  construcción específica.    Se logra así, estimular la creatividad colectiva e individual, mejorar cualitativamente la  producción grupal, potenciar las instancias de diálogo constructivo y reestructurar la  percepción previa acerca del abordaje de un problema.    En síntesis, el aprendizaje colaborativo aporta modelos para la estructuración social de  las experiencias educativas, el abordaje por proyecto actúa como estructurante y da  continuidad al proceso educativo y la construcción colectiva potencia las instancias de  diálogo enfocando al grupo sobre una problemática común.    Modelo de colaboración    Se presenta un modelo de colaboración general como soporte conceptual para el  desarrollo de PECSCs, estructurado a partir de los diferentes niveles de desarrollo de un  proyecto educativo: Nivel de investigación y desarrollo, Nivel de Diseño, Nivel de  ejecución.                                    Nivel de  Investigación  Nivel de  Diseño Nivel de  Ejecución   Nivel de investigación y desarrollo: En este nivel se diseñan y construyen herramientas  y modelos teóricos para si mismo y para los niveles superiores; es el ámbito dónde se  realiza la investigación y se define las políticas y los esquemas de colaboración a  soportar.    El nivel de diseño: En el contexto del Trabajo Cooperativo Soportado por  Computadoras CSCW, un grupo de docentes participa del diseño de un Proyecto  Educativo Colaborativo Soportado por Computadoras PECSC, documentan su  ejecución, realizan evaluación de proceso y final, y elaboran conclusiones teóricas  potencialmente transferibles a otras instancias educativas.    El nivel de ejecución: En este ámbito un grupo de alumnos recorre las fases de  desarrollo de un PECSC, para esto disponen de herramientas que posibilitan la  implementación de diferentes esquemas de colaboración  y herramientas de  construcción colectiva pertinentes al proyecto.    Cada nivel se compone de tres espacios que en conjunto lo estructuran, posibilitando la  concreción de instancias particulares de PECSCs.    En el espacio de construcción se desarrolla la colaboración primaria con la interacción  intra-grupo. Este espacio soporta la interacción intra e inter grupos para la construcción  colectiva. Es soporte de la autogestión del proyecto para la definición de acciones,  interfase, auto-evaluación, agenda interna, etc.  Por ejemplo, en el nivel de diseño, en el espacio de construcción se diseña, documenta  su ejecución y evalúa, cada uno de los proyectos. En cada caso se utiliza una  herramienta de construcción colectiva: Wiki, para la formulación del proyecto y  elaboración de conclusiones teóricas; Blog, de registro para documentar el avance de la  experiencia y Foro de discusión para soportar la evaluación de proceso. Este proceso es  apoyado por herramientas de colaboración y de construcción colectiva disponibles en  PEDCO [3].  En el nivel de investigación, el grupo de investigación avanza en diferentes líneas: el  desarrollo de modelos y esquemas de colaboración generales, construcción de  soluciones conceptuales y técnicas, en lo referente a herramientas de colaboración,  construcción colectiva y laboratorios remotos. En general el proyecto de investigación  adopta el esquema de rompecabezas como estructurante de la colaboración.    En el espacio de apoyo se realiza la colaboración cercana con especialistas. Es el  espacio que aporta solidez multidisciplinar y mantiene el desarrollo del proyecto dentro  de una línea teórica. Con él, interactúan los grupos del espacio de construcción y se  realiza la transferencia interna.   El vínculo con especialistas en el campo disciplinar y didáctico fortalece las  posibilidades de diseñar, ejecutar y evaluar PECSs conceptualmente sólidos. Favorece  la construcción de conclusiones teóricas, desde la solidez conceptual, fuertemente  vinculadas al campo de la praxis y construidas colectivamente.   Investigadores vinculados al proyecto realizan aportes desde otras perspectivas teóricas,  especialistas en otras disciplinas, docentes y alumnos realizan observaciones desde la  perspectiva del usuario.     En el espacio de socialización se realiza la colaboración externa. Aquí se presentan  resultados del desarrollo más allá de los límites del proyecto. Se presenta lo  desarrollado al medio abriendo la posibilidad de que colaboradores externos aporten  puntos de vista específicos. El grupo se posiciona como autor, logrando la trascendencia  de sus producciones y completando el aprendizaje. El grupo de alumnos y el de  docentes interactúan más allá del ámbito del aula.  Los resultados de las experiencias educativas, especialmente los que están en el campo  de las construcciones teóricas, son socializados en espacios académicos, científicos y  propios a la implementación del modelo de colaboración. Se promueve la consideración  del docente como profesional reflexivo, principal autor de las construcciones teóricas de  referencia para el área y se potencia la transferencia horizontal.    Proyectos Interinstitucionales    PECSC: Una mirada desde la Estadística sobre el fracaso escolar    Alumnos de diferentes escuelas desarrollan un proyecto de investigación de tipo  estadístico tendiente a la producción y análisis de información de valor institucional, en  particular se trabaja sobre el fenómeno de fracaso escolar.   Los alumnos participan en forma activa de cada fase en el desarrollo del proyecto:  definición del objeto de estudio, diseño de la experiencia, trabajo de campo,  organización y procesamiento de la información, presentación de los datos estadísticos y  análisis e interpretación de la información  A partir de este proyecto se busca promover a nivel institucional la lectura crítica de la  realidad social e institucional, entendiéndola desde sus particularidades y a partir de su  lógica, construir estrategias de abordaje. A nivel de alumnos se busca que los grupos  logren posicionarse como sujetos críticos y activos y desarrollar estrategias y patrones  para el modelado, organización, producción y procesamiento de información.    Nivel de Ejecución:   • Espacio de Construcción: Ciento catorce alumnos, en el marco del aprendizaje  colaborativo soportado por computadoras CSCL, elaboran instrumentos, diseñan  planes de acción y organizan de tareas. Este proceso es apoyado por  herramientas de colaboración y de construcción colectiva.   La experiencia se estructura a partir de las fases de desarrollo de un proyecto de  investigación de tipo estadístico, las construcciones parciales y totales son de  carácter colectivo y la interacción es estructurada a partir de un esquema de  colaboración de tipo espejo.  • Espacio de Apoyo: Especialistas en estadística apoyan el desarrollo  estableciendo diálogos constructivos con los grupos, analizando desde una  perspectiva científica las producciones, explicitando debilidades y participando  de instancias de formación en la disciplina.  Docentes del área informática  coordinan la interacción general y facilitan el desarrollo de habilidades  instrumentales y conceptuales necesarias para el modelado, organización,  producción y procesamiento de información. Otros docentes y las conducciones  institucionales aportan puntos de vistas multidisciplinares y dan carácter  institucional al proyecto     • Espacio de Socialización: Los avances y resultados de la investigación son  presentados en instancias institucionales y en medios de comunicación. Las  producciones trascienden el espacio áulico posicionando al alumno en un activo  en la producción de conocimiento de relevancia social y abriendo un espacio de  colaboración externa.          PECSC: Las noticias de hoy en boca de los adolescentes    Alumnos de diferentes escuelas participan de la redacción de un periódico on-line, en el  que los hechos son presentados desde la perspectiva del adolescente, proponiendo una  mirada alternativa al punto de vista adulto presente en los medios de comunicación.   Los alumnos participan en forma activa en la producción de contenido y en la definición  de la fisonomía de la publicación.  A partir de este proyecto se busca posicionar en un rol activo del alumno frente a las  TICs y a la producción de contenido, fortalecer los procesos de lectura y escritura en los  adolescentes y abrir un espacio de expresión desde el que se lea y escriba la realidad  desde la perspectiva del adolescente. Se promueve que los grupos desarrollen  habilidades relacionadas a la expresión escrita mediada por TICs.    Nivel de Ejecución:   • Espacio de Construcción: Sesenta y cuatro alumnos, en el marco del aprendizaje  colaborativo soportado por computadoras CSCL, construyen acuerdos acerca de  la estructura de la publicación, redactan artículos, comentan artículos de otros  grupos.  Este espacio se soporta con las herramientas disponibles en el espacio  de redacción de un sistema de publicación   La experiencia se estructura a partir de las fases propias a la redacción de un  artículo y las de definición de la estructura de una publicación. La interacción en  la redacción de artículos se estructura a partir de un esquema de colaboración  tipo rompecabezas y las definiciones de estructura de la publicación en espejo.  • Espacio de Apoyo: Especialistas en redacción de textos y periodismo apoyan el  desarrollo colaborando en la definición de estructuras generales y acuerdos.  Docentes de las áreas de informática y de lengua coordinan la interacción  general y facilitan el desarrollo de habilidades instrumentales y conceptuales  necesarias para la producción de contenido.   • Espacio de Socialización: La divulgación de las producciones escritas es central  a este proyecto. Esta se da de forma inmediata y continúa por lo el reposicionamiento del alumno frente a las TICs y a la producción de contenido se  da tempranamente incidiendo positivamente en explicitar el carácter protagónico  del alumno en el proceso de enseñanza aprendizaje.  Conclusiones y trabajo futuro  El enfoque didáctico presentado se posiciona como meta-estrategia aportando un marco  de referencia al desarrollo de construcciones didácticas específicas y definidas por las  particularidades propias de cada instancia de enseñanza aprendizaje. Estas instancias  pueden implementar diferentes esquemas de colaboración, estilos de conducción de  proceso, formas de agrupamiento, modalidades de evaluación y acreditación, etc.    Otras aplicaciones que se encuentran en diferentes estadios de ejecución incluyen los  PECSCs Ciudad Virtual, Robótica en la Escuela [4].    En el espacio de construcción del nivel de investigación se continua desarrollando  mejoras a las herramientas de soporte, con colaboración con los grupos de estos  proyectos.	﻿Proyectos Interinstitucionales , Ambiente Colaborativo	es	20857
43	Modelo de equilibrio para agentes robóticos bípedos	"﻿    La necesidad de los robots ha cambiado en los últimos años de automatización industrial a sistemas de robots  amigables. La plataforma de robot humanoide PINO es una base interesante para avanzar en el desarrollo de agentes de  asistencia al hombre en la vida diaria. En este trabajo se presenta un modelo de sensor que tiene por objetivo determinar  la inclinación absoluta de un agente móvil. Se adjuntan las evaluaciones de comportamiento obtenidas empíricamente.     1 –Introducción     Típicamente, los robots tradicionales son usados para la automatización industrial, actúan en  ambientes separados de la esfera de las vidas y actividades humanas. A medida que se  incrementa  la edad promedio de las sociedades, se espera la incorporación de robots que asistan en las  actividades del hombre en sus ambientes comunes tales como oficinas, casas y hospitales. En  especial, se espera el surgimiento de robots humanoides debido al antropomorfismo, aplicabilidad  de la locomoción, comportamiento dentro del ambiente en que vive el hombre, etc. Para cubrir esta  demanda, en estos años se han desarrollado una gran variedad de robots humanoides.   Uno de ellos es WABIAN (Waseda Bipedal humANoid) construido por la Universidad  Waseda [1]. A WABIAN lo sucedió WABOT-1 (Waseda roBOT-1), el cual es el primer robot  humanoide de tamaño normal construido en 1973 en el laboratorio del profesor Kato. Él fue pionero  en el desarrollo de robots humanoides y WABIAN es el robot tradicional. WABIAN-RII, tiene un  total de 43 grados de libertad, una altura de 1890 [mm], ancho de 902 [mm], y 131.4 [kg] de peso.  Tiene una figura  humanoide con dos piernas, dos brazos, dos manos, y dos ojos, y es capaz de  caminar.   H6 y H7 son robots humanoides construidos  por la Universidad de Tokio [2]. H6 mide 1370  [mm] de alto y 590 [mm] de ancho, y tiene 35 gr.de libertad. Su peso es de 55 [kg] gracias al uso de  tecnología de aeronaves en el armazón del cuerpo, lo que le proporcionó una estructura fuerte y  liviana. H6 y H7 pueden caminar hacia arriba y hacia abajo por escalones de 25 [cm] de alto y  pueden reconocer rostros humanos. JOHNNIE [3] es un robot bípedo antropomorfo autónomo  construido por la Universidad Técnica de Munich. Es capaz de caminar sobre camino parejo o  desparejo y en curvas. Puede caminar a aproximadamente 2 km/hora.   El robot humanoide más impresionante es el robot humanoide HONDA, cuando P2, el segundo  prototipo de robot humanoide HONDA, fue mostrado en 1996 después de diez años de  investigación en secreto, el mundo de la robótica fue sacudido. P2 es el primer robot humanoide  inalámbrico del mundo, que puede caminar y subir y bajar escaleras [4]. Reduciendo las  dimensiones de P2 (1820 [mm] de altura, 600 [mm] de ancho, 210 [kg] de peso incluyendo las  baterías, 6 grados de libertad/pierna, 7 grados de libertad/brazo, 2 grados de libertad/mano), P3  (1600 [mm] de alto, 600 [mm] de ancho, 130 [kg] de peso incluyendo baterías, 6 grados de  libertad/pierna, 7 grados de libertad/brazo, 1 grado de libertad/mano) apareció en 1997 con la  misma movilidad que P2 [5]. En 2000, con una nueva reducción sobre P3, ASIMO (Advanced Step  in Innovative Mobility) apareció  con el tamaño de un niño (1200 [mm] de altura, 450 [mm] de  ancho, 43 [kg] de peso incluyendo baterías, 6 grados de libertad/pierna, 5 grados de libertad/brazo,  1 grado de libertad/mano, 2 grados de libertad/cabeza) y nueva tecnología para caminar (i-WALK)  [6]. La introducción de la tecnología i-WALK permite a ASIMO caminar continuamente mientras  cambia de dirección, y le dio al robot mayor estabilidad en respuesta a movimientos bruscos.   MK.5 es un robot humanoide de tamaño compacto con 24 grados de libertad construido por  la Universidad Aoyma Gakuin, con 356 [mm] de altura y 1.9 [kg] de peso [7]. El objetivo es  desarrollar robots humanoides móviles compactos, de bajo precio y con posibilidades de expansión.  Para lograr esto, todas las articulaciones son conducidas por servo módulos en modelos  radiocontrolados como PINO construido por ERATO [8].   Este trabajo es parte de un proyecto de investigación para la creación de un agente robótico  humanoide basado en el desarrollo GNU PINO [8]. Esta plataforma robótica cuenta únicamente con  el desarrollo de los efectores, por esto la tarea del proyecto es la creación de la lógica de control,  modelos de comportamiento y de percepción. En lo que respecta a percepción, la relacionada con el  mantenimiento del equilibrio es el objeto del presente trabajo.     2 - Enfoques metodológicos utilizados    Es conocida la importancia del factor equilibrio cuando nos referimos al andar en bípedos.  Básicamente el caminar puede considerarse similar a la corrección de una potencial caída. Esta  caída es medida por un juego de sensores, entre los cuales son de destacar los relacionados con el  mantenimiento del equilibrio.  La idea principal es el desarrollo de un sistema de sensores de equilibrios, cuyos módulos  son idénticos salvo por su orientación.  Utilizando un sensor de aceleración el resultado final de la medición es la inclinación  absoluta, proporcional al seno de la aceleración sensada.     3 – Procedimientos y resultados iniciales    Para la creación del sensor de equilibrio, se utilizó el integrado ADXL202 de Analog  Devices [9][10]. Se trata de un acelerómetro capacitivo de dos ejes con salida digital. La medida de  aceleración es informada a través de modulación de ancho de pulso (PWM) el cual es proporcional  a la medición. Mediante un microcontrolador se decodifica la señal emitida por el integrado para  obtener la aceleración estática en ambos ejes.    Cuando se observa el sensor microscópicamente, se puede apreciar una estructura  micromecanizada que parece una ""H"". Los delgados y largos brazos de la ""H"" están fijos al  substrato1. Los otros elementos están libres para moverse, lo forman una serie de filamentos finos,  con una masa central, cada uno actúa como una placa de un condensador variable, de placas  paralelas.  La aceleración o desaceleración en el eje “SENSOR”, ejerce una fuerza a la masa central. Al  moverse libremente, la masa desplaza las minúsculas placas del condensador, provocando un  cambio de capacidad. Este cambio de capacidad es detectado y procesado para obtener la salida  deseada.  Este trabajo se puede dividir en tres tareas:   1. Medición  Fundamentalmente se realiza a partir del acelerómetro en conjunto con ciertos componentes  externos pasivos. El mismo realiza la medición de aceleración, la que es transmitida al  microcontrolador mediante PWM.  2. Procesamiento de las señales PWM  Se implementó en un microcontrolador PIC 16F877A (8K FLASH) de la familia de  Microchip, el cual trabaja a 20 Mhz. Este se encarga de detectar los flancos de subida y bajada de  ambas salidas (X e Y) proporcionadas por el acelerómetro y guarda la variación de los mismo  mediante el uso de un contador de 16 bits.                                                    1 Soporte en forma de lámina sobre el que se efectúa la deposición de películas delgadas.  Una vez realizado un muestreo, una serie de cálculos convierten los tiempos obtenidos en la  medida de aceleración (Fig.6 y ecuación de aceleración). El acelerómetro es sensible a las  variaciones térmicas. Las experiencias fueron realizadas a una temperatura constante de 22º C, el  microcontrolador contiene en su memoria de programa constantes obtenidas en la calibración a  dicha temperatura.  El código fuente fue realizado en una variante simplificada del lenguaje C para  microcontroladores PIC, la detección de los flancos se realizo con ASSEMBLER para ganar  eficiencia en el proceso. Finalmente, el microcontrolador transmite mediante el uso de su UART los  resultados en formato ASCII.    3. Recepción de datos   La recepción de datos se realiza a 9600 baudios, mediante un software desarrollado en  lenguaje Delphi. El mismo permite la visualización de los datos entrantes.  Una vez obtenidos los datos, el programa permite guardar los mismos en formato de texto  sin atributos.    Pruebas realizadas  Determinación del ángulo de inclinación de un plano    Inicialmente se realiza la calibración del sensor para calcular las constantes de aceleración a  0 g (cero gravedad) y la sensibilidad (variación de lectura por unidad de g), obteniéndose así cuatro  constantes (dos para cada eje). Estas constantes son específicas de cada sensor y dependientes de la  temperatura de trabajo.  La calibración se realiza rotando el dispositivo de la Fig. 3,  360 grados a 2 rpm.     Tomando a Y como eje de giro (Figura 3), se determina la inclinación del plano XY (plano  horizontal en la figura) a partir de los datos de aceleración de los vectores X+ e Y+ del sensor.          Figura 3 Dispositivo sobre el que se monta el acelerómetro.       Las aceleraciones de los vectores X+ e Y+ del sensor (Fig. 4) sirven de base para el cálculo  de inclinación de la proyección en el plano ZX de dichos vectores. A partir de esta información se  determina la inclinación del plano XY. (Fig.5)     Inclinación del plano XY 0 1 2 3 4 5 6 7 1 21 41 61 81 101 121 tiempo rad ian es angulo proy. vector Xsens vs Z angulo proy. vector Ysens vs Z angulo inclinación del plano XY Aceleración de Ejes del sensor -1.5 -1 -0.5 0 0.5 1 1.5 1 21 41 61 81 101 121 tiempo Ac ele rac ión Eje X Eje Y Figura 4 Gráfica de aceleración de  vectores X e Y del sensor                            Figura 5 Gráfica de inclinación del  plano XY     El cálculo de aceleración de X e Y del sensor se realiza a partir de los tiempos T1x, T1y y T2  generados por el sensor. (Figura 6)        Figura 6 Técnica de decodificación de alta velocidad para el ADXL202      Aceleración = [(T1/T2) – (T1/T2 a 0g)]/ sensibilidad      4 - Consideraciones finales    Los resultados obtenidos en este estudio indican una buena sensibilidad del dispositivo, los  pasos a seguir incluyen el montaje de un sistema de sensores con la misma base de cálculos e  integración de señales, y pruebas en condiciones similares a las que se estima estará sometido el  agente."	﻿Agentes robóticos bípedos , Modelo de equilibrio	es	21141
44	Entorno de código móvil seguro	﻿ En este trabajo se presenta la línea Código Móvil Seguro del grupo de investigación “Procesadores de Lenguajes” del Departamento de Computación de la Universidad Nacional de Río Cuarto. Se presenta una técnica para garantizar código móvil seguro, denominada ProofCarrying Code based on Static Analysis (PCC-SA), cuya principal ventaja es que el tamaño de las pruebas generadas es lineal respecto a la longitud de los programas. A fin de demostrar la aplicabilidad de esta tecnica, se han implementado prototipos de un compilador certificante y un verificador de codigo basados en ella. En este trabajo tambien se presentan nuevas lineas de trabajo iniciadas, entre las cuales se destacan la extensión del prototipo de compilador certificante, la paralelización de PCC-SA y un modelo de seguridad para programas concurrentes. Palabras Clave: Código Móvil Seguro, Análisis Estático, Verificación de Código, Certificación de Código, Compiladores Certificantes. 1 Introducción La interacción entre sistemas de software por medio de código móvil es un método poderoso que permite instalar y ejecutar código dinámicamente. De este modo, un servidor puede proveer medios flexibles de acceso a sus recursos y servicios internos. Pero este método presenta un riesgo grave para la seguridad del receptor, ya que el código móvil puede utilizarse también con fines maliciosos, dependiendo de las intenciones de su creador o un eventual interceptor. Existen diversos enfoques tendientes a garantizar la seguridad en un ambiente de codigo movil. Una técnica que despertó mucho intéres en el último tiempo es Proof-Carrying Code (PCC). ProofCarrying Code (PCC), una tecnica propuesta por Necula y Lee [5] para garantizar codigo movil ∗Este trabajo ha sido realizado en el marco de proyectos subsidiados por la SECyT de la UNRC y por la Agencia Córdoba Ciencia y por la NSF. seguro, ha despertado mucho interes y ha generado activas lineas de investigacion con una importante produccion [1, 2, 3, 5, 6]. Sin embargo, PCC esta limitado a propiedades que puedan expresarse en un sistema de tipos formal. La idea básica de PCC consiste en requerir al productor de código la evidencia necesaria, en este caso una prueba formal, de que su código satisface las propiedades deseadas. Esta evidencia constituye un certificado que puede ser verificado independientemente, es decir, no requiere autentificación del productor. Proof-Carrying Code based on Static Analysis (PCC-SA) [7] es un entorno de ejecución de código móvil seguro basado en PCC y analisis estatico. El objetivo es proporcionar una solución en aquellos casos en los cuales la política de seguridad no puede ser verificada eficientemente por un sistema de tipos formal, como es el caso de verificar inicialización de variables y accesos válidos a arreglos. PCC-SA utiliza un código intermedio de más alto nivel que PCC: un árbol sintáctico abstracto (AST) anotado con información del estado del programa. Este tipo de representación intermedia permite realizar diversos análisis estáticos para generar y verificar la información necesaria, y permitiendo ademas aplicar ciertas optimizaciones al código generado. La principal ventaja de PCC-SA reside en que el tamaño de la prueba generada es lineal con respecto al tamaño de los programas. Además, la complejidad de la generación de las anotaciones y la verificación de la seguridad del código también es lineal con respecto al tamaño de los programas. En este trabajo se presentan las líneas de investigación abordadas en el campo del código móvil. En primer lugar se presentan los resultados obtenidos en el último año: el entorno de ejecución PCC-SA (sección 2.1), el prototipo de compilador certificante desarrollado (sección 2.2) y el prototipo verificador de la prueba desarrollado (sección 2.3). En la sección 3 se presentan los trabajos inciados que incluyen la extensión y optimización del prototipo de compilador certificante 3.1, la paralelización del entorno de ejecución PCC-SA (sección 3.2) y una extensión de PCC en entornos concurrentes (sección 3.3). 2 Resultados Obtenidos 2.1 El Entorno de Ejecución PCC-SA La figura 1 muestra la interacción de los componentes del entorno PCC-SA. Siguiendo la notación de Necula [5] los cuadrados ondulados representan código y los rectangulares representan componentes que manipulan dicho código. Además, las figuras sombreadas representan entidades no confiables, mientras que las figuras blancas representan entidades confiables. Consultar [7] para más detalles de PCC-SA y del prototipo desarrollado. Figura 1: Vista global del entorno de ejecución de PCC-SA. En nuestra implementacion, el Compilador toma como entrada el código fuente, escrito en un simple lenguaje imperativo denominado Mini, y produce el código intermedio, en forma de un árbol sintáctico abstracto. Este código intermedio es una representación abstracta del código fuente que puede ser utilizado independientemente del lenguaje fuente y de la política de seguridad. El Compilador es un compilador tradicional que realiza los típicos análisis lexicográficos, sintácticos y semánticos. El Generador de Anotaciones (GenAnot) efectúa diversos análisis estáticos, generando la información necesaria para producir las anotaciones del código intermedio de acuerdo a la política de seguridad. La política de seguridad que definimos garantiza seguridad de tipos y de memoria, además de garantizar que no se leen variables no inicializadas y que no se accede a posiciones no definidas sobre los arreglos. La formalización de la política de seguridad es presentada en [7]. En aquellos puntos del programa en donde las técnicas de análisis estático no permiten determinar fehacientemente el estado, se insertan chequeos en tiempo de ejecución para garantizar de esta forma seguridad durante la ejecución del codigo. En caso de encontrar que en algún punto del programa forzosamente no se satisface la política de seguridad, el programa es rechazado. El último proceso del lado del productor de código es realizado por el Generador del Esquema de Prueba. Éste, teniendo en cuenta las anotaciones y la política de seguridad, elabora un esquema de prueba considerando los puntos críticos y sus posibles dependencias. Esta información se encuentra almacenada en el código intermedio. El Compilador, el Generador de Anotaciones y el Generador del Esquema de Prueba conforman el Compilador Certificante. El consumidor de código recibe el código intermedio con anotaciones y el esquema de prueba. Este esquema de prueba es básicamente el recorrido mínimo que debe realizar el consumidor de código sobre el código intermedio y las estructuras de datos a considerar, a fin de verificar el codigo recibido. El Verificador del Esquema de Prueba es el encargado de corroborar el esquema de prueba generado por el productor de código. Por último el Verificador de la Integridad de la Prueba verifica que el esquema de prueba haya sido lo suficientemente fuerte para poder demostrar que el programa cumple con la política de seguridad. Esto consiste en verificar que todos los puntos críticos del programa fueron chequeados por el Verificador del Esquema de Prueba o bien contienen un chequeo en tiempo de ejecución. Esto es necesario porque si el código hubiese sido modificado durante su envío, el esquema de prueba puede no contemplar ciertos puntos del programa potencialmente inseguros. El Verificador de Esquema de Prueba y el Verificador de la Integridad de la Prueba componen el Verificador de la Prueba. 2.2 Prototipo de Compilador Certificante Con el objetivo de experimentar y analizar el enfoque propuesto, se desarrolló un prototipo del Compilador Certificante descripto en la sección 2.1. Este prototipo se denominó CCMini. Como primera medida se definieron los aspectos de seguridad que se deseaban garantizar, es decir, la política de seguridad. La política de seguridad definida para el prototipo consiste en que toda variable se encuentra inicializada al momento de ser utililizada y en que todo índice de arreglo es un índice válido. El próximo paso consistió en establecer el código intermedio que se utilizaría, un árbol sintáctico abstracto (AST). Luego se definió el lenguaje fuente del compilador, denominado Mini. Finalmente se implementaron los módulos correspondientes al Generador de Código Intermedio, el Generador de Anotaciones, el Certificador y el Generador del Esquema de Prueba. Con esta primera implementación se pretende por un lado poder analizar la factibilidad de su inclusión dentro del framework de PCC-SA y por el otro analizar su efectividad y eficiencia. 2.3 Prototipo de Verificador de la Prueba Para probar la factibilidad de uso de PCC-SA, se desarrolló un prototipo de verificador de la prueba. El prototipo es un verificador que permite descargar programas escritos en un código intermedio y ejecutarlo de manera segura. Debido a que ya se habian definido los aspectos de seguridad que se deseaban garantizar y el AST, el primer paso de esta implementación consistió en desarrollar el Verificador del Esquema de Prueba, el Verificador de la Integridad de la Prueba. Luego se diseñoó el Generador de Código. El prototipo fue implementado usando el lenguaje de programación Java. Al igual que CCMini, el diseño del prototipo de verificador de la prueba asegura que sólo serán descartados aquellos programas que son ciertamente inseguros. Si el prototipo no puede determinar la seguridad de un programa, este inserta chequeos en tiempo de ejecución en los puntos críticos. Utilizando el prototipo desarrollado, se realizaró una serie de experimentos 1. En la mayoría de los casos, el prototipo determinó la seguridad de los programas sin insertar chequeos en tiempo de ejecución. En cuanto al tamaño de las pruebas, los experimentos mostraron que el tamaño de las pruebas es lineal al tamaño de los programas. 3 Trabajos Iniciados 3.1 Extensión y optimización del prototipo de compilador certificante Con el objetivo de determinar la factibilidad de uso de PCC-SA en aplicaciones “reales”, se está trabajando en la extensión del prototipo de compilador certificante CCMini. Para esto, se extenderá el lenguaje fuente de modo de incluir el tratamiento de punteros, invocación de funciones y otras sentencias de flujo de control (for y repeat). Esto traerá aparejado la necesidad de extender la política de seguridad para considerar nuevos puntos críticos. Además, parte del trabajo a realizar consistirá en optimizar el proceso de generación de anotaciones utilizando Pilas de Contextos [7]. El diseño del compilador certificante ser simple y sencillo de manera tal que si éste es aplicado en un ambiente de código móvil seguro, el verificador no consuma demasiados recursos. Esta es una caracteristica importante, ya que el entorno podria ser aplicado a ambientes con recursos limitados como por ejemplo las tarjetas inteligentes (SmartCards). 3.2 Paralelización de PCC-SA En busca de mejorar la performance del framework PCC-SA, se está trabajando en el desarrollo de un nuevo entorno denominado Parallel Proof-Carrying Code based on Static Analysis (Parallel PCCSA) que permitirá verificar la prueba de seguridad en paralelo. La idea básica de Parallel PCC-SA consiste en requerir al productor de código una prueba de que el código satisface las propiedades de seguridad deseadas dividida en q esquemas de pruebas las cuales pueden ser verificadas en paralelo. La figura 2 muestra la interacción de los componentes del entorno. Al igual que en PCC-SA, el Compilador Certificante toma como entrada el código fuente y produce como resultado una representación intermedia anotada. El Generador de Esquemas de Pruebas es el encargado de generar el esquema de prueba y dividir el esquema de prueba en q esquemas de pruebas para que puedan ser verificados independientemente. Una vez obtenidos estos esquemas de pruebas, los mismos son enviados al consumidor de código para realizar su verificación y la ejecución del código. Figura 2: Estructura del entorno Parallel PCC-SA. 1Mas detalles de los experimentos realizados consultar [8] 3.3 Proof-Carrying Code en Entornos Concurrentes El objetivo es definir un entorno de código móvil que garantice la seguridad de programas Java multi-thread. Los programas podrán contar con una especificación en el Java Modeling Language (JML), el cual es un lenguaje de especificación formal que permite especificar contratos en programas Java por medio de invariantes de clases y pre y post-condiciones de métodos. Además, estos programas deberán respetar el nuevo modelo de memoria de Java (JMM), que es parte de Java 5.0 [4] y soluciona las limitaciones que tenía el modelo anterior. Según Pugh [4], el modelo anterior prohibe la aplicación de muchas optimizaciones de código, mientras que el acceso no sincronizado a objetos inmutables (final) puede no ser thread-safe. En esta línea se comenzó a trabajar en un proceso de compilación para una extension del JML introducida por Rodrígez et. al [9]. Los constructores provistos por esta extensión permiten especificar propiedades de no-interferencia y atomicidad de métodos en programas Java multithreaded. Además, permiten realizar la verificación en forma modular. El proceso de compilación genera un archivo .class que incluye la especificación JML en un atributo que instancia una estructura de datos predefinida por la Java Virtual Machine Specification JVMS. Se podría utilizar un compilador estándar Java para generar el .class y luego insertar la especificación JML. Sin embargo, si se considera el nuevo JMM se deben tener en cuenta las restricciones impuestas y analizar las acciones que debe realizar el compilador para no violar ni la especificación JML ni el modelo de memoria. Un componente muy importante de esta línea es el desarrollo de los métodos para poder razonar usando las especificaciones del JML extendido sobre bytecode. Dichos métodos deben permitir verificar la especificación y generar la prueba para que pueda ser utilizada en un ambiente de código móvil.	﻿Código Móvil Seguro , Análisis Estático , Verificación de Código , Certificación de Código , Compiladores Certificantes	es	21201
45	Métodos de acceso por similitud	﻿Métodos de Acceso por Similitud	﻿Similitud , Métodos de Acceso	es	21225
46	Análisis e Implementación del lenguaje AgentSpeak L para agentes inteligentes con arquitectura BDI	﻿ón de dos sistemas BDI que han sido implementados: PRS (Procedural Reasoning System) [3] y dMARS (Distributed Multi-Agent Reasoning System) [4]. A continuación se describirá brevemente los conceptos principales de AgentSpeak(L), más detalles pueden encontrase en [1, 5, 6]. Un agente programado en AgentSpeak(L) podrá actuar en un entorno cambiante donde recibirá continuamente percepciones acerca del entorno en el que se desenvuelve. Como AgentSpeak(L) está basado en la arquitectura BDI, el agente hará un análisis de sus actitudes mentales representadas por los estados de información (creencias), de motivación (deseos o metas) y deliberativo (intenciones). En base a este análisis y con el fin de lograr sus metas, el agente podrá decidir que acciones realizar, las cuales afectaran al entorno. Específicamente en Agentspeak(L), los agentes disponen de un conjunto de creencias y un conjunto de planes, donde estos últimos se activan mediante eventos que se producen debido a cambios o estímulos en el entorno del agente. Los planes organizan de manera jerárquica la ejecución de metas (indicadas por el disparo de eventos) y las acciones del agente. Informalmente, un agente AgentSpeak(L) consiste de: un conjunto de creencias B, que incluye hechos concernientes a las propiedades y características del dominio de aplicación (es decir, una representación del mundo), como así también a los nuevos hechos que son generados por acción del agente; 1Financiado parcialmente por SeCyT Universidad Nacional del Sur (subsidio: 24/ZN09) y por la Agencia Nacional de Promoción Científica y Tecnológica (PICT 2002 Nro 13096) un conjunto de planes P, que es un repositorio que contiene los planes disponibles para usar por el agente. Estos planes están pre-compilados y no requieren ningún tipo de planificación en tiempo de ejecución por parte del agente2; un conjunto de eventos E, que almacena las percepciones del agente acerca de los cambios en su entorno y la solicitud de metas a cumplir; un conjunto de acciones A, que son las que pueden modificar el estado del entorno. Las acciones están explícitamente ordenadas en el cuerpo de cada plan para su futura instanciación y ejecución; un conjunto de intenciones I, donde una intención es una pila de planes parcialmente instanciados3 y donde un pedido de ejecución fue realizado; una función de selección de eventos SE , que elige un evento para procesar de un conjunto de eventos E. Los eventos son acumulados en E a medida que se perciben; una función de selección de plan aplicable SO, que es la responsable de elegir un plan de la librería de planes P y una función de selección de intención SI , que elige una intención para ejecutar del conjunto de intenciones actuales I. En la figura 1 puede verse gráficamente el ciclo que representa el funcionamiento de un agente en AgentSpeak(L).4 Figura 1: Gráfico del intérprete de AgentSpeak(L) [6]. 2La tarea de generar planes automáticamente por el agente no está contemplada en AgentSpeak(L). 3Un plan parcialmente instanciado es una serie de medios propuestos que representa una copia del plan original, ahora bajo el rol de actitud mental que dirige el comportamiento. Notar la diferencia con el plan como norma de comportamiento, que está en el conjunto P. 4La función de revisión de creencias no es parte de la definición original de AgentSpeak(L). Este agregado pertenece al trabajo de Machado et al. en [6] Cuando el agente nota un cambio en el entorno o cuando un usuario externo requiere que el sistema adopte una meta, se genera un evento disparador apropiado. Este tipo de eventos son denominados como eventos externos. Un agente también puede generar eventos internos. Un evento interno es aquel que se dispara por estar indicado en el conjunto de fórmulas a ejecutar de un plan. Los eventos, internos o externos, son asincrónicamente agregados al conjunto de eventos E. La función de selección SE selecciona del conjunto E un evento para procesar. Este evento es removido de E y es usado para unificar con los eventos disparadores de los planes del conjunto P. Los planes cuyos eventos disparadores así unifican son llamados planes relevantes y el unificador es llamado unificador relevante. Luego, el unificador relevante es aplicado al contexto de los planes relevantes obtenidos. En un plan, la condición de contexto indica cuál debe ser la situación del agente y de su dominio para que las acciones del mismo puedan ser ejecutadas. Luego, al aplicar el unificador relevante sobre los contextos se obtiene una sustitución de respuesta correcta sobre los planes, tal que el contexto es una consecuencia lógica del conjunto de creencias base B. Los planes que sustituyen satisfactoriamente su contexto son llamados como planes aplicables u opciones y la composición del unificador relevante con la sustitución de respuesta correcta es llamada unificador aplicable. Por cada evento puede haber muchos planes aplicables u opciones. La función SO elige uno de estos planes. Aplicando el unificador aplicable a la opción elegida se obtienen los medios propuestos para responder al evento disparador. Cada intención es una pila de planes parcialmente instanciados o de marcos de intención. En el caso de un evento externo, los medios propuestos son usados para crear una nueva intención, la cual es agregada al conjunto de intenciones I. En el caso de un evento interno para agregar una meta, los medios propuestos son puestos en el tope de una intención existente (la que generó el disparo del evento interno). Luego, la función de selección SI selecciona una intención para ejecutar. Cuando el agente ejecuta una intención, ejecuta lo que se encuentra en el tope del cuerpo de la intención. En el tope puede haber tres tipos de fórmulas: metas de logro y de testeo y acciones. La ejecución de una meta de logro es equivalente a generar un evento interno para agregar dicha meta a la intención actual. La ejecución de unameta de testeo es equivalente a encontrar una sustitución para dicha meta que la haga consecuencia lógica de las creencias base. Si tal sustitución es encontrada, la meta de testeo es removida del cuerpo del tope de la intención y la sustitución es aplicada al resto del cuerpo del tope de la intención. Si se encuentra una acción entonces ésta se ejecuta y se elimina del cuerpo del tope de la intención. El agente ahora vuelve al conjunto de eventos E y el ciclo entero continúa hasta que no haya más eventos en E o hasta que no queden intenciones ejecutables. AgentWhisper(L): Una implementación en Prolog de AgentSpeak(L) El objetivo de este trabajo es investigar la aplicabilidad de los resultados teóricos desarrollados para la arquitectura BDI. Para el estudio de AgentSpeak(L) se decidió realizar una implementación en Prolog que permita disponer de un prototipo para analizar las características de este lenguaje. A continuación se describe muy brevemente a AgentWhisper(L), una implementación en Prolog de un subconjunto de AgentSpeak(L). Cabe destacar que existen otros análisis e implementaciones de AgentSpeak(L), como por ejemplo los mencionados en [5, 6]. En AgentWhisper(L), las creencias, los planes, los eventos y las intenciones son representadas mediante los predicados dinámicos bel\1, pln\3, evt\2 y int\2 respectivamente, con el objetivo de poder agregarlos o suprimirlos a lo largo del ciclo de ejecución. Las creencias que representan el dominio del agente se representan de la forma bel(b(t)) donde b es un predicado y t representa un término o una serie de términos t1,...,tn. Los eventos son representados con la siguiente estructura: evt(EventoDisparador, NombreIntencion). donde EventoDisparador es un predicado de la forma +!g(t) y NombreIntencion refiere mediante el nombre, a la intención que generó dicho evento disparador. En el caso de que fuera un evento externo, el nombre de la intención es trueIntention. En caso contrario, NombreIntencion es intention_X, donde X es un número natural. La estructura de un plan es la siguiente: pln(EventoDisparador, Contexto, Cuerpo). donde EventoDisparador es una fórmula, Contexto es una lista de predicados de creencia que de ser verdaderos en el momento de la selección del plan, determinan la aplicabilidad del mismo. Cuerpo es una lista de predicados indicando acciones o metas de logro que, por motivos de implementación, su último elemento es true. El conjunto de acciones A es específico del dominio de aplicación del agente. Igualmente, se puede decir que las acciones se representan mediante predicados de la forma nombreAccion(ListaParametros):- <definicionAccion> En este punto es importante aclarar algunas cuestiones de implementación. Debido a la complejidad de formular una función de revisión de creencias, la actualización de la base de datos de creencias y la posible generación de sus eventos indicadores respectivos, queda a cargo de la implementación de las acciones. Cada intención se representa como una pila de planes instanciados. Para lograr el efecto pila, se utiliza el predicado dinámico int\2 para indicar un elemento de la pila/intención. La inserción y supresión de elementos se logra mediante el uso del predicado asserta\1 y del retractall\1 respectivamente. Un elemento de la pila es un plan con sus variables instanciadas. int(NombreIntencion, pln(EventoDisparador, Contexto, Cuerpo)). Las funciones de selección del modelo, al igual que las acciones, dependen del dominio de aplicación del agente. En AgentWhisper(L), la función SE utiliza un criterio de pila de eventos para seleccionar uno; es decir, el primer evento en entrar es el último evento en salir. La función de selección de planes u opciones aplicables SO se comporta de manera similar. SO toma el primer plan aplicable que encuentra en la base de datos. Para la función de selección de intenciones SI se especificó un comportamiento un poco más inteligente. Al momento de elegir una intención para ejecutar en un determinado ciclo del agente, SI evaluará cuál de las intenciones disponibles en I es la más conveniente. Luego de que la función SI eligió una intención para ejecutar, el intérprete de AgentWhisper(L) analiza qué tipo de fórmula es la que se encuentra primera en el cuerpo del plan tope de la intención. Si la primera fórmula es una meta de logro, el accionar del intérprete no varía. Se coloca un evento en el conjunto E correspondiente a la meta de logro recién evaluada y se actualiza la intención removiendo esta fórmula del cuerpo del plan tope. Ahora, si la primera fórmula es una acción, el intérprete la ejecuta, luego la remueve y evalúa la nueva fórmula que ahora está primera en el cuerpo del plan. Si es una acción, repite este proceso nuevamente. Si es una meta de logro, entonces el intérprete abandona esta intención y continúa con el ciclo habitual de ejecución. En resumen, las fórmulas de un plan de una intención serán ejecutadas de corrido siempre y cuando dichas fórmulas sean todas acciones. De esta manera se previene que la secuencia u ordenamiento de las intenciones a ejecutar no influya en el resultado de las acciones. Conclusiones y trabajo futuro Gracias a la implementación de AgentWhisper(L), se pudo ver con mayor claridad algunos puntos oscuros que se encuentran tanto en las definiciones del modelo BDI como en la formalización de AgentSpeak(L). Uno de los puntos no considerados por Rao en [1] es el hecho de no incluir explícitamente el proceso de revisión de intenciones que Bratman et al. en [2] describen como el proceso de filtrado. Quizá AgentSpeak(L) permita esta funcionalidad con el manejo de los distintos tipos de metas de logro pero, de todas formas, esta consideración queda a cargo del programador de agentes y por lo tanto, fuera de la arquitectura. Este problema, conlleva a la ejecución de acciones incoherentes o fuera de contexto cuando hay más de una intención en el conjunto de intenciones I. Otro punto que merece atención es la falta de definiciones operacionales de las funciones de selección SE , SO y SI . Si bien es cierto que son elementos de la arquitectura que son específicos del dominio de aplicación de cada agente, los autores que hablan del modelo AgentSpeak(L) [7, 1, 5] se abstraen de definirlas, a excepción de Machado et al. [6] que brinda una especificación de las funciones extremadamente simple para la implementación de AgentSpeak(L) que realiza. El argumento contra la no explicación del accionar de las funciones se debe a que son procesos del agente donde reside la codificación de una gran parte de su accionar inteligente. Por lo tanto, el hecho de no definir más específicamente la operatoria de dichas funciones hace que los mecanismos de análisis de oportunidades y los mecanismos de revisión de Bratman et al. [2] no estén concretamente visibles en AgentSpeak(L). Como trabajo futuro se planea avanzar en dos direcciones: extender a AgentWhisper(L) para cubrir todos los aspectos de AgentSpeak(L), y estudiar otros desarrollos teóricos y prácticos de la arquitectura BDI.	﻿Arquitectura BDI , Inteligencia Artificial , Agentes Inteligentes	es	21230
47	Framework comparativo de lenguajes de representación de ontologías	﻿ La web semántica es el siguiente estado de la web en su evolución. Es un concepto nuevo para muchos investigadores y desarrolladores que desde hace un tiempo estan trabajando para crear un red semántica de conocimiento que sea provechosa para las personas que la usen. La web semántica está soportada por un concepto denominado Ontología, tomado de la Inteligencia Artificial. Las Ontologías permiten definir el conocimiento de un dominio específico de forma tal que este pueda ser entendido y compartido por diferentes personas. Los lenguajes de representación de Ontologías juegan un papel muy importante en la web semántica como mecanismo para poder representar la información de los dominios de aplicación de forma explicita, para permitir de esta manera que las computadoras puedan finalmente “comprender”los datos que manipulan. Saber elegir entre uno y otro es un punto importante para la implementación de aplicaciones en la web semántica. 1. Introducción La web es quizás la aplicación más conocida y utilizada de la red mundial de información Internet, pero a pesar de esto, los creadores de la web [BLTO01] no tenían pensado desarrollar el tipo de web que existe hoy en día. Sus creadores habían pensado en una web en la que las computadoras no sólo intercambiaran la información que les es presentadas a sus usuarios, sino que además, como lo hacen las personas, pudiesen entender esta información. Esta web, en la que las computadoras muestran información a sus usuarios y además “comprenden”dicha información tiene nombre: Web Semántica. Con los años, la web ha evolucionado a pasos agigantados hasta convertirse en lo que es hoy en día; un medio flexible, económico y adecuado para un gran número de aplicaciones de diversa índole como por ejemplo el comercio electrónico, aplicaciones multimedia, difusión de información, entretenimiento, etc. Este crecimiento, conjuntamente con la evolución de las tecnologías que le dan soporte, la han convertido en una red de información tan grande y desorganizada que resulta muy difícil hoy en día implementar aplicaciones que tengan el grado de eficiencia que requieren los usuarios actuales. Un ejemplo de esto son los buscadores, que sólo realizan búsquedas basadas en patrones o palabras claves sin tener en cuenta la semántica de los datos. Muchos investigadores, institutos internacionales, laboratorios y empresas están trabajando para cambiar esto, para hacer de la web actual aquella que sus creadores alguna vez pensaron construir. La solución a algunos de los problemas de la web actual se llama Web Semántica. 2. Motivación Desarrollar la web semántica no es una tarea fácil de realizar. En primer lugar porque debe pensarse en cómo combinar la web actual con la nueva web y en segundo lugar porque es necesario que muchos investigadores y desarrolladores se pongan de acuerdo en el diseño e implementación de la web semántica y en el desarrollo de las tecnologías que le darán soporte. Actualmente existen numerosas líneas de investigación [Cas] dentro de la web semántica. Algunas de las más importantes son: Metodologías para el desarrollo de Ontologías. Integración de Ontologías. Aprendizaje de Ontologías. Desarrollo de vocabularios en dominios específicos. Agentes. Servicios web. Berners-Lee, Hendler y Lassila [BLTO01] explican que la web semántica “no es una web aparte sino que es una extensión de la actual en la que la información tiene un significado bien definido, posibilitando que los ordenadores y las personas trabajen en cooperación”. El punto de inicio para alcanzar la web semántica es justamente esta idea de convertir la información en conocimiento, de forma tal que las aplicaciones de software que se ejecutan en las computadoras puedan comprender la información de la misma manera que lo hace una persona que navega en la web. Esto significa que las computadoras deben poder distinguir entre los diferentes significados que tienen las palabras o frases de nuestro lenguaje de acuerdo al contexto en el que se encuentran. Si logramos ésto estaremos en presencia de una “red semántica”. Uno de los conceptos que está relacionado con el concepto de web semántica y que forma la base para su desarrollo es el de Ontología. Gruber [Gru93] define el concepto de ontología como “a formal explicit specification of a shared conceptualization”. Según esta definición, una ontología permite la representación de conocimiento de algún dominio específico de forma explícita y formal para permitir que un grupo de entidades (personas o entidades de software) puedan compartir este conocimiento y consensuar un vocabulario común para cooperar entre sí intercambiando la información del dominio. Las ontologías son la herramienta fundamental para especificar el conocimiento explícito e implícito de un dominio de aplicación y poder, de esta manera, entender dicho dominio en término de sus elementos, las relaciones entre sus elementos, funciones y reglas de comportamiento asociadas. El uso de ontologías nos permite modelar la información asociada a un dominio para luego poder crear aplicaciones que “entiendan”ese dominio y de esta manera puedan ser útiles para los usuarios de la web semántica. Ahora bien, si a través de las ontologías podemos definir un modelo explícito de un dominio de aplicación, la pregunta es: ¿cómo logramos que este modelo sea comprendido por las computadoras? Uno de los enfoques es realizar anotaciones explícitas de la semántica de los datos en los recursos (páginas) de la web actual. Para ésto es necesario un lenguaje de representación de ontologías para la web. Existen varios lenguajes de representación de ontologías, cada uno con ventajas y desventajas respecto de los demás e incluso existen estándares de facto en la comunidad de la web semántica. Ejemplos de estos lenguajes son: XOL [RKT99], SHOE (basado en el lenguaje HTML) [LH00], OML [Ken98], RDF [LW99] y RDFShema [BG99] creados por el consorcio W3C [W3C], OIL [Hor00], OWL [MDS03] y DAML+OIL [HvH01] entre otros. La mayoría de estos lenguajes están basados en el lenguaje XML [XML]. Debido a la diversidad de lenguajes que existen para la representación de conocimiento en la web, es necesario poder comparar los lenguajes para conocer las ventajas y desventajas de cada uno y para conocer cual se adapta mejor a las necesidades de representación que nos plantea cada dominio específico. Debido a que existen diferentes formas de comparar los lenguajes, es importante establecer criterios de comparación adecuados que nos ayuden a analizar cada uno de ellos. 3. Trabajo propuesto El propósito de nuestro trabajo es presentar la motivación de nuestra línea de investigación, metas y desarrollos futuros; aportando un framework comparativo de algunos de los lenguajes de representación de ontologías para mostrar ventajas y desventajas de cada uno de ellos y como cada uno se adapta a las distintas necesidades de representación. Lo que se busca es permitir que los desarrolladores e investigadores de la web semántica sepan cómo elegir el lenguaje de representación de ontologías que más se adecúe a las necesidades de representación.	﻿representación de ontologías , lenguajes	es	21258
48	Grupo de procesadores de lenguajes	﻿ En el último tiempo Proof-Carrying Code (PCC) ha despertado un gran interés surguiendo numerosas líneas de trabajo. PCC establece una infraestructura que permite garantizar que los programas se ejecutarán de manera segura. En esta alternativa, el productor de código adjunta al código móvil una demostración, mediante la cual el consumidor del código puede verificar su seguridad antes de la ejecución del programa. Análisis estático es un técnica con un gran potencial para producir la evidencia necesaria para garantizar código móvil seguro. Basándonos en estas dos técnicas se presenta una arquitectura para garantizar la ejecución de código móvil de manera segura. Además se presenta el prototipo implementado que demuestra la factibilidad del uso de la técnica. 1 Introducción La interacción entre sistemas de software por medio de código móvil es un método poderoso que permite instalar y ejecutar código dinámicamente. De este modo, un servidor puede proveer medios flexibles de acceso a sus recursos y servicios internos. Pero este método poderoso presenta un riesgo grave para la seguridad del receptor, ya que el código móvil puede utilizarse también con fines maliciosos, dependiendo de las intenciones de su creador o un eventual interceptor. Existen diversos enfoques tendientes a garantizar que las aplicaciones son seguras, una técnica que despertó mucho intéres en el último tiempo es PCC [6, 1]. La idea básica, de PCC, consiste en requerir al productor de código la evidencia necesaria, en este caso una prueba formal, de que su código satisface las propiedades deseadas. Esta evidencia constituye un certificado que puede ser verificado independientemente (no requiere autentificación del productor). Este tipo de esquemas, que producen software confiable durante la compilación, pueden ser dividido en dos pasos. El primer paso consiste en compilar el código fuente e introducir anotaciones en el archivo objeto (es decir bytecode para JAVA, assembly para C, o cualquier otro lenguaje intermedio). En la actualidad se pretende que estas anotaciones sean generadas e introducidas de manera automatizada para que PCC pueda ser utilizado en la industria. En el segundo paso, el código anotado es tomado por un verificador que chequea que el código anotado no viole las condiciones de seguridad impuestas. Si la verificación tuvo exito es seguro ejecutar la aplicación final. Muchos trabajos sobre PCC estan basados en sistemas de tipos, pero existen muchas casos en los cuales la política de seguridad deseada no puede ser verificada eficientemente por un sistema de tipos formal, como es el caso de verificar inicialización de variables y accesos válidos a arreglos. Esta falta de eficiencia se refiere tanto a nivel de razonamiento sobre propiedades del código como a nivel de performance. Además muchos aspectos relativos a seguridad no pueden ser representados por sistemas de tipos. ∗Este trabajo ha sido realizado en el marco de proyectos subsidiados por la SECyT de la UNRC y por la Agencia Córdoba Ciencia †Universidad Nacional de Río Cuarto,{pancho,nordio,jaguirre,marroyo}@dc.exa.unrc.edu.ar ‡Universidad Nacional de La Plata, gbaum@sol.info.unlp.edu.ar §Stevens Institute of Technology (New Jersey, EE.UU.), rmedel@cs.stevens-tech.edu La información necesaria para garantizar estos aspectos de seguridad puede ser generada mediante el análisis estático de flujo de los programas [3, 4]. De esta manera se aprovechan los beneficios del esquema PCC y del análisis estático. En esta línea de trabajo se definió la arquitectura de un entorno de ejecución de código móvil seguro. Este trabajo está estructurado de la siguiente manera: en la sección 2 se presenta la arquitectura junto con un breve análisis de las motivaciones y ventajas de la misma. En la sección 3 se comentan las características más relevantes del prototipo implementado para corroborar la factibilidad del enfoque. Por último se encuentran algunas conclusiones extraídas. 2 Nuestra Propuesta Proof-Carrying Code based on Static Analysis (PCC-SA) está centrado en poder brindar una solución en aquellos casos en los cuales la política de seguridad no puede ser verificada eficientemente por un sistema de tipos formal, como es el caso de verificar inicialización de variables y accesos válidos a arreglos. Esta falta de eficiencia se refiere tanto a nivel de razonamiento sobre propiedades del código como a nivel de performance. V. Haldar, C Stork y M. Franz [4] argumentan que en PCC estas ineficiencias son causadas por la brecha semántica que existe entre el código fuente y el código móvil de bajo nivel utilizado. Por estas razones, PCC-SA utiliza como código intermedio de más alto nivel como por ejemplo un grafo de flujo de control o un árbol sintáctico abstracto anotado con información de tipos. Además utiliza análisis estático para generar y verificar esta información. Sobre la representación intermedia se realizan diversos análisis estáticos y optimizaciones de código. Análisis estático es una técnica muy conocida en la implementación de compiladores que en los últimos años ha despertado interés en distintas áreas tales como verificación y re-ingeniería de software. La verificación de propiedades de seguridad puede ser realizada por medio de análisis estáticos debido a que este permite obtener una aproximación concreta del comportamiento dinámico de un programa antes de ser ejecutado. Existen una gran cantidad de técnicas de análisis estático, que balancean el costo entre el esfuerzo de diseño y la complejidad requerida, que pueden ser usados para realizar verificaciones de seguridad - por ejemplo, array-bound checking o escape analysis - [3]. Si bien las técnicas de análisis estático requieren un esfuerzo mayor que el realizado por un compilador tradicional - que solo realiza verificación de tipos y otros análisis simples de programas - el esfuerzo requerido, comparándolo con verificación formal de programas, es mucho menor. No hay que dejar de mencionar que se debe llegar a un acuerdo entre precisión y escalabilidad. Se debe asumir el costo de que en algunos casos se pueden rechazar programas por ser inseguros cuando en realidad no lo son. Es conveniente aclarar que si bien el análisis estático es un enfoque importante para garantizar seguridad no soluciona todos estos problemas. En muchos casos este no puede reemplazar controles en tiempo de ejecución, testing sistemático o verificación formal. Además no se avizora ninguna técnica que pueda eliminar todos los riesgos de seguridad, y por las ventajas que presenta el análisis estático debería ser parte del proceso de desarrollo de aplicaciones seguras [3]. Esta técnica podría ser utilizada, por ejemplo, en conjunción con PCC tradicional. Este enfoque, si bien usa análisis estático sigue los lineamientos de PCC y Compiladores Certificantes. En este enfoque se pueden insertar chequeos en tiempo de ejecución para ampliar el rango de programas seguros en los cuales el análisis estático no puede asegurar nada. Es decir, el mecanismo usado para verificar que el código es seguro puede ser una combinación de verificaciones estáticas - en tiempo de compilación - y de chequeos dinámicos - en tiempo de ejecución -. Cabe resaltar que, en muchos casos, es necesario insertar estos chequeos en tiempo de ejecución no solo por las limitaciones de un análisis estático particular sino porque los problemas a resolver son no computables como por ejemplo garantizar la seguridad al eliminar la totalidad de array-bound checking que en su caso general es equivalente al problema de la parada. La figura 1 muestra la interacción del sistema propuesto. Al igual que en [6] los cuadrados ondulados representan código y los rectangulares representan componentes que manipulan dicho código. Además, las figuras sombreadas representan entidades no confiables mientras que las figuras blancas representan entidades confiables. Figura 1: Vista Global de la arquitectura Proof-Carrying Code based on Static Analysis. El Compilador toma como entrada el código fuente y produce el código intermedio. Este código intermedio es una representación abstracta del código fuente y puede ser utilizado independientemente del lenguaje fuente y de la política de seguridad. El Compilador es un compilador tradicional que realiza los típicos análisis léxico y sintáctico verificando únicamente el tipo de las expresiones. El Generador de Anotaciones efectúa diversos análisis estáticos generando la información necesaria para producir las anotaciones del código intermedio de acuerdo a la política de seguridad. En aquellos puntos del programa en donde las técnicas de análisis estático no permiten determinar fehacientemente el estado, se insertan chequeos en tiempo de ejecución para garantizar de esta forma la seguridad de código. En caso de encontrar que en algún punto del programa forzosamente no se satisface la política de seguridad, el programa es rechazado. El último proceso llevado a cabo por el productor de código es realizado por el Generador del Esquema de Prueba. Este, teniendo en cuenta las anotaciones y la política de seguridad, elabora un esquema de prueba considerando los puntos críticos y sus posibles dependencias. Esta información se encuentra almacena en el código intermedio. El Compilador, el Generador de Anotaciones y el Generador del Esquema de Prueba conforman al Compilador Certificante. El consumidor de código recibe el código intermedio con las anotaciones del código y el esquema de prueba. Este esquema de prueba es básicamente el recorrido mínimo que debe realizar el consumidor de código sobre el código intermedio y las estructuras de datos a considerar. El Verificador del Esquema de Prueba es el encargado de corroborar el esquema de prueba generado por el productor de código. Por último el Verificador de la Integridad de la Prueba verifica que el esquema de prueba halla sido lo suficientemente fuerte para poder demostrar que el programa cumple con la política de seguridad. Esto consiste en verificar que todos los puntos críticos del programa fueron chequeados por el Verificador del Esquema de Prueba o en su defecto contienen un chequeo en tiempo de ejecución. Esto se debe a que, si el código del productor hubiese sido modificado en el proceso de envio al consumidor, el esquema de prueba puede no haber contemplado ciertos puntos del programa potencialmente inseguros. El Verificador de Esquema de Prueba y el Verificador de la Integridad de la Prueba componen el Verificador de la Prueba. 3 El Prototipo Desarrollado Con el objetivo de verificar la factibilidad del enfoque propuesto se desarrolló un prototipo de la arquitectura descripta en el punto 2. Como primera medida se definió los aspectos de seguridad que se deseaban garantizar, es decir, la política de seguridad. Esta puede ser expresada en dos reglas: 1. toda variable está inicializada al momento de ser utilizada. 2. todo acceso a una posición de un arreglo es válida. Luego se definió el lenguaje fuente que es básicamente un subconjunto de C. Un programa fuente es una función que debe tomar por lo menos un parámetro y debe retornar un valor. Tanto los parámetros como los valores que retorna deben ser de tipo básico. Los tipos básicos son int y boolean, a diferencia de C en el cual cero significa falso y cualquier entero distinto de cero significa verdadero; además cuenta con arreglos unidimensionales de tipo int o boolean. Otra diferencia con C es que la asignación es una sentencia y no una expresión. Además, opcionalmente los parámetros de tipo entero pueden ser acotados al ser declarados. Para esto la declaración del parámetro va acompañada por un rango que indica el menor y el mayor valor que puede tomar el dato de entrada. El próximo paso consistió en establecer el código intermedio que se utilizaría. Como representación intermedia se definió un árbol sintáctico abstracto ya que esta representación permite realizar distintos análisis estáticos (también, si así se deseara, puede ser usado para generar una gran cantidad de optimizaciones). En la implementación se limita el análisis al cuerpo de las funciones para hacer el análisis más eficiente y escalable a programas largos. Otra consideración a mencionar es el punto medio considerado entre flow-sensitive analysis - el cual considera todos los flujos posibles del programa - y flow-insensitive analysis - el cual ignora el control de flujo -. Si bien se considera el control de flujo en algunos casos, como por ejemplo en los ciclos, se limita a reconocer ciertos patrones en el código. Luego se implementó el Generador de las Anotaciones del Código Intermedio. El objetivo de este módulo es generar la información necesaria para determinar la inicialización de las variables y el rango de las variables utilizadas. Este último objetivo permite asegurar si un valor es válido como índice de arreglo. Además se determinará la precondición y la postcondición. Para lograr con los objetivos antes mencionados se identificarán todas las posibles secuencias de ejecución de los programas. Es decir, el grafo permitirá generar información acerca de que pasará en tiempo de ejecución. Esto se realiza en tres pasos: 1. Identificación de rangos de variables que no son modificadas dentro de un ciclo. 2. Identificación de rangos de variables que son modificadas por valores acotados dentro de un ciclo. 3. Identificación de rangos de variables inductivas que son modificadas dentro de un ciclo. El último módulo del productor implementado, el Generador del Esquema de Prueba, identifica las variables críticas (aquellas que intervienen como índices de arreglos y sus dependencias) y los puntos del programa en que son referenciadas. Con esta información produce el esquema de prueba, el cual es el camino mínimo que el consumidor debe recorrer para verificar la seguridad del código. El Verificador del Esquema de Prueba chequea la consistencia del grafo recibido. Luego, recorriendo el camino de prueba, verifica que toda variable este inicializada y que todos los accesos dentro del camino de prueba sean seguros. Todos los nodos visitados por esta entidad son marcados. Finalmente, el Verificador de la Integridad de la Prueba realiza un recorrido exhaustivo del grafo para garantizar que la entidad anterior visitó todos los nodos que contienen una variable crítica. Si el programa es seguro el Generador de Código produce el código objeto a partir del grafo. 4 Conclusiones y Trabajos Futuros Se han desarrollado los lineamientos de un mecanismo de seguridad basado en técnicas de compilación y análisis estático. Para garantizar la seguridad del consumidor de código esta técnica sigue las ideas de Proof-Carrying Code. La factibidad de este enfoque ha sido probado mediante el desarrollo e implementación de un prototipo del ambiente. El prototipo está conformado por un compilador certificante para un subconjunto del lenguaje C y el verificador de la prueba que garantizan las siguientes propiedades de seguridad: 1. toda variable está inicializada al momento de ser utilizada. 2. todo acceso a una posición de un arreglo es válida. Como consecuencia de la información necesaria para poder garantizar el punto 2 se obtiene la evidencia que permite determinar, en muchos casos, las cotas de las variables en cada punto de ejecución del programa. Otra consecuencia es poder determinar, en ciertos casos, la postcondición. Si bien no siempre se puede garantizar en tiempo de compilación que el acceso a un arreglo es válido, el compilador inserta chequeos en tiempo de ejecución, con lo cual, se garantizan las propiedades de seguridad. Además, como generalmente los accesos a arreglos se realizan mediante variables inductivas, el compilador puede determinar en la mayoría de los casos la validez de los accesos sin insertar chequeos. Para poder determinar la certeza de esta afirmación, se realizará un testing del compilador con la participación de un grupo de programadores (estudiantes, docentes y investigadores del Dpto. de Computación de la UNRC). Como trabajo futuro se desarrollará una herramienta que permitar conocer cual es el porcentaje de programas que requieren verificaciones dinámicas de las propiedades de seguridad que se garantizan para un lenguaje de programación del mundo real, como por ejemplo, bibliotecas C. Si bien el lenguaje fuente del compilador es simple puede ser extendido facilmente. Sería interesante extender el lenguaje permitiendo invocación a funciones. Esto se puede realizar sin mayores dificultades ya que, al obtener la postcondición de una función, esta se puede acotar; además si consideramos punteros se podría verificar la existencia de aliasing por medio de análisis de alias basado en flujo (alias analysis based on flow) y realizar scape analysis. Otro aspecto a tener en cuenta es el desarrollo un lenguaje assembly tipado para que en una etapa posterior, el compilador lo generara. También, sería muy importate contar con un sistema formal que verifique las propiedades de seguridad.	﻿código móvil seguro , procesadores de lenguajes	es	21287
49	Integrando modelos en UML y especificaciones formales	﻿En el presente informe se dan los lineamientos generales de un proyecto de desarrollo de software para la integración de una herramienta gráfica de diseño de diagramas de clases en UML y un generador de especificaciones formales en RSL. Esta integración permitirá que un ingeniero de software realice especificaciones formales asociadas a modelos semi-formales en UML trabajando en un ambiente integrado de desarrollo.  1   Introducción  El Lenguaje Unificado de Modelado (UML) [1], [17], [19] es un lenguaje gráfico para modelar y  especificar sistemas de software. Hoy en día se ha transformado de hecho en el lenguaje estándar de  modelado orientado a objetos. Sin embargo, aunque las notaciones en UML son fácilmente comunicadas al usuario final, su semántica es informal y, en consecuencia, puede resultar ambigua.  En la literatura se pueden encontrar varios trabajos que tratan sobre la formalización de la semántica  y la sintaxis de distintas partes de UML (ver [2], [4], [5], [6], [7] [12], [13], [16]).  En [3], [8], [15] y [18] se hace uso de RSL (the RAISE Specification Language) como base formal  para los diagramas de clases de UML. RSL es un lenguaje de especificación formal [9], el cual recibe su nombre del método RAISE. RAISE (Rigorous Approach to Industrial Software Engineering)  [10] consiste de un número de técnicas y estrategias para realizar desarrollo formal. Su lenguaje,  RSL, es un lenguaje de especificación de amplio espectro. Permite el uso de diferentes estilos de  especificaciones: aplicativo o imperativo; secuencial  o concurrente; directo (explícito) o axiomático  (implícito); algebraico (con tipos de datos abstractos) u orientado al modelo (con tipos de datos  concretos).  En este trabajo presentamos los puntos principales de un proyecto de desarrollo de software para  integrar una herramienta gráfica para la construcción de diagramas de clases en UML con un generador de especificaciones formales en RSL basado en la semántica y sintaxis presentadas en [8].  Esta nueva herramienta integrada de desarrollo permitirá al Ingeniero de Software construir sus propios diagramas de clases y generar a partir de éstos las correspondientes especificaciones formales  en RSL asociadas a los mismos.  Una de las premisas del proyecto está orientada a favorecer el desarrollo y empleo de software de  libre distribución.  2   Antecedentes  El proyecto cuenta ya con una herramienta de desarrollo propio, UML2RSL, la cual permite la generación automática de especificaciones formales en RSL a partir de diagramas de clases en UML.  Dicha herramienta toma como entrada un archivo XML, el cual contiene una descripción de un diagrama de clases, lo analiza sintácticamente y produce como salida una especificación formal en  RSL.   Sin embargo, dado que para generar este tipo de archivos XML, usados como entrada por el traductor, de momento es necesario hacer uso de una herramienta comercial, se ha empezado a trabajar en  la integración de nuestro traductor con una herramienta gráfica de libre distribución basada en  UML. En este sentido, se ha optado por llevar a cabo dicha integración con Argo UML.  2.1 UML2RSL  Como se muestra en la figura 1, UML2RSL usa como entrada un diagrama de clases producido por  una herramienta comercial, el cual se encuentra almacenado en un archivo en formato XMI, un formato basado en XML [11], [14] para intercambio de información entre herramientas de UML.  Analiza sintácticamente el archivo XML de entrada, y si la entrada es válida, produce una especificación RSL basada en las semánticas propuestas en [8].  Esta herramienta fue desarrollada en Java y corre en cualquier plataforma Java 2. Hace uso de una  API muy utilizada para procesar archivos XML: la API DOM (Document Object Model) [11], [14].  Haciendo uso de la API DOM, se analiza el documento XML para verificar que es un documento  XML válido, en cuyo caso es representado como un árbol DOM. DOM nos provee un conjunto de  APIs para manipular los nodos de un árbol DOM. Haciendo uso de estas APIs se analiza si el árbol  corresponde a la descripción de un diagrama de clases de acuerdo al DTD XMI y no a otro documento XML, además de controlar que el diagrama de clases sea sintácticamente correcto de acuerdo  a la sintaxis formal presentada en [8]. A medida que se lleva a cabo este análisis se va creando una  nueva estructura dinámica en memoria que contiene toda la información del diagrama relevante para  la traducción.      Figura 1. Diagrama de componentes  2.2   Argo UML  Argo UML es una herramienta desarrollada en Java para el diseño de modelos en UML. Permite  crear la mayoría de los diagramas estándares de UML, en particular los diagramas de clases que son  los que a nosotros nos interesan en este caso.  Argo UML se encuentra disponible bajo una licencia de código abierto y cuenta con la capacidad de  soportar la generación de archivos XMI. Usa este mecanismo estándar de grabación de los modelos  creados, para facilitar el intercambio con otras herramientas.   Una descripción mas completa sobre la herramienta y el proyecto pude obtenerse en  http://argouml.tigris.org  3   La integración de ambas herramientas  La integración puede ser llevada a cabo en diferentes formas:  Dado que existen una variedad de formatos XMI generados por distintas aplicaciones, en  particular este es el caso entre el formato de los archivos XMI de salida producidos por Argo  UML y el formato de los archivos XMI de entrada aceptados por UML2RSL, la integración  podría llevarse a cabo agregándole un módulo al traductor de manera tal que sirva de puente  entre la salida XMI producida por Argo UML y el tipo de formato XMI reconocido por  UML2RSL.  Crear una nueva versión del traductor compatible con Argo UML, en donde se cambie el  módulo correspondiente al análisis sintáctico del archivo XMI de entrada, de manera tal que  ahora pueda reconocer los archivos XMI compatibles con Argo UML.  Dado que Argo UML es una herramienta “Open Source”, teniendo acceso a su  código fuente se puede trabajar en la integración dentro del código de Argo UML del módulo del traductor que es encargado de producir las especificaciones formales en RSL. En este caso, no es  necesario trabajar con los archivos XMI sino con las estructuras generadas internamente por  Argo UML y aquella que usa el traductor para generar las especificaciones.	﻿Especificaciones Formales , Modelos en UML	es	21300
50	Diferentes enfoques paralelos aplicados en la simulación de un problema físico usando el método de monte carlo	﻿Diferentes Enfoques Paralelos aplicados  en  la Simulación   de un problema físico usando  el método de Monte Carlo    Línea de Investigación: Distribución y Paralelismo      E.Carreño, F. Piccoli, M.Printista  Dpto de Informática  UNSL  mprinti@unsl.edu.ar    H. Velasco  Instituto de Matemática  Aplicada San Luis  UNSL  hvelsco@unsl.edu.ar      RESUMEN     Se ha implementado un modelo (DAGES Model), que desarrolla el cálculo del  factor de tasa de dosis (dose-rate factor)  debida a Gama Emisores depositados en el  suelo. Se desea saber el efecto que causa la exposición a la altura de un metro para  fuentes distribuidas en el suelo usando el método de Monte Carlo (MMC),  en un  sistema de computadoras paralelas (Cluster).   El propósito de este estudio es llevar a cabo el análisis de posibles enfoques para  implementar el cálculo paralelo de dicha Tasa de Dosis. Se ha empleado el modelo de  computación Bulk Synchronous Parallel  con el objeto de analizar, implementar  (usando la librería PUB) y hacer una predicción de los costos para los posibles  enfoques.   Además, los mismos enfoques han sido implementados usando la librería  estándar MPI (Massage-Passing-Interface), por su portabilidad para otros sistemas  similares y porque además está diseñada para permitir la máxima performace en una  gran variedad de sistemas.   Se discute las ventajas y desventajas de los enfoques planteados así como  también las ventajas y desventajas de usar una u otra librería en la implementación.  La mayor ventaja del método de Monte Carlo es su flexibilidad y simplicidad  para simular los movimiento de un fotón en una geometría arbitraria y en condiciones   complejas. Dado que el error límite del método de Monte Carlo es inversamente  proporcional a la raíz cuadrada del número de muestras estadísticas, se requiere un gran  número de muestras para alcanzar una exactitud satisfactoria. Por consiguiente, la  principal desventaja del método de Monte Carlo es que el método es  computacionalmente intensivo.   Sin embargo el método es muy adaptable a la computación paralela, esto es, al  algoritmo se le puede aplicar una descomposición de datos de forma tal que las tareas en  las que se descompuso el problema involucran excesivas computaciones, baja  proporción de interacción y comunicaciones con pequeñas cantidades de datos  (Descomposición de Granulo Grueso “Coarse grained”).        Podemos decir que la computación paralela se introduce ya sea para mejorar la  performance del método de Monte Carlo o para lograr una reducción en el tiempo de  cálculo o ambos.   En este trabajo se presenta la versión secuencial de la Simulación por Monte  Carlo  y se discute el diseño, implementación y el análisis de costo de la versión BSP de   tres enfoques paralelos.	﻿Enfoques Paralelos , problema físico , método de Monte Carlo	es	21322
51	Sistema experto en análisis de fallas en líneas eléctricas de transmisión	"﻿ Los Sistemas eléctricos de Transmisión están sometidos a  distintos tipos de fallas, que degradan la  capacidad de transporte de energía, y provocan como consecuencia un elevado costo en penalidades a la empresa  responsable del transporte. Frente a las dificultades que plantea la obtención rápida de un diagnóstico, se estableció  como objetivo el desarrollo de un Sistema Experto que procese en tiempo real la información adquirida y que frente a un  suceso característico de una falla, emita un diagnóstico y asista a los especialistas y los operadores a identificar  rápidamente el origen del problema y efectuar las operaciones que correspondan.    Palabras Claves: Alarma, Eventos, Sistema Experto (SE), Falla, Diagnóstico, Sistema de protección.    1. Introducciòn     Los Sistema de Transmisión de Energía Eléctrica (STEE) están sometidos a diversos fenómenos  (contingencias) que producen distintos tipos de fallas (perturbaciones) eléctricas. Entre los  fenómenos físicos causantes de una falla eléctrica, podemos mencionar entre otros: el viento, el  incendio de campo, la caída de una torre, las maniobras, las descargas atmosféricas. Estos  fenómenos pueden originar diversos tipos de fallas: monofásica, bifásica, trifásica, sobretensión  etc.    Ante una falla, la Empresa de Transporte de Energía Eléctrica debe hacerse cargo de la  reposición de las partes afectadas y de los costos de las penalidades que le impone el Ente  Regulador, encargado de reglamentar la relación entre los diferentes agentes del Mercado.     El análisis de las fallas es una tarea esencial del especialista en protecciones. Luego de ocurrida  la falla el especialista accede a la información capturada por el Registrador Cronológico de  Eventos (RCE), que es almacenada en una base de datos. Con esta información realiza el  diagnóstico del tipo de falla y el probable origen de la misma.     La reposición del servicio y por ende las penalidades por indisponibilidad, depende de la obtención  de un diagnóstico rápido y confiable. Este es un aspecto crítico del problema, dado que no  siempre el especialista se encuentra disponible. Por ejemplo en caso de una falla fuera de los  horarios normales de trabajo, el especialista debe concurrir a su lugar de trabajo, acceder a la  información, analizarla y emitir un diagnóstico, todo el tiempo transcurrido se traduce en un  elevado costo por penalidades.     La minimización del tiempo post-falla está íntimamente relacionada con el conocimiento de las  causas que originaron la falla y el estado post-falla del STEE  La mayor parte del tiempo que insume este proceso es empleado en tareas poco relevantes, tales  como:    ♦ El tiempo requerido para convocar al especialista,  ♦ El tiempo necesario para obtener los datos adquiridos por los Registradores de Eventos  de las Estaciones, que es la información que documenta la falla.    Frente a esta problemática se planteó una  línea de investigación cuyo objetivo es el desarrollo de  un Sistema Experto en el análisis de las fallas ocurridas en las líneas eléctricas de transmisión  (SAF),       Se ha encontrado un único antecedente con características similares y relacionado con la temática  del presente proyecto es el desarrollado en Francia por Dong-Yih Bau y Patrick J. Brézillon [Dong  92] “Model-Based Diagnosis of Power-Station Control System” Este sistema fue desarrollado  conjuntamente entre la Universidad de Paris y Electricité de France. El presente trabajo se  diferencia del proyecto francés en que además del objetivo mencionado, se intenta identificar el  origen o fenómeno físico causante de la falla  Este trabajo se estructura de la siguiente manera: en la sección 2 se hace una breve descripción  de los objetivos del proyecto, en la sección 3 se describe el análisis de viabilidad, en la sección 4  se describe la metodología utilizada para el desarrollo, en la sección 5 se presenta el esquema de  razonamiento del sistema, en la sección 6 se presentan las conclusiones y futuras líneas de  investigación.    2. Descripción del proyecto    El ámbito de desarrollo del presente proyecto es el Sistema de Transmisión de alta tensión de la  República Argentina cuya operación y mantenimiento está a cargo de TRANSENER S. A.. La red  es de aproximadamente 8800 Km. De línea, cuenta con 28 estaciones transformadoras. El ámbito  de utilización del sistema es la Estación Transformadora Chocón Oeste, en la Región Sur.     SAF debe procesar en tiempo real la información adquirida por el Registrador de Eventos y frente  a un suceso característico de una falla, debe analizar la falla y emitir un diagnóstico que asista a  los especialistas y los operadores a identificar rápidamente el origen del problema y efectuar las  operaciones que correspondan.                            Fig. 1 Esquema del sistema trabajando en la estación    A la izquierda de la figura 1 se puede observar el unifilar de una estación, los eventos allí  generados son almacenados en una base de datos por el Registrador Cronológico de Eventos  (RCE), ante la ocurrencia de una falla, el RCE, habilita al SE. Este sistema lee los eventos  registrados y emite un diagnóstico que permite a los operadores revisar el origen probable de la  falla y facilita a los especialistas el análisis de la situación.    Paralelamente al análisis de la falla, el sistema experto ejecutará ante cada falla una supervisión  sobre la actuación de las Protecciones, detectando cualquier indicio de defecto, en una típica  acción predictiva, a fin de evitar actuaciones futuras no deseadas.    3. Análisis de Viabilidad    Para analizar la viabilidad del proyecto, se utilizó el método propuesto por Alonso, Maté y Pazos  [Gómez.99] que permite obtener un valor acerca de la viabilidad de desarrollo de un sistema  experto. Este método permite trabajar con valores lingüísticos, los cuales son cuantificados en  intervalos difusos.    El método seleccionado utiliza una serie de características agrupadas sobre cuatro dimensiones:  ♦ Plausibilidad de un Sistema Experto  ♦ Justificación de un Sistema Experto     	    	  	      	 	    	 	    	    ♦ Éxito de un Sistema Experto  ♦ Adecuación de un Sistema Experto    El conocimiento que utilizan los especialistas en la solución del problema proviene mayormente de  la experiencia, que se traduce en una serie de heurísticas. El valor obtenido para SAF fue  altamente satisfactorio, sobre una escala de 1 a 10 se obtuvo un valor de 7.9.    4. Metodología    La metodología seleccionada para el desarrollo de este sistema, es la metodología I.D.E.A.L.  desarrollada en 1995 por Alonso 		 . La metodología I.D.E.A.L. incorpora un ciclo de vida en  espiral cónico en tres dimensiones. Está basada en el modelo en espiral de Böehm, en el que  cada fase del ciclo de vida finaliza con el desarrollo de un prototipo.     A continuación se describe en forma breve las diferentes etapas del Ciclo de Vida.     • Adquisición de Conocimientos: los problemas abordados con la tecnología de la Ingeniería  del Conocimiento intentan emular a través de un software, el quehacer de un experto humano  al desempeñar una determinada tarea. Una de las actividades que requiere mayor esfuerzo,  por su complejidad es la adquisición de conocimientos, por medio de la cual se intenta  descubrir el proceso de solución del problema.  En la actualidad, la adquisición de conocimientos constituye el verdadero cuello de botella en la  construcción de SE.  • Conceptualización: en esta fase se estructuran los conocimientos adquiridos. Esta actividad  está constituida por dos tareas fundamentales: una de Análisis, basada en la detección de  conocimientos estratégicos, tácticos y fácticos, y la actividad de Síntesis donde quedan  expresados dichos conocimientos en forma estructurada.  • Formalización: se define una adecuada representación de los conocimientos, que garanticen  su correcta manipulación. Es el primer acercamiento a la máquina en lo que respecta a su  implementación.  • Implementación: se transforman los conocimientos formales en un modelo computable.  • Evaluación: se establece el grado de experiencia alcanzado por el sistema. Esta evaluación es  realizada por expertos en el área, quienes analizan el desempeño del sistema, determinando la  calidad de las respuestas que brinda el sistema experto ante diferentes problemas a resolver.    5. Esquema de razonamiento de SAF    El problema se analiza como si se tratara de dos sistemas que interactúan mutuamente como se  muestra en la figura 2.    Fig. 2 Interacción de dos sistemas    Se llama “T” a un sistema que realiza una determinada función (transmisión de la energía).  “P” a  un sistema que ejerce funciones de control (proteger las líneas de transmisión). Así una falla en  “T” genera la acción inmediata del sistema de control “P”. Al sistema “P” le llegan señales que le  indican que debe actuar rápidamente ejerciendo funciones de control sobre el sistema “T”.      Las funciones de control, “P” las ejecuta a través de la apertura de interruptores,  desvinculando  de esta manera el segmento afectado por la falla.    Se puede obtener información de la falla observando directamente el sistema “T”  o bien se puede  inferir observando el sistema de control “P”.Las anormalidades en “P” (sistema de protección) son  detectadas analizando el comportamiento cuando este actúa para eliminar una falla en “T”.     Los conocimientos estratégicos obtenidos en la fase de Conceptualización constituyen los pasos  modulares que guían al SE para la solución del problema. El siguiente esquema (figura 3) muestra  la estrategia de solución que sigue SAF en el análisis de la falla.    3 Analizar el comportamiento de la protección 1 Diagnosticar la falla 1.1 Leer la información de la falla 1.2.3 Analizar el tipo de falla 1.2.1 Analizar el movimiento de los interruptores Analizar la falla 3.1 Analizar Sistema 1 y Sistema 2 3.2 Analizar coherencia respecto de la falla 1.1.1 Leer las alarmas 1.1.2 Leer movimiento de los interrutpores 1.2 Analizar el sistema 1.3.1 Analizar la ubicación de la falla 1.3 Analizar las alarmas 1.3.2 Determinar fases afectadas 1.3.1.1 Analizar tiempos de actuación 1.3.1.2 Establecer la protección actuante 2 Analizar el origen de la falla 1.2.2 Analizar el estado final de las líneas y barras 1.3.1.3 Analizar la línea afectada  Fig. 3 Estrategia de solución del problema   El análisis y la obtención de los pasos modulares son de vital importancia en la construcción de un  SE, dado que permiten tratar un problema complejo descomponiéndolo en partes menores y  abordables.    El primer paso que debe realizar SAF luego de ocurrida una falla, es seleccionar sólo los eventos  que resulten relevantes para el análisis del fenómeno ocurrido. Los eventos relevantes para el  análisis consisten en las alarmas de las protecciones actuantes y el movimiento de los  interruptores.    Una vez obtenido los eventos relevantes, el SE debe analizar el comportamiento del sistema de  transmisión durante la falla y cómo responde a esta el sistema de protección. El SE obtiene un  primer indicio acerca de la falla observando directamente el sistema de transmisión (el movimiento  de los interruptores). La observación posterior del comportamiento del sistema de protección  permite confirmar y refinar el diagnóstico.    El origen de la falla, es el fenómeno físico causante de la falla eléctrica. La experiencia ha  demostrado que los comportamientos característicos de estos fenómenos, se ven reflejados en los  eventos que registran la falla. Una adecuada lectura de estos patrones permite deducir con  aproximación, el fenómeno físico originario de la falla. Este es un conocimiento de tipo  especulativo o hipotético que depende en gran medida de la experiencia del experto.     Un análisis estadístico de la historia de las fallas sufridas en el sistema de transmisión de  Transener  ha permitido corroborar estos patrones y en algunos casos precisarlos aún más  generando nuevo conocimiento que fue ingresado a la base de conocimientos del sistema.      El análisis del comportamiento de la protección consiste en definir si actuó en forma correcta ante  la falla. Las anormalidades son detectadas analizando el comportamiento del sistema de  protección, cuando este actúa para eliminar una falla. Esto lo realiza de dos maneras:    ♦ Analiza la interacción de ambos sistemas (el sistema de transmisión y el sistema de  protección). El sistema de transmisión por si solo otorga suficiente información acerca de  la falla. Luego el análisis de la protección permite detectar anormalidades en el  comportamiento de la misma ante la falla.    ♦ Analiza la lógica de actuación del sistema de protección y detecta los desvíos que esta  tiene (tanto cualitativos como cuantitativos).    La base histórica con los eventos ocurridos en los últimos siete años. Ha permitido poner a prueba  y precisar el diagnóstico del sistema    6. Conclusiones y Futuras líneas de investigaciòn    De la experiencia obtenida en el desarrollo de SAF, resulta claro que la mayor dificultad y el mayor  esfuerzo se concentra  en la fase de adquisición de conocimientos. La principal razón es que los  expertos trabajan con un conocimiento compilado producto de años de experiencia en el tema, lo  cual dificulta su estructuración.   Se espera con la aplicación del sistema SAF, lograr mejoras en los siguientes aspectos:    ♦ Tiempo de respuesta en la obtención de la falla (obtiene el diagnóstico en forma  inmediata)  ♦ Fiabilidad (Se espera que minimice los errores del análisis)  ♦ Disponibilidad (SAF esta disponible las 24 hs. del día)    Los siguientes puntos representan extensiones al sistema y futuras líneas de investigación.    ♦ Extender las capacidades SAF para fallas en equipos de la estación.  ♦ Incluir como fuente de información para el análisis del sistema, al registrador de falla. (en  la actualidad solo utiliza el RCE). Esto último implica la construcción de un módulo  específico que procese la información del mismo y alimente a SAF.  ♦ Incorporar un factor de incertidumbre en el análisis del origen de la falla. Al analizar el  origen de la falla, la incertidumbre juega un papel muy importante, dado que la afirmación  que se hace acerca de la causa del problema, es de carácter probabilística. La  cuantificación de dicha probabilidad, permitiría enriquecer en gran medida el diagnóstico."	﻿diagnóstico , alarma , eventos , Sistema Experto SE , falla , sistema de protección	es	21359
52	Desarrollo de herramientas inteligentes para la web semántica	"﻿Desarrollo de Herramientas Inteligentes   para la Web Semántica  Laboratorio de Investigación y Desarrollo en Inteligencia Artificial  Laboratorio de Investigación y Desarrollo en Ingeniería de Software  Laboratorio de Investigación en Visualización y Computación Gráfica  Laboratorio de Investigación y Desarrollo en Sistemas Distribuidos    Departamento de Ciencias e Ingeniería de la Computación  Instituto de Investigación en Ciencia y Tecnología Informática (IICyTI)  Universidad Nacional del Sur  dcc@cs.uns.edu.ar     El objetivo general de este proyecto, en el marco de las recomendaciones del World Wide  Web Consortium (W3C), es el desarrollo de las herramientas necesarias para la creación de la  infraestructura de soporte cognitivo y la explotación por parte de agentes autónomos del  conocimiento almacenado en la Web. Esto incluye lenguajes de representación de  conocimiento, herramientas para la creación, mantenimiento y visualización interactiva de  ontologías, y máquinas de inferencia especializadas que puedan razonar con ellas y también  colaborar en su visualización inteligente. El grupo responsable de este proyecto, creado en el  Departamento de Ciencias e Ingeniería de la Computación de la UNS, está integrado por:  Guillermo Simari, Alejandro García, Fernando Tohmé, Telma Delladio, Diego Martínez y  Sergio Gómez  (Laboratorio de Investigación y Desarrollo en Inteligencia Artificial); Elsa  Estévez y Pablo Fillottrani (Laboratorio de Investigación y Desarrollo en Ingeniería de  Software); Silvia Castro y Sergio Martig (Laboratorio de Investigación y Desarrollo en  Visualización y Computación Gráfica) y por Jorge Ardenghi y Javier Echaiz (Laboratorio  Investigación y Desarrollo en Sistemas Distribuidos).   Introducción  La World Wide Web ha cambiado en forma dramática la disponibilidad de información en  formato electrónico. La cantidad de documentos en línea crece aceleradamente habiendo llegado  a algunos miles de millones durante el corriente año. Al mismo tiempo, varios cientos de  millones de usuarios realizan búsquedas sobre este repositorio, y acceden y contribuyen al  mismo en forma continua. No es sorprendente entonces que estas tareas sean cada vez más  dificultosas y menos efectivas.   La mayor parte del contenido de la Web está diseñado, naturalmente, para que sea inteligible  para los seres humanos. Por otro lado, en la actualidad los programas que deben manipularla  carecen de las capacidades para interpretar el significado (la semántica) de su contenido. El  reconocimiento de este problema llevó a Tim Berners-Lee, creador de la World Wide Web, a  proponer una nueva forma de contenido en la Web. Este contenido deberá ser significativo para  los propios sistemas computacionales, i.e. deberá ser interpretable por ellos. La nueva estructura  de la Web, creará un entorno en el que los agentes de software puedan recorrer las páginas  entendiendo su significado. Esta extensión de la Web, denominada por Berners-Lee “Web  Semántica”, provee a la Web de significado bien definido, lo que facilitará la cooperación entre  seres humanos y agentes de software, dándole a estos últimos la habilidad de comprender el  contenido de la Web, razonar con él y así aumentar su efectividad.   En octubre de 1994 se creó el World Wide Web Consortium [W3C] con el objetivo de llevar a la  World Wide Web hacia el total desarrollo de su potencial introduciendo protocolos comunes que  promuevan su evolución y garanticen su interoperabilidad. Para poder responder a las  expectativas crecientes de los usuarios de la Web, el consorcio W3C está diseñando la próxima  generación de la Web proponiendo las tecnologías que ayudarán a hacer que ésta adquiera una  estructura robusta, escalable y adaptiva para el manejo de la información.   Las metas a largo plazo del W3C son: proveer el acceso universal, el desarrollo de la Web  Semántica y el desarrollo de la llamada Web of Trust, i.e. una guía para la consideración de las  cuestiones legales, comerciales y sociales que este desarrollo tecnológico ha introducido. Los  principios fundamentales de diseño son: Interoperabilidad provista a través de especificaciones  de lenguajes y protocolos de la Web, Simplicidad y Modularidad para dar cabida a la Evolución  permitiendo que las nuevas tecnologías puedan introducirse con mayor facilidad, Arquitectura  Descentralizada de manera que el crecimiento de la Web pueda realizarse sin la dependencia de  estructuras centralizadas.  Relevancia del Problema  La Web Semántica es un esfuerzo para dotar a la Web con conocimiento embebido que pueda ser  accedido por las aplicaciones. El resultado esperado es introducir la posibilidad de que las  páginas no sólo puedan ser consultadas por cadenas de caracteres como en la actualidad, sino que  puedan ser consultadas por el conocimiento que almacenan. Esta extensión de la Web dará la  oportunidad para el desarrollo de aplicaciones, tales como los agentes autónomos, que exploten  de manera intensiva la posibilidad de acceder a ese conocimiento.   La Web Semántica se encuentra en las primeras etapas de construcción. Es por esta razón que es  de extrema importancia en este momento desarrollar tecnología propietaria cuando la  oportunidad está abierta.  La Web no sólo es inmensa, sino que también es dinámica, evolutiva, potencialmente  inconsistente, y en general las respuestas obtenidas son dependientes del contexto. Como nuevo  espacio de desarrollo, existe un potencial enorme para realizar en este momento aportes  innovadores que permitan el uso eficiente de la Web para aplicaciones aprovechables en todos  los sectores: institucionales educativos, de salud, gubernamentales y empresariales de todo nivel.  La posibilidad de desarrollar Servicios de Web que puedan explotar la información semántica  apropiada, hará accesible la información por medio de aplicaciones autónomas y contribuirá al  desarrollo de contenidos que permitirán la difusión de información de todo tipo en forma mucho  más efectiva. El uso de estándares y recomendaciones ofrecidos por la W3C permitirá realizar la  investigación y el desarrollo de las herramientas propuestas como software de código abierto y  compatible con otros desarrollos. La construcción de ontologías, con la contribución desde el  sector del conocimiento, aceptadas por el sector público y privado llevará a una mejor  vinculación entre todos los actores permitiendo una articulación fluida de los desarrollos  particulares.   Estas características hacen de la Web Semántica un área de investigación de gran importancia.  Casi todas las áreas teórico-prácticas en Ciencias e Ingeniería de la Computación deberán aportar  contribuciones significativas para lograr avances importantes. En particular, la Inteligencia  Artificial, la Computación Gráfica y la Visualización, la Ingeniería del Software y los Sistemas  Distribuidos, son de particular interés para este proyecto.  Objetivos Específicos  La tarea de los investigadores es la de agregar capacidades de representación de conocimiento y  razonamiento a la red. Esto significa proveer a los agentes de software con la habilidad de  reconocer la representación, de usar reglas para realizar inferencias y de planear acciones en  función de metas propias y requeridas por sus usuarios. Es posible desarrollar los elementos  necesarios para estas tareas en el marco de XML (eXtended Markup Language) y RDF  (Resource Description Framework). El lenguaje XML da la posibilidad de agregar estructura a  los documentos sin darles significado. Este significado es introducido por los elementos de RDF  que permiten expresar relaciones entre elementos en la red que resultan individualizados por  medio de URIs (Uniform Resource Locators).   Este marco se completa con la creación de Ontologías. Una ontología, desde el punto de vista de  la Inteligencia Artificial, define las palabras y conceptos (o significados) utilizados para  describir y representar un área del conocimiento, estandarizando de esa manera los significados.  El uso de ontologías permite definir con claridad qué existe de manera única, facilitando el uso  de la información en el razonamiento y la acción por medio de los agentes de software.   En este contexto, el objetivo de la visualización es ayudar a encontrar rápidamente la  información semántica relevante y a crear, modificar y explorar su estructura fácilmente. La  Visualización permite mejorar la percepción de la estructura de grandes espacios de información  por parte del usuario y provee distintas facilidades de interacción. Es necesario contar con  técnicas de visualización que se constituyan en herramientas para visualizar ontologías. Éstas  deben brindar la posibilidad de visualizar no sólo los esquemas de las ontologías y las instancias  relacionadas sino también su integración de una manera efectiva. En este sentido se hace clara la  necesidad de contar con herramientas específicamente diseñadas que permitan visualizar este  tipo de ontologías con una gran cantidad de instancias explotando la naturaleza jerárquica y  solapada de su jerarquía de clases y posibilitando así la realización de una variedad de tareas  según el tipo de usuario y el estadío del ciclo de vida de la ontología en el que se esté trabajando.  El Desarrollo Basado en Componentes (DBC) es una metodología para construir sistemas de  software complejos, que enfatiza la construcción de sistemas mediante el ensamblaje de bloques  funcionales existentes. Por otro lado, el desarrollo de software orientado a agentes extiende el  desarrollo tradicional de software basado en componentes. Los componentes encapsulan agentes  inteligentes, facilitando la comunicación entre ellos, la integración y la flexibilidad.  De este  modo, la tecnología basada en esta clase de componentes resulta la más adecuada para el  desarrollo de herramientas de software inteligentes que descubran, utilicen, y visualicen  información suministrada. Las ventajas provistas por el uso de componentes de software, por el  desarrollo basado en componentes, y por arquitecturas específicas para un dominio de aplicación   son especialmente adecuadas en aplicaciones científicas de alta performance, complejas y que  generalmente requieren la combinación de la experiencia de distintos grupos de investigación.  En particular, debido a la complejidad del software a construir se deberán asociar componentes  de muy diversos niveles de complejidad y funcionalidad. Por ejemplo, habrá componentes  específicamente dedicados a definir la semántica del modelo, otros que proveerán servicios  inteligentes para la realización de las  búsquedas, y selección de información, y otros  componentes responsables de proveer los servicios de visualización. Por este motivo, es de vital  importancia la definición de los frameworks y las arquitecturas adecuados para cada uno de los  dominios específicos [AHLM03, BCK98, CKK02, DKW01]. Para cumplir con este objetivo, se  estudiarán y analizarán distintas arquitecturas y frameworks en base a los servicios que deberán  proveer los distintos tipos de componentes. En este caso, el aporte del trabajo de investigación  consiste en definir nuevos frameworks y arquitecturas para los diferentes dominios que abarca la  explotación de la Web Semántica.  La arquitectura a desarrollar deberá enfatizar que sus subestructuras se puedan entender y  modificar fácilmente. Asimismo, deberá integrar los aspectos específicos de inteligencia artificial  en lo referente a los componentes inteligentes, responsables de seleccionar los distintos sitios de  interés para la búsqueda requerida, con facilidades de recuperación de documentos, y con las  herramientas de búsqueda, selección, y visualización de la información solicitada.  De igual  modo, deberá satisfacer todos los atributos de calidad necesarios para los sistema de búsqueda,  recuperación, y visualización de información, como por ejemplo performance y facilidad de uso  entre otros.   En el desarrollo del proyecto se buscarán resultados teórico-prácticos que extiendan el trabajo  previo de los investigadores integrantes del Grupo. En el área de Representación de  Conocimiento y Razonamiento se dispone del desarrollo de la máquina de inferencia para el  razonamiento rebatible, Defeasible Logic Programming (DeLP) [GS04], y se extenderán estos  desarrollos para incluir la noción de utilidad en la máquina de inferencia básica de DeLP. El  desarrollo de los formalismos de razonamiento rebatible estuvo en un principio fundamentado en  analogías con procesos de decisión, estas similitudes dieron lugar a la aplicación del  razonamiento rebatible a la formalización de procesos de negociación. El razonamiento rebatible  puede dar una forma de determinar las utilidades de un conjunto de opciones. Estas utilidades no  son numéricas, sino simplemente en forma de orden de preferencias. Se explorará la introducción  de una caracterización general de ""utilidad de un argumento"" y su uso en el marco de la DeLP.  Una consecuencia inmediata de esta variante del razonamiento rebatible va estar en su aplicación  a problemas de planeamiento, en los que un agente inteligente tiene que comparar entre distintos  planes potenciales de acuerdo a criterios tales como el mejor uso de los recursos disponibles,  menor riesgo, etc. Finalmente, esta introducción de utilidades en la DeLP permitirá también  resolver el problema de la formación de creencias por parte de agentes racionales, que tan  importante es en el área del razonamiento bayesiano. De hecho, la introducción de utilidades en  la actualización rebatible de creencias [FKS02] permitirá un tratamiento no-numérico del  razonamiento con incertidumbre y de la dinámica racional del conocimiento.  Desde la perspectiva de la Visualización, se buscarán metáforas adecuadas de representación e  interacción y se explorarán tales metáforas para poder analizar su aplicabilidad en este contexto.  Se analizarán las representaciones visuales existentes en el campo de Visualización de  Información para su aplicabilidad en el contexto de Visualización de Ontologías en la Web  Semántica. Se explorarán representaciones visuales alternativas y se determinarán las  interacciones requeridas por cada técnica para lograr una interacción efectiva con la Web  Semántica.  El aporte desde la Ingeniería de Software será definir arquitecturas de software basadas en  componentes adecuados para cada una de las herramientas. Se definirán frameworks de  componentes específicos para los distintos dominios de aplicación y métricas para evaluar  atributos internos y externos de frameworks y arquitecturas. Se propondrá la arquitectura y los  frameworks adecuados para cada una de las herramientas."	﻿Desarrollo de Herramientas Inteligentes , Web Semántica	es	21377
53	Evaluación de algoritmos de ruteo en redes de computadoras	﻿ En este trabajo, se presenta una línea de investigación que se encarga del análisis de algoritmos  de ruteo de paquetes en una red de computadoras. Nuestra herramienta trabaja en la capa de aplicación  dentro de un protocolo de redes, obteniendo una colección de métricas que determinan el mejor ruteo  de paquetes en cada momento. Presentamos los aspectos más relevantes del diseño de la herramienta,  sus alcances y futuras etapas en este proyecto, como así también enlaces de interés.    Palabras Claves:   Ruteo con brújula. Redes sin cables. Sistemas Operativos.     Introducción  La geometría computacional [2,5,14,15] se encarga de resolver problemas geométricos de modo  constructivo. Se interesa por demostrar la existencia de la solución a un problema y por encontrar los  algoritmos y estructuras [3,4,6,13] de datos eficientes, medidos por su complejidad espacial y temporal.   En ocasiones, la Geometría brinda soluciones más eficientes en problemas que no parecen ser  geométricos, tal es el caso del ruteo de paquetes en una red de computadoras. Si bien este es un tema  que ha sido estudiado debido al gran uso de las redes de computadoras, en la actualidad se investiga la  forma de reducir los costos en cuanto ha espacio y tiempo de los algoritmos de enrutamiento. Es aquí  donde entra en juego la aplicación de la Geometría Computacional y la teoría de grafos, ya que permite  modelar fielmente a una red de computadoras.   Se han definido un conjunto de algoritmos que proponen una nueva forma de enviar un paquete  desde una estación de trabajo S1 a otra S2 [7,8,11,12]. Estos algoritmos usan información geográfica  como, por ejemplo, latitud y longitud, para detectar dónde se encuentra la estación de trabajo destino.  Teniendo en cuenta que en la actualidad se dispone de un dispositivo que permite recuperar de manera  instantánea esta información (Global Position Systems - GPS ), estos algoritmos adquieren importancia  debido a que pueden representar una mejor alternativa a los ya existentes.   El algoritmo de Ruteo por Brújula [10] adquiere importancia ya que propone una forma de  enviar un paquete que intenta minimizar el uso de almacenamiento. En términos matemáticos podemos  modelizar este tipo de problemas como grafos geométricos planos.  Este algoritmo trabaja de la  siguiente forma: desde un vértice inicial s se desea viajar hasta un vértice destino d de un grafo  geométrico. La información disponible en cada nodo es local, sólo se conoce las coordenadas del punto  y las de sus vecinos inmediatos en el grafo. Cuando se alcanza un nodo v, continúa por la arista  incidente a v cuya pendiente es la más próxima a la pendiente del segmento que conecta a v con el nodo  destino d.   Si bien el Ruteo por Brújula, al igual que otros algoritmos tales como el Ruteo por Brújula  Aleatorizado, Ruteo Ávido, entre otros,  brindan una forma diferente de concebir el envío de paquetes,  es importante contar con una herramienta que posibilite su evaluación y comparación con los  algoritmos de enrutamiento existentes.   En este sentido, a partir de las nociones de ruteo, hemos concebido un Simulador. Esta  herramienta trabaja en la capa de aplicación dentro de un protocolo de redes, obteniendo una colección  de métricas que determinan el mejor ruteo de paquetes en cada momento. Nuestro trabajo consiste en el  desarrollo de esta herramienta, para la evaluación de algoritmos de ruteo de paquetes en redes, con el  fin de analizar las bondades y dificultadas de estos nuevos algoritmos.    Concepción del Simulador  La estructura del simulador a alto nivel de abstracción se muestra en la figura 1:        Alg 1 Alg 2 Alg 3  Métricas  Tabla 1 Tabla 2 Tabla N  Simulador  Algoritmos para la Evaluación  Tablas de Resultados de la Evaluación de Algoritmos    Figura 1: Abstracción del Simulador  La principal componente de la herramienta es el módulo Simulador, el cual define  red de  computadoras virtual como un grafo dirigido G=(P,E), donde P es el conjunto de computadoras que  conforman la red  y  E es la relación de comunicación entre ellas. Esta componente se encarga de  simular el envío de mensajes en la red virtual existente.    Este simulador tiene dos posibilidades para llevar a cabo el estudio del comportamiento de los  algoritmos a evaluar: una interfaz gráfica y un modo automático de evaluación.  La interfaz gráfica se encarga de visualizar la red de computadoras y de mostrar los nodos  visitados por el algoritmo de enrutamiento que se está evaluando. Esta interfaz da la posibilidad de  ejecutar el algoritmo en su totalidad o paso a paso; éste último modo de operación facilita el estudio del  comportamiento de algoritmos particulares y la observación de los valores de las métricas que están  siendo evaluadas.  El modo automático de evaluación está pensado para ejecutar cada uno de los algoritmos  disponibles en el simulador para un gran número de casos de prueba (Algoritmos para la evaluación);   de esta forma, se recolectan los valores que toman las métricas que se desean evaluar. Como resultado  de este proceso el simulador emite un conjunto de relaciones (Tablas de Resultados de la Evaluación  de Algoritmos), donde se pueden observar los resultados de las métricas.  Se desea evaluar, en principio, las métricas: Sobrecarga de nodos, distancia de enlace y  Sobrecarga del centro de la red.    Los algoritmos implementados para esta tarea fueron los siguientes: Ruteo por Brújula,  Búsqueda de pasos (en Profundidad y por Niveles), Dijkstra y Centro del grafo[1], con el objetivo de  poder medir la distancia de enlaces de los algoritmos a ser evaluados y detectar si los algoritmos de  enrutamiento sobrecargan el centro del grafo.     La Estructura Interna del Simulador  En la figura 2 se presenta un esquema del simulador.    Depth Breath  1  Ruteo  Por  Bruj.  Métricas  Sobrecarga de nodos  Distancia de enlace  Tabla   Dijkstra  Tabla  Depth 1  Tabla  Breath  2  Algoritmo para la detección del centro del grafo  Algoritmos para la generación de grafos aleatorios  Breath   2  Motor para la  Simulación  Generación de Tablas  Tabla  Breath  1  Depth  Tabla  Ruteo   Brujula  Interfaz Gráfica de  Evaluación    Figura 2: Visión Interna del Simulador    La interfaz gráfica y el modo automático de evaluación fueron descriptos en la sección previa,  pero éste último merece un explicación en mayor detalle.  Para realizar la evaluación de manera automática se necesita una estrategia de generación de  grafos aleatorios, algoritmos para la detección de los nodos centrales del grafo y un motor para la  simulación.  En cuanto a la primera, el simulador provee dos estrategias, las cuales tienen en común la  generación de los nodos del grafo (se utiliza el generador de números aleatorios del lenguaje de  programación para obtener las coordenadas x e y de cada uno de los nodos de la red de computadoras),  y difieren en la construcción de la relación que vincula los nodos.   El primer método consiste en asignar a cada arco del grafo una probabilidad y luego establecer  un rango de probabilidades en forma aleatoria. Los arcos que forman parte de la relación son aquellos  que se encuentran en el rango de probabilidad establecido.   El segundo método consiste en generar una distancia n, así los arcos que forma parte de la  relación son aquellos que unen nodos cuya relación de distancia es menor o igual que esa distancia n.  Con el fin de medir la sobrecarga de los nodos en la red, se implementaron el algoritmo de  búsqueda de pasos propuesto por Floyd y los procedimientos que toman como entrada este resultado y  realizan el cálculo de los nodos centrales de un grafo. Estos algoritmos se requieren para validar la  hipótesis de que los algoritmos de pasos incorporados en el simulador sobrecargan el centro del grafo.  El comportamiento del motor de la simulación consiste en la generación aleatoria los nodos  origen y destino, para la transmisión del mensaje; además, realiza la recolección de datos para luego  enviar esta información hacia el módulo que se encarga de generar las tablas que son las salidas del  simulador.     Conclusiones y Visión de Futuro  Si bien el diseño e implementación de las rutinas del simulador no presentan un alto grado de  complejidad, es importante señalar que éste es un prototipo, ya que se pretende que el simulador pueda  evaluar una mayor cantidad de métricas y además compruebe el comportamiento de algoritmos de  ruteo que se utilizan en la redes móviles.  Por otra parte, en esta primera versión del prototipo, el mensaje que se envía es de una unidad,  pero cabe destacar que en la realidad los mensajes están formados por un conjunto de paquetes. Este  hecho dificulta aún más el diseño e implementación del motor de la simulación ya que se debe pensar si  los segmentos de paquetes que conforman un mensaje llegan por un mismo camino o por otros  diferentes a la máquina destino y si, además, se contempla la posibilidad de que los mensajes puedan  alterarse por errores producidos durante su viaje desde el nodo origen al nodo destino. Esto representa  un gran desafío para este grupo de investigación y abre un abanico de posibilidades para la realización  de trabajos futuros.   De las conclusiones y las futuras etapas del proyecto, se definen diferentes líneas de trabajo  interdisciplinario. Una línea dentro de la Geometría Computacional y Teoría de grafos con el objetivo  de la definición de modelos analíticos, estudio de técnicas algorítmicas y estructuras de datos  adecuadas. Una línea en el desarrollo de software para la implementación de los modelos analíticos de  la línea anterior con el objetivo de probar empíricamente las hipótesis planteadas. Una línea dentro de  la Ingeniería de Software con el fin de aplicar Ingeniería Inversa e Ingeniería de Software para realizar  el desarrollo de los módulos necesarios dentro de marcos formales en Ciencias de La Computación. Y  por último una línea de aplicación de teoría de Sistemas Operativos y Redes.	﻿Redes sin cables , Ruteo con brújula , Sistemas Operativos	es	21398
54	Negociación basada en argumentación en sistemas multi agente	﻿s de alguna práctica de negociación del mundo real  (v.g. subastas).  En esta investigación hemos adoptado la arquitectura BDI para caracterizar a los agentes. La capacidad  cognitiva de cada agente se modela a través de actitudes mentales,  tales como Creencias, Deseos e  Intenciones, y Obligaciones. El comportamiento de un agente queda determinado por sus motivaciones,  su conocimiento y sus capacidades efectoras. Cada miembro en un Sistema Multi-Agente desarrolla  planes e interactúa con otros miembros del grupo para alcanzar metas propias o atender pedidos de  colaboración. Su actitud mental  puede cambiar a su vez como consecuencia de sus acciones y de la  interacción con otros agentes. Aunque los agentes tienen una actitud colaborativa, sólo intentarán  cooperar cuando la propuesta recibida no esté en conflicto con sus propias metas. Así, una solicitud  puede provocar un cambio en las intenciones del agente que la recibe, quien intentará usar sus  creencias y habilidades para atender el pedido.    Objetivos   Nuestro trabajo de investigación  propone un protocolo de negociación para un grupo de agentes BDI  que mantienen una conversación tratando que sus intereses, inicialmente divergentes, converjan. Si se  llega a un acuerdo, el proceso tiene éxito y  la comunicación termina.  La interacción se inicia cuando agente solicita cooperación para alcanzar una meta. La actitud del  grupo es colaborativa, pero puede existir conflicto entre las intenciones de sus miembros. Los agentes  interactúan intentando influir en los planes y preferencias buscando alcanzar un acuerdo compartido.   Algunos elementos cognitivos son compartidos por todos los miembros del sistema. El conocimiento  de cada agente integrará en forma consistente  sus propias creencias y  las creencias compartidas. Para  asegurar la consistencia cuando un agente modifica el conocimiento compartido todos los demás deben  comunicar su conformidad.   La negociación puede terminar en forma exitosa o no. En el primer caso el conocimiento compartido se  modifica para incorporar nuevas creencias y el estado mental de ambos agentes se modifica como  consecuencia de la interacción. El éxito de la negociación depende en gran medida en la habilidad de  los agentes para comunicarse e intercambiar conocimiento. Por lo tanto, el rol del lenguaje es  fundamental; su expresividad determina la capacidad para dar énfasis al mensaje por parte del que lo  envía y la capacidad del receptor para elegir un curso de acción que sea compatible con su función.  La negociación puede fracasar porque el receptor de la solicitud no tiene el conocimiento o la  capacidad para satisfacerla o porque hay conflicto con sus propias metas. En este último caso, el agente  que hizo el pedido original tratará de cambiar sus planes. Si no fuera posible, volverá al diálogo e  insistirá en su pedido. Así, el agente que recibe el pedido tratará de revisar sus propios planes e  intenciones en un intento de efectivizar su colaboración.  Ninguno de los agentes puede modificar directamente las intenciones de los otros miembros del grupo,  pero sí en forma indirecta, a través del diálogo. Cada intervención puede pensarse como un tipo  especial de acción que puede modificar la actitud mental del receptor. Nuestra propuesta ofrece un  conjunto de primitivas de negociación específicas, que permitan influenciar la efectividad de los actos  comunicativos que se intercambian en un diálogo. Cada solicitud considera la actitud mental del  receptor, el tipo de acto comunicativo utilizado y el rol que ocupa cada agente en el grupo.    Negociación basada en Argumentación  Durante la negociación, se establece un diálogo en el que los agentes intercambian mensajes que  expresan propuestas y contra-propuestas. Estos mensajes son acciones que tratan de modificar las  actitudes mentales de los demás participantes. La expresión acto comunicativo  expresa el hecho de que  la comunicación es un tipo particular de acción.   Ambos participantes están interesados en alcanzar un acuerdo y tienen una actitud cooperativa. Cuando  más información incluye cada propuesta, más se acelera el proceso de negociación. Si  q es parte del  conocimiento compartido, y  el agente a necesita p como parte de un plan para lograr q, puede  comunicar su intención al agente b, evitando conflictos futuros.    El diálogo pueden iniciarse también cuando un agente a necesita la colaboración de otro agente b,  para  poder ejecutar un plan para obtener q.  El primero elabora entonces una propuesta en la cual requiere  un literal p. El agente b puede aceptar la propuesta y atender el requerimiento o rechazarla debido a  que p está en conflicto con  su propio plan para alcanzar s. En el primer caso, el literal puede formar  parte de sus creencias o bien puede elaborar un plan que le permita obtenerlo. Si  b acepta la propuesta,  agrega p al conocimiento compartido.  Si  b rechaza la propuesta,  a intenta elaborar otro plan para q que no requiera  p, pero si no lo logra  puede insistir con su requerimiento. Ante la insistencia,  b busca otro plan para s que no esté en  conflicto con p. Si lo encuentra, acepta el requerimiento y agrega p al conocimiento compartido. En  caso contrario, vuelve a rechazar la propuesta.  El agente a revisa entonces sus intenciones, pero si decide persistir en q, puede exigir p a  b. Este  último, revisa sus propias intenciones, e intenta abandonar su propósito de alcanzar s,  comprometiéndose con una nueva meta r para la cual pueda elaborar un plan sin conflictos con p.   Cada agente tiene conocimiento parcial del mundo y conocimiento parcial del estado mental del resto  de los agentes. Cuando un agente incluye un argumento en su propuesta o contrapropuesta de algún  modo muestra parte de su estado mental.   Colaboración y Comunicación  En el modelo adoptado en este trabajo, las creencias de un agente se construyen a partir de las  percepciones propias y de algunos elementos cognitivos compartidos por todos los miembros del grupo.  Cada individuo conocerá estos elementos, razonará sabiendo que el resto también los conocen y saben  que son compartidos. El conocimiento de cada miembro del sistema es incompleto y frecuentemente es  insuficiente para la deducción de un hecho específico. Sin embargo, este hecho podría ser deducido si  se integra todo el conocimiento distribuido. El grupo de agentes puede ser además heterogéneo y en  consecuencia sus miembros tendrán habilidades diferentes.  Cada agente  construye entonces planes a partir de su propio conjunto de acciones, pero puede solicitar  colaboración cuando su conocimiento o sus habilidades son insuficientes. La colaboración requiere que  los agentes tengan capacidad para comunicarse. Así, el rol del lenguaje de comunicación entre agentes,  ACL, es central, puesto que permite interactuar y compartir conocimiento. Aún cuando técnicamente la  comunicación tiene lugar por medio de mensajes sobre una red que utiliza un protocolo de bajo nivel, a  nivel conceptual los agentes mantienen conversaciones guiadas por sus propósitos.   La especificación de un ACL comprende tres niveles: un Protocolo de Interacción, un Lenguaje de  Interacción y un Lenguaje para representar el Conocimiento Compartido. El protocolo de interacción  de cada agente es un patrón de conversación que controla su interacción con otros agentes y permite  estructurar la comunicación. El lenguaje de interacción es el medio a través del cual los agentes  comunican actitudes e intercambian conocimiento. Este lenguaje debe permitir que los agentes asistan  a otros agentes, demanden servicios, distribuyan tareas y controlen su ejecución, se comprometan  a  ejecutar una tarea o rechacen el compromiso, reporten  su evolución y el éxito o fracaso final.  El lenguaje para representar el conocimiento compartido debe garantizar la preservación de la  semántica. Es decir, un concepto, objeto o entidad debe tener el mismo significado uniforme en las  diferentes aplicaciones, aún cuando sea referenciado por diferentes nombres. Una alternativa para  mantener un cuerpo de conocimiento compartido acerca del dominio es a través de una ontología  común.     Un Protocolo de Interacción  La negociación puede pensarse como un proceso de búsqueda distribuida sobre un espacio de acuerdos  potenciales. En la mayoría de los casos, solo una porción de la búsqueda  resultará satisfactoria para los  deseos particulares de un agente.  En el modelo propuesto por este trabajo, cuando un miembro del grupo realiza  una propuesta dentro de  su espacio de acuerdos aceptables, otro la  recibe y puede aceptarla, rechazarla o realizar una  contrapropuesta. En este último caso, el primer agente analiza la contrapropuesta y tiene las mismas  opciones.  La aceptación indica que se ha alcanzado un punto de acuerdo compartido. El rechazo significa que la  negociación ha terminado sin éxito. En el proceso, es posible que uno o ambos participantes deban  relajar sus demandas o aceptar un compromiso. En esta metáfora, la negociación requiere que los  agentes tengan ciertas capacidades mínimas. Deberán ser capaces de: realizar una propuesta dentro del  espacio de situaciones aceptables, aceptar o rechazar una propuesta y elaborar una contrapropuesta.  Las palabras utilizadas pueden tener una influencia en la efectividad del mensaje. El uso de expresiones  tales como insisto o demando influyen en la efectividad de los mensajes. Por otra parte, la aceptación  no solo dependerá de las palabras utilizadas sino que también se debe tener en cuenta la actitud mental  del receptor con respecto al pedido y su rol dentro del grupo.      Lenguaje de Interacción  Un lenguaje de interacción utilizado por agentes BDI deberá ser capaz de expresar actitudes mentales.  De esta forma, el lenguaje brinda las funciones esenciales que permiten la comunicación entre los seres  humanos. En Filosofía y en Lingüística se ha desarrollado un modelo formal de la comunicación  humana conocido como Teoría de los actos del habla o Speech Act Theory. Este modelo ha servido  como base para el desarrollo de lenguajes que están orientados hacia la comunicación en Sistemas  Multi-agente.   Los lenguajes de interacción basados en esta teoría capturan las características esenciales de la  comunicación humana y llevan a un modelo adecuado para el desarrollo de agentes artificiales. La idea  es que la comunicación es una clase especial de acción y como tal provoca cambios en el entorno.  Cuando un agente se comunica, no solo expresa sentencias que pueden ser verdaderas o falsas, también  ejecuta acciones tales como requerimientos, sugerencias, promesas, etc. El acto de habla es la unidad  mínima de comunicación. Cada expresión es un acto de habla y denota una acción.  Un lenguaje de interacción para agentes, basado en la Teoría de los actos del habla, está compuesto por  un conjunto de primitivas de comunicación. Generalmente, es posible definir un conjunto de primitivas  que captura los aspectos esenciales de la comunicación en forma independiente del dominio de  aplicación La descripción semántica de este conjunto determina el modelo de comunicación.   Nuestra propuesta, parte de un ACL con primitivas básicas de comunicación y lo extiende para incluir  primitivas de negociación, que reflejen la intención del agente en el mensaje: Require_for, Insist_for y  Demand_for      Trabajo Futuro  El comportamiento de un agente aislado está determinado por sus motivaciones individuales, sus  creencias acerca del mundo y sus propias habilidades. Esta caracterización es insuficiente para modelar  un agente que participa en un contexto social con una actitud cooperativa. En nuestro trabajo, la actitud  del grupo es colaborativa, pero puede existir conflicto entre las intenciones de sus miembros.  Los  agentes interactúan intentando influir en los planes y preferencias buscando alcanzar un acuerdo  compartido.  La interacción puede mejorar considerablemente si el contexto contiene algunas normas que permitan  establecer acuerdos generales sobre el comportamiento. Las normas no solo hacen que los individuos  actúen de cierta manera específica sino que también condicionan su comportamiento social y  estructuran su interacción con el resto de los agentes. El modelo propuesto se extenderá para reflejar el  impacto de las normas, roles y relaciones en el proceso de negociación.	﻿Negociación , Sistemas Multi Agente	es	21446
55	Planning en agentes inteligentes	﻿ Los agentes inteligentes necesitan satisfacer sus objetivos, para esto una de las técnicas mas  utilizadas es planning. El problema es que los algoritmos de planning no fueron pensados  especialmente para interactuar con agentes, sino para que estos aprovechen sus virtudes a  fin de armar un plan que logre satisfacer sus objetivos. La mayoría de los  agentes  inteligentes poseen información extra sobre el ambiente en el que se encuentra, esta  información conforma el estado mental del agente (creencias, preferencias, objetivos, etc.).  La idea principal del proyecto es utilizar internamente en la construcción del plan la  información del estado mental del agente.      Introducción     Los agentes inteligentes de software son componentes computacionales que actúan autónomamente  para alcanzar un conjunto de objetivos. Para satisfacer estos objetivos, uno de los algoritmos más  utilizados es el de planning. Un algoritmo de planning construye un plan de acción a partir de un  estado inicial de su mundo, un estado final que desea alcanzar y un conjunto de acciones que le  podrían permitir llegar al objetivo.   Los algoritmos de planning son actualmente utilizados por componentes que representan agentes  inteligentes en forma aislada, invocando estos algoritmos sólo enviando los tradicionales datos de  estado inicial, estado final y acciones. Sin embargo, los agentes necesitan que estos algoritmos  contemplen su estado mental. En el estado mental de los agentes se representan aquellas actitudes  mentales que le permiten decidir entre varios caminos a seguir sin que estos estén explícitamente  codificados en su implementación. Así, los objetivos, las preferencias, los compromisos, las  intenciones, entre otras actitudes mentales, son representadas en el estado mental de un agente.  Esta necesidad de los agentes se evidencia en el diseño de agentes de interfaz, por ejemplo, cuando  preferencias y compromisos guían las definiciones de que planes resultan viables o no, y que planes  permiten a sus agentes obtener más ventajas que con otros. Para alcanzar esta funcionalidad con los  algoritmos de planning tradicionales, es necesario realizar un análisis de cada plan posible filtrando  los planes que contradicen fuertemente preferencias y compromisos y luego generando un ranking  para maximizar ventajas. Lamentablemente, el tiempo consumido en este proceso es demasiado  grande para hacerlo viable en aplicaciones de agentes de interfaz ya que éstos requieren interacción  constante con un usuario humano a partir de estos resultados.    Planning  Tomando como algoritmo de experimentación un algoritmo que retorna un plan con orden parcial,  el algoritmo UCPOP [Weld 94], considera que este recibe como datos iniciales tres elementos como  se puede observar en el esquema de algoritmo presentado a continuación. El primer dato es un plan  nulo que contiene el estado actual, el segundo es una agenda de objetivos y el tercero es un conjunto  de acciones con sus precondiciones y efectos.  En el armado del plan, tres elementos son definidos, <A,O,L> en el algoritmo. A es el conjunto de  acciones que forman el plan, O es el conjunto de restricciones de orden y L es el conjunto de  enlaces causales definidos durante la generación del plan. Un enlace causal especifica la razón por  la cual una acción es ingresada a un plan, o sea, que efecto de una acción es condición necesaria  para la ejecución de otra.     POP( <A,O,L>, agenda, X )   1. Terminación: Si la agenda de objetivos está vacía, retorna <A,O,L>.   2. Selección del objetivo: Sea <Q,Aneed> un par de la agenda (por definición        Aneed ∈ A y Q es un elemento de la precondición de Aneed).   3. Selección de la acción: Sea Aadd = choose de una acción que alcanza Q        (con una nueva acción de X, o una acción existente en A, la        cual puede ser consistentemente ordenada como anterior a Aneed). Si tal        acción no existe, se retorna fracaso. Sea L’ = L U {Aadd  -Q-> Aneed}, y sea        O’ = O U {Aadd < Aneed }. Se Aadd es una nueva instancia, entonces           A’ = A U {Aadd} y O’ = O U { A0 < Aadd < A š } (caso contrario A’ = A).   4. Cambios en el conjunto objetivo: Sea agenda’ = agenda - {<Q,Aneed>}.       Si Aadd es una nueva instancia, entonces para cada conjunto Qi de       sua precondiciones, agregar <Qi,Aadd> a agenda’.   5. Protección de enlaces causales: Para cada acción At que puede        amenazar un enlace causal Ap -R-> Ac ∈ choose L’ que es una restricción       de orden consistente, hacer       a. Agregar At < Ap a O’, o       b. Agregar Ac < At a O’ (promoción).   6. Invocación recursiva: POP(<A’, O’, L’>, agenda’, X).    El esquema de algoritmo anterior expone los principales análisis realizados sobre los datos enviados  para la generación de un plan de acción. En éste se remarca una invocación a una función choose,  ya que es un punto importante que hace que el plan resultante, si buscamos sólo uno cualquiera, sea  más eficiente. Es el único punto que se sugiere especializar en los diferentes dominios en los cuales  se puede aplicar un algoritmo de planning.    Estados mentales  El estado mental de un agente es el sub-componente computacional que tiene la responsabilidad de  registrar no sólo el conocimiento que tiene sino también registrará otras actitudes mentales que  permitirán procesar la toma de decisiones en forma autónoma del agente. Entre estas actitudes  mentales, los objetivos son muy relevantes ya que son los que guían el comportamiento del agente.  No menos importantes son las intenciones, que representan lo que el agente quiere. A partir de las  intenciones, el agente decide intentar alcanzar alguna de ellas, momento en el cual se convierten en  objetivos. Otras actitudes mentales influencian la decisión de que intención se convierte en objetivo  y cómo estos objetivos son alcanzados. Entre estas últimas, se puede mencionar las creencias y las  obligaciones.  Por ejemplo, goal(box(182,londres)) especifica que el agente quiere que la caja 182 esté en  Londres. Para ellos existen varios planes de acción posible que un agente puede calcular con un  algoritmo de planning. Pero, si se considera que el agente tiene en su estado mental  preference(visit(paris),9) que especifica que el agente tiene una preferencia alta (nueve) de visitar  Paris, un plan que pase por Paris será de mayor preferencia que uno que no lo haga. Más aún, si  visitar Paris no es una preferencia sino un objetivo goal(visit(paris)) la importancia de esta  restricción es aún mayor ya que cumpliría otro de sus objetivos, aunque no sea parte del planteo  inicial.  Varios marcos formales se han construido para representar formalmente estas actitudes y las  relaciones entre ellas [Cohen 90] [Singh 99] [Wooldridge 00]. Lamentablemente, no se han podido  definir caminos de materialización prácticas de estos formalismos. Sin embargo, estas  formalizaciones pueden ser utilizadas para enriquecer representaciones lógicas respetando estas  definiciones de actitudes mentales. Particularmente, utilizamos módulos lógicos.   Los módulos lógicos permiten trabajar con cláusulas lógicas a nivel de programación,  particularmente, el lenguaje JavaLog [Zunino 01] permite utilizar módulos lógicos y combinarlos  con las operaciones definidas en [O’Keefe 85].  Por ejemplo, la figura 1 expone un ejemplo de un agente que define en tres módulos lógicos  preferencias que utilizará en forma aislada o combinada en diferentes contextos.  aPersonalAssistant  context2 = [ preference(A,7):- request(A, business, Partic, Date, Hour, Place),   member(X,P), boss(X).                      preference(A,10):- request(A,golf,-,-,-,-).]  context3 = [ preference(A,10):- request(A,golf,-,-,-,-).] context1 = [ preference(A,10):- request(A, business, Partic, Date, Hour, Place),  member(X,P), boss(X).                      preference(A,10):- request(A,golf,-,-,-,-).]   Figura 1: Módulos lógicos en un agente.    Estos módulos lógicos permiten combinar a través de un conjunto definido de operaciones clausulas  lógicas tipo Prolog. Enriqueciendo estos módulos con actitudes mentales se permitirá materializar la  propuesta que se expone a continuación.    Solución Propuestas  Los agentes inteligentes que en la toma de decisiones generan planes en tiempo de ejecución  requieren que los planes que generan respeten su estado mental. Así, si contamos con dos planes  posibles para alcanzar un objetivo pero uno de ellos viola nuestras preferencias, se tomará el  restante para su ejecución. Una simple opción es la de generar todos los planes posibles y luego  procesarlos considerando cada una de las actitudes mentales del estado mental del agente.  Lamentablemente, los tiempo de ejecución hacen que esta solución no sea viable en agentes que  conversan con usuarios humanos [Berdun 02].  Para resolver este problema, se propone el desarrollo de un algoritmo de planning específico para  agentes, el cual considere internamente las actitudes mentales durante la generación de los planes.   Para esto es necesario establecer puntos de interacción con el estado mental del agente, de manera  tal que el agente se transforme en la guía del algoritmo, con esto se logra filtrar los planes a medida  que se generan,  evitando llegar a un plan que no este de acuerdo con las preferencias del agente.  Adicionalmente, y en base a las preferencias y objetivos del agente,  se busca armar una solución  que se aproxime a la óptima, pero con un costo computacional mucho menor que el de recorrer todo  el espacio de soluciones en búsqueda de la mejor.  Los trabajos iniciales se desarrollan sobre el algoritmo Ucpop. En base a este, se estudiaron posibles  puntos de interacción con el agente, así como también que es lo que el agente necesita del algoritmo  para poder trabajar con su estado mental. Esto permitió arribar a soluciones diferentes de las que se  conseguían en una ejecución simple del algoritmo Ucpop. Las soluciones que se consiguen, de  acuerdo con el estado mental del agente, se encuentran de acuerdo con las preferencias, objetivos y  restricciones del agente.    Conclusiones  El avance del proyecto permite hoy que un agente guíe la construcción del plan. Esto ha permitido  obtener soluciones que varían de acuerdo al estado mental del agente. Particularmente se trabajo  especialmente sobre el algoritmo Ucpop, logrando modificaciones que le permiten al agente  manipular la creación del plan, y poder aplicar sus preferencias y obligaciones a medida que se  gesta el plan, lo cual significa una mejoría temporal substancial respecto al trabajo realizado en  [Berdun 02]. Paralelamente se construyó una herramienta que permite realizar seguimiento gráfico  del plan en desarrollo y del estado mental del agente.	﻿agentes inteligentes , Planning	es	21465
56	Integración de los estándares UML y WfMC para el modelado de workflows	﻿ Un proceso de negocio es un conjunto de tareas lógicamente relacionadas que se ejecutan para obtener un cierto resultado de negocio. Un proceso de negocio  incluye tanto recursos humanos como materiales. Los procesos de negocio pueden ser controlados y administrados por un sistema basado en software,  proceso de negocio automatizado de esta manera se denomina workflow. Esta automatización resulta en una importante potenciación de las virtudes de dicho proceso. La WfMC (Workflow Management Coalition) surge con el fin de establecer una estandarización  que permita la interoperabilidad de las diversas implementaciones de workflows. El estándar propuesto  incluye un metamodelo de los procesos de workflow (Metamodelo Workflow) y un lenguaje (WPDL) de especificación textual de procesos. Por otro lado, tenemos la notación UML (Unified Model Language) cuyo uso es ampliamente difundido y aceptado a lo largo de todo el ciclo de desarrollo de sistemas de software, y que, a través de sus diagramas de actividades, puede ser utilizada en el modelado de procesos de negocio. El poder expresivo del metamodelo Grafos de Actividades de UML resulta menor que el del Metamodelo Workflow. Ambos metamodelos permiten el  modelado de  los mismos conceptos, con la diferencia de que el metamodelo Workflow ofrece un mayor nivel de detalle. En esta línea de investigación proponemos una integración del metamodelo de Grafos de Actividades de UML para igualarlo con el Metamodelo Workflow haciendo posible la integración de ambos estándares. Introducción Un proceso de negocio es un conjunto de tareas lógicamente relacionadas que se ejecutan para obtener un cierto resultado de negocio. Un proceso de negocio [1] incluye tanto recursos humanos como materiales, además de procedimientos de negocio, todos están orientados a producir un beneficio para la organización. Los procesos de negocio  pueden ser controlados y administrados por un sistema basado en software [2,3],  proceso de negocio automatizado de esta manera se denomina workflow. Esta automatización resulta en una importante potenciación de las virtudes de dicho proceso. Se obtienen mejoras en cuanto a rendimiento, eficiencia y productividad de la organización. Las tecnologías de administración de flujos de trabajo (Workflow Management - WFM) [4] han tenido un crecimiento notable en una gran variedad de industrias. Hay cada vez más productos que aprovechan la riqueza de las tecnologías WFM para el planeamiento y ejecución de procesos de negocios. La característica principal que presentan estos sistemas es la automatización, parcial o total, de los procesos de negocios logrando una interacción entre los recursos humanos y aquellos basados en máquinas. La WfMC (Workflow Management Coalition) surge con el fin de establecer una estandarización que permita la interoperabilidad de las diversas implementaciones de workflows [5]. El estándar propuesto  incluye un metamodelo de los procesos de workflow (Metamodelo Workflow) y un lenguaje (WPDL) de especificación textual de procesos. Por otra parte, existe la notación UML cuyo uso es ampliamente difundido y aceptado a lo largo de todo el ciclo de desarrollo de sistemas de software, y que, a través de sus diagramas de actividades, puede ser utilizada en el modelado de procesos de negocio. Sin embargo, dicha notación no cumple el estándar de la WfMC [6] . El poder expresivo del metamodelo Grafos de Actividades de UML [7,8] resulta menor que el del Metamodelo Workflow. En esta línea de investigación proponemos una extensión del metamodelo de Grafos de Actividades de UML [9] para igualarlo con Metamodelo Workflow haciendo posible la integración de ambos estándares. El mecanismo básico de extensión se basa en la identificación de coincidencias estructurales o paralelismos entre elementos de ambos metamodelos.. Por lo tanto, es lógico que la propuesta de extensión esté basada en el mecanismo de especialización a través de herencia de metaclases, en su mayoría pertenecientes a los paquetes State Machines y Activity Graphs. Las nuevas metaclases representan conceptos presentes en los procesos de negocio pero agregan especificidad orientada al cumplimiento del estándar de la WfMC. Esta línea de trabajo  presenta brevemente en la sección 1 la estructura del metamodelo UML. En la sección 1.2 el lenguaje UML. Los diagramas de actividades de UML son descriptos en la sección 1.3.  La sección 2  explica básicamente el  metamodelo Workflow y  en la sección  3 se propone una extensión del metamodelo de grafo de actividades de UML basándose en el metamodelo Workflow. Finalizando, en la sección 4, se encuentran  las  conclusiones y futuras extensiones. 1. Metamodelo UML UML ha sido definido por el Object Management  Group (OMG)  [9] quien establece en su especificación una descomposición lógica de paquetes, tales como Foundation, Behavioral Elements, and Model Management, a su vez éstos están descompuestos en sub-paquetes. El metamodelo es descripto de un forma semi-formal a través de los siguientes puntos: Sintaxis abstracta: Es establecida mediante la utilización de diagramas de clases UML más una descripción en lenguaje natural. Reglas de buena formación: Son establecidas mediante la utilización de Object Constraint Language OCL y lenguaje natural. Semántica: La semántica es descripta en lenguaje natural, pero puede incluir algún otro tipo de notación adicional. En resumen, el metamodelo de UML es descripto combinando notación gráfica, lenguaje formal y lenguaje natural. 1.1 El Lenguaje de Modelado Unificado (UML) UML es el lenguaje estándar para el modelado de software , el cual permite la visualización, especificación, construcción y documentación de los elementos intervinientes en el modelado de un sistema. También da a los desarrolladores la posibildad de visualizar los resultados de su trabajo en esquemas estandarizados, proporcionando elementos, relaciones y diagramas. Dentro  de esta última categoría se encuentran  los diagramas de Actividades. 1.2 Diagramas de Actividades Los diagramas de actividades definidos por UML pueden ser utilizados para dos objetivos diferentes, para modelar aspectos dinámicos de los sistemas, es  decir procesos computacionales  y para el modelado organizacional, es decir, para la ingeniería de procesos de negocio y modelado de flujos de trabajo [10]. Cuando se modela un flujo de trabajo se pone especial énfasis en las actividades, tal y como son percibidas por los actores que colaboran con el sistema.  Un diagrama de actividades es básicamente una proyección de los elementos de un grafo de actividades, un caso especial de máquina de estados en la cual la mayoría de los estados son estados de actividad y en la cual todas o casi todas las transiciones se disparan al terminar la acción en el estado origen. La OMG (Objet Manangement Group) define mediante un diagrama de clases la sintaxis abstracta para los grafos de actividades [9]. 2. Metamodelo Workflow A los procesos de workflow le concierne la automatización de procedimientos dónde documentos, información o tareas son pasadas entre participantes, de acuerdo a un conjunto de reglas para alcanzar o contribuir al propósito general del negocio [6]. Mientras que los workflows pueden ser manualmente organizados, en la práctica la mayoría de los workflows están organizados dentro del contexto de un sistema IT que provee soporte computarizado para la automatización de los procedimientos. Un proceso workflow se define como la automatización parcial o total de un proceso de negocio [11], con el objetivo de lograr la interacción de diversas actividades realizadas por personas y máquinas. El metamodelo dado por la WfMC es descripto, mediante entidades, relaciones y atributos. Las entidades más importantes y a la vez más usuales que contiene una definición de un proceso [12]. Los elementos presentes son apropiados para definiciones de proceso simples, por lo que éste es un metamodelo básico [16]. Los desarrolladores de productos workflow pueden extender el metamodelo agregando nuevos tipos de objetos para el uso específico en sus productos. Cada una de las entidades tiene asociado un conjunto de atributos que la caracterizan. Algunos atributos son obligatorios y otros opcionales. Cuando se necesite describir características adicionales, el usuario puede definir atributos extendidos. 3. Extensión del metamodelo de grafo de actividades de UML basándose en el metamodelo Workflow El mecanismo utilizado para la extensión  consiste primero en identificar las coincidencias estructurales o paralelismos entre los elementos de ambos metamodelos [18]. Partiendo de la base de que tanto un diagrama de actividades como una especificación en WPDL permiten expresar procesos de negocio, se buscan similitudes y diferencias entre ambos mecanismos de modelado. En general, ambos metamodelos permiten expresar los mismos conceptos, con la diferencia de que el metamodelo Workflow ofrece un mayor nivel de detalle. Por lo tanto, es lógico que la propuesta de extensión esté basada en el mecanismo de especialización a través de herencia de metaclases, en su mayoría pertenecientes a los paquetes State Machines y Activity Graphs. Las nuevas metaclases (figura 2) representan conceptos presentes en los procesos de negocio agregando un mayor nivel de detalle orientado al cumplimiento del estándar de la WfMC. Primeramente, por cada entidad del metamodelo Workflow de la Interfase 1 [12], se presenta la especificación dada por la WfMC. En base a esta especificación, expresada en tablas de atributos, se construye un diagrama de clases equivalente [19]. Posteriormente se propone, para cada entidad, una extensión del metamodelo de grafos de actividades de UML para que cumplan con el estándar de Workflow y para aumentar su poder expresivo. La extensión se logra incorporando nuevas metaclases al metamodelo UML. que representan conceptos de Workflow pero no en el metamodelo UML. La incorporación se realiza a través de herencia, agregando las nuevas metaclases como subclases de las metaclases originales que representan el concepto más parecido . El resultado de la extensión es la adición de un nuevo paquete al metamodelo UML  [20], que llamamos Workflow Processes, (figura 1). Por medio de este nuevo paquete se aumenta el poder expresivo de UML y se logra que el diagrama de actividades que especifica un proceso de negocio verifique la definición de proceso de workflow y que el proceso de negocio resultante posea una potencial interoperabilidad con otros procesos de workflow. Figura 1: Estructura de paquetes Common Behavior State Machines Activity GraphsWorkflow  Processes Use CasesCollaborations WorkflowTransition Activi tyGraph (f rom Activ ity Graphs) Transition (f rom State Machines) State (f rom State Machines) StateMachine (f rom State Machines) * 0..1 +transi tion 10..1 +top TaggedValue (f rom Extension Mechanisms) ModelElement (from Core) * 0..1 +behavior +context * 0..1 +taggedValue WorkflowRelevantData ExtendedLibrary SimulationData instantiation [0..1] : InstantiationKind duration [0..1] : Integer cost [0..1] : String workingTime [0..1] : Integer waitingTime [0..1] : Integer Workf lowParticipant WorkflowApplication Parameter (f rom Core) WorkflowProcess ** +globalData restrictTo * 0..1 +data 0..1 0..1 +library 0..1 0..1 +ti meEsti mation * 0..1 +participant * 0..1 +application * 0..1 +parameter Figura 2: Sintaxis abstracta de Workflow Process  resultante de la extensión La figura 2 muestra una parte la sintaxis abstracta de la extensión. Trabajos recientes [15,16,17] han dirigido sus esfuerzos en líneas de investigación tendientes a relacionar  los conceptos de proceso workflow y grafo de actividades, sin dar demasiados detalles acerca de  las similitudes entre  ambos estándares. 4. Conclusiones y futuras extensiones Esta línea de investigación propone esencialmente la integración de dos estándares utilizados en el modelado de procesos de negocio: el estándar UML de diagrama de actividades y el estándar establecido por la  WfMC para definición de procesos de workflow. La integración permite incorporar al estándar UML la capacidad de generar modelos de proceso de workflow que satisfagan el estándar de la WfMC. Para ello es necesario extender el metamodelo de grafos de actividades aumentando  su especificidad sintáctica y semántica. Podemos identificar tres ventajas principales incorporadas a UML a partir de la integración de los metamodelos: portabilidad, estandarización y simulación de procesos. Portabilididad y estandarización son dos características que están muy relacionadas entre sí y recaen en la posibilidad de poder modelar, haciendo uso la extensión realizada a UML, un proceso workflow que cumpla con el estándar establecido por la WfMC. De este modo se incorpora también la portabilidad del proceso workflow al cumplir con el estándar anterior, permitiendo que el mismo proceso pueda ser ejecutado por diferentes motores workflow y que  pueda interactuar con otras definiciones de procesos. La extensión también permite al usuario especificar datos utilizados en la simulación de un proceso de workflow. De esta manera, la simulación puede ser realizada directamente a partir de la definición de proceso favoreciendo la retroalimentación entre las etapas de definición, análisis y  reingeniería del proceso. Es decir, permite observar si realmente se está modelando lo que se desea y lo que se espera del proceso. Hasta el momento se ha logrado la expresión de ambos estándares en términos equiparables (diagramas de clases). Las diferencias encontradas se basan fundamentalmente en el hecho de que ambos estándares están pensados para la formulación de distintos tipos de procesos. Los diagramas de actividades de UML tienen por finalidad servir de herramienta para visualizar, especificar, construir y documentar procesos en general. En particular, un diagrama de actividades puede ser utilizado para modelar procesos de negocio. Por su parte, el estándar de la WfMC surge como una forma de unificar la representación de diversos elementos presentes en diferentes implementaciones de procesos de negocios automatizados, por lo que tiene por finalidad servir de herramienta para visualizar, especificar, construir y documentar procesos de workflow. Se ha propuesto también una extensión al metamodelo de grafos de actividades de UML. Mediante especialización se han incorporado nuevas metaclases  aumentando la precisión semántica y el nivel de detalle del metamodelo  UML.  La incorporación del concepto de proceso de workflow, además de adaptar UML a las nuevas tecnologías Workflow, permite integrar el modelado de procesos de negocio ejecutables. Continuando con esta línea de investigación, se proponen los siguientes trabajos futuros: • La definición de una sintaxis concreta para el nuevo paquete Workflow Processes. • La extensión de UML propuesta se basa en la especificación  establecida por la OMG versión 1.3. Actualmente la OMG ha publicado la versión 1.5 en donde se establecen algunas modificaciones al metamodelo. Es por ello que una actividad futura será analizar y rediseñar la extensión de acuerdo a los  cambios incorporados. • La creación de una herramienta CASE para la definición y simulación  de procesos workflows. • El grupo PUML (PreciseUML) tiene propuesto rediseñar a UML como una familia de lenguajes usando una aproximación de metamodelado precisa orientada a objetos. Para lograr esto utiliza MMF(Meta Model Facility). Se propone como trabajo reescribir los elementos de UML necesarios y sobre esta base, extender el nuevo metamodelo UML para el modelado procesos workflow.	﻿Workflow Management Coalition , estándares UML , modelado de workflows	es	21491
57	Integración de técnicas orientadas al cliente y técnicas formales en el desarrollo de software con UML y RUP	"﻿Integración de técnicas orientadas al cliente y técnicas formales en el  desarrollo de software con UML y RUP    Favre, Liliana; Leonardi, Carmen; Mauco, Virginia; Felice, Laura; Pereira, Claudia;  Martínez, Liliana  INTIA  Facultad de Cs. Exactas  UNCPBA    1. Introducción    En los últimos años UML (""Unified Modeling Language"") se ha impuesto  como un estándar de– facto para expresar modelos orientados a objetos. Es un lenguaje diseñado para especificar,  visualizar, construir y documentar “artefactos” de sistemas de software [Booch’99].   A diferencia de los lenguajes visuales que lo precedieron, UML posee una definición semántica  más precisa que combina notación gráfica,  reglas bien formadas expresadas en OCL [OMG’01] y  lenguaje natural. Esta definición semántica da una estructura rigurosa al lenguaje aunque aún varias  de sus construcciones están definidas  débilmente.  Si bien UML estandariza un lenguaje de modelamiento y no impone ningún proceso de  desarrollo fue concebido pensando en procesos dirigidos por casos de uso, centrados en  arquitecturas, iterativos e incrementales. En noviembre de 2001 OMG presenta “Software Process  Engineering Metamodel” (SPEM) que es usado para describir un proceso de desarrollo de software  concreto o una familia de procesos que usan UML. El  proceso más popular que se ajusta a SPEM  es  RUP (“Rational Unified Process”) [Krutchen’00].   La existencia de un lenguaje de modelamiento estándar como UML brinda la posibilidad de  concentrar esfuerzos en la definición de potentes herramientas CASE UML. Pueden mencionarse  entre las numerosas existentes en el mercado a Argo/UML, Together, GDPro, Stp/UML, Rational  Rose, MagicDraw/UML, Rhapsody y Objecteering. Las mismas  asisten en el análisis, diseño e  implementación de sistemas orientados a objetos. Proveen facilidades, si bien limitadas, para  homogeneizar diagramas y realizar comprobaciones que detecten inconsistencias y errores como  asimismo para  procesos de ingeniería directa (forward engineering) e inversa (reverse  engineering).       Estas herramientas evolucionan constantemente, con el objeto de cubrir aspectos no considerados  y así brindar asistencia  en las distintas etapas del proceso de desarrollo de software. Por ejemplo,  las mismas aún no brindan una completa asistencia para la construcción de modelos del negocio  durante las primeras etapas de desarrollo en donde la interacción con los stakeholders es crucial.  Por otro lado, la falta de una semántica formal para UML dificulta la  validación de modelos UML  y  el análisis de consistencia entre las diferentes vistas del sistema. Ésto crea dificultades en el  desarrollo de sistemas complejos y críticos donde es crucial realizar un análisis riguroso de las  propiedades  expresadas por los modelos y  detectar ambigüedades e inconsistencias desde los  primeros niveles de un diseño. Asimismo, los procesos de generación de código son bastante  limitados y  no permiten transformar toda la información registrada en los modelos UML.  La  ingeniería inversa soporta un subconjunto de la notación UML y requiere de la habilidad del  programador para mantener la integridad entre modelos UML y el código. Tampoco brindan  asistencia para la optimización de modelos (refactoring).  Teniendo en cuenta los aspectos mencionados anteriormente,  se propone en esta investigación  definir las bases para extender la funcionalidad de las CASE UML existentes. Este proyecto  pretende definir modelos y estrategias que partiendo desde el modelo de negocios permitan obtener  código orientado a objetos. Para esto se presenta la incorporación de modelos de requisitos  orientados al cliente y un conjunto de heurísticas para poder guiar la construcción del modelo de  negocios propuesto por RUP, más concretamente los Business Use Cases y el Modelo de Objetos  del Negocios. Asimismo, se presenta la integración de UML con técnicas formales que permitiría el  uso de verificación formal y  la definición de procesos rigurosos de ingeniería directa e inversa que  tiendan a la automatización facilitando el reuso y la evolución del software.   Se describen en la sección 2 la propuesta para la definición de modelos del negocio a partir de  modelos de requisitos orientados al cliente. La sección 3 describe las bases de la integración de  UML con técnicas formales y los resultados logrados en cuanto a la definición de procesos  rigurosos de generación de código. La sección 4 presenta conclusiones y extensiones de esta  investigación.     2. Definición del Modelo de Negocios de RUP a partir de modelos orientados al cliente    “Se puede considerar  al problema del desarrollo de software como un problema de construir  un artefacto. Este  artefacto  será instalado en el mundo con el cual va a interactuar [Jackson’95]”.  Esa parte del mundo en la cual los efectos del artefacto serán  sentidos,  evaluados  y aprobados en  caso de  éxito, se denomina Dominio de Aplicación o Universo de Discurso, UofD [Leite’95]. El  UofD es el contexto general en el cual el software será desarrollado, operado y mantenido. Incluye  todas las fuentes de información y personas o sectores relacionados con la aplicación. El UofD es  donde se originan los requisitos [Jackson’95; Jacobson’99], por lo que, sino se define  apropiadamente no será posible enfocarse en los mismos. Con este objetivo, se propone para esta  etapa modelar el UofD a partir de modelos en lenguaje  natural y orientados al cliente, favoreciendo  la interacción con el cliente, muy importante en esta etapa. Estos son:  el Modelo  Escenarios para  definir el comportamiento del UofD [Leité97],  el modelo  del Léxico Extendido del Lenguaje  (LEL) que describe el vocabulario del mismo [Leité95], y finalmente el Modelo de Reglas de  Negocio que  describe las  políticas de la organización [Leonardí98, Leité98].   A partir de estos modelos se pueden derivar un modelo conceptual  de objetos que permite a  los ingenieros de software visualizar  el UofD en términos de los conceptos que se manejarán  a lo  largo de todo el proceso de desarrollo, haciendo que  la transición desde el modelado conceptual al  diseño e implementación sea un  proceso más natural. En el contexto de RUP y UML este modelo  es el modelo de Negocios.   Manipular esta información para obtener un   modelo conceptual de objetos   no es una tarea  trivial, ya que debe filtrarse y modelarse en términos de los conceptos que  se manejarán en la  solución, en nuestro caso, objetos  y relaciones.  La definición de clases es un proceso dual, así  como se definen clases  determinando abstracciones relevantes para el sistema,  es igualmente  importante poder descartar clases. Por esta razón, es crucial   poder  manipular la información de los  modelos basados en lenguaje natural para poder definir y descartar las abstracciones. Por esta razón,  en esta etapa se propone una estrategia basada en heurísticas que asista al ingeniero en la  construcción del modelo de negocios a partir de los modelos de  requisitos. El uso de las heurísticas  mejora los aspectos de traceability entre los modelos generadores y los generados. El siguiente  gráfico muestra las actividades de esta etapa, en donde en cada una de ellas se define un conjunto de  heurísticas  que guían la  construcción del Modelo del Negocio. La estrategia completa de esta etapa  se describe en [Leonardi'03]. Actualmente se han desarrollado prototipos que permiten la semiautomatización de ciertos aspectos de esta etapa [Petersen'01; Antonelli'99], pero no en el contexto  de RUP/UML.     3.  Ingeniería directa e inversa de modelos estáticos UML  La falta de una semántica formal para UML pone límites a la potencia de las herramientas CASE  UML. Éstas proveen poco soporte para la validación de modelos UML especificados en OCL y  además no permiten transformar toda la información registrada en los modelos UML. Por ejemplo,  una clase especificada en OCL brinda información importante para la identificación de clases y  “frameworks” que puedan ser adaptados y reutilizados. A partir de diagramas de clase con  especificaciones en OCL, también podrían generarse precondiciones y postcondiciones de  operaciones e invariantes de una clase en lenguajes orientados a objetos que las soporten, como por  ejemplo EIFFEL.   Una alternativa para evitar estas limitaciones es integrar a UML con técnicas formales. Se  propone una integración con especificaciones algebraicas. Las bases de este enfoque son la  definición de un lenguaje de modelamiento algebraico, la definición de un modelo de componentes  reusables y de procesos transformacionales que permiten transformar modelos UML/OCL a  especificaciones algebraicas y a código orientado a objetos.   Con respecto a la formalización algebraica se optó por diseñar un lenguaje de modelamiento,  denominado GSBLoo [Favre’01]   cuyas primitivas reflejaran los conceptos de UML en forma  directa. La característica esencial y original del mismo es que su diseño está centrado en  abstracciones de relaciones (relation-centric). El lenguaje provee una jerarquía de tipos  constructores para relaciones de dependencia, agregación, composición, y asociaciones. El lenguaje  puede verse como un “intermediario” entre UML y otros lenguajes algebraicos, en particular se  trabaja con los lenguajes CASL [COFI’00 ] y RSL [George’95  ].   Con respecto a la especificación formal de componentes reusables se definió un modelo que  integra, en diferentes niveles de abstracción, especificaciones formales y código orientado a objetos.  Un componente reusable consiste de tres diferentes niveles de abstracción (especialización,  realización e implementación) que conectan especificaciones algebraicas incompletas,  especificaciones algebraicas completas y  código orientado a objetos respectivamente. El primer  nivel consta de dos vistas, una algebraica y otra en OCL. Se definieron formalmente operadores de  reuso (Rename, Combine, Extend y Hide). A partir del modelo se definió un método para el reuso  de diseños y código basado en la  adaptación e integración de componentes en los tres niveles.  En  particular para el método RAISE [George’95] para el cual no se hace referencia explícita a la  reusabilidad de especificaciones, se ha definido un modelo [Felice’03] para describir la estructura  de una componente reusable en los distintos niveles de abstracción. Este modelo permitirá la  Definición  Modelo Business Use-Cases (BUC) Identificación de clases Refinamiento de Responsabilidades de las clases Definición Diagrama de  Objetos LEL modelo escenarios meta modelo LEL meta modelo BUC Modelo BUC Reglas de Negocio no funcionales Lista de clases heurísticas de refinamiento heurísticas de Identificación clases heurísticas definición BUC Lista  de clases refinadas heurísticas definición diagrama Reglas de Negocio funcionales diagramas de Actividades  y colaboración Diagrama  de Objetos Realización del modelo de  BUC diagramas de interación o actividades heurísticas  definición integración de especificaciones de componentes en el lenguaje RSL y clases concretas en un  lenguaje orientado a objetos.  El  proceso de generación de código se basa en establecer correspondencias formales entre  diferentes lenguajes.  Las transiciones entre los diagramas UML y todas las especificaciones  intermedias se realizan aplicando operadores de transformación que preservan la integridad entre  especificaciones y código. Toda la información contenida en los modelos (por ejemplo  asociaciones,  multiplicidades y “constraints” OCL) tiene su contrapartida en  las especificaciones  algebraicas y tendrá implicancias en el código generado.   El proceso traduce en una primera etapa, modelos estáticos UML a una especificación algebraica.  Un aspecto importante a destacar en esta etapa es la transformación de precondiciones,  postcondiciones e invariantes de clases OCL  a axiomas en la especificación algebraica. Se obtiene  así una especificación a partir de la cual es posible realizar un análisis riguroso del comportamiento  modelado con técnicas de razonamiento basadas en especificaciones algebraicas.  La especificación  obtenida será la base para la generación de implementaciones que se construyen a partir de  esquemas de código reusables y transformaciones de especificaciones  a código orientado a objetos.  Se prevé abordar refactoring de modelos UML y la ingeniería inversa de sistemas legacy a partir  de las bases propuestas.       4. Conclusiones    Este proyecto pretende integrar las experiencias obtenidas por dos líneas de investigación que ya  han tenido resultados en forma independiente: por un lado trabajos desde el área de ingeniería de  requisitos [Leonardí03, Leonardí01, Leonardí98, Leité98] y por el otro de especificaciones  formales [Favré02, Favré01, Maucó02]. De esta forma, la estrategia integraría modelos  orientados al cliente que ya han sido probados en el área de ingeniería de requisitos en el contexto  de un desarrollo riguroso de software orientado a objetos. Resultados parciales de esta integración  se encuentran en [Felicé02]. Para esto es necesario no sólo lograr un mayor grado de formalización  en las heurísticas sino también refinar las heurísticas para obtener especificaciones más precisas de  los modelos (por ejemplo, profundizar las heurísticas de tratamiento de las reglas del negocio para  generar constraints en OCL).  Estos modelos son la entrada para el proceso de ingeniería forward.   Asimismo, también se trabaja en la prototipación de etapas claves vinculadas a las extensiones  propuestas. Los resultados logrados muestran que sería factible extender la funcionalidad de las  herramientas CASE existentes y aumentar el grado de automatización de los procesos que soportan.  Las actividades posteriores incluirán la integración de estos resultados con CASE UML."	﻿UML Unified Modeling Language , lenguaje diseñado , definición semántica	es	21493
58	Especificación en RSL de componentes basadas en streams	﻿ Un stream es una secuencia finita o infinita de mensajes transmitidos sobre un canal.  Los streams tienen propiedades las cuales hacen posible modelar software en dominios  específicos. RAISE es un método riguroso para el desarrollo de software. RSL, es el  lenguaje de especificación formal usado por el método RAISE. Se han desarrollado  herramientas que permiten verificar automáticamente las especificaciones de los  módulos de software construidos. Aquí radica la ventaja del uso del método RAISE.  En esta línea de investigación se estudia el formalismo de los streams y las  componentes de software basadas en ellos. Se presentan aquí las especificaciones en  RSL de las propiedades básicas de streams y de la componente scan, la cual recorre  una secuencia de datos en tiempo serial aplicando una operación binaria, esta  operación binaria es subespecificada para permitir su reuso. Junto con la componente  scan se muestran sus aplicaciones en la construcción de nuevas componentes.    Palabras Claves: RAISE, RSL, streams, componente, especificación, esquema.    1. Introducción  Las componentes de software pueden ser especificadas por el formalismo de streams [4,5,6,7,8]  que permite el estudio de las componentes de software basadas en las historias de entradas y salida de  la misma.   La idea de las especificaciones de componentes, con el formalismo de streams, consiste en el  reuso de componentes existentes, es decir la construcción de una nueva se lleva a cabo usando las  definidas previamente.  Si bien la formalización de componentes de software usando stream brinda otra visión del  estudio de las mismas, carece de herramientas automáticas que permitan chequear la correctitud de las  especificaciones hechas usando éste formalismo. Es aquí donde el uso de método RAISE[1,2,3]  (Rigorous Approach to Industrial Software Engineering) junto con sus herramientas automáticas juegan  un rol central, ya que esta basado sobre  los principios de desarrollo separado, desarrollo paso a paso,   inventar, verificar y rigurosidad.  El punto importante usado en este trabajo es el concepto de desarrollo separado. Si se desea  desarrollar sistemas de gran tamaño debemos ser capaces de descomponer su descripción en  componentes y componer el sistema a partir de éstas. Cada componente puede ser especificada por  diferentes ingenieros, pero se presenta el problema que cada uno escribe una función que otro quiere  usar o reusar.  Es simple chequear el nombre de la función,  sus parámetros,  y su tipo de resultado,  pero no se puede afirmar lo mismo de la semántica de la función.  Los esquemas de RAISE son usados  para definir la semántica de cada componente.  El lenguaje usado para las especificaciones es RSL (RAISE Specification Language). RSL  permite a los ingenieros especificar esquemas usando el contexto y esquemas parametrizados. Si una  función definida no está en el esquema, esta función se busca en el contexto. Este se construye  escribiendo el nombre del esquema al comienzo de la definición de uno nuevo.  Otra característica usada en los esquemas de especificación de streams es el esquema  parametrizado. El uso de esquemas parametrizados indica los tipos que el esquema recibe como  parámetro permitiendo así la generalización de esquemas. Así, se definen las componentes usando estas  características. Esto permite el reuso de las componentes ya especificadas.  En las secciones siguientes se describe brevemente el formalismo de streams, las  especificaciones en RSL y los pasos que se siguieron en la investigación.  2. Especificación en RSL de Componentes Basadas en Streams  Para realizar el estudio del formalismo de streams y llevar a cabo la especificación en RSL, se  tomaron los siguientes aristas:    1. Especificación en RSL de los transformadores de streams primitivos.  2. Especificación en RSL de componentes basadas en streams existentes.  3. Creación de nuevas componentes basadas en streams con sus correspondientes especificaciones  en RSL.  A continuación se describen las tareas realizadas en cada uno de los aspectos mencionados  arriba.  1. Especificación de los Transformadores de Streams Primitivos  Los streams modelan una sucesión temporaria de mensajes sobre canales de una red de  componentes de comunicación. Dado un alfabeto A, el conjunto A* de streams finitos comprende todas  las secuencias A=<a1,a2,.....,ak> de longitud |A|=k >0 con los elementos ai  A (i  [1,k]). Los streams  finitos son generados desde el stream <> agregando elementos al frente del stream: <a1,a2,.....,ak>= a1   a2 ..... ak  <>. La concatenación de dos streams finitos: A= <a1,a2,.....,ak> and B= <b1,b2,.....,bl>.  Produce el stream A&B =< a1,a2,.....,ak, b1,b2,.....,bl> en [5] se encuentra una síntesis completa de los  operadores de streams. La especificación en RSL de estos operadores primitivos es directa porque los  mismos vienen incorporados en el lenguaje. Los transformadores de streams usados en este formalismo  se definen en base a los primitivos y siguen una lógica similar a la de la programación funcional y sus  especificaciones pueden ser vistas en [9].  2. Especificación en RSL de Componentes basadas en Streams  Como se dijo en apartados anteriores el aporte que hace el formalismo de streams a la  especificación de componentes de software es que permite el estudio de éstas basándose en las historias  de entrada y salidas.  Se han especificado, usando el formalismo de streams, un conjunto de componentes sencillas  que adquieren importancia por su generalidad, la cual se ve reflejada por su amplio uso en la creación  de nuevas componentes, una de éstas  es el scan. El scan es una componente dependiente del estado con  un puerto de entrada y uno de salida. Ésta, consume un stream de entrada de datos y emite el stream de  segmentos iniciales donde dos elementos son combinados bajo una operación diádica. El resultado de  esta componente depende del valor del estado inicial y de la operación de combinación.     El comportamiento del scan puede visualizarse de la siguiente manera:            La especificación en RSL de esta componente puede ser vista en la figura 1.   scheme Function_Scan=         class         type          B,          A           value          scan: B x (BxA → B) → (A* → B*)          scan(s,f)(A) is            if A=<..> then               <..>               else                <.f(s,hd(A)).> ^ scan(f(s,hd(A)),f)(tl(A))            end  end    Figura 1: Especificación en RSL de la componente scan  Existen dos teoremas que adquieren importancia porque ayudan a la especificación de nuevas  componentes, tal es el caso del Teorema de la Composición Paralela y el de Composición Serial.  El Teorema de la Composición Paralela afirma que:  (scan(s,⊕) x scan(t,⊗))° syn = unzip°scan((s,t), ⊕x⊗)  donde x denota la función producto y ° la composición de funciones.  El Teorema de la Composición Serial  es el siguiente:  scan(s,⊕) x scan(t,⊗)= map2(first) ° scan((s,t),Θ)  donde:  (x,y) Θ z = (x ⊕ (y ⊗ z), y ⊗ z)  El transformador de stream map2 aplica la función first a todos los elementos de una tupla de  streams y el transformador first retorna el primer elemento de una tupla. Las especificaciones en RSL  de estos teoremas pueden ser vistos en [9].  3. Especificación en RSL de Nuevas Componentes Basadas en Streams  En el apartado anterior se presentó la componente scan y se destacó su generalidad para  especificar nuevas componentes, se presentó además la especificación en RSL de la misma y se  delinearon los teoremas de la Composición Paralela y de la Composición Serial. En esta sección se  Scan(s,⊕)  ....  x3  x2  x1  ……  (s⊕ x1) ⊕ x2) ⊕ x3  (s⊕ x1) ⊕ x2  s⊕ x1  muestra la especificación con streams de una nueva componente: Un Contador Módulo N que usa al  scan.  El contador  consume un stream de números naturales con la siguiente regla: La entrada es  acumulada mientras  no ocurra un comando reset. Después de un comando reset el contador comienza  nuevamente desde cero.  Como un primer paso se fija la interfaz sintáctica de la componente. Sea N={0,1,2,3,....} el  conjunto de los números naturales . Para un número N 1, sea N={0,.....,N-1} los restos módulo N.  Un contador módulo se describe por el transformador de stream de orden superior  COUNT: N→[(N {reset})→N  Donde reset denota el comando reset. La componente combina el estado interno con la entrada  bajo la operación ⊕: N x (N {reset})→N validando:         0  si x = reset  s ⊕ x =       s+x mod N  si x  N    El comportamiento de la entrada salida de un contador módulo esta dado por el transformador  de stream:  COUNT(s) = SCAN(s, ⊕)  De esta manera se muestra como la componente scan puede ser usada para especificar nuevas  componentes. Además del contador se han definido otras componentes que hacen uso del scan: Una  celda de Memoria y un Concordador de Paréntesis, cuyas especificaciones pueden ser vistas en [9].    3 Conclusiones  El hecho de especificar en RSL, las funciones primitivas de streams, la componente scan junto  con sus propiedades y sus aplicaciones,  permitió observar que: la especificación de los operadores  primitivos de stream es directa, la especificación de componentes se facilita por el reuso de las mismas  y por su similitud con la programación funcional, el chequeo automático provisto por RSL es una gran  ayuda para la correctitud de la especificación de la componente basada en stream.  Como un caso de estudio se esta analizando la posibilidad de especificar en RSL una  arquitectura para la transmisión de videos en la web. Para esto se pretende en primera instancia realizar  una especificación usando streams y luego traducirla a RSL permitiendo así el chequeo automático de  las propiedades y comportamiento de las componentes.	﻿Componentes , RAISE , RSL , streams , especificación , esquema	es	21494
59	Nueva generación de componentes y su utilización en la web semántica	﻿ La Web semántica es una extensión de la Web actual, que permitirá encontrar, compartir y combinar la información más fácilmente. Fundamentalmente, se basa en tener datos definidos y asociados, a fin de facilitar los procesos de búsqueda, la integración, y el reuso a través de varias aplicaciones. La nueva generación de componentes de software para la Web semántica está basada en agentes inteligentes. Integrando estos conceptos, planteamos nuetra línea de investigación donde se propone estudiar la definición, la especificación de la semántica, y el ciclo de vida de componentes de software basados en agentes, para su utilización en la Web semántica. 1. Introducción La Web semántica se basa, fundamentalmente, en tener datos definidos y asociados a fin de facilitar los procesos de búsqueda, la integración, y el reuso a través de varias aplicaciones. De esta manera, se facilita la interacción de personas y computadoras trabajando cooperativamente, y así se intenta brindar nuevas funcionalidades, que produzcan mayor eficiencia. Asímismo, la integración de procesos de negocios del tipo B2B entre empresas, y la nueva generación de aplicaciones de comercio electrónico, requieren cubrir mayor cantidad de aspectos del negocio, y por ende, que las aplicaciones sean cada vez más complejas, y extensibles. Por otro lado, el desarrollo de software orientado a agentes extiende el desarrollo basado en componentes tradicional, prometiendo sistemas basados en componentes más flexibles, y con menores tiempos de desarrollo e implementación. En este trabajo, se presenta nuestra línea de investigación, en la que se propone estudiar la nueva generación de componentes de software basados en agentes inteligentes, y su utilización en la Web semántica. 2. Web Semántica y Tecnología de Agentes en CBD Actualmente, cada vez que se necesita buscar información sobre un determinado asunto nadie duda en acceder a Internet. La Web se ha convertido en la herramienta de consulta más amplia disponible a una gran variedad de usuarios, desde estudiantes a profesionales, incluyendo personas que por algún interés personal acceden a buscar información sobre un determinado tema, como por ejemplo información turística, información médica, artística, etc. Para poder ejecutar satisfactoriamente estas búsquedas se necesita que los conceptos disponibles tengan un significado completo, representado de tal manera que sea factible encontrarlos fácilmente cuando se disparan los procesos de búsqueda. En este proceso, no sólo es importante la representación del significado de los distintos elementos, sino la relación existente entre diferentes conceptos. Esto, da origen a lo que se denomina la Web Semántica, que es una extensión de la Web actual que permitirá encontrar, compartir, y combinar la información más fácilmente. Según Bernes Lee [8], la Web semántica se trata de que “la información debe ser reunida, de forma que un buscador pueda comprender, antes de ponerla, simplemente, en una lista”. Su intención se basa, en que la información esté almacenada de tal forma, que pueda ser comprendida primeramente por los procesadores, antes que por las personas. A modo de ejemplo, supongamos que se está intentando confirmar la asistencia a un evento, a través de una página en la Web. Este evento se realiza en otra ciudad, e incluye charlas de determinados profesionales. Analicemos las necesidades de la persona que desea asistir. En primer lugar, se quiere resolver la forma en que se va a trasladar. Imaginemos la situación en la que con un simple click del mouse, se le mostrarán al usuario las distintas posibilidades de transporte, presentándole información clara sobre horarios, tiempos de viaje y costo del pasaje. Además, tendría que ser posible que con otro simple botón de confirmación, se tuviera la posibilidad de confirmar su pasaje, efectuar el pago del mismo vía una transacción electrónica, y registrar también su inscripción en el evento. Podría solicitarse también que la información del evento actualice el cronograma de tareas en su dispositivo handheld, y reciba un mail con la descripción de las charlas del evento, y la información de las páginas personales de los disertantes. Del mismo modo, se podría resolver la reserva del hotel, la contratación del servicio de traslado, y otros tantos servicios requeridos por el asistente al congreso. A fin de poder ofrecer esta funcionalidad integrada, se requiere la facilidad para lograr distintos servicios, como por ejemplo poder asociar información de distintas bases de datos, compartir contenidos entre las mismas, y disponer de aplicaciones críticas que descubran nuevas posibilidades, y tengan la factibilidad de combinar distintas clases de servicios de la Web. Este nuevo enfoque, propone enriquecer la potencialidad de la Web a través de estructurar la información, agregándole componentes semánticos que puedan ser procesados de manera automática. Estos nuevos formatos, vienen definidos en XML, Extensible Markup Language [9], y RDF, Resource Description Frameworks, los cuales incluirían la definición de ontologías. El término ontología proviene de la filosofía, pero ha sido utilizado en Inteligencia Artificial. Una ontología, según la definición de Gruber [1], la describe como una especificación explícita y formal sobre una conceptualización compartida. De esta manera, las ontologías proveen la definición de conceptos y relaciones de algún dominio, que proporcionan un vocabulario concensuado para definir redes semánticas de información interrelacionadas. Durante los últimos años, se han desarrollado diversos lenguajes y estándares para la definición de ontologías, entre ellos los ya mencionados, y más recientemente OWL, Web Ontology Language [2]. Por otro lado, los componentes se han convertido en la forma más eficiente de integrar aplicaciones interempresarias, reemplazando a los objetos como la forma de construir aplicaciones distribuidas [5]. Existen varios modelos de componentes, cada uno de los cuales ofrece beneficios, y a su vez, presentan algunos problemas. Los más conocidos en el mercado son la familia de COM/COM+ y (D)COM de Microsoft [4]; CCM, CORBA Component Model del Object Management Group [6] [7], y EJB, Enterprise Java Beans de Sun Microsystems. Paralelamente, el desarrollo de software orientado a agentes extiende el desarrollo de componentes tradicional. De la misma manera que los modelos, los diagramas de colaboración y de interacción, los patrones, y la programación orientada a aspectos ayudan a construir componentes y sistemas basados en componentes; las mismas técnicas pueden ser utilizadas para los agentes y los sistemas basados en agentes [3]. Los sistemas multi agentes, aquellos formados por un grupo de agentes que colaboran para lograr un objetivo, son críticos para grandes interacciones de comercio electrónico, especialmente del tipo B2B, tales como provisionamiento de servicios, negociación, cadena de suministros, y entrega. De este modo, la tecnología basada en componentes que encapsule agentes autónomos, que pueden comunicarse entre sí, adaptables e integrables, prometen el desarrollo más flexible, de sistemas evolutivos distribuidos, y complejos. Un agente, o un conjunto de agentes compatibles, se construirán combinando aspectos y componentes que representen las capacidades claves. A fin de lograr esto, se requiere investigar como integrar las distintas tecnologías tales como HTTP, XML, agentes, servicios electrónicos, workflows, infraestructuras de componentes, y otros, para producir la nueva generación de tecnologías de componentes de software flexibles, apropiadas para la rápida construcción de las nuevas aplicaciones. Las nuevas ideas de la Web semántica pueden ser de suma utilidad en distintas aplicaciones tales como, comercio electrónico, gestión de conocimiento corporativo, librerías digitales, enseñanza a distancia, procesamiento de lenguaje natural, turismo, y otras. Dentro de este nuevo enfoque, existen distintas líneas de estudio, entre las que podemos citar: lenguajes de definición de ontologías, metodologías de desarrollo de ontologías, integración de ontologías, cómo usar reglas al traducir ontologías, cómo representar el conocimiento ontológico que no está capturado en las descripciones lógicas, agentes inteligentes, servicios web, y otros. Por otra parte, los sistemas multi-agentes pueden revelar interesantes clases de “comportamiento emergente”, a partir de agentes autónomos que trabajen juntos, de una manera que no fue explícitamente programada, produciendo potencialmente resultados no esperados, o no previstos. Los agentes pueden descubir otros agentes, y formar grupos dinámicos a fin de proveer nuevo comportamiento. En suma, la tecnología de agentes, tiene el potencial de ser más flexible que la programación tradicional, con la que se obtienen componentes estáticos. De esta manera, surge una nueva generación de componentes de sofwtare basados en agentes inteligentes. Integrando estos conceptos, nuestro plan de investigación se basa en el estudio de técnicas para la definición de componentes basados en agentes inteligentes, la definición, y la especificación de su semántica, y su utilización en aplicaciones para la Web semántica. Estudiaremos como ensamblar la tecnología de componentes, a fin de soportar componentes que encapsulen agentes, y cómo especificar la semántica de estos componentes, a fin de que puedan ser utilizados por las herramientas de la Web semántica.	﻿Web Semántica , Utilización , Nueva Generación de Componentes	es	21497
60	Un modelo de objetos para bills of materials complejos	﻿que les exige  incrementar su competitividad constantemente. Una de las estrategias empleadas con este fin es el  aumentar lo máximo posible los niveles de integración entre todas las actividades que ellas  desarrollan, tanto de gestión administrativa como de producción de bienes físicos, lo cual en gran  medida implica compartir información y automatizar operaciones. Internet es el más reciente  capítulo en el camino hacia la automatización que comenzó por los años 60 con el CAD (Computer  Aided Design), el MRP (Material Requirement Planning)  y los primeros sistemas de inventario.  Durante estas primeras etapas, la automatización afectaba sólo a áreas individuales de la empresa,  no se incluían a las relaciones entre las unidades de negocios dentro de la misma compañía ni las  relaciones hacia fuera, con clientes y proveedores. Posteriormente, el MRP evoluciona al MRP II  que apunta a la planificación de todos los recursos de manufactura dentro de la empresa. El  siguiente paso, el ERP (Enterprise Resource Planning), se centra en la integración de diferentes  áreas de la organización, permitiendo que éstas tengan una visión integral de los distintos procesos  intra-empresa. Recién, con el surgimiento de Internet, se hace posible a integración entre empresas.   Internet al mismo tiempo que brinda a las empresas la capacidad de compartir información, las ha  expuesto más que nunca a un contexto de creciente competitividad debido a su exposición a  mercados globales.   Las empresas tuvieron que cambiar sus formas de organizarse y hacer negocios para poder  adaptarse a los nuevos requerimiento: productos personalizados, con ciclos de vida más cortos, de  menor costo y mayor calidad.  Estos nuevos requerimientos, han ocasionado una explosión en la  variedad de los productos, incorporando una exigencia nueva a los sistemas de automatización  existentes.  Hoy en día, una empresa industrial debe ser lo suficientemente ágil para responder  a los frecuentes  cambios que le presenta el mercado respecto a la demanda de sus productos. Para lograr esto es  importante que cada organización comparta un modelo de producto común que abarque todas las  etapas del ciclo de vida del mismo.  En el presente trabajo, solo abordaremos el modelo de producto en lo que respecta al Bill of  Materials (BOM). Se presentará una definición de BOM, los requerimientos de un BOM para dar  soporte al nuevo contexto de producción y una primera aproximación de un modelo que de  respuesta  a estos requerimientos.    Definiciones y desafíos del Bill of Materials.  Una primera definición de BOM, sostiene que el Bill Of Materials es una lista estructurada de las  partes que se utilizan para obtener un producto (Levy 1986). Gráficamente se representa con un  grafo en el cual los nodos son las diferentes partes (productos terminados, semiensamblados,  componentes y materias primas) y los arcos son las relaciones estructurales entre estas partes.  Reconocido desde hace tiempo como un bloque esencial en el éxito de los sistemas ERP, los  modelos de BOM administran actualmente mucha más información que la de una simple estructura  de producto (Bourke 2000). La nueva situación del mercado, hizo surgir nuevos enfoques de  BOMs, como los propuestos por Scheer (1998), Van Veen y Wortmann (1999), Olsen y colab.   (1997), Chung y Fisher (1994), Hvan y colab. (2003) entre otros, que presentan modelos para  adaptar el BOM a las nuevas necesidades planteadas. Estos enfoques apuntan principalmente a  resolver los problemas que ocasiona el manejo de múltiples variantes, es decir pequeñas  modificaciones sobre una estructura de producto básica común. Pero hay muchos otros aspectos que  deben ser tenidos en cuenta en una representación BOM para lograr una eficiente integración inter e  intra-empresa:   •  Representación de estructuras complejas de composición, descomposición e híbridas.  • Adecuado manejo de subproductos, coproductos y material scrap  • Manejo de los datos con mínima redundancia y rapidez en el acceso a la información.  • Integración de los diferentes tipos de BOMs existentes en la empresa.  • Representación de  restricciones entre componentes  • Gestión de versiones de productos  • Incorporación no solo de componentes sino también otros recursos necesarios para la  fabricación del producto.  Estructura del producto  Tradicionalmente, el BOM ha sido utilizado para representar de que forma un producto se  componía de otras partes, las cuales podían ser materias primas o ensamblados intermedios que eran  producidos en la organización (los cuales también tenían su representación BOM). Este modelo  simple no permite representar el caso de aquellas industrias, en las que además de  contar con  productos con estructuras de composición, fabrican productos donde alguna de las partes se deriva  de la descomposición de una (o más) materia prima no atómica. En este tipo de industrias  (frigorífica, petroquímica, etc.) es necesario representar también estructuras de descomposición  entre materias primas y sus derivados. Aquí un determinado producto, puede al mismo tiempo,  formar parte de una estructura de composición y de una estructura de descomposición.   Como ya se dijo, un producto puede tener 1 o más estructuras.  Dependiendo del producto de que se  trate, una estructura puede ser:   i) una composición: partes que se unen para fabricar un producto  ii) una  descomposición: derivados de una materia prima no atómica  iii) una estructura simple: componentes o materias primas atómicas, que no se componen a  partir de otras partes, ni se descompone  en componentes derivados.    Modelo propuesto  El objetivo final es el desarrollo de un framework que permita derivar estructuras BOMs que  cumplan con los requerimientos mencionados anteriormente. En este trabajo se describe un  proyecto en desarrollo, por lo cual algunos de los requerimientos ya están cubiertos en el modelo  actual, mientras que otros están siendo analizados para ir incorporándolos al modelo presentado de  manera de ampliar su dominio de aplicación.   La propuesta tiene un enfoque orientado a objetos (Boosch y colab. 1999) y se enmarca dentro de  los denominados BOMs generativos (Van Veen y Wortmann, 1992): aquellos en los que la  estructura de cada variante particular no se almacena sino que se genera, a partir de una estructura  base, en el momento en que se necesita  En la Fig. 1 se presenta en modelo inicial, en el cual la clase Producto representa la familia de  productos (conjunto de productos con estructuras similares). La estructura de la familia esta dada  por la clase Estructura, la cual se especializó en 3 subclases: Composición, Descomposición y  Simple. Los dos primeros casos, tienen una relación de agregación con la clase Producto ya que  ambos representan productos compuestos. Siguiendo esta relación es posible obtener los  componentes o derivados de un producto.  Una estructura de composición, Composición, se relaciona con los productos que son partes  componentes para la fabricación del mismo, mientras que una estructura de descomposición,  Descomposición, agrupa a los productos que se obtienen como derivados de dicho producto.   Las asociaciones: “Relación de composición”, “Relación de descomposición” y “Estructura de”  permiten representar la estructura base.  Mientras que las “reglas” para generar la estructura de una  determinada variante se describen a través de:  “Variante de”,  “Modificación” e “Inclusión”.  Otra problemática considerada es la representación de los coproductos y/o subproductos, es decir  productos que se obtienen durante el proceso de fabricación de otro producto, y que poseen un valor  comercial equiparable (Clement y colab. 1992). Esta situación se está modelando mediante la  relación “Deriva de “ qu e permite identificar en que Estructura de producción se obtiene una dada  Variante como co-producto.  Sim p le Re laci on   Com posi ci on Re laci on   Descom posici on Va rian te   S im p le Inclusi on M od i fi caci on Descom po sicion Cam b io Signo Va lo r Com posi ci on Producto Caracteristi cas N omb re V a lor Va ri a nt e  com puesta Estructu ra 1 ..n 1 Va rian te 1 ..n1 Varian te  de 1 ..n 1 ..n De riva  de Estu rctu ra  de   Figura 1: Modelo Propuesto  En los contextos productivos que se pretenden administrar se puede presentar el caso en que un  mismo producto pueda ser parte al mismo tiempo de una relación de composición y de una de  descomposición. Ya que puede ser un derivado de una materia prima no atómica, que se utilice en  la fabricación de otro producto. En la Figura 2 se presenta un ejemplo de la industria frigorífica en  el que el producto Carne Dicing FCB29 es derivado de una de las estructuras del producto Nalga  Adentro CT (Nalga Ad CT1), y al mismo tiempo forma parte de la composición del producto  Dicing CEE.  Variantes de producto  El modelo considera la existencia de productos similares, que tienen estructuras comunes. Este  conjunto, recibe el nombre de familia de productos y un elemento particular del mismo se  denomina variante.  Las diferencias que puede tener un integrante de una familia respecto de la estructura de la misma  pueden dividirse en : i) variaciones en el BOM Básico por agregado o sustracción de alguna parte  componente de la estructura base; ii) variaciones en los valores de los parámetros que caracterizan a  las partes componentes de la estructura base y iii) variaciones en el número de veces que una parte  está en la estructura de su padre.   Entre un producto (incluyendo su estructura) y sus variantes el modelo plantea 3 relaciones: i)  “ Variante de”; ii) “ Modificación” y iii) “ Deriva de” que se muestran en l a figura 1.  La relación “ Variante de” asocia un producto con sus variantes. La clase Variante se especializa en  las clases Variante simple y Variante compuesta que representan las variaciones de un producto  simple o de un producto compuesto respectivamente (aquel que tiene  una estructura de  composición o de descomposición).   La relación “modificación” está representada por la clase Modificación que agrupa un conjunto de  cambios que pueden hacerse a una estructura para una variante dada.  Estos cambios pueden ser:  eliminación de todo o parte de un componente de la estructura o agregado de algún nuevo  componente. El tipo de cambio y la cantidad de ese cambio están representados por los atributos  signo  y valor de la clase cambio.  En la Figura 2, se presenta una vista parcial del modelo aplicado a un pequeño caso que se presenta  en la industria frigorífica. En el se observan las estructuras de composición, descomposición, y  solamente 2 variantes de productos.  La relación   “ Deriva de” une la variante con una o más estructuras. Si se trata de una variante que  se obtiene mediante una descomposición, la relación se establecerá entre dicha variante y la  estructura de descomposición de la cual es hija (relación entre V1 Ctro.Nalga Suiza y Nalga Ad  ST en el ejemplo). En cambio,  si se trata de un producto que surge de una composición, la relación  se establece entre la variante en cuestión y  la estructura de la que es padre (relación entre Caja  Dicing CEE V1 y la estructura Caja Dicing CEE en el ejemplo).  En este ejemplo, el producto Centro Nalga Suiza puede obtenerse a partir de dos estructuras de  descomposición posibles: i) a través de Nalga Ad CT 1, una de las estructuras de descomposición  asociada al producto Nalga Adentro CT y ii) a través de Nalga Ad ST, estructura de  descomposición del producto Nalga Ad s/tapa.  Si se tiene una variante del producto Centro Nalga Suiza que solo es posible si el producto se  obtiene de la descomposición Nalga Ad ST, entonces se establece una relación “deriva de”  entre  esta instancia de descomposición y la instancia correspondiente a la variante  en cuestión (V1 Ctro.  Nalga Suiza en el ejemplo). Siguiendo esta relación, es posible obtener que productos son  coproductos o subproductos de esta variante.  CCT 1 Cuadril c/tapa Tapa Cuadril Cuadril s/tapa CCT 2 Cuadril CT exp Grasa Comes tible Nervios y pellejos CCT 3 Cuadril corte D-UK Conser va 1era TC 1 TC 2 TC 3 Tapa cuadril exp Grasa Comestible Nervio y pellejo Conser va 1era Tapa Cuadril EEUU Carne Slicing FCB28Cuarto Traser o Cuarto Tr 2 Cuarto Tr 1 BIfe Angost o Bola Lomo Colita Cuadril Grasa Comes -tible Cza cuadra da Garron Lomo Nalga Adento CT Peceto Tortug uita Conser va 1era Hueso s/carne Nervio y pellejo Nalda Ad CT 1 Nalga Ad CCT 2 Centro Nalga Suiza Carne Slicing FCB28 Carne Dicing FCB29 Nalga Ad s/ tapa Tapa Nalga Nalga Ad ST V 1 Ctro. Nalga Suiza Deriva de Caja Dicing CEE DICING CEE 1  DicingCEE Sal Gelatin a Caja Dicing CEE Tubo Cristal Caja Adhesi vo CCC Etiq. a c/cod. barra Caja Dicing CEE V1 Variant of Deriva de Descomposición Produc to Composición Estructura de Relacion Descomposición Variante de Relacion Composición Variante Referencias   Figura 2: Modelo de objetos para la industria frigorífica  Restricciones  Otro aspecto importante a considerar es que en las relaciones de composición y descomposición  planteadas anteriormente, no todas las combinaciones de partes son válidas,  ya sea  por razones  tecnológicas o comerciales. Surge, entonces, la necesidad de tener un mecanismo que permita  expresar qué restricciones existen entre las partes al momento de definir una estructura. Se pueden  establecer dos grandes grupos de restricciones entre partes: de obligatoriedad y de incompatibilidad.  Ambas deben definirse para poder lograr estructuras válidas, y según Olsen (1997), todas las  restricciones deberían especificarse en la estructura genérica de manera de simplificar la  especificación de una variante del producto.  Si bien esta característica no se describen en el modelo, se consideran que las restricciones que  pueden  existir se organizan en las siguientes categorías:  i) restricciones en la cantidad, mínima o  máxima cantidad en que una familia de producto puede ser  parte componente del padre de la  estructura en la que está  la restricción; ii) restricciones de incompatibilidad o de obligatoriedad  entre variantes de productos simples y iii) restricciones entre instancias de familias de producto.  Conclusiones y trabajos futuros  La definición de un modelo BOM  es una tarea demasiado compleja y requiere de un continuo  desarrollo y mejoramiento de los modelos existentes para ampliarlos, de manera de solucionar las  limitaciones de éstos frente  a las exigencias de las nuevas estrategias de producción.  El modelo de objetos propuesto, el cual deberá ser formalizado, contempla solo algunos de los  aspectos cruciales que deben ser tenidos en cuenta en una representación BOM, a saber: manejo de  variantes, de estructuras híbridas y de coproducto-subproducto intentando minimizar el volumen de  información almacenado. Se continuará trabajando para incorporar al modelo, en primera instancia  el manejo de restricciones y con posterioridad se analizarán y modelarán los aspectos relacionados  con:  • Diferentes tipos de BOM: En el nuevo contexto de producción, el modelo de producto  deberá soportar, la integración de toda la cadena de suministro. En esta cadena, a cada área  involucrada le interesa ver al producto desde una perspectiva particular. Por ejemplo:  mientras que a producción le interesa que de que manera ensamblar un producto, a  planificación le interesan los posibles proveedores de la materia prima, sus precios y  tiempos de entrega y al área de control de calidad le interesa que test aplicar a una materia  prima en función del producto en el que participa.  Se pretende desarrollar un modelo en el  que cada área de la organización, tenga acceso a la información del BOM, desde una  perspectiva diferenciada dentro de un modelo global.  • Unificación BOM-Routing: El routing detalla el método de manufactura de un ítem  particular. Se especifica para cada nodo del BOM convencional e incluye las operaciones a  llevar a cabo, su secuencia, los centros de trabajo involucrados, los tiempos de setup y  producción. Tradicionalmente existe una separación en la estructura de datos del taller, un  aspecto contiene  la estructura del producto (BOM) y otro la ruta a seguir por cada  componente del producto en su proceso de fabricación (routing).   • Manejo de versiones: con el tiempo las diferentes variantes de un producto (o el producto  en si mismo) irá cambiando de acuerdo a lo que dicta la demanda de los clientes. Esta  evolución histórica es lo que se conoce como “versión” de un producto. Este concepto  deberá ser incorporado al modelo.	﻿modelo de objetos , bills of materials complejos	es	21501
61	Técnicas espaciales frecuenciales y morfológicas para restauración de huellas dactilares deterioradas	﻿Técnicas espaciales, frecuenciales y morfológicas para restauración de huellas dactilares deterioradas Moler, E.1 , Ballarin, V. 1, Blotta, E. 1, Meschino, G. 1, Pastore, J. 1 , Incháurregui, A. 2 1 Laboratorio de Procesos y Medición de Señales, Facultad de Ingeniería, Universidad Nacional de Mar del Plata,  J.B.Justo 4302, B7608FDQ Mar del Plata, Tel: 02234816600, int. 255/Fax: 0223-4810046. egmoler@fi.mdp.edu.ar 2 Dirección de Registro de Personas Desaparecidas de la  Pcia. Bs. As. Calle 55 Nº 930. Piso 1. CP 1900 Introducción Este proyecto se enmarca en un trabajo interdisciplinario entre especialistas de Procesamiento Digital de Imágenes (PDI) y Antropología Forense. El objeto de estudio son huellas dactilares deterioradas, almacenadas en microfilms y en papel, que contienen información útil para realizar una identificación positiva de personas desaparecidas. Este proyecto se realiza en forma conjunta con integrantes del Equipo Argentino de Antropología Forense (EAAF) y la Dirección de Registro de Personas Desaparecidas de la Provincia de Bs.As., integrado por antropólogos, arqueólogos, médicos y peritos forenses, cuyos objetivos fundamentales son entregar los restos a los familiares de la persona desaparecida, principalmente de la última dictadura militar y aportar pruebas a las causas judiciales correspondientes (Cohen Salama M., 1994; Reichs Kathleen J., 1997). En una investigación preliminar estas organizaciones recolectan documentos provenientes de distintos ámbitos: administrativos, gubernamentales, judiciales, periodísticos y de distintas organizaciones defensoras de los derechos humanos para ser presentados a la justicia como evidencia. Es de interés para este proyecto restaurar huellas dactilares que se encontraron en microfilms (250 dactilogramas) y en  papel (20 dactilogramas), que fueron extraídas en el período del régimen militar 1976-1983, figurando exclusivamente en la ficha dactilar: NN masculino o NN femenino. Como el objetivo de que estas huellas no fue el de una identificación real de las personas sino simplemente cumplimentar un trámite administrativo, no se tomaron los mínimos recaudos para que en la ficha se observaran las características esenciales de una huella digital para su posterior identificación. Por lo tanto el problema de visualización se presenta en que dichas huellas son ininteligibles al ojo humano por diversas alteraciones: borrosas y poco visibles (tomadas casi sin tinta), superpuestas, desplazadas, con manchas que no corresponden a huellas, incompletas y con exceso de tinta. Además, en el proceso de microfilmación, las fichas dactilares se redujeron 41 veces. Al querer recuperarlas por un simple lector de microfilm se obtiene una imagen de pésima calidad como para efectuar cotejos dactiloscópicos e identificar cadáveres. Existen gran cantidad de investigaciones sobre nuevos algoritmos en la disciplina de PDI que permiten realzar zonas poco visibles de una imagen. Cada una presenta ventajas y desventajas. Debido a las complejidad del deterioro de las imágenes tanto microfilmadas como en papel, es necesario, además de aplicar las técnicas conocidas, explorar otras técnicas de realce, adoptando distintos modelos matemáticos para desarrollar nuevos algoritmos para estas aplicaciones específicas. Se deben desarrollar nuevas técnicas en el dominio espacial, en el dominio frecuencial, como así también ahondar nuevos modelos y/o posibles combinaciones de los existentes para satisfacer las demandas de esta problemática. Materiales y Métodos Los dactilogramas en papel se digitalizan a través de un scanner (HP SCAN Jet II cx, 1200 dpi). Las huellas de los microfilms se adquieren a través de un microscopio LEICA DMLB conectado a una cámara de video. El software utilizado, hasta el momento, incluye software comercial (Image Pro Plus v4.1 de Media Cybernetics (Image Pro-Plus, 1999) y algoritmo DHP de Image Content Tecnology LLC) y software desarrollado por integrantes del Laboratorio (PDI v2.0, desarrollado en lenguaje Delphi). También se utiliza Matlab 5.3 en la etapa de prueba de los algoritmos. Hasta el momento se aplicaron las siguientes técnicas: − Técnicas de realce en el dominio espacial Para realzar las imágenes se utilizan algoritmos de contrast-stretching, ecualización (lineal, logarítmica, exponencial, entre otras). También se aplican filtros lineales y no lineales como: pasabajos, pasaltos, mediana, máximo, high-boost, k-próximo y sigma (Gonzalez, R. & Woods, R., 1992; Haralick, R. and Shapiro, L., 1992; Jain, A., 1989). Una estandarización en el pre-procesamiento se logra cuando se utiliza la técnica de Histéresis Diferencial (Klaus-Ruediger, P., 1996) que permite extraer y resaltar determinados rangos de niveles de contraste. El proceso de Histéresis se complementa al restar la imagen de salida de la imagen de entrada, la imagen resultado conserva las pequeñas variaciones y reduce las grandes. − Técnicas de realce en el dominio frecuencial Múltiples características de una huella que no pueden ser descriptas en el dominio espacial surgen con claridad en el dominio frecuencial. Las técnicas desarrolladas en este dominio son útiles, especialmente, para la supresión de ruido y filtrados de realce. Se aplica la Transformada de Fourier bidimensional y a partir de su espectro se aplican distintos filtros. En especial se trabaja con los filtros Spike-boost y Spike–cut, que permiten realzar y suprimir, respectivamente, bandas frecuenciales de interés (Moler, E et al., 1998). Nuevas líneas de interés a trabajar: − Técnicas basadas en Morfología Matemática La Morfología Matemática es una teoría basada en conceptos de Geometría, Álgebra, Topología y Teoría de Conjuntos, creada para caracterizar propiedades físicas y estructurales de diversos materiales (Serra, J., 1982). En la actualidad la Morfología Matemática ha alcanzado el status de una poderosa herramienta para el Procesamiento Digital de Imágenes. Utilizando conceptos morfológicos se pueden desarrollar filtros que permiten realizar realces de interés para determinadas zonas de una huella . También se permite la caracterización de una forma a través su esqueleto, que es representativo de la forma en sí, realizando una mejor caracterización de la huella para su identificación (Ge, Y. and Fitzpatrick, J. , 1996). − Técnicas basadas en el modelo AM – FM de reacción y difusión Este enfoque se basa en ecuaciones diferenciales parciales (PDEs) y la modelización de imágenes AM-FM. La reconstrucción de la textura de la imagen ocurre vía reacción y difusión generadas por las PDEs (Scott T. et al., 2001).  En el proceso de difusión, la imagen se suaviza en forma adaptiva, preservando las características y los bordes importantes. La reacción reproduce la información de las texturas del borde de la región reconstruyéndolas en las regiones ocluidas de la imagen. Se usan y diseñan filtros de Gabor en el proceso de reacción usando un análisis de componentes principales AM-FM. A diferencia de técnicas de restauración anteriormente descriptas, que dependen de la interpolación o la continuidad de componentes conectadas dentro del conjunto de niveles de la imagen o la estimación de texturas, los procesos de reacción – difusión propuestos entregan una transición sin uniones notables entre la región recreada y las regiones de la imagen restaurada. Al aplicar esta técnica se desea reparar las partes perdidas u ocultas de las huellas digitales Resultados Hasta el momento se procesaron 100 huellas dactilares aplicando las técnicas descriptas para su restauración y  posterior identificación por un perito. La Fig. 1 muestra una sección de una imagen original y el resultado obtenido de aplicar una combinación de filtros espaciales no lineales (High-Boost y K-próximos vecinos). La Fig. 2 muestra una sección de una imagen original y el resultado obtenido de aplicar una combinación de filtros pasabajos y la técnica de Histéresis Diferencial La Fig. 3 muestra una sección de una imagen original y el resultado obtenido de aplicar una combinación de filtros espaciales y frecuenciales. El método de PDI fue convalidado por Peritos dactiloscópicos y el Juez que atendía la causa. Luego de las presentaciones correspondientes a la Justicia, se realizaron dos identificaciones positivas  a través de los dactilogramas procesados. Se deben continuar aplicando las técnicas conocidas y explorando nuevas, para realizar una contribución en un método científico-tecnológico en el área forense a través del PDI para obtener resultados que involucran aspectos estrictamente humanos con un alto impacto en lo afectivo. (a) (b) Fig.1 (a) Imagen original; (b) Imagen procesada utilizando de filtros espaciales no lineales (a) (b) Fig.2 (a) Imagen original; (b) Imagen procesada utilizando de filtros pasabajos e Histéresis Diferencial (a) (b) (c) Fig.2 (a) Imagen original; (b) Imagen procesada utilizando de filtros espaciales;  (c)  Imagen procesada utilizando filtros espaciales y frecuenciales.	﻿Técnicas frecuenciales , Técnicas espaciales , Técnicas morfológicas , restauración de huellas dactilares deterioradas	es	21526
62	Estimación y compensación de movimientos en video codificado	"﻿ Los sistemas distribuidos multimedia requieren transferencia de datos continua sobre periodos de tiempo relativamente altos, sincronización en el manejo de los diferentes tipos de datos (ejemplo: voz, sonido y video), espacios de almacenamiento extremadamente grandes, manejo de tiempo real y técnicas especiales de indexación y recuperación de los datos de tipo multimedia, además de otros problemas que surgen a partir de éstos. En los últimos años han ido apareciendo nuevas aplicaciones en el mundo de las telecomunicaciones, que requieren una continua mejora de los equipos terminales y de los canales de comunicación. El desarrollo y la explotación de nuevos sistemas de comunicación como la videoconferencia o la videotelefonía, deben tratar de compaginarse con la necesidad de aprovechar canales de transmisión de baja capacidad, que en un principio fueron diseñados para transmitir voz o texto. La necesidad anterior ha impulsado el diseño de técnicas de codificación que permitan trabajar con unas tasas de transmisión restringidas, manteniendo una calidad aceptable[1], para ello se hace hincapié en el estudio de técnicas de segmentación como también en la implementación de métodos de estimación y compensación de movimiento. Esa creciente necesidad de incrementar la interconexión de las cada vez más poderosas estaciones de trabajo multimedia da como resultado una evolución de las comunicaciones en búsqueda de las redes (sus características) que soporten la transmisión de este tipo de información multimedia. Palabras Claves: Compensación de movimiento, estimación de movimiento, segmentación, video, MPEG, Tiempo Real.                                                  1 Magister en Informática - Profesor Adjunto Dedicación Exclusiva, Facultad de Informática, U.N.L.P. E-mail: crusso@lidi.info.unlp.edu.ar 2 Magister en Informática - Profesor Adjunto Dedicación Exclusiva, Facultad de Informática, U.N.L.P. E-mail: hramon@lidi.info.unlp.edu.ar 3 Calle 50 y 115 Primer Piso, La Plata (1900), Argentina, Pcia. de Bs. As. Teléfono 54-21-227707 lidi@ada.info.unlp.edu.ar Introducción Las técnicas de compresión de datos juegan un papel fundamental en el desarrollo de las aplicaciones multimedia debido a los grandes requerimientos de espacio que exige el audio, las imágenes, la animación y el video como a los limitados anchos de banda de las redes que no permiten la transmisión de altos volúmenes de datos multimedia (como el video) en tiempo real. Los métodos de compresión, aprovecha la redundancia espacial de una imagen (áreas uniformes), la correlación entre puntos cercanos y la menor sensibilidad del ojo a los detalles finos de las imágenes fijas (JPEG) y, para imágenes animadas (MPEG), se saca provecho también de la redundancia temporal entre imágenes sucesivas[1] [2] [3]. Se pueden obtener grandes factores de compresión teniendo en cuenta la redundancia entre imágenes sucesivas. Esto involucra al eje del tiempo. Este proceso se denomina codificación inter o codificación temporal. La codificación temporal permite altos factores de compresión, pero con la desventaja de que una imagen individual existe en términos de la diferencia entre imágenes previas. Si una imagen previa es quitada en la edición, entonces los datos de diferencia pueden ser insuficientes para recrear la siguiente imagen. El estándar ISO MPEG (Motion Pictures Experts Group) utiliza esta técnica. Compresión de Video en el estándar MPEG [4] En el año de 1990, la ISO, preocupada por la necesidad de almacenar y reproducir imágenes de video digitales y su sonido estereofónico correspondiente, creó un grupo de expertos que llamó MPEG (Moving Pictures Expert Group) procedentes de aquellas áreas implicadas en el problema (telecomunicaciones, informática, electrónica, radio difusión, etc). El primer trabajo de este grupo se conoció como la norma ISO/IEC 11172, mucho más conocida como MPEG-1, en el año 1992. La idea inicial era la de permitir el almacenamiento y reproducción en soporte CD-ROM con un flujo de transmisión de datos del orden de 1,5 Mbits/s, transportando tanto imagen como sonido. El estándar MPEG además de aprovechar la redundancia espacial intrínseca de una imagen fija utilizada en la codificación JPEG, aprovecha la redundancia temporal que aparece en la codificación de imágenes animadas, permitiendo encontrar similitudes entre las imágenes sucesivas de video. [5] Debido a que la calidad en la compresión de video en el estándar MPEG-1 era de baja calidad y no servía para otras aplicaciones, se creó la norma ISO/IEC 13818, mucho más conocida con el nombre de MPEG-2. Esta norma permite un flujo de transmisión hasta el orden de los 20 Mbits/s, transportando tanto imagen como sonido. Norma que se utilizaría en la televisión de alta definición. En la actualidad, se está trabajando en una norma llamada MPEG-4 y está encaminada a la transmisión de datos del orden de los 8 a 32 Kbits/s, norma que será utilizada en las aplicaciones de video conferencia o video teléfono. La compresión de video utiliza los mismos principios que JPEG con pérdidas, a la que se le añaden nuevas técnicas que, juntas, forman el MPEG-1, que permiten reducir considerablemente la cantidad de información necesaria para la transmisión de imágenes sucesivas muy correlacionadas temporalmente. [7] Estas técnicas, llamadas de ""predicción con compensación de movimiento"", consisten en reducir, con un mínimo de información adicional, la mayoría de las imágenes precedentes (incluso las que le siguen). Esto requiere un dispositivo de estimación de movimiento en el decodificador, que es la parte más compleja. Tratándose de imágenes en movimiento o animadas, la descompresión deberá poder hacerse en ""tiempo real"" durante la reproducción. Por otro lado, la necesidad de un tiempo de sincronización y de una respuesta de acceso aleatorio a una secuencia no demasiado largos (0.5 segundos máximo) limita el número de imágenes que pueden depender de la misma primera imagen a diez o doce para un sistema de 25 imágenes por segundo. [6] Es de nuestro interés poner énfasis en el estudio de la optimización de algunos de los procesos involucrados en la compresión de video como la segmentación que será la base para aplicar los métodos de compensación y estimación de movimiento. Compensación y Estimación de Movimiento La compensación de movimientos [7] es un proceso mediante el cual se mide eficazmente el movimiento de los objetos de una imagen a otra. De este modo se consigue medir qué tipos de movimientos redundan entre imágenes. Las técnicas, llamadas de ""predicción con compensación de movimiento"", consisten en reducir, con un mínimo de información adicional, la mayoría de las imágenes precedentes (incluso las que le siguen). El objetivo de la compensación de movimiento es la reconstrucción del cuadro en proceso, empleando el cuadro de referencia y la información de movimiento. Esto requiere un dispositivo de estimación de movimiento en el decodificador, que es la parte más compleja. Tratándose de imágenes en movimiento o animadas, la descompresión deberá poder hacerse en ""tiempo real"" durante la reproducción. Por otro lado, la necesidad de un tiempo de sincronización y de una respuesta de acceso aleatorio a una secuencia no demasiado largos (0.5 segundos máximo) limita el número de imágenes que pueden depender de la misma primera imagen a diez o doce para un sistema de 25 imágenes por segundo. La estimación de movimiento [8][9]es uno de los problemas fundamentales en el tratamiento de vídeo digital. Su objetivo es calcular el campo de vectores que describe el movimiento aparente entre dos cuadros de una secuencia. Es importante hablar de movimiento aparente, porque no hay que olvidar que las señales de imagen en movimiento son la  proyección en el plano, en instantes discretos de tiempo, de escenas tridimensionales. Esto supone una pérdida de información que hace necesario distinguir entre el movimiento real que se proyecta sobre el plano y el movimiento aparente que, junto con su velocidad, es lo que el observador percibe mediante la variación del contenido visual de las imágenes de la secuencia. Una consecuencia de lo anterior es que, en situaciones en las que hay cambios en la iluminación o existe una ausencia del suficiente gradiente espacial en la imagen, el movimiento real y el flujo óptico (velocidad aparente) no tienen por qué coincidir. Temas de investigación Nuestro estudio se centra  en: • las técnicas de estimación y compensación • el análisis de la segmentación de las imágenes para lograr un mayor nivel de estimación y compensación, para esto se estudia la segmentación por bloques fijos, por bloques variables. • la segmentación basada en regiones tomando los objetos dentro de la imagen. • estudio de la complejidad de los algoritmos de búsqueda y matching. • eficiencia y tiempo de respuesta sobre canales reales."	﻿Tiempo Real , Compensación de movimiento , estimación de movimiento , segmentación , video , MPEG	es	21529
63	Programación en lógica rebatible una semántica declarativa	﻿Programación en lógica rebatible: una semántica declarativa Laura A. Cecchi Departamento de Informática y Estadística UNIVERSIDAD NACIONAL DEL COMAHUE  e-mail:   lcecchi@uncoma.edu.ar PALABRAS CLAVE: Extensiones de la Programación en Lógica. Semántica Declarativa de Extensiones de la Programación en Lógica. Semántica de Juegos. Sistemas Argumentativos. Introducción La Programación en Lógica Rebatible [GS99, GSC98, Gar00] (de ahora en más P.L.R.) es una extensión de la Programación en Lógica (P.L.) con una nueva clase de reglas, las reglas rebatibles. Estas reglas permiten representar conocimiento tentativo, aumentando, de este modo, el poder expresivo de la P.L.. El razonamiento no monotónico basado en el análisis dialéctico  constituye la semántica operacional de la P.L.R..  Para verificar si una consulta es consecuencia de un programa lógico rebatible, este formalismo utiliza un análisis dialéctico de argumentos y contraargumentos. Así, una consulta q tendrá éxito si existe un argumento	﻿Sistemas Argumentativos , Extensiones de la Programación en Lógica , Semántica Declarativa de Extensiones de la Programación en Lógica , Semántica de Juegos	es	21643
64	Un modelo de planificación para un agente social	﻿ que captura el proceso de planificación de un agente social. La construcción de un plan de un agente social es diferente a la de un agente aislado porque no considera exclusivamente  sus metas, su conocimiento y sus habilidades sino también las metas, el conocimiento y las habilidades de los miembros de la comunidad en la que participa. Cada agente cumple además un rol social que determina el modo en que actúa y se relaciona con los demás.  El modelo intentará reflejar el impacto de los roles y relaciones entre agentes en el proceso de planificación. Creencias, deseos e intenciones Desde un punto de vista concreto un agente es una entidad computacional capaz de percibir, reaccionar y actuar sobre un entorno. Estas capacidades perceptivas, reactivas y efectoras requieren de cierto nivel de razonamiento práctico que permita seleccionar la acción más adecuada respecto a la meta y al estado del entorno en el que se haya situado. Es decir, la capacidad cognitiva del agente es la que le permitirá actuar adecuadamente en cada contexto particular. El razonamiento práctico de un agente involucra dos procesos fundamentales: decidir qué metas van a perseguirse y elegir un plan que establezca cómo  alcanzarlas. El proceso de decisión requiere considerar un espectro de opciones alternativas, seleccionar algunas de ellas y comprometerse a cumplirlas. La opciones elegidas conforman las intenciones del agente, influyen en sus acciones, restringen el razonamiento práctico futuro y persisten de alguna manera en el tiempo. El proceso de planificación consiste en elegir un conjunto de acciones que  le permitan satisfacer sus intenciones. Un agente modifica sus intenciones cuando decide que no va a poder alcanzarlas o desaparecen las razones que provocaron que fueran elegidas entre otras alternativas. En este sentido las intenciones de un agente están ligadas a sus creencias, a su conocimiento acerca de lo que cree que está dentro de sus posibilidades. Un aspecto clave en la definición de un modelo de agentes es mantener un equilibrio que le permita a cada entidad comprometerse con sus intenciones pero al mismo tiempo reconsiderar su factibilidad cuando sea necesario. El balance entre compromiso y reconsideración va a depender de las características del entorno. En un entorno dinámico el agente debe mantener una actitud más reactiva que en otro menos cambiante. Los modelos de creencias, deseos e intenciones (BDI) basados en el razonamiento práctico, brindan los elementos esenciales  para representar la actitud mental de un agente racional que actúa en un ambiente dinámico, sujeto a cambios bruscos y frecuentes. Las creencias conforman el conocimiento del agente acerca del entorno que lo rodea. Sus deseos e intenciones se refieren al estado que desea alcanzar y representan sus motivaciones y compromisos. Los modelos BDI convencionales no consideran los aspectos que caracterizan a un agente situado en un entorno social. El objetivo de nuestro trabajo es extender el modelo BDI de modo que el estado mental de cada cada agente individual refleje también su naturaleza social. La revisión de las intenciones de un agente social estará fuertemente ligada a los deseos e intenciones de los demás agentes. Cuando hay conflicto cada agente puede tener que reconsiderar sus intenciones y reconciliar las metas con los demás miembros del grupo. En este caso la comunicación debe soportar alguna forma de negociación. La participación de cada agente en la negociación estará influenciada por su actitud frente al conflicto y el rol que cumple en el grupo. El modelo debe capturar la influencia de los roles y las relaciones entre los diferentes miembros de una comunidad de agentes, sobre la actitud mental de cada uno de ellos. Las creencias de un agente social se construyen a partir de su percepción del mundo que lo rodea pero también incluyen su conocimiento acerca del conocimiento de otros agentes del grupo.  Existirán algunos elementos cognitivos compartidos por todos los integrantes del sistema. Cada individuo conocerá estos hechos pero además razonará sabiendo que los demás también los conocen y saben que son compartidos. El conocimiento de cada agente es incompleto y con frecuencia ningún miembro de la comunidad será capaz de deducir un hecho específico a partir de él, pero si podría hacerlo si reune todo el conocimiento distribuido en el grupo. Nuestra propuesta parte de un modelo BDI convencional incluyendo creencias, deseos e intenciones y un mecanismo que representa el proceso deliberativo y la capacidad de razonamiento de medios y fines. El modelo se extiende para incluir los roles y relaciones de los miembros del grupo y cierta capacidad social que le permita a cada agente  considerar su conocimiento acerca de los otros miembros del grupo. Las creencias de un agente van a incluir entonces su conocimiento acerca del conocimiento de los demás integrantes del entorno social. El proceso de planificación de un agente social La idea básica subyacente en nuestro modelo es la de mundos posibles. El conocimiento de un agente acerca de su entorno es incompleto y sus creencias  le permiten considerar diferentes mundos posibles. Cualquiera de las alternativas es una representación del mundo válida para el agente, de acuerdo a sus creencias. Un agente conoce un hecho si ese hecho es válido en todos los mundos que considera posibles. Cuando un individuo incorpora conocimiento algunos mundos dejan de ser posibles para él. Así, cuanto mayor será el conocimiento del agente, menor sea la cantidad de mundos posibles. Cada agente elabora planes acuerdo a su conocimiento y sus acciones modifican el entorno. La percepción de los cambios en el entorno modifica a su vez sus creencias, generándose una interacción dinámica y continua. En el caso de un agente social el proceso de planificación estará fuertemente influenciado por lo que sabe acerca del conocimiento de los demás. Las nociones de conocimiento compartido y distribuido son cruciales para alcanzar un acuerdo en un proceso de planificación. Usaremos una lógica modal como lenguaje de representación de conocimientos. El lenguaje brindará operadores que permitirán expresar el conocimiento compartido y distribuido que intervendrá en el proceso de planificación. La idea es reflejar cómo la percepción de las creencias de otros agentes constituye en sí misma un conocimiento útil para el proceso de planificación. La semántica del modelo permitirá darle una interpretación a cada una de las fórmulas del lenguaje. Por último, nuestra propuesta claramente no es la única alternativa posible, existen diferentes formas de representar conocimiento y la riqueza de cada una de ellas estará fuertemente ligada al dominio de aplicación. La intención de este trabajo es elaborar un modelo adecuado para soportar la planificación de un agente racional que participa en un entorno social.	﻿agente social , modelo de planificación	es	21646
65	Todos los Portales Educativos son iguales	﻿¿Todos los Portales Educativos son iguales? Análisis de portales para Educación Superior Vice-decano Lic. Javier Díaz LINTI - Facultad de Informática - Universidad Nacional de La Plata jdiaz@unlp.edu.ar Lic.  Maria Alejandra Osorio LINTI - Ce.S.P.I - Universidad Nacional de La Plata aosorio@isis.unlp.edu.ar AC. Ana Paola Amadeo LINTI - Facultad de Informática - Universidad Nacional de La Plata pamadeo@info.unlp.edu.ar El gran auge en el uso de las computadoras y redes tecnológicas brinda acceso a numerosas fuentes de información potenciando un cambio significativo en las relaciones organizacionales, operaciones financieras, relaciones entre usuarios, nuevos patrones de participación entre estudiantes, etc. La utilización de estas tecnologías  darán como resultado la eliminación de las limitaciones de Tiempo y Espacio, brindándole al usuario la posibilidad de acceder virtualmente a un número ilimitado de recursos, en el momento que así lo requiera. Actualmente, en la Internet, existen gran variedad de portales educativos, que brindan información útil y diferentes servicios a estudiantes  de distintos niveles: primarios, secundarios, terciarios y/o universitarios; así como también a educadores e instituciones educativas. Hemos realizado una recopilación y comparación de las distintas herramientas y servicios que brindan  una selección de portales,  de habla hispana, tales como: http://www.universia.net http://www.educ.ar http://www.universired.com.ar http://www.estudiantesnet.com.ar http://www.netuniversitaria.com    o   http://www.reduniversitaria.com http://www.educaguia.com http://www.canaleducación.com http://www.universidades.org http://www.puertouniversitario.com  Aspectos Evaluados:  Información general del portal.  Seguridad.  Información Curricular.  Incorporación de material al sitio.  Servicios adicionales.  Comunicaciones sincrónicas.  Comunicaciones asincrónicas.  Actividad socializadora  Encuestas.  Formación en línea.  Tipo de búsquedas.  Información general del portal:	﻿Portales Educativos , Educación Superior	es	21716
66	Una especificación precisa para patrones GoF	﻿Una Especificación Precisa para Patrones GoF Alejandra Cechich Departamento de Informática y Estadística Universidad Nacional del Comahue Buenos Aires 1400, (8300) Neuquén,  Argentina E-mail: acechich@uncoma.edu.ar Richard Moore International Institute for Software Technology United Nations University P.O. Box 3058, Macau E-mail: rm@iist.unu.edu PALABRAS CLAVES: Patterns ♦ Métodos Formales ♦  Orientación a Objetos ♦ 1. Introducción Una especificación formal que permita definir en forma precisa la semántica de patrones de diseño, en particular patrones de Gamma o GoF [1], puede ser la base para definir una herramienta que ayude a desarrollar software más seguro. En ciertos dominios, certificar el uso de métodos y técnicas es indispensable para asegurar la calidad del software producido. Sin embargo, una notación más formal para patrones – que permita especificar en forma más segura, consistente y completa – es todavía un desafío. Nuestro primer trabajo en esa dirección y su extensión  para incluir características semánticas implícitas en la estructura de un patrón [2][3], basados ambos en RSL (RAISE Specification Language) [4], fue originalmente comunicado en [5]. Esa formalización incluía sólo algunos patrones del catálogo de Gamma, aunque para verificar eventualmente que un diseño utiliza algún patrón es necesario incluir mayor cantidad (y diversos tipos) de patrones. Por otra parte, variaciones de los patrones también deberían considerarse. En este resumen, presentamos una semántica precisa para patrones con la que finalmente ha sido modelado todo el catálogo de Gamma. Esa semántica ha servido también para iniciar la formalización de variaciones de esos patrones y  de otros aplicables al análisis de dominios en lugar de al diseño de software. Futuras extensiones son abordadas al final. 2. Un modelo para la definición precisa de patrones. Para definir en forma precisa la semántica que sustenta a los patrones de diseño orientados a objeto, se han definido tres tipos de acciones: invocación, instanciación e invocación a sí mismo o a una clase superior en la jerarquía de herencia. Una invocación representa una interacción que corresponde a una relación de asociación o agregación: objetos de una clase requieren a objetos de otra clase para cumplimentar alguna acción en respuesta a la ejecución de un método. Alguna variable (generalmente el “nombre” de la relación) en la primer clase representa al objeto que recibe el requerimiento, mientras que el éste consiste del nombre del método que debería ejecutarse y de un grupo de parámetros para ese método. En la especificación RSL, las variables han sido representadas por el tipo “Variable_name” y los parámetros del requerimiento por el tipo “Actual_Parameters”, que es básicamente sólo una lista de variables. Luego, el requerimiento es modelado usando el tipo “Actual_Signature” y la invocación con el tipo “Invocation”: Invocation :: call_vble : Variable_Name  call_sig : Actual_Signature, Actual_Signature :: meth_name : Method_Name  a_params : Actual_Parameters, Actual_Parameters = Wf_Variable_Name* Una instanciación por otra parte consiste del nombre de la clase a ser instanciada junto con una lista de parámetros: Instantiation :: class_name : Class_Name   a_params : Actual_Parameters Las invocaciones a sí mismo o a una clase superior son análogas a las invocaciones comunes, excepto que se producen sobre la misma clase o sobre una superclase respectivamente. El tipo “Invocation” es usado para modelar esas situaciones, pero en estos casos la variable invocada es la variable específica “self” o “super” respectivamente: self, super:: Variable_Name De manera similar se han modelado los requerimientos y las asignaciones de variables que constituyen el cuerpo de un método concreto o implementado. El resultado de aplicar ese método, sus parámetros formales – una lista de parámetros cada uno modelado como una variable que no puede ser “self” o “super” del tipo “Wf_Vble_Name” – con opcionalmente el nombre de una clase indicando el tipo de la variable y también su nombre, han sido modelados como: Method :: f_params : Wf_Formal_Parameters meth_res : Result    body : Method_Body, Result = Variables, Parameter == var(Wf_Vble_Name) | paramTyped(paramName : Wf_Vble_Name, className : Class_Name) El estado de una clase se define en forma análoga como un conjunto de variables. Luego, se especifican  varias condiciones de consistencia sobre esta estructura y el modelo se aplica al catálogo completo de patrones de Gamma. Detalles de esas especificaciones pueden verse en [6][7][8] y [9]. 3. Otras aplicaciones y/o ampliaciones del modelo formal. Entre otras ampliaciones, el modelo formal de patrones ha sido extendido para incluir otras variaciones. Por ejemplo, un diseño compuesto por varios patrones relacionados posee propiedades adicionales derivadas de la interacción. Por ello, formalizar la composición de patrones – definida como la aplicación conjunta de patrones donde una clase puede poseer diversos roles, cada uno asociado a un patrón diferente -– es una tarea aún más compleja. Nuestro primer trabajo en esa dirección puede verse en [10]. La generalidad de nuestra especificación formal favorece su aplicación a otro tipo de patrones. Por ejemplo, los patrones de análisis de Fowler [11] modelan un dominio completamente distinto, pero la descripción de su estructura y comportamiento siguen lineamientos similares a los de los demás patrones. Así, reusando las especificaciones formales un patrón de análisis se define como: Analysis_Pattern =  G. Pattern_Name  x  PS.WF_Pattern_Structure Algunas de sus propiedades han debido definirse especialmente, es decir no han sido reusadas desde el modelo original. Por ejemplo, cada relación de herencia se clasifica ahora como completa o incompleta y se modela con una definición variante;  la noción de instanciación de una estructura usada para especificar restricciones semánticas se define como el tipo “Instances”; etc. Inheritance_type = = complete | no_complete inheritance_type:  R.Wf_Relation → G.Inheritance_type Instances :: Instance-set Instance ::   fixed_object : G.Concrete_Object   connected_objects : G.Concrete_Object-list Las diversas restricciones de consistencia aplicadas sobre estos tipos, así como la formalización de propiedades semánticas propias de los patrones de análisis, pueden verse en [12]. Por ejemplo, el operador semántico que restringe a los objetos conectados por una asociación a formar un grafo acícilico dirigido (dag), se modela como una función en la que toda instanciación de una clase particular de la estructura – conectada por una relación recursiva – cumple con la restricción mencionada. dag_type:   PR.Role_Type x PR.Signature_Type  →   PS.Wf_Pattern_Structure → Bool dag_type(ro, rid)(psc, psr) ≡ ∀ cl: C.Wf_Pattern_Class, re: R.Wf_Relation, co: Wf_Instances •  cl ∈ psc ∧ re ∈ psr ∧ is_instantiation(co, psr) ∧  C.role_type(C.p_role(cl) = ro ∧  R.signature_type(re) = rid  ∧ R.relation_type (re) = G. association  ∧ exists_loop(re, psr) ⇒ is_dag(co) La formalización de otras propiedades asociadas al comportamiento de los patrones de análisis también han sido direccionadas. Algunas de esas especificaciones pueden verse en [13]. 4.   Futuras extensiones El modelo formal de patrones es un producto que se desprende del proyecto de investigación “Orientación a Objetos y Métodos Formales para la Especificación de Dominios”. La especificación completa del catálogo de Gamma permite iniciar ahora la construcción de una herramienta de software que verifique en forma semi-automática diseños realizados con patrones. Así, cualquier desarrollo podría certificar que el uso dado a los patrones no contradice su semántica, de acuerdo a como ha sido formalmente definida. La construcción de esta herramienta es parte de las actividades actuales de nuestro proyecto, así como la aplicación del modelo formal a dominios específicos.	﻿Especificación Precisa , Patrones GoF , Patterns , Métodos Formales , Orientación a Objetos	es	21723
67	Streamlines y LIC Visualización de Campos Vectoriales	﻿Streamlines y LIC: Visualización de Campos Vectoriales Claudio Augusto DELRIEUX, Julián Ariel DOMINGUEZ y Andrés Pablo REPETTO Universidad Nacional del Sur, Av. Alem 1253, (8000) Bahía Blanca, Argentina usdelrie@criba.edu.ar, jdoming@uns.edu.ar, repetto@uns.edu.ar 1.  Descripción de la línea de trabajo En los sistemas no lineales y caóticos la Visualización Científica es especialmente útil debido a la imposibilidad que presentan estos sistemas en general para hallar soluciones analíticas. Para estos sistemas se han creado numerosos métodos para representar mapas vectoriales (vectores, partículas, íconos, Streamlines, Hyperstreamlines, Spot Noise, LIC, etc.) [1,3,7,8,9] de las cuales traeremos en relieve dos técnicas destacadas en el tema: Streamlines y LIC. En esta línea de investigación buscamos un análisis de las características de estos dos métodos, aplicados a sistemas bidimensionales, y estudiamos las posibilidades de desarrollar un nuevo método que intente combinar propiedades ventajosas de ambos. 2.  Streamlines Esta técnica se basa en la representación de trayectorias, mediante el posicionamiento inicial de una “semilla” y la visualización del camino que recorre al dejarla evolucionar. En este método es de fundamental importancia la posición inicial de la semilla, pues de ella depende que tan “parejo” será el cubrimiento de las zonas importantes como puntos fijos, ciclos límite o atractores. Entre las mejoras que se pueden introducir al método encontramos trabajos realizados sobre alternativas para la distribución de las semillas [5,6] para lograr controlar el cubrimiento del diagrama, como también investigaciones efectuadas sobre el uso del color para mejorar la percepción de las características clave del sistema. Para solucionar el problema del cubrimiento del diagrama de fases, introducimos aquí la idea del “Path Search”, que consiste en el recorrido “hacia atrás” de la trayectoria, hasta encontrar la “fuente”. De ahí se recorre la trayectoria hasta llegar al “sumidero”, o morir después de cierta cantidad de iteraciones en un ciclo límite. Esto lo podemos apreciar en la Figura 1. Figura 1. Diagrama de fases sin y con path search. Otra técnica derivada de los streamlines podría ser la graficación del campo a través del uso de la desaturación en trayectorias cortas. Sin embargo, como aquí utilizaremos una longitud de trayectorias fija, se pierde la información de “velocidad”. En la figura 2 observamos esta técnica. Como conclusión, esta técnica tiene la ventaja de ser rápida, y la utilización de metáforas como la del color le da valor añadido para la visualización del espacio de fases de un sistema Figura 2. Disminución de la saturación. 3.  Line Integral Convolution (LIC) La idea básica de este tipo de métodos es la convolución local entre la textura y un segmento de trayectoria, para cada punto del mapa. Cabral y Leedom [1] introdujeron en 1993 la idea de efectuar la evaluación de la convolución sobre un segmento de trayectoria (Line Integral Convolution, o LIC). La LIC se hace atractiva para la visualización de sistemas desconocidos, ya que el resultado visual no depende de la disposición de las semillas. Es muy conveniente también porque la visualización del flujo es contínua y las principales características del sistema quedan al descubierto. En este método, al igual que con los streamlines,  la calidad depende de la longitud de la curva convolucionada, o ancho de ventana (kernel), pero también depende de la función peso del kernel. En la Figura 3 se presenta el mismo sistema dinámico, graficado con esta técnica y utilizando distintas longitudes de kernel L. El mejor resultado se logra a con mayor longitud, pero su costo computacional se vuelve inaceptable. La función peso del kernel puede adoptar distintas formas, y su variación ha sido utilizada en trabajos anteriores [1,11,12,16] para producir una sensación de sentido del flujo, o para lograr animaciones mediante su variación. Figura 3. Cuatro evaluaciones, con L=10, t=9.4s., L=20, t=20.2s. L=40, t=41.8s., y L=80, t=91.7s. A partir de una contínua búsqueda de mejoras en el método, surgieron nuevos tipos de LICs que mejoran en uno a varios aspectos el algoritmo original. El primero de éstos busca adaptar el paso en la evaluación a diferencias finitas hasta encontrar un nuevo texel vecino en la trayectoria. Este LIC adaptativo produce imágenes en un tiempo diez veces menor, con calidad indistinguible del original (ver Figura 4). Figura 4. LIC adaptativo. L=20, t=3.8s y L=80,t=13.4s. La forma del kernel también sirve para producir un efecto de sentido, ya que si utilizamos un filtro “sólo hacia adelante”, o positivo, la LIC resultante mostrará los nodos atractivos como fuentes de color, emanando o dispersando a su alrededor el color del nodo; en cambio si el kernel es “sólo hacia atrás”, o negativo, el efecto que resulta es el mismo pero para los nodos repulsivos. Este es el principio básico de los métodos LIC “direccionales” como OLIC y FROLIC [16], los cuales se tratan de LICs Orientadas por la forma del kernel. Además es utilizada para varios de los métodos de animación de LICs, usando funciones kernel que varían entre cuadro y cuadro [1,11]. En resumen, la LIC tiene como ventaja principal un cubrimiento parejo del diagrama de fases del sistema, dando una idea completa del comportamiento del flujo; y como principal desventaja el costo computacional que tiene su evaluación. 4.  Influencia de la textura El aspecto de la textura juega un rol muy importante en la generación de la LIC, aunque por mucho tiempo no se la haya considerado. Es clásico encontrar en estos trabajos ruido como textura de entrada, a lo sumo adaptado en frecuencia al campo a visualizar [17]. Como veremos,  la textura de entrada puede ayudarnos a visualizar el campo que estamos describiendo con el gráfico, dado que es un medio para introducir determinadas características geométricas, las que –al interactuar con el campo del sistema dinámico– permiten hacer visibles los rasgos característicos del sistema. En la Fig. 5 podemos ver algunos ejemplos de un mismo sistema dinámico convolucionado con diferentes texturas. En el caso a no tenemos ningún  elemento distintivo, en b se agregan cuadrados de color, y se distinguen por su forma, color y posición en la textura. Los casos c y d son similares al a, sólo que la uniformidad de contraste permite una superficie pareja en c, En d vemos que se distingue  cada cuadrado como “elemento” de una distribución pareja, separado uno del otro por un color constante (blanco), mientras que el cambio de color le agrega orientación. En los casos e y f, así como el k, es notable el resalte que toma una línea, o elemento rectilíneo, cuando el campo circula en esa dirección. Tal propiedad se hace visible en las rayas de un solo sentido, y aún más interesante en el caso k provocando un efecto de “luz de color” dependiente del ángulo del flujo. El g y h son similares, sólo que el hecho que el color de las líneas dependa del ángulo, junto a la mayor variedad de ángulos “resaltables”, provoca un mayor colorido en la LIC de salida. Los casos i y j poseen segmentos de recta en todas las direcciones, causando un recorrido más suave, como de grandes streamlines. En j incluimos la característica distintiva que los elementos de cada porción de círculo tengan un color dependiente de la dirección en la que está, provocando un efecto que combina las características del caso g y del i, dando curvas más suaves, cuyo color denota la dirección en esa sección de la trayectoria. Es de destacar el comportamiento de los trazos rectilíneos, ya que tiene en cierta manera es parecido al adoptado por los streamlines. Si la textura posee trazos verticales y horizontales (caso e) se produce un aliasing similar al que ocurre por una distribución constante en el sembrado de los streamlines (Fig. 1a); y si se usan segmentos de dirección aleatoria (casos g y h), el resultado se asemeja más al de los streamlines sembrados con una distribución de Poisson. Figura 5. Doce evaluaciones con diferentes texturas. 5.  Conclusiones y trabajo futuro Hemos revisado dos métodos de visualización de campos vectoriales, intentando entender sus principales características, ventajas y desventajas. Como conclusión, queremos hacer notar que utilizando L mayores conseguiremos que las características propias de la textura se acentúen, permitiendo una visualización más clara y hasta más agradable del sistema. Creemos que esta idea también es importante a la hora de buscar un nuevo método que nos permita graficar LICs con una mayor longitud de convolución. Agregamos a los resultados obtenidos la importancia de la textura y la posibilidad de crear animaciones mediante la variación de la textura de entrada. Algunas técnicas introducidas permiten obtener LICs con mayor longitud de kernel en tiempos interactivos, por lo que se hace provechoso seguir estudiando la posibilidad de crear un método que combine las ventajas de Streamlines y LIC.	﻿LIC , Streamlines , Visualización , Campos Vectoriales	es	21749
68	Interfaces en la visualización de información	﻿Interfaces en la   Visualización de Información    Lic. Sandra Di Luca    Lic. Sergio Martig     Mg. Silvia Castro      Guillermo Trutner   Lic. Mercedes Vitturini      Departamento de Ciencias e Ingeniería de la Computación - Universidad Nacional del Sur  Laboratorio de Investigación en Visualización y Computación Gráfica  Universidad Nacional del Sur – Bahía Blanca  Instituto de Investigación en Ciencia y Tecnología Informática (IICyTI)  {sdiluca,gtrutner}@criba.edu.ar, {smartig,smc,mmv}@cs.uns.edu.ar      Introducción    La creciente cantidad de aplicaciones en Visualización de Información hace que la misma sea  actualmente un área de activo desarrollo. Así como los usuarios creativos empujan los límites de las  herramientas actuales, los diseñadores serán presionados para proveer aún mayor funcionalidad.     La visualización y la exploración de colecciones de información se vuelve cada vez más dificultosa  a medida que el volumen y la diversidad aumenta. El proceso de extracción de información es un  proceso impreciso y, cuando los usuarios pretenden un acercamiento al sistema, tienen a menudo un  entendimiento vago de cómo lograr sus objetivos.     Existen diversas técnicas de Visualización de Información que ayudan en la tarea de exploración de  la información. Estas son atractivas ya que permiten tanto la presentación de la información bajo  distintos aspectos como una exploración controlada de la misma por parte del usuario. Si bien estas  tareas son propias de la Visualización de Información, deben integrarse adecuadamente a la  Interface. En este contexto la interface podrá asistir a los usuarios a formular sus consultas, navegar  y seleccionar entre las fuentes de información disponibles y entender los resultados de la  visualización.     Las mejoras logradas en las interfaces de usuario para consultas en Bases de Datos y búsquedas en  documentos de texto están dando como resultado nuevos productos. En la actualidad son posibles   acercamientos gráficos originales y de manipulación directa tanto en la formulación de consultas  como en la visualización de información y en la interacción.    Mientras los prototipos han tratado solamente con un tipo de dato (uni-, bi-, y tri-dimensional, datos  temporales y multidimensionales, árboles y redes), los futuros productos comerciales deberán  combinar varios. Estos productos necesitarán proveer una buena integración con el software  existente y soportar tareas como zoom, filtros, detalles sobre demanda e historia. Estos métodos son  atractivos porque presentan la información rápidamente y permiten una exploración controlada por  parte del usuario. Si todos ellos deben ser completamente efectivos, requeriremos estructuras de  datos avanzadas, displays de alta resolución y nuevas formas de entrenamiento de los usuarios. En  la actualidad, por ejemplo, están siendo construidas varias interfaces de usuario para especificar  filtros avanzados que están siendo evaluadas para incluirlas en distintos proyectos comerciales.    Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 934 La búsqueda de información con estructura compleja (incluyendo, entre ellos, gráficos, imágenes,  sonido o video) y la presentación de la información misma brindan una gran oportunidad para el  diseño de interfaces de usuario avanzadas y motores de búsqueda poderosos.     Se necesita de una mejor integración con la sicología perceptual y con la toma de decisiones, ya que  son fundamentos teóricos y medidas prácticas para la elección a través de las diversas técnicas de  visualización. Los estudios empíricos ayudarán a ordenar la situación específica en la cual la  visualización sea mas útil. Finalmente, las herramientas de software para la construcción de  visualizaciones innovadoras, integradas adecuadamente a la interface, facilitarán el proceso de  exploración.    Así es que el objetivo de nuestro trabajo consiste en delinear criterios para el diseño de Interfaces  que asistan al usuario particularmente en todas las etapas del proceso de Visualización de  Información. Además cabe señalar que, como una aplicación significativa de la Visualización de  Información lo constituye la Visualización de Bases de Datos, se comenzará teniendo en cuenta los  principios para el diseño de Interfaces instanciados adecuadamente a Bases de Datos. El enfoque  sobre una aplicación ayudará a cubrir la distancia entre las propuestas teóricas y las aplicaciones de  las mismas y es un modo de establecer el valor de las técnicas de Interfaces.     Diseño de la Interfaces a lo largo del proceso de Visualización de Información    Debido a la gran variedad de dominios de la Visualización de Información, el desafío es diseñar un  ambiente que permita a los usuarios llevar a cabo, de manera intuitiva, una variedad de tareas de  visualización. Aunque diferentes dominios de aplicación requieren representaciones visuales  distintas, muchos de estos comparten operaciones de transformación de los datos y manipulaciones  similares a lo largo del pipeline de visualización.    Las interfaces deben ayudar en las distintas etapas del proceso de visualización: deben soportar el  proceso de búsqueda mediante la especificación de las consultas, debe ayudar en el proceso mismo  de la exploración de la información presentada y deben, sin duda, ser un apoyo en el proceso de  interacción a lo largo de todo el proceso, de modo tal que el usuario pueda satisfacer su necesidad  de acceder y explorar la información.    En todas las etapas de la Visualización de Información, debemos contar con interfaces apropiadas  que apoyen al usuario en sus tareas específicas. Desde el punto de vista del diseño de las interfaces,  los usuarios tienen habilidades, diferencias y predilecciones muy variadas; son muchos los factores  que pueden conducir tanto al rechazo como a la aceptación de las distintas técnicas de Interfaces;  además, esta diversidad plantea la necesidad de un diseño muy flexible y de la incorporación de  características, en cada etapa, que las haga efectivas. De acuerdo a esto podemos detallar lo  particular en cada uno de los procesos involucrados en la Visualización de Información:    - En la etapa de la presentación inicial de la visualización  se le debe proveer al usuario una buena  manera de comenzar el proceso exploratorio. Una pantalla en blanco no brinda una forma  adecuada de comenzar este proceso.  Al comienzo, un punto importante es que la interface   ayude a los usuarios a seleccionar las fuentes de datos a las que quiere interrogar y que éstas se  presenten de manera útil. A partir de este momento, el usuario tratará de satisfacer la o las metas  uqe tenga y dará por comenzado el pipeline de visualización.    - Luego comenzará la interacción con la visualización introduciendo consultas, observando los  resultados y modificando las consultas en un ciclo interactivo hasta ver satisfechas sus  Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 945 necesidades de información. Esta segunda estapa lo constituye la especificación de las  consultas. La formulación de una consulta debe permitir al usuario seleccionar colecciones,  describir metadatos o conjuntos de información de acuerdo a lo pedido en la misma y también  debe permitir especificar información que puedacoincidir con la información en la base de  datos. A partir de la consulta, el sistema creará la información derivada que será mostrada de  alguna manera adecuada. En este punto deben verse las posibles interafaces y sus características  de acuerdo al estilo de interacción  elegido para la especificación de las consultas.    - Se pondrá además especial énfasis en las etapas específicas que correspondan a la presentación  de la Información. En esta etapa se deben tener en cuenta los distintos métodos que hay para la  presentación de datos multidimensionales y los desafíos que plantean las Interfaces en lo  referente a mostrar los datos y sus interrelaciones.     - La interacción del usuario en cada una de las etapas del pipeline de visualización provocará  seguramente una realimentación. Es por ello que el sistema deberá brindar un ambiente en el que  la interface sea clara y amigable para lograr aprovechar al máximo la flexibilidad, creatividad y  conocimiento general que el usuario posee del problema real. De esta manera se establecerá un  flujo interactivo coherente entre el usuario y el sistema de visualización.    Actualmente se está trabajando en el diseño de un prototipo integrado de visualización, en el que la  interacción del usuario esté presente en las distintas etapas del proceso de Visualización. En esta  primera etapa se está realizando un primer abordamiento en la visualización de Base de Datos  seleccionando dentro del conjunto de técnicas generales de visualización a las Coordenadas  Paralelas por considerar la multidimensionalidad de los datos. Nuestro objetivo es el diseño de  interfaces efectivas en cada una de las etapas del proceso de visualización siguiendo los principios  para el diseño de la misma e instanciándolos adecuadamente de acuerdo a la aplicación señalada.      Conclusiones    La importancia de la Interacción Humano-Computadora está siendo reconocida como un campo  importante de Ciencias de la Computación, y de acuerdo a lo visto surge sin duda el rol relevante de  la interface en todo el proceso de acceso e interacción con la información.    Por ello es necesario contar con interfaces efectivas que apoyen las distintas tareas a realizar en todo  el proceso de visualización. Las interfaces deben seguir los principios de diseño de interfaces de  usuario y cada uno de estos principios debe ser instanciado adecuadamente dependiendo de la etapa  particular en la que el usuario interactúa con la visualización.	﻿Visualización de Información , Interfaces	es	21822
69	Interacción en la visualización de información	﻿Interacción en la Visualización de Información    Sergio Martig – Silvia Castro – Sandra Di Luca    Laboratorio de Investigación y Desarrollo en Visualización y Computación Gráfica (VyGLab)  Instituto de Investigación en Ciencias y Tecnología Informática (IICTI)  Departamento de Ciencias e Ingeniería de la Computación  Universidad Nacional del Sur - Bahía Blanca  [smartig/smc]@cs.uns.edu.ar      Introducción  En un sistema de Visualización, un conjunto de interacciones bien diseñadas puede utilizarse para  responder a una gran variedad de preguntas. A pesar de esto, aún no hay una sistematización que  permita identificar qué técnicas de interacción aplicar en los distintos puntos del proceso de  Visualización de Información. Esta es la razón de por qué se debe caracterizar la interacción en las  distintas etapas del proceso de Visualización.     1 Interacción   El término interacción implica la existencia de dos partes que se influencian recíprocamente en su  accionar, para que tenga lugar deben existir como mínimo dos actores. Desde una óptica  computacional, interacción es el fenómeno que ocurre cada vez que un usuario (el humano) se  involucra en la operación de un producto computacional (el sistema).   Para que la interacción sea efectiva, debe ser completa. Debe contemplar los dos componentes,  al  humano y a la computadora. Ambos componentes son complejos y difieren mucho en la manera en que  ellos se comunican, requieren un protocolo de comunicación que permita una interacción tan rápida y  natural como sea posible.  La interface debe en consecuencia ser el medio efectivo de traducción entre el humano y la  computadora, para que la interacción sea exitosa. El proceso de traducción puede fracasar en distintos  puntos y debido a diferentes razones.   El uso de modelos de interacción pueden ayudarnos a entender lo que está ocurriendo exactamente y  detectar los problemas que puedan surgir en ese proceso. Ellos también nos proveen de un marco  válido para la comparación de los distintos estilos de interacción  y  el análisis de los posibles  problemas.  Sistema Usuario Input (Entrada) Output (Salida)     Modelo de Interacción (Abowd y Beale)    Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 1256 2 Visualización de Información  El campo de Visualización de Información surgió en respuesta a la búsqueda de los investigadores de  herramientas que favorecieran el análisis y el entendimiento de datos abstractos mediante el uso de de  computación gráfica interactiva y el uso de técnicas de visualización. Los datos abstractos no son  inherentemente geométricos y presentan desafíos a los investigadores en Visualización porque evidentemente no  es obvio poner los datos abstractos en formas visuales efectivas.  Concretamente podemos decir que la Visualización de Información es el uso interactivo de representaciones  visuales de datos abstractos soportadas en computadoras con el objetivo de amplificar el conocimiento. El  principal objetivo de esta área de la Visualización es la representación gráfica adecuada tanto de los datos con  parámetros múltiples como de las tendencias y las relaciones subyacentes que existen entre ellos. Su propósito  no es la creación de las imágenes en sí mismas sino el insigth, es decir, la asimilación rápida de información o  monitoreo de grandes cantidades de datos.  La Visualización de Información puede ser vista como una herramienta de exploración, tiene que asistir e  incentivar al usuario en el proceso de análisis del espacio de información. Como dice Jacques Bertin en su  Semiology of Graphics: “Un gráfico no es dibujado una vez y para siempre; es construido y reconstruido hasta  que revela todas las relaciones entre los datos...Un gráfico nunca es el fin en sí mismo; es un momento en el  proceso de la toma de decisiones”.     2.1 Modelo de referencia:  La Visualización puede ser vista como un proceso de transformación que convierte valores de datos en  vistas gráficas. En Visualización Científica, varios investigadores han examinado el uso de un modelo  de red de flujo de datos para construir visualizaciones.  El modelo tradicional de red de flujo de datos usado en Visualización Científica es insuficiente para  describir a la Visualización de Información. Esto en parte se debe  a que los requerimientos de la  Visualización de Información difieren de los de la Visualización Científica. En Visualización de  Información nos enfrentamos con el hecho de que los datos abstractos no tienen un mapeo espacial  inherente lo que nos lleva a tener que resolver cuestiones acerca de cómo representar los datos  visualmente, qué tipo de interacción exploratoria incluir y cómo estructurarla. Necesitamos un modelo  que no solo describa los procesos de transformación de los datos, sino también los distintos estados de  los datos. En particular lo que necesitamos es un modelo que permita unificar el modelo de interacción  con el del proceso de visualización.   El modelo (Pipeline de Visualización de Información) propuesto por Ed Huan-shi Chi [2], que surge  como modificación del modelo de Stuart Card [1] consta de cuatro estados de datos y de procesos de  transformación entre ellos.  En los extremos se encuentran los datos y las vistas respectivamente y sus  transformaciones van derivando en los estado intermedios. El modelo contempla la interacción del  usuario tanto en los diferentes estadíos de los datos, como en las transformaciones.    Datos Crudos Abstracción Analítica Abstracción Visual Vista Transformación de los Datos Transformación de Visualización Transformación del Mapeo Visual INTERACCIÓN     Pipeline de Visualización de Información    Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 1267 3 Interacción en la Visualización de Información  Avances en las interfaces para visualización de información han mostrado que el análisis visual no solo  se beneficia de buenos métodos de representación, sino también de buenas interacciones con esas  representaciones. Estas interfaces permiten que los usuarios realicen las operaciones de análisis  directamente sobre la representación visual, teniendo realimentación directamente a nivel visual.  En un sistema de visualización un conjunto de interacciones bien definidas pueden ser usadas para  responder a una amplia gama de necesidades. En una primera instancia el diseño de un conjunto de  interacciones bien definidas requiere de conocimiento en el dominio específico de la aplicación, esto es  natural ya que cada disciplina y las cuestiones particulares dentro de las mismas  determinan los  análisis a los que someterán los datos. Afortunadamente aunque sea frecuente que distintos dominios   de aplicación requieran representaciones  visuales diferentes, varios de ellos pueden compartir estados  intermedios de los datos, o requerir manipulaciones a nivel de vistas similares o incluso necesitar de las  mismas transformaciones de datos.   En el marco de un modelo conceptual para la Visualización de Información, podemos analizar y  categorizar las similitudes entre los distintos dominios de aplicación y caracterizar las interacciones que  puedan tener lugar en las distintas etapas del proceso. El objetivo es lograr caracterizar los distintos  tipos de interacción que puedan requerirse en procesos típicos de visualización de información. Se  quiere lograr una caracterización del espacio de diseño de las interacciones considerando:  - la etapa  del pipeline sobre la que se quiere interactuar  - el estado de los datos  determinando y evaluando los estilos de interacción apropiados para cada caso y su impacto en el resto  del pipeline.    4 Conclusiones  La persona que, en posesión de un dominio del conocimiento, está frente a una visualización, puede  decidir reacomodar los datos o posiblemente adicionar datos relevantes; ésta es la esencia de la  visualización interactiva: un examen y una interpretación iterativos de los datos presentados  gráficamente.  De esto se desprende que un mero dibujo estático no es suficiente para extraer  información de los datos. El gráfico debe ser construido y reconstruido (manipulado) hasta que todas  las relaciones que subyacen en los datos hayan sido percibidas.   Esto pone en evidencia la imperiosa necesidad de explorar los datos interactivamente. Esta capacidad  es tan valiosa que se ha invertido mucho esfuerzo en la creación y la implementación de herramientas  de visualización interactiva que exploten este potencial. El análisis visual se beneficia no sólo con  buenos métodos de representación visual sino también recibe beneficios de buenas interacciones con  estas representaciones.   Si bien es indudable la importancia de la interacción en el proceso de Visualización de Información,  aún no hay una sistematización que permita identificar qué técnicas de interacción aplicar en los  distintos puntos del proceso de Visualización. De este hecho surge la necesidad de caracterizar la  interacción en las distintas etapas del proceso de Visualización.	﻿Visualización de Información , Interacción	es	21829
70	La visualización de algoritmos como recurso para la enseñanza de la programación	﻿os de lectura mecánica y sobre todo de lectura  comprensiva [27] [28].   En relación con la enseñanza de la programación se destaca la importancia de las actividades  comprendidas en:   - construcción de algoritmos   - lectura comprensiva de algoritmos  Existe consenso en considerar un enfoque que parta de la resolución de problemas, para el primer  ítem, y que se apoye en técnicas de visualización de algoritmos y programas para el segundo caso.  Se aborda en este trabajo el estudio realizado sobre visualización de software, especialmente  pensado para la lectura comprensiva de algoritmos y programas.  La visualización de software es el uso de gráfica de computadoras y animación interactiva para  ilustrar y presentar programas, procesos y algoritmos. Se basa en el uso de diseño gráfico, de  animación, de sonido, de video y tiene como característica sobresaliente la interacción entre el  usuario y la computadora.  La visualización de la conducta dinámica de los programas y de sus modelos abstractos, los  algoritmos, y consecuentemente, el beneficio psicopedagógico de su aplicación en la enseñanza,  constituyen una gran área de investigación que aún no ha sido completamente explorada.   La motivación especial que condujo a realizar este trabajo es la necesidad de mejorar las  herramientas existentes para facilitar la comprensión de los aspectos de la conducta dinámica de los  algoritmos en las distintas fases del ciclo del software, en el diseño, en el desarrollo, en la búsqueda  de errores, en la depuración y en el mantenimiento.  Generalmente, para lograr una comprensión acabada de un programa se realiza, por un lado, el  estudio del código fuente, lo que es incómodo y de dificil aplicación y, por otro lado, la  construcción de casos de prueba para explicar la conducta de un programa, lo que es una tarea  penosa y especulativa. Estas dificultades motivan el desarrollo de programas especiales que son  usados para ayudar a explicar la conducta de otros programas [1].  Visualización de software  La Visualización de Software comprende la visualización interactiva de dos áreas fundamentales:  la de algoritmos y la de programas, que en ambos casos, se pueden considerar transversalmente  como presentaciones estáticas o dinámicas.    Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 2123  Estática    Dinámica   Algoritmos  Programas  Visualización de Software      La visualización de algoritmos consiste en la exposición de abstracciones de alto nivel que  describen semánticamente el software. La representación estática  de los mismos está basada en la  esquematización de la estructura de su especificación. La representación dinámica, que  habitualmente se llama animación de algoritmos, muestra su comportamiento en tiempo de corrida.  La visualización estática de programa realiza un análisis del texto del mismo y provee la  información que es válida para todas las ejecuciones independientemente de su entrada [15] [11].  Una representación dinámica del programa provee información acerca de su ejecución sobre un  conjunto de datos de entrada [15]. La información de la ejecución se puede ofrecer en forma  instantánea, o en un tiempo posterior (análisis post mortem). El análisis post mortem puede realizar  computaciones grandes para condensar la información de la ejecución y presentarla en una forma  útil. Los métodos no son excluyentes.   Las herramientas en tiempo de corrida pueden ser pasivas o interactivas. En un sistema pasivo, la  herramienta presenta información al usuario, pero el usuario tiene poco control sobre la actividad  del mismo; en un sistema interactivo, el usuario puede tener control externo sobre la información  que se está exhibiendo, puede estar disponible para modificar la computación que está siendo  observada o bien el dato que está siendo procesado [11].  Los sistemas para comprender programas son usados en una variedad de aplicaciones. El debugger  es uno de los programas más comunes que ayudan a la comprensión del software. Otras de las  aplicaciones de sistemas de comprensión de programas es la encargada de poner a punto la  efectividad o performance. Una tercera aplicación de la comprensión es la instrucción y orientación  del software, que resulta de interés para aprender importantes algoritmos, estructuras de datos o  técnicas de programación. Con las herramientas de comprensión de programas las personas  afectadas pueden beneficiarse por la información provista por ellas, antes de consultar el código  fuente.  Antecedentes de la visualización de software.  La preocupación frente a la dificultad en la comprensión del software ha sido constante. Entre las  distintas mejoras se pueden mencionar las orientadas hacia la estructura lógica y expresiva del estilo  de los programas, tal como el diseño top down y el refinamiento paso a paso [14], la programación  estructurada [15], la modularidad [16], y las herramientas de software [17]; en la claridad y potencia  expresiva de los lenguajes de programación [18], y en el desarrollo de aproximaciones orientadas a  objeto para diseño y desarrollo [19].   El embellecimiento de la escritura, el uso de espaciado, identación, y disposición para que el código  fuente fuera más fácil de leer son algunas de las primeras ideas de visualización que surgieron en un  principio. Los Prettyprinters son programas que sistemáticamente identan el código fuente de un  programa de acuerdo a su estructura sintáctica. SEE Program Visualizer [3] es un sistema que  automáticamente establece un programa en C de acuerdo a una guía de estilo basada sobre  principios de diseño. El sistema WEB mejora la escritura de programas combinando el texto del  programa fuente con la documentación en un sola publicación usando un lenguaje sofisticado [2].   A medida que se progresa en la capacidad de representación gráfica de las computadoras, se  desarrolla software que permite visualizar el comportamiento de los programas. El primer trabajo  Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 2134 que ilustra la ejecución de un programa es el que presenta Baecker a mediados de los 70 [3]. El  sistema Balsa es uno de los primeros que permite la creación de las visualizaciones de algoritmos  genéricos en Pascal [4]. El sistema agrega al código primitivas de animación las cuales activan las  funciones de representación gráfica que ofrece el sistema. A finales de los años 80 y en la década de  los 90 surgen la gran mayoría de los sistemas que contemplan ahora la visualización de programas  concurrentes o paralelos como los sistemas Voyeur [5] o Belvedere [6].  Como sucesores del sistema Balsa, surgen los sistemas Tango [7], Zeus [8] y [9]. Estos dos últimos  permiten la visualización de ejecuciones de programas concurrentes. El sistema Pavane [10]  permite la visualización de programas declarativos.   Visualización de algoritmos.  La Visualización de Algoritmos presenta un aspecto distintivo de los otros tipos de visualizaciones  ya que no existen entidades tangibles a las cuales asirse para diseñar la visualización. Se debe  decidir lo que se quiere visualizar y cómo se va a obtener la información necesaria para hacerlo  [23].  En las visualizaciones de algoritmos, en las que se utilizan grandes niveles de abstracción, puede  resultar difícil especificar la apariencia visual de los objetos lógicos presentes. En este caso el  sistema de visualización debe ofrecer un conjunto de transformaciones y entidades gráficas para  realizar la especificación.   La visualización del algoritmo en cualquiera de sus dos formas, dinámica o estática, puede ser muy  difícil de relacionar con las construcciones del programa que la codifican y que constituyen las  especificaciones de su comportamiento. Es decir, se presenta allí una brecha entre la representación  del algoritmo y la especificación del programa subyacente [4].   Contribución pedagógica de la Visualización de Algoritmos  Se debe disponer de sistemas amigables con el usuario (facilidad de interacción hombre-máquina,  en cuyo diseño se debe poner especial interés en la percepción humana). Pero, además, la  experiencia de los usuarios con el lenguaje de programación y su nivel de familiaridad con la propia  visualización de software resultante, influyen sobre la calidad del sistema de visualización.   La Visualización de Algoritmos presenta un medio audio-visual interactivo multimedial cuyas  bondades en el campo de la psicopedagogía han sido ampliamente comprobadas. La Visualización  de Algoritmos es una disciplina joven y, por ello, tanto la experiencia en sí como la evaluación de  los resultados deben ajustarse a ella. Las animaciones de algoritmos presentan importantes  beneficios educativos [24] [25] [26]:  • Logran un incremento de la motivación.  • Facilitan el desarrollo de destrezas.   • Asisten en el desarrollo de habilidades analíticas.  • Ofrecen un buen soporte al docente.  • Permiten la exploración, de las peculiaridades de un algoritmo, jugando interactivamente.   Distintos centros educativos emplean estos sistemas como apoyo al aprendizaje [13] [24].  Sistema de Visualización de Algoritmos propuesto  Se propone un Sistema de Visualización Interactiva de Algoritmos orientado a una programación  estructurada y modular.  El sistema debe proveer multiventanas en las que se debe exhibir la estructura estática del  algoritmo, la estructura estática de los datos, el menú de selección de la representación de datos y  aquellas ventanas que permiten el ingreso de los valores de los datos y de su representación con los  cuales se realizará la corrida del programa.  Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 2145 Durante la ejecución del programa, el sistema debe permitir la visualización del dinamismo de la  activación de los distintos módulos en ejecución, la historia del paso por los distintos módulos y  fundamentalmente, el comportamiento del algoritmo a través de la animación de los datos.  Ventanas adicionales, deben estar activas para permitir la interacción del usuario con la  visualización de manera que pueda interrumpirla, fijar la imagen, modificar su velocidad de  presentación, modificar los valores de entrada, modificar la representación de los datos y sus  colores, activar la audición ya sea de errores o de pasos claves. La audición ofrece algunas ventajas  respecto de la observación permanente de las ventanas activas. Escuchar un sonido determinado  ayuda al usuario a detectar una acción cuya ejecución se está llevando a cabo en ese momento, sin  necesidad de observar la pantalla.  Conclusión  La animación de algoritmos y la visualización de programas ayudan a los estudiantes a  comprender los conceptos de software y también a los docentes en su tarea de enseñar dichos  conceptos.  Independientemente de la calidad de un sistema de visualización se deben tener en cuenta algunos  factores importantes que influyen sobre la efectividad del mismo, como son la experiencia de los  usuarios con el lenguaje de programación y su nivel de familiaridad con la propia visualización de  software resultante.  Actualmente se están estableciendo los lineamientos básicos de un sistema de visualización de  algoritmos basados en un ambiente estructurado y modular, que constituya un entorno de  aprendizaje efectivo para ayudar a los estudiantes en la comprensión y desarrollo de algoritmos y  programas.	﻿enseñanza de la programación , visualización de algoritmos , recurso	es	21859
71	Estudio de Técnicas apropiadas para modelar aplicaciones de Hidroinformática en el contexto de los Sistemas de Información Ambiental	﻿de los Sistemas de Información Ambiental – Urciuolo Adriana, Iturraspe Rodolfo, Sandoval Sandra, Parson Ariel Universidad Nacional de la Patagonia San Juan Bosco – Sede Ushuaia, Darwin y Canga, (9410) Ushuaia. TE/FAX: 430892 e-mail: urciuolo@tdfuego.com, iturraspe@tdfuego.com, sandrasandoval@ciudad.com.ar, a-p rson@impsat1.com.ar Resumen El objetivo general de la línea de investigación es el estudio de técnicas apropiadas para el desarrollo de sistemas de hidroinformática,  que permitan manejar la complejidad, representar en forma adecuada la información y simular los procesos relacionados con el ambiente natural, en el contexto de un Sistema de Información ambiental. En forma específica durante la primera etapa del proyecto, se pretende definir técnicas adecuadas para modelar los sistemas de hidroinformática, mediante la aplicación de un enfoque combinado a un campo particular del dominio: los modelos matemáticos lluvia-escorrentía. Se proponen microarquitecturas de diseño OO (en base a la aplicación de patrones de diseño) aptas para el desarrollo de modelos hidrológicos en el contexto de un Sistema de Información Ambiental y soluciones evolutivas para la optimización de los parámetros del modelo. El proyecto se desarrolla en la Fac. de Ingeniería de la UNPSJB. Asesores externos del proyecto: Dra. Silvia Gordillo (UNLP), PhD. Zbigniew Michalewicz. Introducción Los Sistemas de Información ambiental se relacionan con el manejo de los datos correspondientes a los distintos componentes interactuantes del ambiente: el suelo, el agua, el aire y las especies existentes. El presente proyecto de investigación en desarrollo, focaliza su atención en el tratamiento de la información correspondiente a uno de dichos componentes: el agua. La gran demanda de información ambiental y de herramientas apropiadas para su organización y análisis, así como la necesidad de que las organizaciones de manejo ambiental cuenten con sistemas de información eficientes, motiva actualmente en forma creciente el interés por esta área de investigación. La complejidad de los sistemas relacionados al ambiente hídrico, generó el desarrollo de un gran número y diversidad de modelos computacionales (Bavovic, 1996),  que a través de años de evolución dieron surgimiento a un nuevo paradigma que tiene sus bases en la hidráulica computacional: la hidroinformática (Abbot, 1991), el cual se  ocupa del  desarrollo y aplicación de modelos matemáticos y tecnología avanzada de información a problemas de hidrología, hidráulica e ingeniería ambiental. Provee sistemas de soporte de decisión basados en computadoras para ser utilizados por ingenieros, autoridades del agua y organismos gubernamentales de la gestión hídrica y ambiental (IHE Delf, 1996). Los problemas más comunes que manejan estos sistemas son: modelado de ríos, cuencas, agua subterránea, transporte de sedimentos, transporte de contaminantes, calidad de aguas, predicción de crecidas, riesgos hidrológicos, cambio climático, manejo y remediación de suelos y aguas subterráneas contaminadas, conservación de calidad de aguas, etc. El tratamiento de estos problemas, normalmente se realiza en las instituciones, mediante el uso de modelos matemáticos que simulan distintas situaciones, asumiendo que los resultados de los mismos, una vez ajustados, corresponden al comportamiento del sistema hidrológico real. Dentro de este dominio, los modelos computacionales aplicables al campo de la hidrología superficial conocidos como de lluvia-escorentía, sufrieron un proceso de cambios y evolución, determinado por los avances en la computación, a los cuales se fueron adaptando; se distinguen cinco generaciones de modelos. Si bien en la última se han incorporado componentes de inteligencia artificial encapsulando conocimiento de expertos y de otras metodologías para sistemas soporte de toma de decisión, se Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 3389 observan aún problemas en el software, derivados de la falta de utilización sistemática de técnicas de ingeniería de software apropiadas para el dominio. Pueden citarse: 1. Falta de metodologías adecuadas para el manejo de la complejidad. La complejidad en el manejo de la información y el modelado de ambientes naturales está presente por muchas razones, sin embargo podríamos citar como fundamentales las siguiente causas: · Gran cantidad de datos a procesar. Las técnicas clásicas de almacenamiento y manejo de datos no son eficientes en la mayoría de los casos. · Escala temporal: los fenómenos ambientales cambian a través del tiempo. · Necesidad de representación espacial de la información. · Los objetos ambientales presentan estructuras complejas, generalmente se componen de subobjetos. · Los procesos hidrológicos involucran información de tipo continua y modelos no lineales · Necesidad de considerar la influencia humana sobre los ecosistemas. · Manejo de la incertidumbre. · Consideración de restricciones legales, normativas existentes, etc. · Requerimiento del estudio de complejas conexiones lógicas entre datos, dada la diversidad de áreas de estudio interrelacionadas. 2. Falta de integración de modelos a sistemas de información hídrica y ambiental. La mayoría de modelos tradicionales existentes en el mercado, se ejecuta en forma aislada de los sistemas de información hídrica y ambiental de las organizaciones, sin poder utilizar la información disponible en los mismos. En la actualidad, algunos modelos se han comenzado a integrar a SIG (Sistemas de Información Geográfica) para beneficiarse de sus prestaciones de almacenamiento y análisis de datos, pero dicha integración se consigue a través de complejos programas de interface para adecuar las estructuras de datos de modelos y SIG y no a través de una arquitectura común de diseño. Lo mismo sucede en cuanto a la integración con otros modelos de hidrología (por ejemplo de calidad de aguas, transporte de sedimentos, etc.) o de otras áreas, que permitirían agregar funcionalidad a la existente o modificar los métodos de cálculo. Temas de investigación y desarrollo De lo expuesto, surge la necesidad de analizar metodologías apropiadas para manejar la complejidad y de caminar hacia una infraestructura común integradora de desarrollo del software para el dominio, que permita la ampliación de la frontera de utilización a nuevos problemas y usuarios y brinde flexibilidad para la actualización y/o extensión de los modelos existentes en las organizaciones. Las actuales investigaciones en hidroinformática incluyen el estudio de técnicas apropiadas para el manejo de la complejidad y de la incertidumbre. Se observa que en el software correspondiente a modelos de quinta generación, ha comenzado a utilizarse el enfoque OO, pero básicamente a nivel de los componentes de control del modelo. Abbot postula que la OO debería jugar un rol importante en esta generación, en la modelación de los tipos de datos (Abbot, 1991). No obstante lo propuesto, si bien se encuentran en nuestros días aplicaciones que utilizan este enfoque, sólo se observa la definición de algunas jerarquías de herencia y relaciones básicas, sin el objetivo de obtener diseño reusable. Dentro de este proyecto de investigación, se propone el desarrollo de microarquitecturas de diseño OO para este tipo de sistemas, basadas en la utilización de patrones, que permitan solucionar en parte, los Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 33940 problemas planteados: representación adecuada de la información continua, espacio-temporal, flexibilidad en la elección del modelo a utilizar por los componentes de un sistema hidrológico real, modelos integrados a Sistemas de información. El planteo del proyecto implica una concepción inversa a la tradicional en el modelado de recursos hídricos, donde el énfasis fue siempre puesto en los procesos físicos siendo representados y  la forma en la cual obtener los parámetros de un ambiente particular jugaba un rol menor. El énfasis se pone  en primer lugar en la descripción del ambiente de un sistema hidrológico real y luego en la definición de los modelos de procesos que puedan utilizar los datos disponibles. Se utilizan metodologías apropiadas para desacoplar la funcionalidad, de modo que distintos modelos independientemente del objetivo de la simulación tengan acceso a los datos observados y almacenados en sistemas de información. Por otra parte, dada la necesidad de manejar la incertidumbre, se hace necesaria la optimización de parámetros de los modelos para que se ajusten al sistema real que representan. Existe en la actualidad un incremento cada vez mayor de la utilización de técnicas de computación evolutiva aplicadas a los sistemas de información ambiental (Goldberg, 2000), como herramienta de optimización de funciones  y  de toma de decisiones; es intención de este proyecto, probar asimismo su adaptabilidad a procesos característicos de las aplicaciones de  hidrología superficial, como la optimización de parámetros, Se propone para ello el estudio de diferentes técnicas evolutivas, en particular de algoritmos genéticos a los efectos de determinar la conveniencia de su uso para este fin. Etapas del Proyecto 1. Análisis del Dominio Se realizaron los siguientes estudios: Conceptos básicos del paradigma de hidroinformática, Modelos hidrólogicos e hidráulicos existentes, Análisis de aplicaciones de software existentes y de documentación  relacionadas con los Sistemas de Información Ambiental y con el dominio de estudio en particular, Análisis y comparación de metodologías utilizadas en el dominio. 1.1. Requerimientos candidatos: Se definieron los siguientes requerimientos candidatos para el software del dominio de los sistemas de hidroinformática: · Proveer una forma adecuada y flexible de modelar los procesos del mundo natural. · Adaptarse tanto a los objetivos de simulación numérica, resolviendo las ecuaciones correspondientes, como al manejo de la gran cantidad y complejidad de la información disponible. · Proveer medios para intercambio de datos o integración entre los sistemas de simulación hidráulicos o hidrológicos con herramientas de análisis de impactos ambientales y económicos. · Facilitar su integración con SIG (Sistemas de información geográfica) a los efectos de manejar el ingreso y almacenamiento de datos, así como la presentación de resultados mediante una interface adecuada. · Utilizar herramientas adecuadas de simulación y optimización. · Proveer facilidades para su extensión y modificación, así como para la inclusión de rutinas de simulación existentes 1. 2.  Definición del contexto Se realizó el análisis del contexto de las aplicaciones correspondientes a Sistemas de información hídrica  y a Modelos lluvia/escorrentía, considerando diferentes niveles de diferente abstracción. Se definieron los siguientes tipos de objetos, para cada uno de los niveles expuestos: Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 3401 · Nivel de Información hídrico/ambiental: Objeto-Hidrológico. Este nivel está compuesto por los objetos del mundo real que interesa almacenar en un “inventario hídrico” (cuenca, lago, planta, etc.) Las entidades observadas se clasificaron en hidrológicas, climáticas, tecnológicas y ambientales. · Nivel de representación espacial: GeoObjeto-Hidrológico: en este nivel, se adiciona comportamiento geográfico a aquellos objetos identificados en el nivel anterior, que deban incluir su representación espacio/temporal. Los objetos de este nivel pueden ser ubicados en un mapa de acuerdo a diferentes representaciones (raster, vector, etc.) · Nivel de escenarios de simulación: Objeto-Hidrológico-Escenario. En este nivel los objetos tienen comportamiento hidrológico, es decir pueden simular diferentes procesos de la hidrología superficial, mediante distintos métodos. Se analizaron los procesos naturales involucrados y los métodos de simulación correspondientes a diferentes escenarios de modelos lluvia-escorrentía. 2. Diseño Se definieron microarquitecturas de diseño, en base a patrones OO, apropiadas para: · Composición flexible del sistema hidrológico real a representar. Se utiliza el patrón de diseño Composite (Gamma, 1997), que permite representar jerarquías todo-parte. · Representación espacial/temporal de los objetos de un sistema hidrológico real. Se utiliza el “Modelo para aplicaciones geográficas”, Gordillo, 1998. Este modelo propone utilizar el patrón Decorator (Gamma, 1997), el cual permite adicionarle características espaciales. Además resuelve la definición de la Locación del objeto (información referente a la posición y al tiempo) y considera la geometría y sistema de referencia utilizado para el objeto. · Adición de “comportamiento hidrológico” a los objetos del sistema de información seleccionados. Se utiliza el patrón de diseño Decorator, (Gamma, 1997) a los fines de adicionar comportamiento específico a los objetos del sistema de información. · Representación de procesos correspondientes a modelos conceptuales ESMA y modelos distribuidos basados en el proceso físico. Se utiliza el patrón de diseño Strategy, que define una familia de algoritmos, encapsula cada uno y los hace intercambiables · Integración de diferentes escenarios de simulación al nivel del Sistema de Información hídrica. Se utilizan los patrones Strategy y Mediator. Esta etapa del Proyecto aún no ha sido finalizada Sistema hidrológico real Modelo métodoRouting devQsal() Objeto_Hidro-Escenario SubcuencaCurso Almacen. devQsal() devQsal() devQsal() MétodoRouting propagarQ() Muskingum propagarQ()  Aplicación Patrón Strategy Otros ......... Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 3412 3. Diseño de Procesos utilizando algoritmos genéticos (Próxima etapa) En esta etapa del proyecto, se realizará el estudio de distintas técnicas de computación evolutiva y se utilizarán algoritmos genéticos para la optimización de parámetros en modelos lluvia-escorrentía. Conclusiones y trabajos futuros Los resultados obtenidos hasta el presente permiten establecer que mediante el uso de técnicas de modelación apropiadas, pueden ser resueltos gran parte de los problemas mencionados en relación a los sistemas de hidroinformática. En particular, el uso de patrones de diseño OO permite obtener: una forma conveniente de modelar información de tipo continua y su representación espacio/temporal, flexibilidad en la selección del método y/o modelo hidrológico a utilizar por parte de los distintos componentes de un sistema de información, composición flexible del sistema hidrológico real a modelar, facilidad para extender las aplicaciones a diferentes tipos de modelos hidrológicos y ambientales y modelos desarrollados en un contexto de sistemas de información ambiental. Durante la próxima etapa del proyecto, se pretende demostrar que los algoritmos genéticos constituyen una valiosa herramienta para el desarrollo de estos sistemas. En futuros trabajos se espera poder investigar sobre técnicas que permitan la incorporación a estos sistemas de restricciones legales, normativas vigentes y otras reglas; se considera que en este aspecto, sería conveniente analizar la utilización de Inteligencia Artificial en Hidroinformática.	﻿Hidroinformática , Técnicas apropiadas , Sistemas de Información Ambiental	es	21918
72	Integración de bloques IP en diseños SoC	﻿ La metodología de diseño de sistemas SoC basándose en bloques IP (Hard o Soft) presenta diversos retos, desde las cuestiones relativas a la síntesis y validación de cada bloque IP, hasta las características de integración física y verificación funcional del sistema completo. Nos propusimos abordar la temática del diseño de bloques IP basándonos en descripciones de hardware por lenguaje (en particular VHDL) y avanzar en los mecanismos para re-utilizar dichos bloques como parte de distintos sitemas. Introducción La complejidad del hardware ha crecido de modo sostenido en los últimos años a punto tal que es posible diseñar -y fabricar- computadores digitales completos (CPU, Memoria y E/S) o colocar múltiples procesadores en un único chip de silicio con diversas soluciones tecnológicas (por ejemplo: ASIC’s, CPLD’s, FPGA’s y Systems-on-Chip entre otras) y se proponen metodologías de diseño con distintos niveles de abstracción – flexibilidad que se basan en esas nuevas soluciones tecnológicas [1,2]. En la metodología de diseño System-on-a-Chip (SoC) el diseñador combinará bloques prediseñados y pre-verificados en un chip para implementar funciones complejas, poseyendo un limitado conocimiento de la estructura interna de esos bloques. Debe asegurarse entonces, el correcto diseño físico (temporizados, potencia, tamaño) y un comportamiento funcional verificable del sistema. Temas de Investigación y Desarrollo Son de interés los siguientes temas: · Integración de bloques IP en diseños SoC. El ensamble de un conjunto de bloques en un diseño jerárquico presentará varias opciones y por lo tanto podremos encontrar diferentes problemas que deberán ser administrados con distintas estrategias. La selección de un bien diseñado y bien documentado bloque IP reduciría el esfuerzo de integración [5,7]. · Verificación a nivel sistema. Las estrategias de verificación a nivel de sistema en un diseño SoC usan el esquema “divide y conquista” de un sistema jerárquico. Se comienza con la verificación de los                                                   1 Profesores Ordinarios Dedicación Exclusiva Workshop de Investigadores en Ciencias de la Computación WICC 2002 Página 40016 bloques elementales, luego las interfaces entre bloques y finalmente el sistema completo (chip). En particular éste último paso es una verificación funcional que se realiza con aplicaciones de complejidad creciente [6]. · Re-usabilidad de bloques IP. El temporizado, el consumo de potencia y las estructuras de test de los bloques “Hard IP” son tópicos definidos en la etapa de diseño del bloque propiamente dicho.y que facilitan o no su re-usabilidad.. Los mecanismos de interface previstos para cada bloque tambien constituyen un elemento definitorio de su re-usabilidad [6]. En todos los casos asumimos que la herramienta básica para generar un bloque IP será un Lenguaje como VHDL. En particular, utilizaremos para la ‘compilación a silició dispositivos lógicos programables complejos (CPLD). Algunos resultados obtenidos. Líneas de Investigación en curso. Ademas de los primeros estudios académicos mencionados en [1] y en los que se utilizó VHDL como herramienta de descripción, trabajamos en la idea de incorporar múltiples procesadores (bloques IP) en un único chip [3,4], surgiendo diferentes inconvenientes y por consiguiente problemas a resolver o, al menos, abordar. En particular nos centramos en el estudio de políticas para la verificación funcional de SoC, las características de interface entre bloques IP y el análisis mas profundo de las especificaciones de diseño para facilitar la aplicación de test a los bloques IP.	﻿bloques IP Intellectual Property , Arquitectura de Computadoras , SoC System on a Chip , VHDL	es	21941
73	Frameworks para el desarrollo de sistemas multi agentes	"﻿ En esk artículo se presenta un análisis de los frameworks Brainstorm/J y Framas  desarrollados en el Instituto de Sistemas ISIST AN. El análisis tiene como objetivo  determinar las ventajas y las limitaciones de cada uno de ellos en el desarrollo de  di""t""!""so' !inos de agentes inteligentes. Entre los tipos estudiados se encuentran agentes  f<<acti\!n"", cognitivos, y móviles.  :;ntroduccióf¡  Varias experiencias en el desarrollo de agentes han sido realizadas en los últimos años. ll'itas  han origifliidu la necc',idad del crecimiento del área de ingenieríh de software, espedali:r.ándcL.t  para d des'!m¡Jlo de sistema;.; multi-agentes. Erl este; Ct)mexto, arquitectllrtls de sOfrW:!f'  ambientes especializados para ei desarrollo de agentes fueron propuestos, juntamente ,:C,,1  metodologías especialmente dirigidas a este tipo de sistemas.  Dentro de las herramientas de desarrollo de agentes, algunos frameworks f!.leron realizados  <:llntemplandu generaimente caracteríí.ticas específicas tales como movilidad o reacción, Los  :'--'lnlr:'works permitee¡ no sólo el desarrollo de agentes con estas características sine· la  ,Ji: rliaci6n (1,; i:Ódigg). generalme.nte por especialización, paa mnldearlü a difert'ntee dC01!!'jP:':  ,.::) ;',te con!e""tG, do.. frameworks des3!""fi)iJacios er;o el 1nstituHl de Sissemas ¡ SIST 1"" N d:  '. Jn¡vcrr:idat! Nacional del Centro han sido rrsteados y comparados en este artículu.  Fstc artÍCulo está organizado de la siguiente manera. Seguidamente, dos secciones presentan los  JSpcctos m¡ís relevantes oe cada uno dé los fnmcworh. A partir de ellos un an:ílii:b ,>  prcc('niado ;\ !LlVés dI"" cemoaraci[lIll:s ('·""PIIcífi('as,  El Framwnrk FraMllS  Para él desarrollo de FraMas [Avancini OOJ se utilizó un enfoque hottom-up, es decir que  rril11err) se construyeron aplicaciones de agente:; particulares y se fuu ahstrayendo  ;'pmportamicnlo común de estas para generar las clases del framework.  :\ metodolngf,1 de rli',eño seleccionadil es b razón por Ié! cual e! framework no está bas:td:l en  ¡ iñllna arqujj,,,cturll de diseño particular. sino que consiste en !:n sistema orientado :: ohjj:;):··:  oi,)nde los rr:,t;lemas de diseño se solucionaron utilizando sólc patrones aplicables i,..icia[:n  ñk  a sistemas mulli-agentes particulares.  h1 .::stc framework. un agente posee un comport.:tmiento básico al cual :-;e le adic;ona  comp()rtal11it''11(l ¡nle :igemm>. l:s eSTel:ommortamient() d (iue define al ohjet(; é!l!en!"" C( l:r.n  i ntcligentc.  El fr:·unework posee una clase llamada Bl/sic.AgentActiol/s la cual dehe ser heredada peor el  objeto inicial del ag''ñte. El objeto inicial es aquel que proVG\! todo el compor:amicnt(. no  inteligente. o sea. el conjunto de acciones simples que es capaz de ejecutar cuando el  componente <le decisión lo indique.  i 'n Frrd\1:ls. e' comportamiento inteligente corresponde a la comunicación, selecció!l dd la  '1f/':xima ,1cl'Íón y aprendizaje de las preferencjaa; del usuario. Este comportamiento tí!""ico de  ::.I!enles ..:.:' :lIiir;nna por medio de decor:ldorc-s (también conocidos como wrappcr,,). Esta ma r1e""';  de agregar comportamiento a un agente está definida por el patrón de diseño Decoraior l Garrlma  lJ'i]_  I J patrón Decorato, también conocido como !V¡'llpper permite agregar dinár,1ici1m,,ñ!('  re:-;ponsahilldades a l'n objeto. Lo:,; de.:oradores provven una alternativa tlcxihle (J la herenc;;¡  para exlcnc1er funcionalidad. En !a figura 1 se presenta la estructura genérica del patrón  f)cco/""ll!or_ 1\ nartir de este patrón de diseño, las acciones básicas que un agente puede rea!izar  Wicc 2000 - lJ4  encapsuladas en un objeto BasicAgentActions puede ser decorado dinámicamente con otros  objetos que son responsables de la comunicación entre agentes, el manejo de preferencias, las  reacciones, las decisiones deliberativas, etc.  Component  ·operationO ' 1     I I  ConcreteComponent Decorator +component   operationO  opePationO .................................. ...................  component. operationO;  lf  I 1  ConcreteDecoratorA ConcreteDecorator8  l_addedState  operronO··············   operationO  addedBehaviorO  .--------""..  superO;  ¡  added8ehaviorQ;  Figura 1. Bases del Framework FraMaS.  El Framework Brainstorm/J  Ei framework Brainstorm/J [Zunino 00] ha sido desarrollado en el lenguaje Java siguiendo los  lineamientos de la arquitectura de agentes Brainstorm [Amandi 97]. Esta arquitectura prescribe  agentes compuestos de un objeto base responsable de las acciones simples y determinísticas de  los agentes y un conjunto de meta-objetos que interfieren en su computación haciendo que estas  acciones sean combinadas convenientemente para alcanzar objetivos específicos, o para  adicionar funcionalidad inherente a agentes inteligentes.  Las ventajas de este enfoque es que no son necesarias modificaciones en el código para  modificar la estructura de un agente, ya que todo comportamiento es administrado a través de  retlexión computacional.  La figura 2 muestra una estructura básica de la arquitectura Brainstorm, la cual fue mapeada a  Java por el framework Brainstorm/J.  Figura 2. Esquema parcial de la arquitectura Brainstorm.  Wicc 2000 - 95  Comparación  Ambos frameworks permiten construir sistemas multi-agentes en el lenguaje Java. La primera  diferencia es su concepción. Brainstorm/J mapea componentes arquitecturales de Brainstorm al  lenguaje Java, teniendo que soportar para ello un sistema de meta-objetos. FraMaS es concebido  p1rt!r de la abstracción de implementaciones específicas. Los resultados de las  experimentaciones han mostrado que FraMaS tiene una orientación más marcadas haci2. agentes  de interfaz, resultado proveniente de la utilización de implementaciones en este contexto en la  abstracción de clases del framework.  En cuando a su funcionalidad, la tabla presentada a continuación resume varios aspectos de  estos frameworks.  l- Descripción Brainstorm/J --FraMaS  l Comunicación Mensaje send esplícito Mensajes entre objetos  i Movilidad Dévil (uso de mü) Dévil (uso de rmi)  I Deliberación Manejo de conversaciones Soporte para incorporación de I ! Estrategias deliberativas estrategias deliberativas.   .. __ .- !  ¡ Reacción Uso de templates Implementación esxplícita I [  Aprendizaje Interferencia del meta-objeto Implementación explícita ----, I  I    learnmg en el prooeso  deliberativo  !  Tabla 1. Comparación de Brainstom/J y FraMaS.  !  ;  La tabla anterior muestra a!gunos aspectos de funcionalidad junto con su solución. Se puede  obseIVar que FraMaS implementa una forma de comunicación más natural en ambiente  orientados a objetos, pero ofrece menos soportes en aspectos deliberativos.  Otros puntos importantes considerados en el análisis son el tiempo de respuesta a un pedido  determinado y la complejidad de modificación de las estructuras comportamentales de los  agentes en tiempo de ejecución. El resultado del análisis es el siguiente:  ¡--------:---;--:-----r-------;;;:----:-----'-;;-------'¡-----;;:;-----;;;-----l  Descripción I Brainstorm/J ¡ FraMaS  Tiempo de resp. ¡ 10% más por uso de meta-objetos I 4% más por delegación  Modificabilidad I Transparente I Control de wrappers  Tabla 2. Usabilidad de los frameworks.  Conclusiones  En este artículo fue presentado un análisis comparativo entre dos frameworks para sistemas  multi-agentes que fueron concebidos utilizado dos metodologías diferentes: abstracción por  ejemplos y dirección por una arquitectura específica. Este análisis conjuntamente con las  exneriencias realizadas han mostrado la utilidad de cada uno de ellos en diferentes contextos de   ....querimientos de sistemas de agentes."	﻿Sistemas Multi agentes , frameworks Brainstorm J y Framas	es	22149
74	Tecnologfa multimedial aplicada al Análisis de aptitudes y entrenamiento inicial en Informática	"﻿Tecnologfa multimedial aplicada al Análisis de aptitudes y entrenamiento  inicial en Informática  Cristina Madoz1, Gladys Gorga1,Armando De Giusti1, Raul Champredonde3  Motivación  LID!. Laboratorio de Investigación y Desarrollo en Informática.  Facultad de Informática. UNLP.  50 Y 115. 1 er Piso. La Plata  La iniciación de una carrera universitaria en general y en Informática en particular refleja una  serie de dificultades de los alumnos que se podrían sintetizar en 5 puntos:  • Falta de una adecuada orientación vocacional.  • Poco entrenamiento en pensar y expresar rigurosamente conceptos.  • Dificultad de aprendizaje de los temas básicos.  • kscasa valoración por el trabajo sistemático.  • Gran disparidad de conocimientos y formación previa  Normalmente frente a alumnos con estas dificultades, los primeros cursos de una carrera  universitaria de Informática intentan introducir la programación estructurada (desde un paradigma  imperativo), fundamentada en la expresión y especificación lógica de algoritmos. El resultado general  es una alta tasa de deserción.  Por otro lado, se debe tener en cuenta la escasa práctica y evaluación de actividades  relacionadas con la ciencia informática en la currícula de los cursos pre universitarios. Esta actividades  no le resultan suficientes al alumno para descubrir si reúne las aptitudes específicas para una futura  actividad profesional en el campo de la Informática. Cabe destacar, por otra parte, que la mayoría de  los temas impartidos en este ciclo del proceso de aprendizaje resultan de carácter informativo para el  alumno. Por ejemplo el conocimiento y manejo de productos tales como procesadores de texto,  planillas de cálculo, etc, tienen al alumno como un mero espectador en ese proceso de aprendizaje.  Una de las metas propuestas es que el proceso de aprendizaje tenga carácter formativo, es  decir, que el alumno deje de tener una actitud pasiva y se convierta en protagonista de este proceso, y  consecuentemente desarrolle capacidades creativas que le serán indispensable en su futura vida  profesional.  En este contexto el tema multimedial ha tenido un auge muy importante en los últimos años y  promete ser una herramienta poderosa que facilite la motivación, la definición vocacional y el  descubrimiento de aptitudes para resolver problemas del mundo real utilizando una computadora.  Los resultados de numerosas investigaciones han afirmado que los jóvenes aprenden más de la  mitad de lo que saben a partir de información visual, por lo tanto nuestro desafio es construir  herramientas de enseñanza - aprendizaje que contengan imágenes y multimedia.  1 Profesor Adjunto Dedicación Exclusiva.  1 Investigador Principal CONICET. Profesor Titular Dedicación Exclusiva.  3 Jefe de Trabajos Prácticos Dedicación Exclusiva. Becario CONICET.  LIDI - Facultad de Informática. UNLP - Calle 50 y 115 ler Piso, (1900) La Plata, Argentina.  TElFax +(54)(221)422-7707. http://Iidi.info.unlp.edu.ar  Wicc 2000 - 194  Este concepto está basado en la Teoría de las Múltiples Inteligencias, propuesta por Howard  Gardner quién sostiene que las personas deben ser tratadas teniendo en cuenta que pueden estar  dotadas de diferentes tipos de inteligencia desarrolladas en mayor o menor grado. Identifica  inicialmente siete tipos de inteligencia: verbal/lingüística, visuaVespacial, musical, lógica/matemática,  corporal/cinestésica, interpersonal, intrapersonal. Gardner argumenta que la enseñanza más efectiva  debe incorporar las siete formas de inteligencia. La teoría establece que todas las formas de  inteligencia son necesarias para que la persona pueda funcionar productivamente en la sociedad.  Tener en cuenta todos estos aspectos en una herramienta de enseñanza - aprendizaje presenta  numerosas ventajas: un clima positivo que apoya, motiva y promueve el éxito del alumno, una  variedad de estrategias instruccionales perfeccionadas y extendida, una diversidad de formas para  abordar el aprendizaje, la renovación del sentido de profesionalismo.  Línea de Investigación y Desarrollo  Todos los aspectos presentados en el punto anterior nos condujeron a plantear la necesidad de  desarrollar una herramienta multimedial interactiva.  Los problemas referidos a la transición secundaria - universidad, así como la necesidad de  nivelación de los alumnos en un contexto motivador, nos llevaron a realizar una serie de trabajos, uno  de los cuales se convirtió en un Proyecto aprobado por la Secretaria de Extensión Universitaria de la  UNLP y que se refiere a desarrollar el Curso de Ingreso sobre un ambiente multimedial interactivo y  probar su utilización como complemento y/o alternativa del método tradicional de enseñanza en el  curso de ingreso.  En una segunda etapa se incorporó a esta herramienta un lenguaje y ambiente visual para la  enseñanza de programación y de este modo el alumno puede trabajar directamente desde el ambiente  con el lenguaje visual y recíprocamente. Esta asociación tiene como objetivo que el alumno pueda  escribir su solución sobre la computadora, ver los resultados de la misma y verificar la solución  propuesta.  Actualmente en el Colegio Nacional dependiente de la Universidad Nacional de La Plata  como otras escuelas de universidades del interior de nuestro país, participan de esta experiencia, en la  que la herramienta propuesta forma parte de la currícula. Es un objetivo a largo plazo que esta  herramienta llegue a utilizarse en todos los cursos pre universitarios para alcanzar a cumplir la meta  que nos propusimos al principio. Es decir, que el proceso de aprendizaje tenga un carácter formativo y  de esta manera permitir descubrir las aptitudes y los aspectos vocacionales en el alumno.  Por otro lado, a principios de 1999, se tomó la decisión de incorporar este recurso multimedial  a Internet. El objetivo de esta decisión es que todos aquellos alumnos interesados en la carrera tuvieran  la posibilidad de interactuar con esta herramienta sin tener la necesidad de estar fisicamente en la  Facultad. Por otra parte, se debe tener presente que el acento de este recurso está puesto en descubrir  aptitudes y afianzar conocimientos ya adquiridos. Se cree que los alumnos ""a distancia"" podrán tener  entonces las mismas posibilidades de disponer de esta información.  Wicc 2000 - 195  Algunos resultados  En el Curso de Ingreso 2000 a la Licenciatura en InfoIlllática se utilizó de modo masivo la  herramienta propuesta con especial interés en el lenguaje visual.  Por otra parte se llevó a cabo una experiencia piloto con un grupo de alumnos ingresantes a  Informática, que motivó el trabajo ""Línea de Investigación de Desarrollo y Evaluación de Lenguaje y  Metodología de Enseñanza de Programación"", presentado en este encuentro.  Aplicaciones futuras  Si bien nuestra experiencia ha sido desarrollada en el ámbito académico de nuestra Facultad,  resulta claro que las aplicaciones de herramientas multimediales interactivas pueden ser de suma  utilidad en otras disciplinas.  Por otra parte no podemos dejar de lado la utilización masiva de la Internet, donde las  tecnologías basadas en computadoras comienzan a resultar útiles para la producción de aprendizaje y/o  entrenamiento con una modalidad distinta a la tradicional. Nuestra idea es experimentar modalidades  semi presenciales con sitios Web interactivos, para la educación en Informática y entrenamiento a no  informáticos.  En gran medida la evolución continua del mundo laboral en cuanto a métodos y herramientas  de organización y producción requiere que los empleados ( ""usuarios"" de la tecnología) renueven y  actualicen sus conocimientos y estén preparados para enfrentar los cambios con flexibilidad mental y  actitud positiva, de modo de favorecer la capacitación y actualización de personas de ambientes no  informáticos."	﻿entrenamiento inicial en Informática , Tecnología multimedial , Análisis de aptitudes	es	22183
75	Sistemas distribuídos de tiempo real	"﻿ o las restricciones de la especificación (en muchos casos a partir de una activación  asincrónica).  La evolución tecnológica en el tratamiento de señales (locales o remotas) y en los  sistemas de comunicaciones ha impulsado enormemente esta área temática, sobre  todo en los aspectos de planificación y desarrollo de software para Sistemas  Distribuidos de Tiempo Real [HAT88][LAP93][SHU92] .  Como cualquier sistema basado en computadora, un SDTR debe integrar software,  hardware, personas, bases de datos, no solo para cumplir con los requisitos  funcionales del sistema, sino tambien con los requerimientos de rendimiento.  Las dificultades principales del desarrollo de software para sistemas de tiempo real  son:  • Controlar hardware en forma directa  • Procesar mensajes que arriban en forma asincrónica, con diferentes velocidades y  diferentes prioridades  • Detectar y controlar condiciones de falla. Prever diferentes grados de recuperación  del sistema.  • Manejar colas y buffers de almacenamiento y mensajes.  • Modelizar condiciones de concurrencia en un conjunto apropiado de procesos.  • Asignar procesos lógicos a procesadores fisicos (si se dispone de ellos).  • Manejar las comunicaciones inter-procesos e inter-procesadores.  • Proteger datos compartidos por procesos concurrentes.  • Organizar (schedule) y despachar la atención de procesos.  • Manejar las restricciones de tiempo y performance.  • Relacionarse con un reloj de tiempo real y evitar desfasajes de procesadores tiempos.  • Testear y poner a punto un sistema que normalmente esta distribuido en diferentes  procesadores.  • Elaborar herramientas de software que permitan simular o emular dispositivos o  eventos de hardware no disponibles en el desarrollo.  • Reducir y estructurar los requerimientos.  • Seleccionar la estructura de hardware adecuada.  Cada una de las actividades del ciclo de vida de los sistemas necesita de técnicas  propias o extendidas para hacer frente a la descripción de este tipo de sistemas:  • En la etapa de planificación es importante contar con estimaciones fiables que  permitan proyectar efectivamente el proceso de desarrollo. La recolección de  métricas que conformen una línea base para la gestión de proyectos de  características comunes.  • En la etapa de análisis deben lograrse especificaciones que permitan represe' Itar  los flujos y procesamientos de control, los flujos de información que son producidos  o recogidos continuamente, las ocurrencias múltiples de la misma transformación,  los estados del sistema y mecanismos que producen las transiciones,  • En la etapa de diseño debe tenerse en cuenta la coordinación entre las tareas, el  procesamiento de interrupciones, el manejo de E/S (que asegura la no pérdida de  datos), la especificación de las ligaduras de tiempo externas, el aseguramiento de  la consistencia de su base de datos.  • En la etapa' de pruebas del sistema deben diseñarse casos de prueba para el  tratamiento de eventos (ej: proceso de interrupciones), la temporización de los  datos, el paralelismo de las tareas. La estrategia de pruebas puede implementarse  a través de la prueba de tareas, de comportamiento, intertareas y del sistema.  • Las herramientas CASE permiten crear modelizaciones para simular el  comportamiento de los sistemas de tiempo real como consecuencia de sucesos  externos, debido a que no siempre es posible probar sobre el sistema.  Objetivos de la línea de investigación  En el marco de los proyectos de investigación del LlDI sobre Sistemas de Tiempo Real  y Sistemas Distribuidos en Automatización de Oficinas se han realizado varias  experiencias de desarrollo de Sistemas Distribuidos de Tiempo Real para áreas de  aplicación muy diferentes.  • Desde el punto de vista de las especificaciones se han desarrollado herramientas  de derivación automática de código, en particular especificaciones en lenguaje  Estelle y CSP derivadas a código ADA ejecutable. También desde Redes de Petri  extendidas se ha derivado prototipos de tareas en ADA. Se han aplicado estos  trabajos especialmente a la concepción y validación de protocolos.  • Dado que el desarrollo de sistemas de tiempo real implica la aplicación de los  conceptos de Ingeniería de Sistemas, se han analizado las extensiones a las  metodologías estructuradas propuestas por Keller y Shumate y posteriormente se  aplicaron las mismas a la modelización y desarrollo de la Ingeniería de Software de  un sistema de supervisión y control de cables telefónicos, que debe detectar cortes  en tiempo real, incluyendo la ubicación de los mismos y el aviso a la unidad policial  correspondiente en menos de 3'.  :,)  :)     :}     ,,  ,, 1,  , 1""  ,,  ,,     ,,  ..  ..  ,,  ..  '.    .J  ,J  J  ,J  ,,)  J  J  J  ,,)  "" J  J  J  J  ,)  ,)  ,)  .>  ,)  ,J  ,)  )  .:>  '\  ,¡I'  "" ..  ""  i  Cada una de las actividades del ciclo de vida de los sistemas necesita de técnicas  propias o extendidas para hacer frente a la descripción de este tipo de sistemas:  • En la etapa de planificación es importante contar con estimaciones fiables que  permitan proyectar efectivamente el proceso de desarrollo. La recolección de  métricas que conformen una línea base para la gestión de proyectos de  características comunes.  • En la etapa de análisis deben lograrse especificaciones que permitan representar  los flujos y procesamientos de control, los flujos de información que son producidos  o recogidos continuamente, las ocurrencias múltiples de la misma transformación,  los estados del sistema y mecanismos que producen las transiciones.  • En la etapa de diseño debe tenerse en cuenta la coordinación entre las tareas, el  procesamiento de interrupciones, el manejo de E/S (que asegura la no pérdida de  datos), la especificación de las ligaduras de tiempo externas, el aseguramiento de  la consistencia de su base de datos.  • En la etapa de pruebas del sistema deben diseñarse casos de prueba para el  tratamiento de eventos (ej: proceso de interrupciones), la temporización de los  datos, el paralelismo de las tareas. La estrategia de pruebas puede implementarse  a través de la prueba de tareas, de comportamiento, intertareas y del sistema.  • Las herramientas CASE permiten crear modelizaciones para simular el  comportamiento de los sistemas de tiempo real como consecuencia de sucesos  externos, debido a que no siempre es posible probar sobre el sistema.  Objetivos de la línea de investigación  En el marco de los proyectos de investigación del LlDI sobre Sistemas de Tiempo Real  y Sistemas Distribuidos en Automatización de Oficinas se han realizado varias  experiencias de desarrollo de Sistemas Distribuidos de Tiempo Real para áreas de  aplicación muy diferentes.  • Desde el punto de vista de las especificaciones se han desarrollado herramientas  de derivación automática de código, en particular especificaciones en lenguaje  Estelle y CSP derivadas a código ADA ejecutable. También desde Redes de Petri  extendidas se ha derivado prototipos de tareas en ADA. Se han aplicado estos  trabajos especialmente a la concepción y validación de protocolos.  • Dado que el desarrollo de sistemas de tiempo real implica la aplicación de los  conceptos de Ingeniería de Sistemas, se han analizado las extensiones a las  metodologías estructuradas propuestas por Keller y Shumate y posteriormente se  aplicaron las mismas a la modelización y desarrollo de la Ingeniería de Software de  un sistema de supervisión y control de cables telefónicos, que debe detectar cortes  en tiempo real, incluyendo la ubicación de los mismos y el aviso a la unidad policial  correspondiente en menos de 3'.  • La necesidad de contar con estimaciones tempranas para sistemas de tiempo real  ha impulsado al estudio de las métricas de Punto Función extendidas. Resultados  de estas investigciones se aplicaron a la evaluación de un sistema distribuido de  comunicaciones utilizado en entrenamiento militar.  • La complejidad de los sistemas a modelizar hace necesaria la utilización de  herramientas CASE . Se ha investigado en la especificación de herramientas  CASE orientadas a tiempo real, incluyendo desarrollos experimentales.  • La ilecesidad de poder especificar y verificar restricciones ae tiempo hace  necesario incorporar a las herramientas de análisis y diseño, la posibilidad de  modelar las especificaciones temporales y poder verificarlas mediante simulación.  Para esto se desarrollaron dos ambientes experimentales centrados en Redes de  Petri extendidas.  Mecanismos de especificación y verificación de restricciones de tiempo  ..  Existen numerosos enfoques que tienden a la especificación y verificación de  restricciones de tiempo entre los que se cuentan los lenguajes de simulación, las  aproximaciones axiomáticas y las Redes de Petri.  En particular las Redes de Petri son un elemento útil para modelar sistemas en los que  existen dependencias secuenciales dinámicas y consideraciones de procesamiento  concurrente, por las siguientes características:  • Son inherentemente paralelas y soportan eventos asincrónicos.  • Permiten una representación explícita de las dependencias causales y de las  interdepedencias entre procesos.  • Permiten la descripción de un sistema en diferentes niveles de abstracción  • Favorecen la verificación y en particular la detección de situaciones críticas (por  ejemplo deadlocks).  • Soportan extensiones para manipular temporizaciones aleatorias y/o  determinísticas.  Para poder utilizarlas en modelizaciones de Sistemas de Tiempo Real se incorporan  restricciones de tiempo y se han extendido las clases de arcos del grafo de Petri para  simular condiciones de funcionamiento. De esta manera pueden modelarse  fenómenos tales como tiempo entre llegadas de eventos externos, tiempos de  reacción, duración de actividades, tiempos de timeout, acciones periódicas, etc.  Actualmente se está investigando la incorporación de las Redes de Petri extendidas a  ambientes CASE orientados a tratamiento de sistemas de tiempo real y asimismo se  están estudiando variantes (en particular Queued Petri Nets y la incorporación de  atributos a los tokens) para analizar mejor fenómenos de espera y de manejo de datos  distinguidos dentro del sistema."	﻿Sistemas Distribuídos , Tiempo Real , restricciones de tiempo	es	22203
76	Una metodología para desarrollar aplicaciones usando programación por restricciones	"﻿ En los últimos años la programación por restricciones ha automatizado la solución de_  problemas combinatorios complejos en muchos dominios tan diversos como planificación,  asignación de recursos, optimización, etc,  ....  En este trabajo describiremos una metodología general que permite desarrollar  programas utilizando la tecnología de restricciones.  En la programáción por restricciones un problema se representa involucrando  incertidumbre, esto es., sus variables, .. y- las restricciones sobre estas variables. En otras  palabras, la parte desconocida del problema se representa con variables restringidas. De  este modo, para cualquier problema dado, su representación consiste en la declaración de  las variables y la colooación de las-restricciones sobre ellas. La resolución de un problema  consiste en encontrar un valor para cada variable de modo que se satisfagan todas las  restricciones. Debido a que es posible que exista más .de una solución para un problema  dado, la elec_ción de una solución, normalmente se determina de acuerdo a un criterio de  optimización.  Para capturar la representación del problema, la metodología propuesta, utiliza la  programación orientada a objetos [Booch, 91], mediante el. uso de clases. Algunas de estas  clases representan, variables restringidas, valores para los dominios de estas variables  restringidas, restricciones, y algoritmos de solución.  De esta manera con la combinación de estos dos paradigmas obtenemos por el lado de la  programación por restricciones eficiencia y declaratividad para abordar y manejar  problemas industriales complejos en los cuales existe una gran explosión combinatoria  [Hente,89] [Jaffa, 87J. Mientras que por el lado de la programación orientada a objetos  obtenemos la facilidad. para capturar abstracciones y mecanismos que se encuentran en el  dominio del problema.  Este trabajo fue desarrollado en el marco del Proyecto CICITCA ""Problemas de Asignación de  Recursos utilizando Técnicas de Inteligencia Artificial""  1.1 Introducción  El objetivo del este trabajo es proponer una metodología para desarrollar aplicaciones  utilizando la tecnología de restricciones.  La metodología utiliza una arquitectura de clases, la cual provee los componentes  necessrios para programar con restricciones. Algunas de estas clases representan, variables  restringidas, valores para los dominios de estas variables restringidas, restricciones, y  algoritmos de solución, la descripción de los componentes se realiza a un nivel lo  suficientemente abstracto que nos permite comprender las actividades que implica la  metodología. Por esta razón se necesitan tanto los conceptos de programación por  restricciones como de los de la programación orientada a objetos, para poder aplicarla.  Las aatividades que componen la metodología son las siguientes:  1. Escribir en lenguaje natural la descripción del problema para clarificar el propósito de  la aplicación y las restricciones involucradas.  2. Diseñar una representación para el problema -un modelo para el problema- basado en la  descripción. En la representación, declaramos lo desconocido en el problema (variables  restringidas) y le colocamos restricciones. Por último se deben establecer los  mecanismos de búsqueda.  3. Usar clases y funciones provistas por la arquitectura mencionada para implementar el  modelo.  4. Diseñar los mecanismos de búsqueda para encontrar las soluciones.  Es importante destacar que la metodología propuesta contiene sólo pautas generales  para abordar un problema, sin embargo en problemas reales éstas se deben extender para  resolver la complejidad que los mismos presentan.  También es necesario aclarar que en problemas industriales la ejecución de todas las  actividades no produce de inmediato una solución para el problema, sino cada actividad  puede producir conocimiento tanto para una la actividad que le anteceda, como para una  que le preceda, el flujo de conocimiento entre las actividades no es unidireccional sino  bidireccional. Así un modelo que solucione el problema es el resultado de la interacción  entre las distintas actividades.  Luego de introducir los conceptos de programación por restricciones y describir la  arquitectura de clases antes mencionada, se plantea un ejemplo pedagógico que permitirá  describir y explicar los distintos pasos de la metodología planteada.  1.2 Descripción de un Problema - Actividad Nro 1  Para explicar cada paso de la metodología se resuelve un problema conocido. El problema  de colorear un mapa, este involucra elegir los colores para los distintos países en el mapa,  de tal manera que a lo sumo se usen solo cuatro colores y que ningún país colindante tenga  el mismo color. Consideremos los seis países Bolivia, Uruguay, Brasil, Argentina, Chile y  Paraguay. Este problema queda bien definido con esta descripción, pero en los problemas  reales necesitan una descripción más elaborada con el objeto de construir posteriormente el  modelo que la representara. Ente los aspectos relevantes de esta descripcion detallada se  encuentra los objetivos, la descripción general , las consideraciones generales, las  consideraciones sobre la complejidad, la entrada detallada y la salida detallada. Para  problemas complejos como problemas de scheduling una opción para abordar la  complejidad es descomponer el problema en una serie de problemas de menor complejidad  Esta descomposición es un proceso iterativo donde un problema, es el resultado de la  evolución de un problema más simple.  Problema objetivo  Primera simplificación  Problema objetivo  -DSegunda simplificación  Problema objetivo  1.3 Diseño de la Representación del problema - Actividad Nro 2  Para problemas simples como el propuesto en el ejemplo bastará con lo siguiente:  • Declaración de las variables restringidas (ó variables dominio).  • Colocación las restricciones.  • Búsqueda de soluciones.  En la práctica sobre problemas complejos el diseño de la representación no es tan fácil,  no solo se hace necesario aplicar el análisis y diseño orientado a objeto [Booch, 91], sino  tambien es necesario coordinar el mismo con una arquitectura subyacente que soporta  programación por restricciones. Por ejemplo para solucionar un problema de scheduling  [Ddiaz,98] se deben combinar estos tres pasos en un modelo objeto, este modelo se debe  dividir a su vez en dos grandes categorias de clase, una que nos permita representar el  problema y otra que articule los medios para solucionar el mismo. Además, el diseñador  también debe tener en cuenta con qué artefactos de software cuenta para poder implementar  los mismos, esto le permite por un Jado, economizar una gran parte del tiempo que  consume la creación de los mecanismos de búsqueda. Por otro lado, el beneficio de pensar  en la arquitectura subyacente es el de articular las entidades creadas de manera que, la sola  colocación de restricciones produzca una gran poda a priori del arbol de búsqueda. Otra  necesidad que debe cubrir el diseñador tiene que ver con el tiempo de respuesta para  encontrar una solución. Esto es un requerimiento muy solicitado por los usuarios.     1.3.1 Declaración de las variables restringidas  Las variables dominio son el medio para representar lo desconocido en este problema- el  color ppra los _países - como variables dominio. Para cada país necesitaremos una variable  domm cUy015 -PJJJibles valores serán 0, 1, 2 Y 3, que representan respectivameote los  colorés azul: blañco, rojo y verde.  Variables dominio':  Bolivia(0,3), Uruguay(0,3), Brasil(0,3),  Argentina(0,3), Chile(0,3), Paraguay(0,3);  1.3.2 Colócando Rostricciones  .  La restricción ""distinta de"" nos permite expresar la idea de que dos variables no pueden  asumir el mismo valor En nuestro ejemplo usaremos esta técnica para tratar el hecho que  cuando dos países son fronterizos, ellos no pueden tener el mismo color. Para el ejemplo,  las siguiente sentencia indica que Brasil y Bolivia no pueden tener el mismo color pues son  fronterizos.  (Brasil != Bolivia)  Para coiocc restricciones, lo haremos utilizando la palabra Colocar, que se usa de la  siguiente manera:  / / Restricciones  Colocar( Brasil != Bolivia);  Colocar( Brasil != Paraguay);  Colocar( Brasil !=-Argentina);  Colocar( Paraguay- r= Argentina);  Colocar( Paraguay != Bolivia);  Colocar( Bolivia != Chile);  _Colocar( Argentina != Chile);  . Colocar( Argentina != Uruguay);  1.3.3 Búsqueda de Soluciones  Generalmente, una simple colocación de restricciones produce una poda del espacio de  soluciones, pero esto en general, no es suficiente para obtener soluciones. De esta manera,  la simple colocación de restricciones normalmente abre una puerta a todas las posibles  solucismes. Cuando queremos encontrar soluciones, necesitamos algo un po,:o más  específico. _  Una de las características de programación por restricciones es que la asignación de  valores a las variables no se realiza ciegamente, más bien, se.produce una propagación de  las restricciones sobre los dominios de las variables. De esta manera se explotan los efectos  de los cambios en algunas variables sobre otras variables que tiene el problema. Los efectos  de la propagación de restricciones es la reducción de los dominios de las variables. De este  modo las restricciones se pueden usar activamente para restringir los posibles valores de  las variables.tan pronto como sea posible. Así, los algoritmos proveen un camino eficiente  para encontrar una solución rápidamente.  Para nuestro ejemplo, la búsqueda de soluciones consiste en elegir una variable que no  esté instanciada (que no posea un valor asignado) e instanciarla (asignarle un valor), y  repetir el proceso hasta agotar las variables. Debido a los efectos de la propagación de  restricciones, puede ocurrir que no se necesiten instanciar todas las variables, sólo una  pequeña parte de ellas.  1.4 Usar clases y funciones provistas por la arquitectura mencionada para  implementar el modelo. - Actividad Nro 3  Para describir esta etapa es necesario describir la arquitectura sobre la cual se implementará  el modelo.  1.4.1. Descripción de la arquitectura.  La arquitectura que se describe a continuación está basada en el modelo de objetos de la  herramienta de llog, denominada Ilog-Solver [ILOG,Sol-R] [ILOG,Sol-U] [ILOG,Sch­ R] [ILOG,Sol-U].  La arquitectura se fundamenta en dos grandes categorías de clases:  • La categoría de clases para la representación  Esta contiene las clases que permiten capturar el conocimiento sobre el dominio del  problema Las abstracciones más importantes de esta categoría de clases es la  denominada variable dominio. Con cada una de estas variables restringidas se  asocia un conjunto de posibles valores llamado, el dominio de la variable. Cuando el  dominio de una variable contiene un solo valor, se dice que la variable está  instanciada.  • La categoría de clases para la solución del problema  LL resolución de un problema consiste en seleccionar un valor perteneciente al dominio  de cada variable de modo que se satisfagan todas las restricciones.  Convenciones para nombres  Los nombres para las variables globales como así también los nombres de las clases se  escriben concatenadamente con la letra de comienzo de cada palabra en mayúscula. Por  ejemplo:  • LisilntVar  Una letra en minúscula comienza los nombres de los argumentos, instancias, y funciones  miembros Por ejemplo  aVar  LisilntVar: :getValue  Además suponemos que estas clases están implementadas sobre el lenguaje de  programación C++ De esta manera, la sintaxis y semántica utilizada es la el lenguaje  C++  1.4.1.1 Clases que representan tipos de datos básicos  Lisilnt  LisiAny  LisiBool  Este tipo representa enteros con signo que son manejados por las variables  Este tipo representa un puntero a cualquier objeto  Este tipo representa valores Booleanos; estos valores son False y True.  1.4.1.2 Clases que representan variables restringidas y expresiones  Una variable restringida es una variable que posee un dominio que define los valores  permitidos para la misma. Una variable restringida (también conocida como variable  dominio) no puede instanciarse o ligarse con ningún valor fuera de los que el dominio 4UC  contiene. Nuestra arquitectura sólo contiene variables restringidas en el dominio de los  enteros.  Una variable restringida entera es aquella cuyo dominio está compuesto por valores  del tipo Lisilnt (enteros). Su dominio se puede representar por un intervalo cuando los  valores son consecutivos y por enumeración de enteros cuando no son consecutivos. La  clase para variables restringidas enteras es LisilntVar.  Las variables restringidas enteras se pueden combinar con operadores aritméticos  confomlando expresiones restringidas enteras. La clase LisintExp es la clase raíz para las  expresiolles restringidas enteras. Una variable restringida entera es en si lmsma una  expresión restringida entera. La clase bisilntVar es una subclase de LisilntExp.  Reversibilidad. Todas las funciones y funciones miembros definidas para la clase  LisilntExp y para la clase LisilntVar son reversibles. En particular, los cambios hechos  usando las funciones de colocación de restricciones son llevados a cabo con asignaciones  reversibles. Por lo tanto, el valor, dominio, y restricciones colocadas sobre cualquier  variable restringida son restablecidos cuando se realiza backtracking.  1.4.1.3 Expresiones Restringidas Enteras LisilntExp  Variables restringidas enteras y enteros ordinarios ( instancias de la clase Lisilnt) se pueden  combinar conjuntamente para formar expresiones. Todo desarrollador puede construir  expresiones aritméticas mediante la combinación de constantes númericas y variables  restringidas usando los operadores suma + , resta -, multtplicación *, y división l.  Cada expresión restringida entera tiene un mínimo y un máximo. Decimos que la expresión  esta ligada, o equivalentemente, instanciada, si su mínimo es igual a su máximo.  El dominio de una expresión restringida entera se puede reducir hasta el punto de llegar  a ser vacIO.  Existe más de un constructor para crear expresiones restringidas enteras, pero el  operaeor = (igual) es el mas usado para crearlas. Por ejemplo. sean dos variables dominio x  e y, el camino para crear la expresión suma es el siguiente:  LisilntExp sum = x + y;  Un segundo camino es el siguiente:  LisilntExp sum;  sum = x + y;  1.4.1:4 Variables Restringidas Enteras LisilntVar  Las variables restringidas enteras están representadas por la clase LisilntVar, el dominio de  una variable restringida entera contiene valores del tipo Lisilnt. Los dominios se  representan por un intervalo cuando los valores son consecutivos, o por enumeración  cuando no son consecutivos.  Constructores  LisilntVar (Lisilnt min, Lisilnt max , char* name). Contruye una variable dominio  cuyo dominio esta comprendido entre los valores min y max inclusive. Ej LisilntVar  prueba ( 0, 4) - variable dominio con dominio {O, 1, 2, 4}  Funciones de Acceso  Lisilnt getMinO. Retoma el menor valor de dominio de una variable restringida ( V.R.)  Entera. Ej Lisilnt domMin = prueba.getMinO;  Lisilnt getMaxO.Retoma el mayor valor de dominio de una V.R. Entera. Ej Lisilnt  domMax = prueba.getMaxO;  Lisilnt getValueO. Retoma el valor de una V.R. entera  LisiBool isBoundO. Retoma LisiTrue si la V.R. entera esta ligada, o lo que es lo mismo,  su mínimo es igual su máximo.  Funciones Modificadoras  Una función modificadora reduce el dominio de una expresión restringida entera, si es que  puede-hacerlo  setValue(Lisilnt value). Remueve del dominio de la V.R. todos los valores que son  diferentes de value. Esta acción dispará una propagación de restricciones, podado los  dominios de las variables que estan relacionados con dicha variable a la que se le aplica  SetValue  1.4.1.5 Restricciones  El me.canismo fundamental por el cual esta arquitectura permite encontrar soluciones son  las restricciones. Estas son condiciones presentes en todo momento luego de que la  restricción es tomada en cuenta por el programa, o sea cuando la restricción es colocada.  Una restricción esta ligada a una o más variables. Los dominios de todas las variables  restringidas del programa deben ser consistentes, lo cual es equivalente a cumplir todas las  restricciones colocadas. Por cada cambio que sufre una variable, se revisan todas sus  restricciones asociadas y en post de conservar consistente los valores de su dominio con las  restricciones asociadas, se puede producir una reducción en su dominio. Esta reducción  puede-producir también una reducción en los dominios de las variables asociadas mediante  restricciones, a esta acción se la denomina propagación de restricciones, la cual tiene como  efecto reducir los dominios de las variables restringidas del programa, dando lugar a una  reducción en el espacio de búsqueda del problema.  La colocación de restricciones se realiza mediante la función LisiTell, la misma coloca  la restricción y produce una primera propagación sobre las variables que asocia dicha  restricción.  Ejemplo:  LisiTell ( x != y );  Restricciones en variables restringidas enteras  Definir y colocar restricciones sobre variables restringidas enteras es una tarea fácil debido  a los operadores de definición de restricciones que brinda LisilntVar. Por ejemplo  supongamos que se tienen dos variables restringidas enteras x e y , lo que se desea es que x  sea siempre mayor que y, el siguiente código muestra como hacerlo.  Lisilnt Var x(0,5);  LisilntVar y(O, 1 O);  LisiTell(x>y);  La tercera línea ha definido y colocado una restricción sobre las variables x e y  lógicamente los dominios de x e y sufrirán modificaciones luego que la tercera linea se  ejecute, dominio de x = { 1,2,3,4, 5}, dominio de y = { 0, 1,2,3, 4}  Restricción igual ""=="". Esta restricción condiciona a que el valor con el cual se liguen  cada una de las variables sean iguales. Por ejemplo, dado el siguiente código:  LisilntVar x(0,5);  LisilntVar y(O,IO);  LisiTell(x == y);  Los dominios de cada variables después de la tercera línea son  x = { 0,1,2,3,4,5}  Y = { 0,1,2,3,4,5}  Los valores de y comprendidos entre 6 y lOse han extraído porque nunca sastisfacerán la  restricción, cualquiera de los valores restantes pueden sastifacer la restricción. Una vez que  una de las variables se lige con .algún valor, la otra se ligará con el mismo valor. La  siguiente sentencia liga el valor de ""x"" a 2, entonces por propagación de restricciones ""y""  queda ligada· a 2  x.setValue(2);  Los d<.?minios de cada variables después de ejecutar la sentencia son:  x={2} y={2}  Restricciones existentes para LisilntVar  Igual a ==  Distinto !=  Menor o igual que <=  Menor que <  Mayor o igual que >=  Mayor que >  1.4.1.6 Arreglos de Variables Restringidas Enteras y Restricciones genéricas  La clase LisilntVarArray es la clase para un arreglo de instancias de LisilntVar.  El arreglo hace más fácil la implementación de restricciones genéricas (restricciones que  se aplican a un grupo de variables). En este contexto, una restricción genérica es una  restricción que se aplica a todas las variables del arreglo. Existen funciones miembros de la  clase arreglo, disponibles para la colocación de tales restricciones genéricas. De esta  rhanera una restricción genérica se coloca y se guarda solamente una vez para todas las  variables en el arreglo.  Constructores  LisilntVarArray ( Lisilnt size, Lisilnt min ,Lisilnt max J. Contruye un arreglo de  variables dominio cuyo dominio esta comprendido entre los valores mín y máx  inclusive. Ej LisilntVarArray A(3,0,4), contiene las siguiente variables  A[O]={O, 1,2,3,4}; A[I]={O,1 ,2,3,4}; A[2]={O, 1,2,3,4}  Funciones de Acceso y asignación  operator 11. LisilntVar& LisilntVarArray::operator[](Lisilnt index) Este operador  retoma una variable restringida entera correspondiente a un índice dado en el arreglo de  variables restringidas enteras. Por ejemplo LisilntVar varl =A[l]  Expresiones Restringidas Enteras con Arreglos  Las siguientes funciones miembros devuelven expresiones restringidas enteras las cuales  permiten formar restricciones genéricas  LisiSum  LisilntExp LisiSum(const LisilntVarArray array). Esta función crea una expresión  restringida entera igual a la suma de los elementos del arreglo.  LisiMax Esta función permite crear una expresión restringida entera igual al máximo de los  elementos del array.  LisiMin Esta función permite crear una expresión restringida entera igual al mínimo de los  elemehtos del array.  Restricciones sobre Arreglos de Variables Restringidas Enteras.  Para formar restricciones sobre arreglos se utilizan las expresiones anteriores y los  operadores para restricciones sobre variables restringidas enteras. Por ejemplo para colocar  que la suma del arreglo A sea igual 3, se tiene el siguiente código:  LisilntVarArray A(3, 0,4);  LisilntExp suma=LisiSum(A);  LisiTell(suma==3);  LisiAlIDiff Es una restricción sobre un arreglo que hace que las variables restringidas de  un arreglo tomen valores diferentes unas de otras, cuando estas son instanciadas.  LisiConstraint LisiAllDiff (const LisilntVarArray array);  Ejemplo. Los valores que toman todas las variables del arreglo A, deben ser distintos entre  sí.  LisilntVarArray A(3, 0,4);  LisjT ell(LisiAllDiff( A»;  1.4.1.7 La categoría de clases para la solución del problema  Clase Goal (Objetivos).  La búsqueda se basa en la idea de goals. Una goal es un objeto cuyo comportamiento lo  define el algoritmo de búsqueda que soporta, esta puede tener éxito o fallar. Conocer cómo  ha sucedido, o sea, si el algoritmo de búsqueda ha podido o no encontrar una solución,  permite tomar decisiones sobre los posibles caminos de búsqueda existentes. Una solución  es alcanzada usando el backtracking y disparando goals. De esta manera uno puede usar las  goals para guiar la búsqueda aunque no conozca nada acerca de camino exacto por el cual  se llega a una solución. Desde luego, si todos los caminos posibles se agotan, una falla se  producirá y ninguna solución será encontrada.  Las goals se pueden combinar usando el operador And y el operador Or, los cuales son  respectivamente LisiAnd e LisiOr. El operador LisiOr se usa para definir un punto de  elección entre dos subgoals para que en caso de que si la ejecución de primera subgoal  conduce a inconsistencias, el estado del sistema anterior a la ejecución de esta se  restablezca, y la segunda goal sea ejecutada.  La cIase Lisilnstantiate  Esta clase deriva de la clase base goal y recibe como argumento una variable restringida y  le asigna un valor del dominio. Entonces automáticamente se propagan las restricciones  colocadas sobre esta variable restringida. Si ningún fallo ocurre durante esta propagación la  goal tennina satisfactoriamente, caso contrario falla, el valor del dominio es descartado y  prueba con otro hasta que la goal sea satisfecha, o no queden más valores en el dominio de  la V.R. en cuyo caso la goal falla.  La clase LisiGenerate  Esta goal recibe como argumento un arreglo de variables restringidas y le asigna a cada  variable restringida un valor de su dominio.  LisiGenerate se puede pensar como una goal que iterativamente selecciona una variable  restringida (o más precisamente, un índice de un arreglo determinado), llama a  Lisilnstantiate para que instance esta variable restringida, luego LisiGenerate llamará  subsecuentemente a LisiInstantiate para instanciar las variables restringidas restantes. En la  selección de las variables restringidas que sé instancian interviene la función chooselndex.  Esta tomará el arreglo a instanciar y determinará cual variable instanciar primero. Estas  funciones se conocen como criterios de selección de variables y uno de los parámetros de  LisiGenerate es el nombre de la función o criterio de selección de variable.  La sintaxis para LisiGenerate es:  LisiGenerate(LisilntVarArray vars, LisiChooseIntIndex chooseIndex  1.5 Mapeo de.l modelo diseñado en la arquitectura propuesta - Búsqueda  de soluciones - Actividad Nro 4  Para el ejemplo propuesto el mapping entre el modelo y la arquitectura propuesta es de la  siguiente manera  1) LisiIntVar Bolivia(O,3), Uruguay(O,3), Brasil(O,3), Argentina(O,3),  Chile(O,3), Paraguay(O,3);  2) LisiIntVarArray coloresParaPaises(6, Bolivia, Uruguay, Brasil, Argentina,  Chile, Paraguay);  II Restricciones  3) LisiTell( Brasil != Bolivia);  LisiTell( Brasil != Paraguay);  LisiTell( Brasil != Argentina);  LisiTell( Paraguay != Argentina);  LisiTell( Paraguay != Bolivia);  LisiTell( Bolivia != Chile);  LisiTell( Argentina != Chile);  LisiTell( Argentina != Uruguay);  • I/Búsqueda de una solución  4) LisiSolve(LisiGenerate( coloresParaPaises»;  II Imprimir  print( ""Bolivia:"", Bolivia);  print ( ""Uruguay:"", Uruguay);  print (""Brasil:"", Brasil);  print (""Argentina:"", Argentina);  print (""Chile:"", Chile);  • print (""Paraguay:"", Paraguay);  Explicación  1) Creo las variables dominio  2) Se construye un arreglo de variables dominio, el cual contendrá todas las variables  domirrio que representan los países, este arreglo constituye una ayuda a la hora de buscar  soluciones.  3) Se colocan todas las restricciones  4) Se implementa la búsqueda de soluciones.  Para problemas complejos, la dificultad para abordar esta fase dependerá de la etapa de  diseño.  Conclusiones.  Fundamentalmente, la metodología planteada, ha demostrado en una forma simple como  construir un programa utilizando la programación con restricciones. En cuanto a la  aplicabilidad, las actividades propuestas son suficientes para problemas simples, mientras  que para problemas reales la misma establece pautas generales de como abordarlos, las  cuales son el fruto de la investigación aplicada que desarrolla el LISI [Rueda,95],  [Ddiaz,98J.  Con respecto a la arquitectura esta tiene como objetivo mostrar de manera sencilla como se  alcanza una implementación de un problema sencillo, como así también cuales son  artefactos básicos con los que cuenta una herramienta para programar por restricciones."	﻿programación por restricciones , desarrollo de aplicaciones	es	22246
77	Visualización de composiciones minerales	"﻿ Se presentan los detalles de implementación de un sistema de visualización de  volúmenes de datos aplicado a problemas específicos de las Ciencias Geológicas. Un  conjunto numeroso de muestras mineralógicas se representa como un volumen dentro de  un espacio de composiciones minerales. Dicho volumen es renderizado por medio de su  superficie, utilizando un modelo de iluminación que simplifica el modelo de Blinn­ Kajiya.  1. Introducción  La exploración de conjuntos de datos provenientes de las ciencias naturales, y en particular de las  Ciencias Geológicas, involucra la aplicación de herramientas y técnicas interactivas de Visualización  [6, 7]. Tal es el caso de los minerales que integran el grupo de los espinelos los cuales presentan una  gran variabilidad composicional, vinculada a su génesis. Por ello algunos de los minerales del grupo  se constituyen en excelentes indicadores petrogenéticos, particularmente los espinelos crómicos  (cromitas) [5, 8]. Para evaluar muestras incógnitas en este último contexto es necesario contar con un  gran número de datos provenientes de poblaciones de distintos ambientes geológicos. Dichos datos se  agrupan en campos composicionales específicos, los cuales constituyen un patrón de referencia para  clasificar las muestras incógnitas.  Sin duda, volcar un gran número de datos para construir dichos campos referenciales y a su vez  graficar los datos de una nueva población son tareas que reqieren de una muy alta inversión de  tiempo, si es que no se realizan en forma automatizada. Por otra parte se hace necesario contar con  herramientas que permitan una visualización de los distintos campos composicionales desde cada uno  de los vértices del prisma composicional de los espinelos, vértices que están definidos por la  composición de los miembros finales del grupo: Esto plantea la necesidad de visualizar los datos en  su dominio natural facilitando de este modo las comparaciones y la integración de múltiples  conjuntos de datos [4].  2. Representación de los datos  Por lo tanto se busca una representación adecuada de composiciones minerales de modo tal que  determinados grupos de muestras puedan ser comparadas con un determinado patrón. Tanto las  muestras como los patrones minerales deben representarse en espacios tridimensionales prismáticos  o tetragonales. Un conjunto ""histórico"" de muestras de un determinado mineral confonna una  tendencia o estructura dentro del espacio de la composición. Esta tendencia es un patrón de  composiciones o proporciones de óxidos que caracteriza un mineral y puede representarse como un  sólido en el espacio de la composición. En la figura 1 se puede ver un espacio prismático adecuado  para la representación de la composición de espinelos y el sólido patrón en su interior.  MgOr20 4  Figura 1: Espacio prismático de datos y sólido patrón.  2.1. Representación del patrón  Debido a la gran cantidad de muestras que constituyen el patrón y a que no es necesario  individualizar los elementos constituyentes del mismo sino su tendencia, un buen modelo para el  mismo lo constituye un volumen. Para representarlo podría elegirse tanto una superficie limitante del  volumen como un sólido. En caso de representarse el mismo como una superficie opaca, podríamos  ver el sólido patrón, pero no tendríamos una buena visualización del conjunto de muestras; si no las  vemos, sabríamos que están dentro del sólido, pero no apreciaríamos su ubicación dentro del mismo.  Si representásemos el mismo como una superficie transparente podríamos apreciar las muestras, pero  no tendríamos información clara del sólido.  La otra posibilidad consiste en obtener una representación volumétrica del patrón. La ventaja en este  caso es que, al renderizarlo con transparencias obtendríamos una percepción de lo que ocurre en su  interior. Sin duda, la exploración interactiva de la visualización constituye un elemento esencial y en  ste caso la velocidad con que obtenemos la misma no es interactiva. Debido a lo expuesto,  onsideramos en representar al sólido mediante una superficie limitante con técnicas B-Spline  stándar [1], pero renderizarlo calculando las intensidades y las opacidades a lo largo del paso visual  omo función del espesor [2]. De este modo logramos una visualización interactiva del sólido patrón  podremos ver los análisis incógnita.  2.2. Representación de las muestras analizadas  Con respecto a los datos que constituyen el conjunto de muestras analizadas, o incógnita, pueden  representarse en este espacio con un ícono adecuado; ahora el problema de clasificación es  determinar si estos íconos están o no dentro del patrón. Para ello, se representaron los mismos con  macropuntos. Se establecieron dos escalas de color, una para los macropuntos que están dentro del  sólido patrón y otra para los que están fuera (ver Figura 2). Por otro lado, se disminuyó la  luminosidad de loo macropuntos a lo largo del paso visual en las dos escalas de color, dando esto una  sensación de profundidad.  Figura 2: Intersección de un conjunto de muestras con el sólido patrón, representada con diferentes escalas  de color.  Cuando la cantidad de muestras analizadas supera un determinado umbral (alrededor de 100), es  necesario encontrar una mejor ayuda para la representación de las mismas como un todo. En este  caso, una posibilidad natural es considerar que el conjunto de muestras conforma otro sólido dentro  del espacio prismático. De este modo tanto el sólido patrón como el que representa el conjunto de  muestras analizadas se visualizan como sólidos traslúcidos. La intersección de ambos sólidos  representa las muestras analizadas que siguen el patrón. Es posible aplicar pseudocoloring en el  modelo de iluminación, para poder producir diferentes efectos en dicha intersección (ver Figura 3).  Una de las simplificaciones más importantes es que no hay necesidad entonces de tener almacenado  un conjunto de datos volumétricos ya que los sólidos traslúcidos pueden ser adecuadamente  representados mediante las superficies limitantes. Así, la representación es lograda no como un gran  conjunto de datos volumétricos sino como una superficie B-Spline. Las superficies están modeladas  mediante caras triangulares. Además la renderización de los volúmenes traslúcidos en tiempo  interactivo se logró con una simplificación del modelo Blinn-Kajiya [2, 6] que puede ser calculada  sobre la conversión scan de los triángulos.  Figura 2: Intersección de un conjunto de muestras con el sólido patrón, representada con diferentes  operaciones de color.  3. Conclusiones y Trabajo Futuro  Cuando la cantidad de muestras es relativamente pequeña, la representación con macropuntos  permitió una visualización adecuada tanto de las muestras incógnita como de su ubicación dentro del  sólido patrón. Cuando la cantidad de muestras analizada crece, este modelo no es totalmente  adecuado, pero se logra una visualización aceptable por medio de los volúmenes traslúcidos. Se  realizaron algunas modificaciones útiles del modelo de iluminación que permitieron una mejor  percepción visual. Con mapas de color adecuados, se varió la precedencia a cualquiera de los  volúmenes con respecto al otro o a la intersección.  La visualización con macropuntos resultó adecuada con conjuntos de datos pequeños. Al aumentar  sensiblemente la cantidad de análisis representados, la visualización tiende a volverse confusa. Es por  ello que, cuando las muestras superan cierta cantidad la visualización con volúmenes traslúcidos es  superior. Sin embargo, si bien la visualización mejora con respecto a la de macropuntos, hay detalles  con respecto a una percepción efectiva de la intersección de ambos conjuntos de muestras que aún no  fueron solucionados. Es por ello que en este momento estamos trabajando en la incorporación de  elementos adicionales que permitan determinar con mayor precisión dicha intersección para mejorar  así la calidad de la visualización."	﻿visualización , composiciones minerales	es	22278
